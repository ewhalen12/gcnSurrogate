{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on multiple topologies\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('./models')\n",
    "from feastnetSurrogateModel import FeaStNet\n",
    "\n",
    "sys.path.append('./readers')\n",
    "from loadGhGraphs import loadGhGraphs\n",
    "\n",
    "sys.path.append('./visualization')\n",
    "from altTrussViz import plotTruss, interactiveErrorPlot\n",
    "\n",
    "sys.path.append('./util')\n",
    "from gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load simulation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_5\n",
      "loading design_6\n",
      "loading design_7\n",
      "loading design_8\n",
      "loading design_9\n"
     ]
    }
   ],
   "source": [
    "trainSets ,valSets ,testSets = {}, {}, {}\n",
    "\n",
    "doeFiles = np.sort(glob.glob(\"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/*1000.csv\"))\n",
    "for doeFile in doeFiles:\n",
    "    designName = doeFile.split('/')[-1].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    allGraphsUnfiltered = loadGhGraphs(doeFile, NUM_DV=5)\n",
    "    allGraphs = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "    trainData, valData, testData = partitionGraphList(allGraphs)\n",
    "    trainSets[designName] = trainData\n",
    "    valSets[designName] = valData\n",
    "    testSets[designName] = testData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train on each group seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsList = []\n",
    "saveDir = './results/topoTest01/'\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on design_5\n",
      "epoch: 0   trainLoss: 1.0356e+00   valLoss:1.0439e+00  time: 1.29e+00\n",
      "epoch: 1   trainLoss: 9.3205e-01   valLoss:1.0446e+00  time: 9.85e-01\n",
      "epoch: 2   trainLoss: 9.2407e-01   valLoss:1.0541e+00  time: 9.83e-01\n",
      "epoch: 3   trainLoss: 8.4189e-01   valLoss:1.0973e+00  time: 1.05e+00\n",
      "epoch: 4   trainLoss: 8.3505e-01   valLoss:1.1607e+00  time: 9.54e-01\n",
      "epoch: 5   trainLoss: 8.1768e-01   valLoss:1.1738e+00  time: 9.43e-01\n",
      "epoch: 6   trainLoss: 7.4498e-01   valLoss:1.0955e+00  time: 9.45e-01\n",
      "epoch: 7   trainLoss: 6.7474e-01   valLoss:9.3193e-01  time: 9.41e-01\n",
      "epoch: 8   trainLoss: 6.6743e-01   valLoss:7.7763e-01  time: 9.98e-01\n",
      "epoch: 9   trainLoss: 6.2234e-01   valLoss:6.7188e-01  time: 9.52e-01\n",
      "epoch: 10   trainLoss: 5.9795e-01   valLoss:6.1265e-01  time: 9.67e-01\n",
      "epoch: 11   trainLoss: 5.8180e-01   valLoss:5.7374e-01  time: 9.56e-01\n",
      "epoch: 12   trainLoss: 5.3583e-01   valLoss:5.4524e-01  time: 1.05e+00\n",
      "epoch: 13   trainLoss: 5.0522e-01   valLoss:5.1771e-01  time: 1.01e+00\n",
      "epoch: 14   trainLoss: 4.6553e-01   valLoss:4.8374e-01  time: 9.83e-01\n",
      "epoch: 15   trainLoss: 4.5168e-01   valLoss:4.5221e-01  time: 1.05e+00\n",
      "epoch: 16   trainLoss: 4.2764e-01   valLoss:4.3013e-01  time: 9.69e-01\n",
      "epoch: 17   trainLoss: 3.7490e-01   valLoss:4.0535e-01  time: 9.61e-01\n",
      "epoch: 18   trainLoss: 3.6532e-01   valLoss:3.8578e-01  time: 1.03e+00\n",
      "epoch: 19   trainLoss: 3.5327e-01   valLoss:3.5931e-01  time: 9.66e-01\n",
      "epoch: 20   trainLoss: 3.1304e-01   valLoss:3.3404e-01  time: 1.03e+00\n",
      "epoch: 21   trainLoss: 3.0637e-01   valLoss:3.1598e-01  time: 9.74e-01\n",
      "epoch: 22   trainLoss: 2.6947e-01   valLoss:2.9204e-01  time: 9.60e-01\n",
      "epoch: 23   trainLoss: 2.6692e-01   valLoss:2.7098e-01  time: 9.66e-01\n",
      "epoch: 24   trainLoss: 2.4391e-01   valLoss:2.5327e-01  time: 9.84e-01\n",
      "epoch: 25   trainLoss: 2.2107e-01   valLoss:2.3369e-01  time: 9.45e-01\n",
      "epoch: 26   trainLoss: 2.0542e-01   valLoss:2.2053e-01  time: 9.50e-01\n",
      "epoch: 27   trainLoss: 2.0912e-01   valLoss:2.0458e-01  time: 9.62e-01\n",
      "epoch: 28   trainLoss: 1.7449e-01   valLoss:1.8193e-01  time: 9.54e-01\n",
      "epoch: 29   trainLoss: 1.6686e-01   valLoss:1.7747e-01  time: 9.79e-01\n",
      "epoch: 30   trainLoss: 1.8243e-01   valLoss:1.7646e-01  time: 9.50e-01\n",
      "epoch: 31   trainLoss: 1.4655e-01   valLoss:1.5208e-01  time: 1.04e+00\n",
      "epoch: 32   trainLoss: 1.4004e-01   valLoss:1.3837e-01  time: 9.42e-01\n",
      "epoch: 33   trainLoss: 1.2887e-01   valLoss:1.3888e-01  time: 9.45e-01\n",
      "epoch: 34   trainLoss: 1.2136e-01   valLoss:1.3135e-01  time: 9.52e-01\n",
      "epoch: 35   trainLoss: 1.1128e-01   valLoss:1.1523e-01  time: 9.72e-01\n",
      "epoch: 36   trainLoss: 9.8839e-02   valLoss:1.0769e-01  time: 9.64e-01\n",
      "epoch: 37   trainLoss: 9.9547e-02   valLoss:9.7808e-02  time: 9.45e-01\n",
      "epoch: 38   trainLoss: 8.7793e-02   valLoss:9.7068e-02  time: 1.04e+00\n",
      "epoch: 39   trainLoss: 8.6331e-02   valLoss:1.0542e-01  time: 9.80e-01\n",
      "epoch: 40   trainLoss: 8.6698e-02   valLoss:8.6742e-02  time: 9.38e-01\n",
      "epoch: 41   trainLoss: 7.7248e-02   valLoss:7.8897e-02  time: 1.03e+00\n",
      "epoch: 42   trainLoss: 7.2936e-02   valLoss:9.4043e-02  time: 9.86e-01\n",
      "epoch: 43   trainLoss: 7.1360e-02   valLoss:7.7727e-02  time: 9.70e-01\n",
      "epoch: 44   trainLoss: 6.5668e-02   valLoss:7.5114e-02  time: 9.72e-01\n",
      "epoch: 45   trainLoss: 6.1696e-02   valLoss:7.9844e-02  time: 1.05e+00\n",
      "epoch: 46   trainLoss: 5.2545e-02   valLoss:6.7776e-02  time: 9.57e-01\n",
      "epoch: 47   trainLoss: 5.1806e-02   valLoss:6.5206e-02  time: 9.37e-01\n",
      "epoch: 48   trainLoss: 5.2547e-02   valLoss:7.3606e-02  time: 9.68e-01\n",
      "epoch: 49   trainLoss: 4.6237e-02   valLoss:6.1434e-02  time: 9.45e-01\n",
      "epoch: 50   trainLoss: 4.8162e-02   valLoss:6.5538e-02  time: 9.42e-01\n",
      "epoch: 51   trainLoss: 4.5461e-02   valLoss:7.2973e-02  time: 1.02e+00\n",
      "epoch: 52   trainLoss: 5.0944e-02   valLoss:7.6196e-02  time: 9.79e-01\n",
      "epoch: 53   trainLoss: 4.1559e-02   valLoss:5.5718e-02  time: 9.66e-01\n",
      "epoch: 54   trainLoss: 4.2153e-02   valLoss:5.9911e-02  time: 9.60e-01\n",
      "epoch: 55   trainLoss: 4.5508e-02   valLoss:6.7605e-02  time: 1.03e+00\n",
      "epoch: 56   trainLoss: 3.9025e-02   valLoss:5.3634e-02  time: 9.69e-01\n",
      "epoch: 57   trainLoss: 4.8236e-02   valLoss:6.3372e-02  time: 9.62e-01\n",
      "epoch: 58   trainLoss: 3.3793e-02   valLoss:6.2426e-02  time: 9.49e-01\n",
      "epoch: 59   trainLoss: 3.4302e-02   valLoss:4.9827e-02  time: 9.43e-01\n",
      "epoch: 60   trainLoss: 3.6031e-02   valLoss:6.6308e-02  time: 9.53e-01\n",
      "epoch: 61   trainLoss: 3.0766e-02   valLoss:7.0233e-02  time: 9.45e-01\n",
      "epoch: 62   trainLoss: 3.0990e-02   valLoss:4.9605e-02  time: 9.78e-01\n",
      "epoch: 63   trainLoss: 3.2123e-02   valLoss:5.2171e-02  time: 9.58e-01\n",
      "epoch: 64   trainLoss: 3.1634e-02   valLoss:5.4162e-02  time: 9.61e-01\n",
      "epoch: 65   trainLoss: 3.3813e-02   valLoss:5.8909e-02  time: 9.53e-01\n",
      "epoch: 66   trainLoss: 3.0395e-02   valLoss:6.1682e-02  time: 9.50e-01\n",
      "epoch: 67   trainLoss: 2.5199e-02   valLoss:4.6916e-02  time: 9.44e-01\n",
      "epoch: 68   trainLoss: 2.9356e-02   valLoss:4.6231e-02  time: 9.46e-01\n",
      "epoch: 69   trainLoss: 2.8811e-02   valLoss:5.4980e-02  time: 9.57e-01\n",
      "epoch: 70   trainLoss: 2.6650e-02   valLoss:6.2660e-02  time: 9.44e-01\n",
      "epoch: 71   trainLoss: 2.5825e-02   valLoss:4.3943e-02  time: 9.42e-01\n",
      "epoch: 72   trainLoss: 2.4862e-02   valLoss:5.1038e-02  time: 1.05e+00\n",
      "epoch: 73   trainLoss: 2.2924e-02   valLoss:5.6241e-02  time: 1.06e+00\n",
      "epoch: 74   trainLoss: 2.1665e-02   valLoss:4.7569e-02  time: 9.93e-01\n",
      "epoch: 75   trainLoss: 2.7695e-02   valLoss:5.0791e-02  time: 9.68e-01\n",
      "epoch: 76   trainLoss: 2.2356e-02   valLoss:5.3318e-02  time: 9.52e-01\n",
      "epoch: 77   trainLoss: 2.2644e-02   valLoss:5.6988e-02  time: 9.72e-01\n",
      "epoch: 78   trainLoss: 2.1712e-02   valLoss:6.2441e-02  time: 9.71e-01\n",
      "epoch: 79   trainLoss: 2.1004e-02   valLoss:4.4499e-02  time: 9.65e-01\n",
      "epoch: 80   trainLoss: 1.9323e-02   valLoss:4.1029e-02  time: 9.77e-01\n",
      "epoch: 81   trainLoss: 1.9698e-02   valLoss:6.3093e-02  time: 9.67e-01\n",
      "epoch: 82   trainLoss: 2.5901e-02   valLoss:5.5829e-02  time: 9.46e-01\n",
      "epoch: 83   trainLoss: 2.1757e-02   valLoss:4.0044e-02  time: 9.50e-01\n",
      "epoch: 84   trainLoss: 1.8929e-02   valLoss:4.5492e-02  time: 9.77e-01\n",
      "epoch: 85   trainLoss: 1.9038e-02   valLoss:4.1659e-02  time: 1.06e+00\n",
      "epoch: 86   trainLoss: 2.0356e-02   valLoss:5.1283e-02  time: 1.05e+00\n",
      "epoch: 87   trainLoss: 1.5232e-02   valLoss:6.0515e-02  time: 1.04e+00\n",
      "epoch: 88   trainLoss: 2.2613e-02   valLoss:3.9302e-02  time: 9.46e-01\n",
      "epoch: 89   trainLoss: 2.4400e-02   valLoss:4.4435e-02  time: 1.03e+00\n",
      "epoch: 90   trainLoss: 1.7291e-02   valLoss:4.2105e-02  time: 9.84e-01\n",
      "epoch: 91   trainLoss: 1.4932e-02   valLoss:4.1147e-02  time: 1.06e+00\n",
      "epoch: 92   trainLoss: 2.4672e-02   valLoss:4.8920e-02  time: 1.04e+00\n",
      "epoch: 93   trainLoss: 1.7944e-02   valLoss:4.8603e-02  time: 1.03e+00\n",
      "epoch: 94   trainLoss: 2.2591e-02   valLoss:4.3619e-02  time: 9.44e-01\n",
      "epoch: 95   trainLoss: 2.5070e-02   valLoss:3.3735e-02  time: 9.49e-01\n",
      "epoch: 96   trainLoss: 2.0142e-02   valLoss:4.5749e-02  time: 9.53e-01\n",
      "epoch: 97   trainLoss: 1.6756e-02   valLoss:4.5651e-02  time: 9.70e-01\n",
      "epoch: 98   trainLoss: 1.4148e-02   valLoss:5.0745e-02  time: 1.00e+00\n",
      "epoch: 99   trainLoss: 1.6134e-02   valLoss:5.0982e-02  time: 9.40e-01\n",
      "loading checkpoint 95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6accb58cf837428e936080afa7b0a520\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6accb58cf837428e936080afa7b0a520\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6accb58cf837428e936080afa7b0a520\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ca468111fbd93579b8248ddcc39e1e37\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-ca468111fbd93579b8248ddcc39e1e37\": [{\"index\": 0, \"train\": 1.0356485843658447, \"val\": 1.0438956673498507}, {\"index\": 1, \"train\": 0.9320478638013204, \"val\": 1.0446435163418453}, {\"index\": 2, \"train\": 0.9240703781445821, \"val\": 1.0541125530446018}, {\"index\": 3, \"train\": 0.8418900767962137, \"val\": 1.097300312585301}, {\"index\": 4, \"train\": 0.8350514968236288, \"val\": 1.1606893068938344}, {\"index\": 5, \"train\": 0.8176847696304321, \"val\": 1.1738095498747296}, {\"index\": 6, \"train\": 0.7449823617935181, \"val\": 1.0954633851846058}, {\"index\": 7, \"train\": 0.6747399965922037, \"val\": 0.9319323444807971}, {\"index\": 8, \"train\": 0.6674296259880066, \"val\": 0.7776282959514194}, {\"index\": 9, \"train\": 0.6223429242769877, \"val\": 0.671881061202536}, {\"index\": 10, \"train\": 0.5979470213254293, \"val\": 0.6126526725237016}, {\"index\": 11, \"train\": 0.5817963282267252, \"val\": 0.5737398655360771}, {\"index\": 12, \"train\": 0.5358304381370544, \"val\": 0.5452374080540957}, {\"index\": 13, \"train\": 0.5052206714948019, \"val\": 0.5177054784467651}, {\"index\": 14, \"train\": 0.4655301372210185, \"val\": 0.4837428422696475}, {\"index\": 15, \"train\": 0.4516773323218028, \"val\": 0.45221270905393696}, {\"index\": 16, \"train\": 0.4276435673236847, \"val\": 0.43012958070849655}, {\"index\": 17, \"train\": 0.3749016324679057, \"val\": 0.4053524426288075}, {\"index\": 18, \"train\": 0.3653179903825124, \"val\": 0.3857792243620174}, {\"index\": 19, \"train\": 0.3532702724138896, \"val\": 0.35930861150673415}, {\"index\": 20, \"train\": 0.3130401372909546, \"val\": 0.33404188027984844}, {\"index\": 21, \"train\": 0.30637335777282715, \"val\": 0.3159759796431495}, {\"index\": 22, \"train\": 0.2694702744483948, \"val\": 0.2920364112920921}, {\"index\": 23, \"train\": 0.26691656311353046, \"val\": 0.2709777969953225}, {\"index\": 24, \"train\": 0.24390616516272226, \"val\": 0.2532732666004449}, {\"index\": 25, \"train\": 0.22106687227884927, \"val\": 0.23369362659717877}, {\"index\": 26, \"train\": 0.2054205040136973, \"val\": 0.22052539343497268}, {\"index\": 27, \"train\": 0.20911970237890878, \"val\": 0.20457738913009288}, {\"index\": 28, \"train\": 0.17449207852284113, \"val\": 0.18192554320450183}, {\"index\": 29, \"train\": 0.16686139007409415, \"val\": 0.17746649202200826}, {\"index\": 30, \"train\": 0.1824311912059784, \"val\": 0.17645869000504413}, {\"index\": 31, \"train\": 0.14655170838038126, \"val\": 0.15207970773594248}, {\"index\": 32, \"train\": 0.14004012445608774, \"val\": 0.13837462122103683}, {\"index\": 33, \"train\": 0.12886851529280344, \"val\": 0.1388839538709295}, {\"index\": 34, \"train\": 0.12135752787192662, \"val\": 0.1313528561818034}, {\"index\": 35, \"train\": 0.1112779105703036, \"val\": 0.11522795204331891}, {\"index\": 36, \"train\": 0.09883895516395569, \"val\": 0.10768853622937093}, {\"index\": 37, \"train\": 0.09954749296108882, \"val\": 0.09780753532821243}, {\"index\": 38, \"train\": 0.0877934421102206, \"val\": 0.09706760409581303}, {\"index\": 39, \"train\": 0.08633079876502354, \"val\": 0.10542307320671777}, {\"index\": 40, \"train\": 0.08669839054346085, \"val\": 0.08674207049490953}, {\"index\": 41, \"train\": 0.0772481436530749, \"val\": 0.07889711464910458}, {\"index\": 42, \"train\": 0.07293612385789554, \"val\": 0.09404296151155399}, {\"index\": 43, \"train\": 0.071359783411026, \"val\": 0.07772721359463132}, {\"index\": 44, \"train\": 0.06566767146190007, \"val\": 0.07511433920848908}, {\"index\": 45, \"train\": 0.06169633815685908, \"val\": 0.0798437615416737}, {\"index\": 46, \"train\": 0.05254535749554634, \"val\": 0.06777649327974629}, {\"index\": 47, \"train\": 0.05180587371190389, \"val\": 0.06520579332114991}, {\"index\": 48, \"train\": 0.05254721641540527, \"val\": 0.07360601311342584}, {\"index\": 49, \"train\": 0.04623690495888392, \"val\": 0.061433744442183524}, {\"index\": 50, \"train\": 0.04816191643476486, \"val\": 0.06553836925067352}, {\"index\": 51, \"train\": 0.04546056191126505, \"val\": 0.07297293758407856}, {\"index\": 52, \"train\": 0.050943693766991295, \"val\": 0.07619615184376016}, {\"index\": 53, \"train\": 0.04155919204155604, \"val\": 0.055717930236520864}, {\"index\": 54, \"train\": 0.04215279966592789, \"val\": 0.05991101586173668}, {\"index\": 55, \"train\": 0.045507571349541344, \"val\": 0.06760545993974018}, {\"index\": 56, \"train\": 0.03902460510532061, \"val\": 0.05363422272623413}, {\"index\": 57, \"train\": 0.04823557039101919, \"val\": 0.06337181108581179}, {\"index\": 58, \"train\": 0.03379346306125323, \"val\": 0.06242600761793761}, {\"index\": 59, \"train\": 0.034302043418089546, \"val\": 0.049827106068497176}, {\"index\": 60, \"train\": 0.036030509819587074, \"val\": 0.06630805859782009}, {\"index\": 61, \"train\": 0.030766092861692112, \"val\": 0.07023299173495078}, {\"index\": 62, \"train\": 0.030989688510696094, \"val\": 0.04960467263676778}, {\"index\": 63, \"train\": 0.03212316893041134, \"val\": 0.052171254457250515}, {\"index\": 64, \"train\": 0.031634364277124405, \"val\": 0.05416240992188385}, {\"index\": 65, \"train\": 0.03381311148405075, \"val\": 0.05890927759551064}, {\"index\": 66, \"train\": 0.030395179366072018, \"val\": 0.061682359994544335}, {\"index\": 67, \"train\": 0.02519925559560458, \"val\": 0.046915859190954104}, {\"index\": 68, \"train\": 0.029356007153789204, \"val\": 0.04623090522902742}, {\"index\": 69, \"train\": 0.02881106175482273, \"val\": 0.054980433921122715}, {\"index\": 70, \"train\": 0.026650484030445416, \"val\": 0.06266004784049949}, {\"index\": 71, \"train\": 0.025824980189402897, \"val\": 0.04394262016393003}, {\"index\": 72, \"train\": 0.024862340341011684, \"val\": 0.051037690054659766}, {\"index\": 73, \"train\": 0.02292444556951523, \"val\": 0.05624063812084151}, {\"index\": 74, \"train\": 0.02166476969917615, \"val\": 0.04756894443805019}, {\"index\": 75, \"train\": 0.02769540126125018, \"val\": 0.050790846020121266}, {\"index\": 76, \"train\": 0.02235625001291434, \"val\": 0.05331758530465541}, {\"index\": 77, \"train\": 0.02264353943367799, \"val\": 0.05698836695713302}, {\"index\": 78, \"train\": 0.021711526438593864, \"val\": 0.06244128639809787}, {\"index\": 79, \"train\": 0.021004314844806988, \"val\": 0.044498689429558536}, {\"index\": 80, \"train\": 0.019322901964187622, \"val\": 0.041028704840250105}, {\"index\": 81, \"train\": 0.01969845872372389, \"val\": 0.06309324401620499}, {\"index\": 82, \"train\": 0.025901055584351223, \"val\": 0.05582921410246787}, {\"index\": 83, \"train\": 0.021756888056794804, \"val\": 0.0400436112260515}, {\"index\": 84, \"train\": 0.018928973625103634, \"val\": 0.04549187826880909}, {\"index\": 85, \"train\": 0.019038379192352295, \"val\": 0.0416591843219336}, {\"index\": 86, \"train\": 0.020356403042872746, \"val\": 0.051283209018745564}, {\"index\": 87, \"train\": 0.01523163914680481, \"val\": 0.06051540021198215}, {\"index\": 88, \"train\": 0.022612875948349636, \"val\": 0.0393024850270228}, {\"index\": 89, \"train\": 0.024399648730953533, \"val\": 0.044435028081621836}, {\"index\": 90, \"train\": 0.017290803293387096, \"val\": 0.04210520671309765}, {\"index\": 91, \"train\": 0.014931897011895975, \"val\": 0.0411473818520015}, {\"index\": 92, \"train\": 0.0246720053255558, \"val\": 0.048919676947493655}, {\"index\": 93, \"train\": 0.017943750756482284, \"val\": 0.048603199674592666}, {\"index\": 94, \"train\": 0.0225911820307374, \"val\": 0.043618941813258934}, {\"index\": 95, \"train\": 0.02507037514199813, \"val\": 0.03373504055694125}, {\"index\": 96, \"train\": 0.02014159628500541, \"val\": 0.04574871553254693}, {\"index\": 97, \"train\": 0.016756066121160984, \"val\": 0.04565101712338488}, {\"index\": 98, \"train\": 0.014148423758645853, \"val\": 0.0507454858980728}, {\"index\": 99, \"train\": 0.016134456731379032, \"val\": 0.05098233983584645}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_5\n",
      "\n",
      "training on design_6\n",
      "epoch: 0   trainLoss: 9.6502e-01   valLoss:1.1972e+00  time: 9.85e-01\n",
      "epoch: 1   trainLoss: 8.9800e-01   valLoss:1.2026e+00  time: 9.93e-01\n",
      "epoch: 2   trainLoss: 8.6612e-01   valLoss:1.2054e+00  time: 9.87e-01\n",
      "epoch: 3   trainLoss: 7.8615e-01   valLoss:1.2015e+00  time: 1.00e+00\n",
      "epoch: 4   trainLoss: 7.9711e-01   valLoss:1.2041e+00  time: 1.15e+00\n",
      "epoch: 5   trainLoss: 7.5902e-01   valLoss:1.1768e+00  time: 1.16e+00\n",
      "epoch: 6   trainLoss: 6.8668e-01   valLoss:1.1238e+00  time: 1.14e+00\n",
      "epoch: 7   trainLoss: 6.7836e-01   valLoss:1.0371e+00  time: 1.00e+00\n",
      "epoch: 8   trainLoss: 6.2324e-01   valLoss:9.2628e-01  time: 1.01e+00\n",
      "epoch: 9   trainLoss: 5.9268e-01   valLoss:7.5886e-01  time: 1.02e+00\n",
      "epoch: 10   trainLoss: 5.5817e-01   valLoss:6.7341e-01  time: 1.02e+00\n",
      "epoch: 11   trainLoss: 5.7479e-01   valLoss:6.2014e-01  time: 1.14e+00\n",
      "epoch: 12   trainLoss: 5.0688e-01   valLoss:5.7041e-01  time: 1.14e+00\n",
      "epoch: 13   trainLoss: 4.7484e-01   valLoss:5.2926e-01  time: 1.14e+00\n",
      "epoch: 14   trainLoss: 4.7193e-01   valLoss:4.9625e-01  time: 1.11e+00\n",
      "epoch: 15   trainLoss: 4.4243e-01   valLoss:4.6817e-01  time: 1.15e+00\n",
      "epoch: 16   trainLoss: 4.0775e-01   valLoss:4.4878e-01  time: 1.14e+00\n",
      "epoch: 17   trainLoss: 3.8089e-01   valLoss:4.1136e-01  time: 1.13e+00\n",
      "epoch: 18   trainLoss: 3.5192e-01   valLoss:3.7755e-01  time: 1.13e+00\n",
      "epoch: 19   trainLoss: 3.1815e-01   valLoss:3.5125e-01  time: 1.14e+00\n",
      "epoch: 20   trainLoss: 3.2285e-01   valLoss:3.2780e-01  time: 1.09e+00\n",
      "epoch: 21   trainLoss: 2.9093e-01   valLoss:3.1543e-01  time: 1.14e+00\n",
      "epoch: 22   trainLoss: 2.8466e-01   valLoss:2.9179e-01  time: 1.12e+00\n",
      "epoch: 23   trainLoss: 2.4919e-01   valLoss:2.8400e-01  time: 1.12e+00\n",
      "epoch: 24   trainLoss: 2.4424e-01   valLoss:2.7049e-01  time: 1.11e+00\n",
      "epoch: 25   trainLoss: 2.1281e-01   valLoss:2.4906e-01  time: 1.09e+00\n",
      "epoch: 26   trainLoss: 2.0991e-01   valLoss:2.6927e-01  time: 1.01e+00\n",
      "epoch: 27   trainLoss: 1.9969e-01   valLoss:2.3865e-01  time: 1.01e+00\n",
      "epoch: 28   trainLoss: 1.8792e-01   valLoss:2.2151e-01  time: 1.01e+00\n",
      "epoch: 29   trainLoss: 1.6979e-01   valLoss:2.1864e-01  time: 1.00e+00\n",
      "epoch: 30   trainLoss: 1.6310e-01   valLoss:2.1185e-01  time: 1.00e+00\n",
      "epoch: 31   trainLoss: 1.5622e-01   valLoss:1.9868e-01  time: 1.01e+00\n",
      "epoch: 32   trainLoss: 1.4102e-01   valLoss:1.9801e-01  time: 1.14e+00\n",
      "epoch: 33   trainLoss: 1.5353e-01   valLoss:1.9053e-01  time: 1.14e+00\n",
      "epoch: 34   trainLoss: 1.3064e-01   valLoss:2.1792e-01  time: 1.14e+00\n",
      "epoch: 35   trainLoss: 1.2042e-01   valLoss:1.9264e-01  time: 1.14e+00\n",
      "epoch: 36   trainLoss: 1.1479e-01   valLoss:1.8209e-01  time: 1.15e+00\n",
      "epoch: 37   trainLoss: 1.1014e-01   valLoss:1.6681e-01  time: 1.10e+00\n",
      "epoch: 38   trainLoss: 1.0836e-01   valLoss:1.6833e-01  time: 1.14e+00\n",
      "epoch: 39   trainLoss: 1.0197e-01   valLoss:1.6320e-01  time: 1.14e+00\n",
      "epoch: 40   trainLoss: 1.0650e-01   valLoss:1.7271e-01  time: 1.14e+00\n",
      "epoch: 41   trainLoss: 9.9479e-02   valLoss:1.7555e-01  time: 1.14e+00\n",
      "epoch: 42   trainLoss: 1.1003e-01   valLoss:1.6118e-01  time: 9.98e-01\n",
      "epoch: 43   trainLoss: 1.0409e-01   valLoss:1.9207e-01  time: 1.01e+00\n",
      "epoch: 44   trainLoss: 8.5244e-02   valLoss:1.4564e-01  time: 1.02e+00\n",
      "epoch: 45   trainLoss: 9.2898e-02   valLoss:1.6406e-01  time: 1.01e+00\n",
      "epoch: 46   trainLoss: 7.8855e-02   valLoss:1.4439e-01  time: 9.84e-01\n",
      "epoch: 47   trainLoss: 8.2777e-02   valLoss:1.5406e-01  time: 1.00e+00\n",
      "epoch: 48   trainLoss: 7.6616e-02   valLoss:1.5639e-01  time: 9.87e-01\n",
      "epoch: 49   trainLoss: 8.1285e-02   valLoss:1.1367e-01  time: 9.92e-01\n",
      "epoch: 50   trainLoss: 7.4310e-02   valLoss:1.6757e-01  time: 1.02e+00\n",
      "epoch: 51   trainLoss: 7.2650e-02   valLoss:1.5179e-01  time: 1.01e+00\n",
      "epoch: 52   trainLoss: 6.6525e-02   valLoss:1.2846e-01  time: 9.87e-01\n",
      "epoch: 53   trainLoss: 6.9597e-02   valLoss:1.2606e-01  time: 1.08e+00\n",
      "epoch: 54   trainLoss: 6.9475e-02   valLoss:1.1264e-01  time: 1.01e+00\n",
      "epoch: 55   trainLoss: 6.6205e-02   valLoss:1.2118e-01  time: 1.01e+00\n",
      "epoch: 56   trainLoss: 6.2446e-02   valLoss:1.8211e-01  time: 1.01e+00\n",
      "epoch: 57   trainLoss: 6.3158e-02   valLoss:1.0662e-01  time: 1.00e+00\n",
      "epoch: 58   trainLoss: 6.2404e-02   valLoss:1.3872e-01  time: 1.07e+00\n",
      "epoch: 59   trainLoss: 6.1365e-02   valLoss:1.1360e-01  time: 9.93e-01\n",
      "epoch: 60   trainLoss: 5.4655e-02   valLoss:1.2358e-01  time: 1.01e+00\n",
      "epoch: 61   trainLoss: 6.4456e-02   valLoss:1.1176e-01  time: 9.88e-01\n",
      "epoch: 62   trainLoss: 5.8066e-02   valLoss:1.3068e-01  time: 1.02e+00\n",
      "epoch: 63   trainLoss: 4.8826e-02   valLoss:1.3800e-01  time: 1.13e+00\n",
      "epoch: 64   trainLoss: 4.9130e-02   valLoss:1.1615e-01  time: 1.09e+00\n",
      "epoch: 65   trainLoss: 4.0903e-02   valLoss:1.2315e-01  time: 1.00e+00\n",
      "epoch: 66   trainLoss: 4.2238e-02   valLoss:9.9269e-02  time: 1.14e+00\n",
      "epoch: 67   trainLoss: 4.1256e-02   valLoss:9.6771e-02  time: 1.08e+00\n",
      "epoch: 68   trainLoss: 4.2670e-02   valLoss:1.0589e-01  time: 9.85e-01\n",
      "epoch: 69   trainLoss: 4.6530e-02   valLoss:1.3436e-01  time: 9.88e-01\n",
      "epoch: 70   trainLoss: 4.1291e-02   valLoss:1.1940e-01  time: 1.12e+00\n",
      "epoch: 71   trainLoss: 3.7500e-02   valLoss:1.0865e-01  time: 1.13e+00\n",
      "epoch: 72   trainLoss: 4.1594e-02   valLoss:9.8479e-02  time: 1.13e+00\n",
      "epoch: 73   trainLoss: 3.3425e-02   valLoss:1.2658e-01  time: 1.01e+00\n",
      "epoch: 74   trainLoss: 3.5455e-02   valLoss:8.6821e-02  time: 1.01e+00\n",
      "epoch: 75   trainLoss: 3.7507e-02   valLoss:8.4499e-02  time: 9.90e-01\n",
      "epoch: 76   trainLoss: 3.5925e-02   valLoss:9.6721e-02  time: 1.01e+00\n",
      "epoch: 77   trainLoss: 4.4351e-02   valLoss:9.7485e-02  time: 1.02e+00\n",
      "epoch: 78   trainLoss: 3.4368e-02   valLoss:1.1787e-01  time: 9.90e-01\n",
      "epoch: 79   trainLoss: 4.1417e-02   valLoss:9.4063e-02  time: 9.80e-01\n",
      "epoch: 80   trainLoss: 4.0505e-02   valLoss:9.2793e-02  time: 1.12e+00\n",
      "epoch: 81   trainLoss: 3.2615e-02   valLoss:1.2102e-01  time: 1.13e+00\n",
      "epoch: 82   trainLoss: 3.1425e-02   valLoss:8.4458e-02  time: 9.79e-01\n",
      "epoch: 83   trainLoss: 4.0771e-02   valLoss:1.3811e-01  time: 1.12e+00\n",
      "epoch: 84   trainLoss: 3.0965e-02   valLoss:9.5038e-02  time: 9.87e-01\n",
      "epoch: 85   trainLoss: 4.0111e-02   valLoss:1.0608e-01  time: 1.03e+00\n",
      "epoch: 86   trainLoss: 3.1482e-02   valLoss:1.2808e-01  time: 1.01e+00\n",
      "epoch: 87   trainLoss: 2.9133e-02   valLoss:8.4000e-02  time: 9.81e-01\n",
      "epoch: 88   trainLoss: 2.8624e-02   valLoss:9.9804e-02  time: 1.13e+00\n",
      "epoch: 89   trainLoss: 2.7609e-02   valLoss:1.0416e-01  time: 1.12e+00\n",
      "epoch: 90   trainLoss: 3.0900e-02   valLoss:9.6270e-02  time: 1.15e+00\n",
      "epoch: 91   trainLoss: 2.8467e-02   valLoss:1.0303e-01  time: 1.16e+00\n",
      "epoch: 92   trainLoss: 4.1350e-02   valLoss:7.3942e-02  time: 1.14e+00\n",
      "epoch: 93   trainLoss: 2.6602e-02   valLoss:9.5644e-02  time: 1.12e+00\n",
      "epoch: 94   trainLoss: 3.3096e-02   valLoss:1.0127e-01  time: 9.95e-01\n",
      "epoch: 95   trainLoss: 2.9955e-02   valLoss:8.0703e-02  time: 1.16e+00\n",
      "epoch: 96   trainLoss: 2.9101e-02   valLoss:8.4173e-02  time: 1.12e+00\n",
      "epoch: 97   trainLoss: 3.2900e-02   valLoss:1.0723e-01  time: 9.86e-01\n",
      "epoch: 98   trainLoss: 2.9785e-02   valLoss:9.3383e-02  time: 9.85e-01\n",
      "epoch: 99   trainLoss: 2.7159e-02   valLoss:1.3627e-01  time: 1.10e+00\n",
      "loading checkpoint 92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ef7c67ab666d49b495264155daa714c9\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ef7c67ab666d49b495264155daa714c9\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ef7c67ab666d49b495264155daa714c9\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-cc84a1e21f9665053b744de96f5d0141\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-cc84a1e21f9665053b744de96f5d0141\": [{\"index\": 0, \"train\": 0.9650207161903381, \"val\": 1.197227531147224}, {\"index\": 1, \"train\": 0.8980012536048889, \"val\": 1.2025614926522528}, {\"index\": 2, \"train\": 0.866123616695404, \"val\": 1.2054282133639962}, {\"index\": 3, \"train\": 0.7861450711886088, \"val\": 1.2014868095241211}, {\"index\": 4, \"train\": 0.7971120476722717, \"val\": 1.2040899900926485}, {\"index\": 5, \"train\": 0.7590203682581583, \"val\": 1.1767761564503114}, {\"index\": 6, \"train\": 0.6866791248321533, \"val\": 1.1237996123317215}, {\"index\": 7, \"train\": 0.6783587137858073, \"val\": 1.0370621194856033}, {\"index\": 8, \"train\": 0.6232369740804037, \"val\": 0.926280226972368}, {\"index\": 9, \"train\": 0.5926763415336609, \"val\": 0.7588589063811081}, {\"index\": 10, \"train\": 0.5581745902697245, \"val\": 0.6734137534750281}, {\"index\": 11, \"train\": 0.5747869312763214, \"val\": 0.6201399420215575}, {\"index\": 12, \"train\": 0.5068753957748413, \"val\": 0.5704108686013906}, {\"index\": 13, \"train\": 0.47483763098716736, \"val\": 0.5292607619496131}, {\"index\": 14, \"train\": 0.471929669380188, \"val\": 0.49625365168753044}, {\"index\": 15, \"train\": 0.4424312512079875, \"val\": 0.46816642051217733}, {\"index\": 16, \"train\": 0.40775038798650104, \"val\": 0.4487804152347423}, {\"index\": 17, \"train\": 0.3808889190355937, \"val\": 0.4113575528824219}, {\"index\": 18, \"train\": 0.35191739598910016, \"val\": 0.37755182775220386}, {\"index\": 19, \"train\": 0.31815123558044434, \"val\": 0.3512471938954183}, {\"index\": 20, \"train\": 0.32285146911938983, \"val\": 0.3278010133653879}, {\"index\": 21, \"train\": 0.2909334401289622, \"val\": 0.3154305928835162}, {\"index\": 22, \"train\": 0.2846616407235463, \"val\": 0.2917936448076809}, {\"index\": 23, \"train\": 0.2491902013619741, \"val\": 0.2840043239260989}, {\"index\": 24, \"train\": 0.24424457550048828, \"val\": 0.27049086341220474}, {\"index\": 25, \"train\": 0.21281023820241293, \"val\": 0.24906438221741053}, {\"index\": 26, \"train\": 0.20991366108258566, \"val\": 0.2692654733373611}, {\"index\": 27, \"train\": 0.19968890647093454, \"val\": 0.23864724338744525}, {\"index\": 28, \"train\": 0.1879175901412964, \"val\": 0.22150963830278703}, {\"index\": 29, \"train\": 0.16978621979554495, \"val\": 0.2186420955695212}, {\"index\": 30, \"train\": 0.16310076415538788, \"val\": 0.21184817020332924}, {\"index\": 31, \"train\": 0.15622299909591675, \"val\": 0.19867943311800007}, {\"index\": 32, \"train\": 0.14101742704709372, \"val\": 0.19801466354099964}, {\"index\": 33, \"train\": 0.15352578461170197, \"val\": 0.19052619587940475}, {\"index\": 34, \"train\": 0.1306409239768982, \"val\": 0.21792118483292008}, {\"index\": 35, \"train\": 0.12041568756103516, \"val\": 0.19263893095948906}, {\"index\": 36, \"train\": 0.11479430397351582, \"val\": 0.18208782445570385}, {\"index\": 37, \"train\": 0.1101393202940623, \"val\": 0.1668134106202396}, {\"index\": 38, \"train\": 0.10835536072651546, \"val\": 0.16833167895674706}, {\"index\": 39, \"train\": 0.10196902851263683, \"val\": 0.16320469510672544}, {\"index\": 40, \"train\": 0.10650347173213959, \"val\": 0.17270999973536366}, {\"index\": 41, \"train\": 0.09947944184144338, \"val\": 0.175545357969693}, {\"index\": 42, \"train\": 0.11002702514330547, \"val\": 0.16117961262352765}, {\"index\": 43, \"train\": 0.1040884479880333, \"val\": 0.19207210144614456}, {\"index\": 44, \"train\": 0.08524424582719803, \"val\": 0.14564455531244339}, {\"index\": 45, \"train\": 0.09289755175511043, \"val\": 0.16405528661346547}, {\"index\": 46, \"train\": 0.07885532826185226, \"val\": 0.14439464406179334}, {\"index\": 47, \"train\": 0.08277689913908641, \"val\": 0.15405634733744794}, {\"index\": 48, \"train\": 0.07661563903093338, \"val\": 0.15638761822548178}, {\"index\": 49, \"train\": 0.08128534257411957, \"val\": 0.11367039696347934}, {\"index\": 50, \"train\": 0.07431008170048396, \"val\": 0.1675690246090569}, {\"index\": 51, \"train\": 0.07264971360564232, \"val\": 0.1517938075983828}, {\"index\": 52, \"train\": 0.06652507682641347, \"val\": 0.12846138931086493}, {\"index\": 53, \"train\": 0.06959694127241771, \"val\": 0.12606450365166422}, {\"index\": 54, \"train\": 0.06947536518176396, \"val\": 0.112638952861609}, {\"index\": 55, \"train\": 0.06620467826724052, \"val\": 0.12118342525705143}, {\"index\": 56, \"train\": 0.06244618445634842, \"val\": 0.1821143699473598}, {\"index\": 57, \"train\": 0.06315816069642703, \"val\": 0.1066226614035528}, {\"index\": 58, \"train\": 0.06240423768758774, \"val\": 0.13871948225682396}, {\"index\": 59, \"train\": 0.061365107695261635, \"val\": 0.11359893664269259}, {\"index\": 60, \"train\": 0.0546548825999101, \"val\": 0.12357949035207706}, {\"index\": 61, \"train\": 0.06445582831899326, \"val\": 0.11175637400312419}, {\"index\": 62, \"train\": 0.05806634450952212, \"val\": 0.13067810406425484}, {\"index\": 63, \"train\": 0.048825553307930626, \"val\": 0.13800227006517904}, {\"index\": 64, \"train\": 0.04913029447197914, \"val\": 0.11615474613521386}, {\"index\": 65, \"train\": 0.040902951111396156, \"val\": 0.12314968572981241}, {\"index\": 66, \"train\": 0.042237538844347, \"val\": 0.09926863138443204}, {\"index\": 67, \"train\": 0.04125618934631348, \"val\": 0.0967705290553298}, {\"index\": 68, \"train\": 0.042669648925463356, \"val\": 0.10589288037563502}, {\"index\": 69, \"train\": 0.04653007040421168, \"val\": 0.1343583087609322}, {\"index\": 70, \"train\": 0.0412906731168429, \"val\": 0.1194020418441613}, {\"index\": 71, \"train\": 0.037499686082204185, \"val\": 0.10865022837943225}, {\"index\": 72, \"train\": 0.04159403716524442, \"val\": 0.09847870806697756}, {\"index\": 73, \"train\": 0.03342463696996371, \"val\": 0.12658330853769761}, {\"index\": 74, \"train\": 0.035455128798882164, \"val\": 0.08682080862302057}, {\"index\": 75, \"train\": 0.037507290641466774, \"val\": 0.08449915425713968}, {\"index\": 76, \"train\": 0.035925413171450295, \"val\": 0.09672113315685203}, {\"index\": 77, \"train\": 0.04435099412997564, \"val\": 0.09748521781336793}, {\"index\": 78, \"train\": 0.03436834365129471, \"val\": 0.11786548738988738}, {\"index\": 79, \"train\": 0.04141716162363688, \"val\": 0.09406322881976073}, {\"index\": 80, \"train\": 0.04050529251495997, \"val\": 0.09279334192871358}, {\"index\": 81, \"train\": 0.032614635303616524, \"val\": 0.12102066177909297}, {\"index\": 82, \"train\": 0.03142548476656278, \"val\": 0.08445775333395297}, {\"index\": 83, \"train\": 0.04077137882510821, \"val\": 0.13811133826513672}, {\"index\": 84, \"train\": 0.030965233221650124, \"val\": 0.0950384986785206}, {\"index\": 85, \"train\": 0.040110510463515915, \"val\": 0.10608390540195008}, {\"index\": 86, \"train\": 0.0314817950129509, \"val\": 0.12808115100833}, {\"index\": 87, \"train\": 0.02913336642086506, \"val\": 0.0840001139505249}, {\"index\": 88, \"train\": 0.02862372932334741, \"val\": 0.09980430513516897}, {\"index\": 89, \"train\": 0.027609320357441902, \"val\": 0.1041551729819427}, {\"index\": 90, \"train\": 0.03089988852540652, \"val\": 0.0962700769132762}, {\"index\": 91, \"train\": 0.028467145437995594, \"val\": 0.10303069926211955}, {\"index\": 92, \"train\": 0.041349723314245544, \"val\": 0.07394247639846678}, {\"index\": 93, \"train\": 0.026602493599057198, \"val\": 0.09564422348867757}, {\"index\": 94, \"train\": 0.03309574661155542, \"val\": 0.10127490191047804}, {\"index\": 95, \"train\": 0.029954676205913227, \"val\": 0.0807034697521616}, {\"index\": 96, \"train\": 0.02910107560455799, \"val\": 0.08417298910380513}, {\"index\": 97, \"train\": 0.03290040170152982, \"val\": 0.10722831092533414}, {\"index\": 98, \"train\": 0.029784542818864185, \"val\": 0.09338266525365826}, {\"index\": 99, \"train\": 0.02715913454691569, \"val\": 0.13626870920847137}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_6\n",
      "\n",
      "training on design_7\n",
      "epoch: 0   trainLoss: 8.8450e-01   valLoss:9.4310e-01  time: 1.05e+00\n",
      "epoch: 1   trainLoss: 8.3724e-01   valLoss:9.4267e-01  time: 1.03e+00\n",
      "epoch: 2   trainLoss: 8.1404e-01   valLoss:9.4263e-01  time: 1.07e+00\n",
      "epoch: 3   trainLoss: 7.3584e-01   valLoss:9.4545e-01  time: 1.04e+00\n",
      "epoch: 4   trainLoss: 7.3168e-01   valLoss:9.3272e-01  time: 1.03e+00\n",
      "epoch: 5   trainLoss: 7.0533e-01   valLoss:9.2125e-01  time: 1.16e+00\n",
      "epoch: 6   trainLoss: 6.3549e-01   valLoss:8.7636e-01  time: 1.04e+00\n",
      "epoch: 7   trainLoss: 6.0489e-01   valLoss:7.8333e-01  time: 1.05e+00\n",
      "epoch: 8   trainLoss: 6.4063e-01   valLoss:6.7551e-01  time: 1.06e+00\n",
      "epoch: 9   trainLoss: 5.5619e-01   valLoss:6.1818e-01  time: 1.04e+00\n",
      "epoch: 10   trainLoss: 5.5086e-01   valLoss:5.9239e-01  time: 1.03e+00\n",
      "epoch: 11   trainLoss: 5.4329e-01   valLoss:5.7101e-01  time: 1.05e+00\n",
      "epoch: 12   trainLoss: 5.6116e-01   valLoss:5.5412e-01  time: 1.04e+00\n",
      "epoch: 13   trainLoss: 4.8820e-01   valLoss:5.3545e-01  time: 1.04e+00\n",
      "epoch: 14   trainLoss: 4.5729e-01   valLoss:5.2001e-01  time: 1.03e+00\n",
      "epoch: 15   trainLoss: 4.5673e-01   valLoss:5.0871e-01  time: 1.06e+00\n",
      "epoch: 16   trainLoss: 4.6520e-01   valLoss:4.9941e-01  time: 1.02e+00\n",
      "epoch: 17   trainLoss: 4.2815e-01   valLoss:4.7680e-01  time: 1.03e+00\n",
      "epoch: 18   trainLoss: 4.2147e-01   valLoss:4.5491e-01  time: 1.06e+00\n",
      "epoch: 19   trainLoss: 4.0539e-01   valLoss:4.4047e-01  time: 1.04e+00\n",
      "epoch: 20   trainLoss: 3.8102e-01   valLoss:4.2040e-01  time: 1.02e+00\n",
      "epoch: 21   trainLoss: 3.7615e-01   valLoss:4.1472e-01  time: 1.06e+00\n",
      "epoch: 22   trainLoss: 3.6376e-01   valLoss:3.9434e-01  time: 1.05e+00\n",
      "epoch: 23   trainLoss: 3.7316e-01   valLoss:3.8393e-01  time: 1.04e+00\n",
      "epoch: 24   trainLoss: 3.3507e-01   valLoss:3.7998e-01  time: 1.02e+00\n",
      "epoch: 25   trainLoss: 3.3329e-01   valLoss:3.5341e-01  time: 1.05e+00\n",
      "epoch: 26   trainLoss: 3.0132e-01   valLoss:3.5544e-01  time: 1.04e+00\n",
      "epoch: 27   trainLoss: 2.7847e-01   valLoss:3.5470e-01  time: 1.02e+00\n",
      "epoch: 28   trainLoss: 2.7878e-01   valLoss:3.0967e-01  time: 1.03e+00\n",
      "epoch: 29   trainLoss: 2.5999e-01   valLoss:3.1468e-01  time: 1.06e+00\n",
      "epoch: 30   trainLoss: 2.8804e-01   valLoss:3.3744e-01  time: 1.05e+00\n",
      "epoch: 31   trainLoss: 2.6903e-01   valLoss:2.9204e-01  time: 1.05e+00\n",
      "epoch: 32   trainLoss: 2.4345e-01   valLoss:2.9136e-01  time: 1.05e+00\n",
      "epoch: 33   trainLoss: 2.3456e-01   valLoss:3.2704e-01  time: 1.04e+00\n",
      "epoch: 34   trainLoss: 2.1756e-01   valLoss:2.7112e-01  time: 1.02e+00\n",
      "epoch: 35   trainLoss: 2.1106e-01   valLoss:2.7134e-01  time: 1.04e+00\n",
      "epoch: 36   trainLoss: 1.9610e-01   valLoss:3.3292e-01  time: 1.04e+00\n",
      "epoch: 37   trainLoss: 2.2493e-01   valLoss:2.5872e-01  time: 1.11e+00\n",
      "epoch: 38   trainLoss: 1.9559e-01   valLoss:2.5255e-01  time: 1.03e+00\n",
      "epoch: 39   trainLoss: 2.1089e-01   valLoss:3.0035e-01  time: 1.02e+00\n",
      "epoch: 40   trainLoss: 1.8281e-01   valLoss:2.4774e-01  time: 1.03e+00\n",
      "epoch: 41   trainLoss: 1.7542e-01   valLoss:2.3745e-01  time: 1.05e+00\n",
      "epoch: 42   trainLoss: 1.7458e-01   valLoss:2.8374e-01  time: 1.04e+00\n",
      "epoch: 43   trainLoss: 1.7162e-01   valLoss:2.3017e-01  time: 1.03e+00\n",
      "epoch: 44   trainLoss: 1.6391e-01   valLoss:2.3728e-01  time: 1.05e+00\n",
      "epoch: 45   trainLoss: 1.5904e-01   valLoss:2.7848e-01  time: 1.05e+00\n",
      "epoch: 46   trainLoss: 1.5662e-01   valLoss:2.4183e-01  time: 1.02e+00\n",
      "epoch: 47   trainLoss: 1.5021e-01   valLoss:2.4639e-01  time: 1.05e+00\n",
      "epoch: 48   trainLoss: 1.4790e-01   valLoss:2.4251e-01  time: 1.05e+00\n",
      "epoch: 49   trainLoss: 1.4083e-01   valLoss:2.2339e-01  time: 1.04e+00\n",
      "epoch: 50   trainLoss: 1.4640e-01   valLoss:2.3865e-01  time: 1.14e+00\n",
      "epoch: 51   trainLoss: 1.2579e-01   valLoss:2.2178e-01  time: 1.02e+00\n",
      "epoch: 52   trainLoss: 1.2341e-01   valLoss:2.1039e-01  time: 1.03e+00\n",
      "epoch: 53   trainLoss: 1.3369e-01   valLoss:2.5188e-01  time: 1.07e+00\n",
      "epoch: 54   trainLoss: 1.2229e-01   valLoss:2.0129e-01  time: 1.06e+00\n",
      "epoch: 55   trainLoss: 1.1161e-01   valLoss:1.9184e-01  time: 1.05e+00\n",
      "epoch: 56   trainLoss: 1.2483e-01   valLoss:2.0622e-01  time: 1.05e+00\n",
      "epoch: 57   trainLoss: 1.1996e-01   valLoss:2.0431e-01  time: 1.19e+00\n",
      "epoch: 58   trainLoss: 1.1150e-01   valLoss:2.0401e-01  time: 1.04e+00\n",
      "epoch: 59   trainLoss: 1.0956e-01   valLoss:1.6930e-01  time: 1.18e+00\n",
      "epoch: 60   trainLoss: 1.0896e-01   valLoss:2.0192e-01  time: 1.03e+00\n",
      "epoch: 61   trainLoss: 1.0208e-01   valLoss:2.2212e-01  time: 1.03e+00\n",
      "epoch: 62   trainLoss: 9.8296e-02   valLoss:1.7323e-01  time: 1.03e+00\n",
      "epoch: 63   trainLoss: 1.0149e-01   valLoss:1.7600e-01  time: 1.12e+00\n",
      "epoch: 64   trainLoss: 1.1645e-01   valLoss:1.6652e-01  time: 1.12e+00\n",
      "epoch: 65   trainLoss: 9.5283e-02   valLoss:1.7749e-01  time: 1.02e+00\n",
      "epoch: 66   trainLoss: 9.7736e-02   valLoss:2.2169e-01  time: 1.02e+00\n",
      "epoch: 67   trainLoss: 1.0332e-01   valLoss:1.5440e-01  time: 1.03e+00\n",
      "epoch: 68   trainLoss: 1.2398e-01   valLoss:1.8830e-01  time: 1.05e+00\n",
      "epoch: 69   trainLoss: 8.7348e-02   valLoss:1.9938e-01  time: 1.04e+00\n",
      "epoch: 70   trainLoss: 9.4871e-02   valLoss:1.5783e-01  time: 1.04e+00\n",
      "epoch: 71   trainLoss: 9.3514e-02   valLoss:1.6754e-01  time: 1.04e+00\n",
      "epoch: 72   trainLoss: 9.1138e-02   valLoss:2.0059e-01  time: 1.06e+00\n",
      "epoch: 73   trainLoss: 9.2781e-02   valLoss:1.7792e-01  time: 1.05e+00\n",
      "epoch: 74   trainLoss: 7.7560e-02   valLoss:1.7543e-01  time: 1.05e+00\n",
      "epoch: 75   trainLoss: 7.7165e-02   valLoss:1.5788e-01  time: 1.05e+00\n",
      "epoch: 76   trainLoss: 8.2771e-02   valLoss:1.5744e-01  time: 1.03e+00\n",
      "epoch: 77   trainLoss: 8.1647e-02   valLoss:1.5457e-01  time: 1.16e+00\n",
      "epoch: 78   trainLoss: 7.5333e-02   valLoss:2.1609e-01  time: 1.05e+00\n",
      "epoch: 79   trainLoss: 9.3834e-02   valLoss:1.5540e-01  time: 1.03e+00\n",
      "epoch: 80   trainLoss: 1.0788e-01   valLoss:1.4576e-01  time: 1.11e+00\n",
      "epoch: 81   trainLoss: 9.6525e-02   valLoss:2.4192e-01  time: 1.02e+00\n",
      "epoch: 82   trainLoss: 8.2530e-02   valLoss:1.5862e-01  time: 1.04e+00\n",
      "epoch: 83   trainLoss: 8.9963e-02   valLoss:1.4232e-01  time: 1.04e+00\n",
      "epoch: 84   trainLoss: 7.8959e-02   valLoss:1.9713e-01  time: 1.04e+00\n",
      "epoch: 85   trainLoss: 7.6100e-02   valLoss:1.7203e-01  time: 1.04e+00\n",
      "epoch: 86   trainLoss: 6.6299e-02   valLoss:1.4837e-01  time: 1.13e+00\n",
      "epoch: 87   trainLoss: 7.3396e-02   valLoss:1.5118e-01  time: 1.05e+00\n",
      "epoch: 88   trainLoss: 7.4533e-02   valLoss:2.1339e-01  time: 1.03e+00\n",
      "epoch: 89   trainLoss: 8.1422e-02   valLoss:1.6788e-01  time: 1.02e+00\n",
      "epoch: 90   trainLoss: 6.8852e-02   valLoss:1.4404e-01  time: 1.02e+00\n",
      "epoch: 91   trainLoss: 7.0250e-02   valLoss:1.8266e-01  time: 1.02e+00\n",
      "epoch: 92   trainLoss: 6.0399e-02   valLoss:1.4399e-01  time: 1.14e+00\n",
      "epoch: 93   trainLoss: 7.1761e-02   valLoss:1.5028e-01  time: 1.04e+00\n",
      "epoch: 94   trainLoss: 6.8894e-02   valLoss:1.6405e-01  time: 1.05e+00\n",
      "epoch: 95   trainLoss: 5.9953e-02   valLoss:1.3467e-01  time: 1.03e+00\n",
      "epoch: 96   trainLoss: 6.3374e-02   valLoss:1.6842e-01  time: 1.11e+00\n",
      "epoch: 97   trainLoss: 6.3007e-02   valLoss:1.4010e-01  time: 1.05e+00\n",
      "epoch: 98   trainLoss: 7.7542e-02   valLoss:1.4649e-01  time: 1.05e+00\n",
      "epoch: 99   trainLoss: 7.3409e-02   valLoss:1.4627e-01  time: 1.05e+00\n",
      "loading checkpoint 95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-4a6fe11803d34be78845c1b9baf9aba2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4a6fe11803d34be78845c1b9baf9aba2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4a6fe11803d34be78845c1b9baf9aba2\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-37b9d1fc7e948b6511cff3a165f3f704\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-37b9d1fc7e948b6511cff3a165f3f704\": [{\"index\": 0, \"train\": 0.884497324625651, \"val\": 0.9431012056216046}, {\"index\": 1, \"train\": 0.8372361262639364, \"val\": 0.9426716475023164}, {\"index\": 2, \"train\": 0.8140378395716349, \"val\": 0.9426270071762027}, {\"index\": 3, \"train\": 0.7358417709668478, \"val\": 0.9454513989319956}, {\"index\": 4, \"train\": 0.7316845258076986, \"val\": 0.932723606246765}, {\"index\": 5, \"train\": 0.7053265571594238, \"val\": 0.9212536127823923}, {\"index\": 6, \"train\": 0.6354886094729105, \"val\": 0.8763601260121774}, {\"index\": 7, \"train\": 0.6048906644185384, \"val\": 0.7833308766216591}, {\"index\": 8, \"train\": 0.6406252781550089, \"val\": 0.6755127111035917}, {\"index\": 9, \"train\": 0.5561940868695577, \"val\": 0.6181760004169687}, {\"index\": 10, \"train\": 0.5508591334025065, \"val\": 0.5923906026097635}, {\"index\": 11, \"train\": 0.5432890355587006, \"val\": 0.571010738507741}, {\"index\": 12, \"train\": 0.5611583789189657, \"val\": 0.5541229175610675}, {\"index\": 13, \"train\": 0.48819581667582196, \"val\": 0.5354508782943918}, {\"index\": 14, \"train\": 0.4572883148988088, \"val\": 0.520008640812227}, {\"index\": 15, \"train\": 0.45673273007074994, \"val\": 0.5087073566392064}, {\"index\": 16, \"train\": 0.46519861618677777, \"val\": 0.4994058084018804}, {\"index\": 17, \"train\": 0.4281539022922516, \"val\": 0.4768034782674577}, {\"index\": 18, \"train\": 0.4214657445748647, \"val\": 0.4549117896153971}, {\"index\": 19, \"train\": 0.40538639823595685, \"val\": 0.44047044771206045}, {\"index\": 20, \"train\": 0.3810223738352458, \"val\": 0.4203955172216175}, {\"index\": 21, \"train\": 0.376147598028183, \"val\": 0.41472201680557597}, {\"index\": 22, \"train\": 0.363760381937027, \"val\": 0.39434318119418565}, {\"index\": 23, \"train\": 0.37316063046455383, \"val\": 0.38392890336129953}, {\"index\": 24, \"train\": 0.33507097760836285, \"val\": 0.379980709058819}, {\"index\": 25, \"train\": 0.33329394459724426, \"val\": 0.35341045175058144}, {\"index\": 26, \"train\": 0.30132095019022626, \"val\": 0.3554352357017773}, {\"index\": 27, \"train\": 0.2784700095653534, \"val\": 0.354698296515616}, {\"index\": 28, \"train\": 0.27877726157506305, \"val\": 0.30967184372625695}, {\"index\": 29, \"train\": 0.25999125838279724, \"val\": 0.314679729666009}, {\"index\": 30, \"train\": 0.288042093316714, \"val\": 0.3374420086146091}, {\"index\": 31, \"train\": 0.26902878284454346, \"val\": 0.29204120485250046}, {\"index\": 32, \"train\": 0.24345339834690094, \"val\": 0.2913574009478368}, {\"index\": 33, \"train\": 0.23455654084682465, \"val\": 0.3270407535687641}, {\"index\": 34, \"train\": 0.21756104131539664, \"val\": 0.27111598673380083}, {\"index\": 35, \"train\": 0.2110574891169866, \"val\": 0.2713425666421514}, {\"index\": 36, \"train\": 0.1960990677277247, \"val\": 0.33292341363374833}, {\"index\": 37, \"train\": 0.22492782274881998, \"val\": 0.25872235293327656}, {\"index\": 38, \"train\": 0.1955863485733668, \"val\": 0.25254516386323506}, {\"index\": 39, \"train\": 0.21089478333791098, \"val\": 0.3003547864155499}, {\"index\": 40, \"train\": 0.18280723690986633, \"val\": 0.2477404146834656}, {\"index\": 41, \"train\": 0.17542286217212677, \"val\": 0.2374468703530039}, {\"index\": 42, \"train\": 0.17457770307858786, \"val\": 0.2837431350705662}, {\"index\": 43, \"train\": 0.1716210295756658, \"val\": 0.23017198881306858}, {\"index\": 44, \"train\": 0.16390629609425864, \"val\": 0.23727734469705158}, {\"index\": 45, \"train\": 0.15903636316458383, \"val\": 0.27848429242321465}, {\"index\": 46, \"train\": 0.15662018954753876, \"val\": 0.24182670064167017}, {\"index\": 47, \"train\": 0.15021211902300516, \"val\": 0.24639475026547356}, {\"index\": 48, \"train\": 0.14789780974388123, \"val\": 0.24250626967598995}, {\"index\": 49, \"train\": 0.14083093404769897, \"val\": 0.2233904578326339}, {\"index\": 50, \"train\": 0.1464019368092219, \"val\": 0.23864868304830184}, {\"index\": 51, \"train\": 0.12578965971867243, \"val\": 0.22178436663105255}, {\"index\": 52, \"train\": 0.12340500950813293, \"val\": 0.21039347511854162}, {\"index\": 53, \"train\": 0.13369432340065637, \"val\": 0.25187528026893874}, {\"index\": 54, \"train\": 0.12229379266500473, \"val\": 0.20128602065853085}, {\"index\": 55, \"train\": 0.11160571128129959, \"val\": 0.19183503029247126}, {\"index\": 56, \"train\": 0.12483419477939606, \"val\": 0.2062199040643733}, {\"index\": 57, \"train\": 0.11995857208967209, \"val\": 0.20431217703433638}, {\"index\": 58, \"train\": 0.11150323102871577, \"val\": 0.2040121235444935}, {\"index\": 59, \"train\": 0.10955596715211868, \"val\": 0.16929967936421572}, {\"index\": 60, \"train\": 0.10896094888448715, \"val\": 0.20191599091480453}, {\"index\": 61, \"train\": 0.10208314657211304, \"val\": 0.2221186680900347}, {\"index\": 62, \"train\": 0.09829611082871755, \"val\": 0.173228811839147}, {\"index\": 63, \"train\": 0.10149418314297994, \"val\": 0.17599987195959935}, {\"index\": 64, \"train\": 0.11644987761974335, \"val\": 0.16652272477383828}, {\"index\": 65, \"train\": 0.09528290232022603, \"val\": 0.17749308697292926}, {\"index\": 66, \"train\": 0.09773620218038559, \"val\": 0.22168600255261278}, {\"index\": 67, \"train\": 0.10331876575946808, \"val\": 0.15439824022663137}, {\"index\": 68, \"train\": 0.12397609154383342, \"val\": 0.1882955797870333}, {\"index\": 69, \"train\": 0.08734796692927678, \"val\": 0.19938074262743746}, {\"index\": 70, \"train\": 0.09487092246611913, \"val\": 0.1578284577699378}, {\"index\": 71, \"train\": 0.09351363529761632, \"val\": 0.16754368635291164}, {\"index\": 72, \"train\": 0.09113773703575134, \"val\": 0.20059183859525043}, {\"index\": 73, \"train\": 0.09278072913487752, \"val\": 0.17792261376787252}, {\"index\": 74, \"train\": 0.07756011684735616, \"val\": 0.17542531747474438}, {\"index\": 75, \"train\": 0.07716537266969681, \"val\": 0.15787757970651406}, {\"index\": 76, \"train\": 0.08277050157388051, \"val\": 0.15743805210675216}, {\"index\": 77, \"train\": 0.08164697637160619, \"val\": 0.15457026776426505}, {\"index\": 78, \"train\": 0.0753331333398819, \"val\": 0.2160851324054723}, {\"index\": 79, \"train\": 0.09383404006560643, \"val\": 0.15539660121107268}, {\"index\": 80, \"train\": 0.10787586619456609, \"val\": 0.14575856906379125}, {\"index\": 81, \"train\": 0.09652520964543025, \"val\": 0.2419191235072773}, {\"index\": 82, \"train\": 0.08252998193105061, \"val\": 0.15861963739842866}, {\"index\": 83, \"train\": 0.08996312941114108, \"val\": 0.1423224023439818}, {\"index\": 84, \"train\": 0.07895868768294652, \"val\": 0.19713267962948453}, {\"index\": 85, \"train\": 0.07610006009538968, \"val\": 0.17202981739494674}, {\"index\": 86, \"train\": 0.06629869590202968, \"val\": 0.14837373200700515}, {\"index\": 87, \"train\": 0.07339646418889363, \"val\": 0.15117794427917236}, {\"index\": 88, \"train\": 0.07453299313783646, \"val\": 0.2133941319986695}, {\"index\": 89, \"train\": 0.08142150690158208, \"val\": 0.1678753997742509}, {\"index\": 90, \"train\": 0.06885230789581935, \"val\": 0.14403651899192482}, {\"index\": 91, \"train\": 0.07024967173735301, \"val\": 0.18266165898078018}, {\"index\": 92, \"train\": 0.06039919828375181, \"val\": 0.14399464795995434}, {\"index\": 93, \"train\": 0.07176077614227931, \"val\": 0.15028183780507082}, {\"index\": 94, \"train\": 0.06889400382836659, \"val\": 0.16404816015037121}, {\"index\": 95, \"train\": 0.059952681263287864, \"val\": 0.1346703942399472}, {\"index\": 96, \"train\": 0.06337435419360797, \"val\": 0.16842060581418788}, {\"index\": 97, \"train\": 0.06300682947039604, \"val\": 0.14010499030593093}, {\"index\": 98, \"train\": 0.07754230250914891, \"val\": 0.1464891804865113}, {\"index\": 99, \"train\": 0.0734088768561681, \"val\": 0.14627189953656247}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_7\n",
      "\n",
      "training on design_8\n",
      "epoch: 0   trainLoss: 8.8766e-01   valLoss:9.0016e-01  time: 1.09e+00\n",
      "epoch: 1   trainLoss: 7.7776e-01   valLoss:8.7512e-01  time: 1.06e+00\n",
      "epoch: 2   trainLoss: 7.7424e-01   valLoss:8.4472e-01  time: 1.07e+00\n",
      "epoch: 3   trainLoss: 7.0403e-01   valLoss:8.5272e-01  time: 1.08e+00\n",
      "epoch: 4   trainLoss: 6.5942e-01   valLoss:9.0604e-01  time: 1.08e+00\n",
      "epoch: 5   trainLoss: 6.2513e-01   valLoss:9.7293e-01  time: 1.11e+00\n",
      "epoch: 6   trainLoss: 5.6999e-01   valLoss:9.5564e-01  time: 1.25e+00\n",
      "epoch: 7   trainLoss: 5.2024e-01   valLoss:8.0793e-01  time: 1.07e+00\n",
      "epoch: 8   trainLoss: 5.4949e-01   valLoss:6.2151e-01  time: 1.10e+00\n",
      "epoch: 9   trainLoss: 4.7895e-01   valLoss:4.8270e-01  time: 1.10e+00\n",
      "epoch: 10   trainLoss: 4.5267e-01   valLoss:4.5090e-01  time: 1.10e+00\n",
      "epoch: 11   trainLoss: 4.2458e-01   valLoss:4.3038e-01  time: 1.09e+00\n",
      "epoch: 12   trainLoss: 3.9712e-01   valLoss:4.0851e-01  time: 1.06e+00\n",
      "epoch: 13   trainLoss: 3.5359e-01   valLoss:3.9590e-01  time: 1.25e+00\n",
      "epoch: 14   trainLoss: 3.8015e-01   valLoss:3.8579e-01  time: 1.09e+00\n",
      "epoch: 15   trainLoss: 3.7477e-01   valLoss:3.6345e-01  time: 1.09e+00\n",
      "epoch: 16   trainLoss: 3.4239e-01   valLoss:3.5306e-01  time: 1.08e+00\n",
      "epoch: 17   trainLoss: 3.2347e-01   valLoss:3.4144e-01  time: 1.07e+00\n",
      "epoch: 18   trainLoss: 3.0239e-01   valLoss:3.2932e-01  time: 1.09e+00\n",
      "epoch: 19   trainLoss: 3.0323e-01   valLoss:3.2259e-01  time: 1.06e+00\n",
      "epoch: 20   trainLoss: 3.1939e-01   valLoss:3.1388e-01  time: 1.06e+00\n",
      "epoch: 21   trainLoss: 2.9341e-01   valLoss:3.0450e-01  time: 1.06e+00\n",
      "epoch: 22   trainLoss: 2.8463e-01   valLoss:3.0672e-01  time: 1.09e+00\n",
      "epoch: 23   trainLoss: 2.8871e-01   valLoss:2.8537e-01  time: 1.09e+00\n",
      "epoch: 24   trainLoss: 2.6716e-01   valLoss:2.7980e-01  time: 1.09e+00\n",
      "epoch: 25   trainLoss: 2.4363e-01   valLoss:2.7648e-01  time: 1.09e+00\n",
      "epoch: 26   trainLoss: 2.5066e-01   valLoss:2.7299e-01  time: 1.17e+00\n",
      "epoch: 27   trainLoss: 2.3793e-01   valLoss:2.6027e-01  time: 1.07e+00\n",
      "epoch: 28   trainLoss: 2.5334e-01   valLoss:2.6317e-01  time: 1.08e+00\n",
      "epoch: 29   trainLoss: 2.2232e-01   valLoss:2.5636e-01  time: 1.08e+00\n",
      "epoch: 30   trainLoss: 2.1927e-01   valLoss:2.4796e-01  time: 1.15e+00\n",
      "epoch: 31   trainLoss: 2.3416e-01   valLoss:2.4084e-01  time: 1.17e+00\n",
      "epoch: 32   trainLoss: 1.9639e-01   valLoss:2.4604e-01  time: 1.09e+00\n",
      "epoch: 33   trainLoss: 2.0139e-01   valLoss:2.3866e-01  time: 1.08e+00\n",
      "epoch: 34   trainLoss: 1.9944e-01   valLoss:2.2900e-01  time: 1.09e+00\n",
      "epoch: 35   trainLoss: 1.9265e-01   valLoss:2.2371e-01  time: 1.15e+00\n",
      "epoch: 36   trainLoss: 1.9339e-01   valLoss:2.2060e-01  time: 1.08e+00\n",
      "epoch: 37   trainLoss: 1.8963e-01   valLoss:2.1808e-01  time: 1.08e+00\n",
      "epoch: 38   trainLoss: 1.6720e-01   valLoss:2.1264e-01  time: 1.26e+00\n",
      "epoch: 39   trainLoss: 1.7223e-01   valLoss:2.1458e-01  time: 1.09e+00\n",
      "epoch: 40   trainLoss: 1.5956e-01   valLoss:2.0769e-01  time: 1.09e+00\n",
      "epoch: 41   trainLoss: 1.5009e-01   valLoss:2.1207e-01  time: 1.08e+00\n",
      "epoch: 42   trainLoss: 1.5750e-01   valLoss:1.8931e-01  time: 1.10e+00\n",
      "epoch: 43   trainLoss: 1.5197e-01   valLoss:1.9285e-01  time: 1.17e+00\n",
      "epoch: 44   trainLoss: 1.4375e-01   valLoss:1.9484e-01  time: 1.07e+00\n",
      "epoch: 45   trainLoss: 1.5638e-01   valLoss:1.8547e-01  time: 1.09e+00\n",
      "epoch: 46   trainLoss: 1.3995e-01   valLoss:1.7722e-01  time: 1.08e+00\n",
      "epoch: 47   trainLoss: 1.4677e-01   valLoss:1.7300e-01  time: 1.07e+00\n",
      "epoch: 48   trainLoss: 1.3318e-01   valLoss:1.7507e-01  time: 1.06e+00\n",
      "epoch: 49   trainLoss: 1.3368e-01   valLoss:1.6214e-01  time: 1.10e+00\n",
      "epoch: 50   trainLoss: 1.2124e-01   valLoss:1.5835e-01  time: 1.08e+00\n",
      "epoch: 51   trainLoss: 1.3558e-01   valLoss:1.5878e-01  time: 1.08e+00\n",
      "epoch: 52   trainLoss: 1.2271e-01   valLoss:1.5487e-01  time: 1.06e+00\n",
      "epoch: 53   trainLoss: 1.2645e-01   valLoss:1.6324e-01  time: 1.06e+00\n",
      "epoch: 54   trainLoss: 1.2284e-01   valLoss:1.5384e-01  time: 1.12e+00\n",
      "epoch: 55   trainLoss: 1.2709e-01   valLoss:1.5292e-01  time: 1.17e+00\n",
      "epoch: 56   trainLoss: 1.1371e-01   valLoss:1.3892e-01  time: 1.08e+00\n",
      "epoch: 57   trainLoss: 1.0861e-01   valLoss:1.6025e-01  time: 1.18e+00\n",
      "epoch: 58   trainLoss: 1.0976e-01   valLoss:1.3514e-01  time: 1.10e+00\n",
      "epoch: 59   trainLoss: 1.1053e-01   valLoss:1.3692e-01  time: 1.08e+00\n",
      "epoch: 60   trainLoss: 1.1538e-01   valLoss:1.6338e-01  time: 1.18e+00\n",
      "epoch: 61   trainLoss: 9.7401e-02   valLoss:1.3331e-01  time: 1.15e+00\n",
      "epoch: 62   trainLoss: 9.6618e-02   valLoss:1.3979e-01  time: 1.09e+00\n",
      "epoch: 63   trainLoss: 1.0236e-01   valLoss:1.4085e-01  time: 1.09e+00\n",
      "epoch: 64   trainLoss: 1.1020e-01   valLoss:1.3006e-01  time: 1.08e+00\n",
      "epoch: 65   trainLoss: 1.0204e-01   valLoss:1.3073e-01  time: 1.20e+00\n",
      "epoch: 66   trainLoss: 1.1475e-01   valLoss:1.3322e-01  time: 1.10e+00\n",
      "epoch: 67   trainLoss: 9.1193e-02   valLoss:1.3366e-01  time: 1.09e+00\n",
      "epoch: 68   trainLoss: 8.8834e-02   valLoss:1.2790e-01  time: 1.07e+00\n",
      "epoch: 69   trainLoss: 9.2139e-02   valLoss:1.2709e-01  time: 1.05e+00\n",
      "epoch: 70   trainLoss: 7.9428e-02   valLoss:1.2979e-01  time: 1.06e+00\n",
      "epoch: 71   trainLoss: 8.9861e-02   valLoss:1.2115e-01  time: 1.09e+00\n",
      "epoch: 72   trainLoss: 7.7500e-02   valLoss:1.3210e-01  time: 1.06e+00\n",
      "epoch: 73   trainLoss: 8.4728e-02   valLoss:1.2831e-01  time: 1.06e+00\n",
      "epoch: 74   trainLoss: 7.8763e-02   valLoss:1.2560e-01  time: 1.23e+00\n",
      "epoch: 75   trainLoss: 8.9710e-02   valLoss:1.1818e-01  time: 1.15e+00\n",
      "epoch: 76   trainLoss: 9.6535e-02   valLoss:1.2143e-01  time: 1.17e+00\n",
      "epoch: 77   trainLoss: 7.4904e-02   valLoss:1.2804e-01  time: 1.09e+00\n",
      "epoch: 78   trainLoss: 8.1459e-02   valLoss:1.2461e-01  time: 1.11e+00\n",
      "epoch: 79   trainLoss: 8.1523e-02   valLoss:1.0686e-01  time: 1.09e+00\n",
      "epoch: 80   trainLoss: 8.2803e-02   valLoss:1.2185e-01  time: 1.10e+00\n",
      "epoch: 81   trainLoss: 8.0057e-02   valLoss:1.2321e-01  time: 1.06e+00\n",
      "epoch: 82   trainLoss: 1.0514e-01   valLoss:1.1680e-01  time: 1.10e+00\n",
      "epoch: 83   trainLoss: 7.1291e-02   valLoss:1.3820e-01  time: 1.05e+00\n",
      "epoch: 84   trainLoss: 7.0047e-02   valLoss:1.1258e-01  time: 1.07e+00\n",
      "epoch: 85   trainLoss: 7.1268e-02   valLoss:1.1549e-01  time: 1.06e+00\n",
      "epoch: 86   trainLoss: 7.9282e-02   valLoss:1.2304e-01  time: 1.15e+00\n",
      "epoch: 87   trainLoss: 7.2235e-02   valLoss:1.1997e-01  time: 1.09e+00\n",
      "epoch: 88   trainLoss: 7.5577e-02   valLoss:1.0630e-01  time: 1.18e+00\n",
      "epoch: 89   trainLoss: 6.6670e-02   valLoss:1.0591e-01  time: 1.15e+00\n",
      "epoch: 90   trainLoss: 6.6312e-02   valLoss:1.0291e-01  time: 1.18e+00\n",
      "epoch: 91   trainLoss: 7.3773e-02   valLoss:1.1087e-01  time: 1.07e+00\n",
      "epoch: 92   trainLoss: 6.6244e-02   valLoss:1.1938e-01  time: 1.06e+00\n",
      "epoch: 93   trainLoss: 6.7933e-02   valLoss:1.1924e-01  time: 1.06e+00\n",
      "epoch: 94   trainLoss: 6.0788e-02   valLoss:9.7227e-02  time: 1.06e+00\n",
      "epoch: 95   trainLoss: 5.7480e-02   valLoss:9.8947e-02  time: 1.06e+00\n",
      "epoch: 96   trainLoss: 6.3587e-02   valLoss:9.9245e-02  time: 1.09e+00\n",
      "epoch: 97   trainLoss: 6.2878e-02   valLoss:1.0836e-01  time: 1.08e+00\n",
      "epoch: 98   trainLoss: 6.2604e-02   valLoss:9.4518e-02  time: 1.16e+00\n",
      "epoch: 99   trainLoss: 5.8506e-02   valLoss:1.0456e-01  time: 1.07e+00\n",
      "loading checkpoint 98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-faaaa1a519134012aab5c1ee936b3631\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-faaaa1a519134012aab5c1ee936b3631\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-faaaa1a519134012aab5c1ee936b3631\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9179817d620cf434511890add1433642\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-9179817d620cf434511890add1433642\": [{\"index\": 0, \"train\": 0.8876554171244303, \"val\": 0.9001581591416012}, {\"index\": 1, \"train\": 0.7777588963508606, \"val\": 0.8751249241866861}, {\"index\": 2, \"train\": 0.7742373744646708, \"val\": 0.8447167554835754}, {\"index\": 3, \"train\": 0.7040318846702576, \"val\": 0.8527197657428958}, {\"index\": 4, \"train\": 0.6594195167223612, \"val\": 0.9060423911583645}, {\"index\": 5, \"train\": 0.625131368637085, \"val\": 0.9729254034658273}, {\"index\": 6, \"train\": 0.5699869294961294, \"val\": 0.9556406608748215}, {\"index\": 7, \"train\": 0.5202430486679077, \"val\": 0.8079282845611926}, {\"index\": 8, \"train\": 0.5494929254055023, \"val\": 0.6215068634289006}, {\"index\": 9, \"train\": 0.478951096534729, \"val\": 0.48270335115699303}, {\"index\": 10, \"train\": 0.4526708523432414, \"val\": 0.45089550464655515}, {\"index\": 11, \"train\": 0.42458399136861164, \"val\": 0.4303793738751362}, {\"index\": 12, \"train\": 0.3971173167228699, \"val\": 0.4085068574554666}, {\"index\": 13, \"train\": 0.35358796020348865, \"val\": 0.39590110167585035}, {\"index\": 14, \"train\": 0.3801467816034953, \"val\": 0.3857916055454148}, {\"index\": 15, \"train\": 0.37477315465609234, \"val\": 0.36344955520945843}, {\"index\": 16, \"train\": 0.34239129225413006, \"val\": 0.35305746787883063}, {\"index\": 17, \"train\": 0.323469618956248, \"val\": 0.3414417414977733}, {\"index\": 18, \"train\": 0.3023865719636281, \"val\": 0.32931700266069835}, {\"index\": 19, \"train\": 0.3032276431719462, \"val\": 0.3225902234359334}, {\"index\": 20, \"train\": 0.3193890005350113, \"val\": 0.31388044316770025}, {\"index\": 21, \"train\": 0.293409804503123, \"val\": 0.30449734659244615}, {\"index\": 22, \"train\": 0.2846258382002513, \"val\": 0.30671718451884333}, {\"index\": 23, \"train\": 0.2887057860692342, \"val\": 0.2853693045951702}, {\"index\": 24, \"train\": 0.26715969542662305, \"val\": 0.2798014487295101}, {\"index\": 25, \"train\": 0.24363364279270172, \"val\": 0.2764820690025334}, {\"index\": 26, \"train\": 0.25065580507119495, \"val\": 0.272991753442006}, {\"index\": 27, \"train\": 0.23792530099550882, \"val\": 0.2602669071337139}, {\"index\": 28, \"train\": 0.25334298610687256, \"val\": 0.26316580627355035}, {\"index\": 29, \"train\": 0.22232484817504883, \"val\": 0.25635907980958345}, {\"index\": 30, \"train\": 0.21926946938037872, \"val\": 0.2479582038725278}, {\"index\": 31, \"train\": 0.2341630756855011, \"val\": 0.24083880800753832}, {\"index\": 32, \"train\": 0.19639447331428528, \"val\": 0.24604369645627835}, {\"index\": 33, \"train\": 0.20138752460479736, \"val\": 0.23865514452120773}, {\"index\": 34, \"train\": 0.19944283862908682, \"val\": 0.2289981103019306}, {\"index\": 35, \"train\": 0.19264901181062064, \"val\": 0.2237134403290434}, {\"index\": 36, \"train\": 0.19339486459891, \"val\": 0.22059904786551165}, {\"index\": 37, \"train\": 0.18962816894054413, \"val\": 0.21808440691825967}, {\"index\": 38, \"train\": 0.16720341642697653, \"val\": 0.2126447223385589}, {\"index\": 39, \"train\": 0.17223092913627625, \"val\": 0.21457981175087668}, {\"index\": 40, \"train\": 0.15955755611260733, \"val\": 0.20768906971163772}, {\"index\": 41, \"train\": 0.15009058515230814, \"val\": 0.21207201498112194}, {\"index\": 42, \"train\": 0.15749855836232504, \"val\": 0.18930569122959343}, {\"index\": 43, \"train\": 0.1519672473271688, \"val\": 0.19285334051690167}, {\"index\": 44, \"train\": 0.14375111957391104, \"val\": 0.1948401065784748}, {\"index\": 45, \"train\": 0.1563784380753835, \"val\": 0.18546952852651616}, {\"index\": 46, \"train\": 0.13994730512301126, \"val\": 0.17722443282104064}, {\"index\": 47, \"train\": 0.14677449812491736, \"val\": 0.17300105044894196}, {\"index\": 48, \"train\": 0.13317742943763733, \"val\": 0.17506736159083192}, {\"index\": 49, \"train\": 0.13368049512306848, \"val\": 0.16213722723639673}, {\"index\": 50, \"train\": 0.12124314904212952, \"val\": 0.15834685744010601}, {\"index\": 51, \"train\": 0.13558388749758402, \"val\": 0.15878213331086077}, {\"index\": 52, \"train\": 0.12270956734816234, \"val\": 0.15487308207795852}, {\"index\": 53, \"train\": 0.12645093351602554, \"val\": 0.1632424947133081}, {\"index\": 54, \"train\": 0.12283854931592941, \"val\": 0.15383524970254964}, {\"index\": 55, \"train\": 0.1270915667215983, \"val\": 0.15292211423662525}, {\"index\": 56, \"train\": 0.11370596041282018, \"val\": 0.13892414101778908}, {\"index\": 57, \"train\": 0.10860591630140941, \"val\": 0.1602476720387737}, {\"index\": 58, \"train\": 0.10975779592990875, \"val\": 0.1351407667734074}, {\"index\": 59, \"train\": 0.11053069929281871, \"val\": 0.13691595779662882}, {\"index\": 60, \"train\": 0.11537634332974751, \"val\": 0.16338347109828005}, {\"index\": 61, \"train\": 0.09740139544010162, \"val\": 0.13331361017966023}, {\"index\": 62, \"train\": 0.09661767135063808, \"val\": 0.139788288416134}, {\"index\": 63, \"train\": 0.10235929985841115, \"val\": 0.14084573092232286}, {\"index\": 64, \"train\": 0.11020295570294063, \"val\": 0.13005999853851757}, {\"index\": 65, \"train\": 0.10204223543405533, \"val\": 0.13072571819389445}, {\"index\": 66, \"train\": 0.11475034554799397, \"val\": 0.13322081575083924}, {\"index\": 67, \"train\": 0.09119320909182231, \"val\": 0.13366498773124208}, {\"index\": 68, \"train\": 0.08883388340473175, \"val\": 0.12790450421851818}, {\"index\": 69, \"train\": 0.09213852882385254, \"val\": 0.12708745024563675}, {\"index\": 70, \"train\": 0.07942839960257213, \"val\": 0.12979283301953087}, {\"index\": 71, \"train\": 0.08986135572195053, \"val\": 0.12115062828848346}, {\"index\": 72, \"train\": 0.07749960198998451, \"val\": 0.13209751652363963}, {\"index\": 73, \"train\": 0.08472772687673569, \"val\": 0.12831420235818736}, {\"index\": 74, \"train\": 0.07876335829496384, \"val\": 0.1256011405752765}, {\"index\": 75, \"train\": 0.08971013377110164, \"val\": 0.11818413611466962}, {\"index\": 76, \"train\": 0.096535158654054, \"val\": 0.12143300561217109}, {\"index\": 77, \"train\": 0.07490364213784535, \"val\": 0.12803756711245687}, {\"index\": 78, \"train\": 0.08145914226770401, \"val\": 0.12460567649557358}, {\"index\": 79, \"train\": 0.0815228670835495, \"val\": 0.10686184570227784}, {\"index\": 80, \"train\": 0.0828026682138443, \"val\": 0.1218509482504386}, {\"index\": 81, \"train\": 0.08005716154972713, \"val\": 0.12321252914800963}, {\"index\": 82, \"train\": 0.1051409939924876, \"val\": 0.11680201403537972}, {\"index\": 83, \"train\": 0.07129066437482834, \"val\": 0.13819783699215837}, {\"index\": 84, \"train\": 0.07004659374554952, \"val\": 0.11258300720213878}, {\"index\": 85, \"train\": 0.07126788049936295, \"val\": 0.11548994754061655}, {\"index\": 86, \"train\": 0.07928173492352168, \"val\": 0.12304307344041902}, {\"index\": 87, \"train\": 0.07223524898290634, \"val\": 0.11996966770298227}, {\"index\": 88, \"train\": 0.07557693372170131, \"val\": 0.10630002482449291}, {\"index\": 89, \"train\": 0.06666983664035797, \"val\": 0.10590731181825201}, {\"index\": 90, \"train\": 0.06631193806727727, \"val\": 0.10291153115772263}, {\"index\": 91, \"train\": 0.07377339899539948, \"val\": 0.11087446407570194}, {\"index\": 92, \"train\": 0.06624393910169601, \"val\": 0.11937868981018525}, {\"index\": 93, \"train\": 0.06793268024921417, \"val\": 0.11924236107410656}, {\"index\": 94, \"train\": 0.06078806643684705, \"val\": 0.09722682369734954}, {\"index\": 95, \"train\": 0.057480463137229286, \"val\": 0.09894663981524193}, {\"index\": 96, \"train\": 0.0635869378844897, \"val\": 0.099245043023041}, {\"index\": 97, \"train\": 0.06287776430447896, \"val\": 0.10836115492404336}, {\"index\": 98, \"train\": 0.06260428080956142, \"val\": 0.09451755394952165}, {\"index\": 99, \"train\": 0.05850589151183764, \"val\": 0.1045576224612348}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_8\n",
      "\n",
      "training on design_9\n",
      "epoch: 0   trainLoss: 1.0742e+00   valLoss:1.3588e+00  time: 1.13e+00\n",
      "epoch: 1   trainLoss: 9.3244e-01   valLoss:1.3202e+00  time: 1.22e+00\n",
      "epoch: 2   trainLoss: 8.3198e-01   valLoss:1.2608e+00  time: 1.14e+00\n",
      "epoch: 3   trainLoss: 7.9700e-01   valLoss:1.1973e+00  time: 1.21e+00\n",
      "epoch: 4   trainLoss: 7.2501e-01   valLoss:1.2067e+00  time: 1.14e+00\n",
      "epoch: 5   trainLoss: 6.4700e-01   valLoss:1.1764e+00  time: 1.13e+00\n",
      "epoch: 6   trainLoss: 6.2242e-01   valLoss:1.1342e+00  time: 1.25e+00\n",
      "epoch: 7   trainLoss: 5.6671e-01   valLoss:1.0049e+00  time: 1.12e+00\n",
      "epoch: 8   trainLoss: 5.4338e-01   valLoss:7.8998e-01  time: 1.12e+00\n",
      "epoch: 9   trainLoss: 4.8971e-01   valLoss:6.0398e-01  time: 1.22e+00\n",
      "epoch: 10   trainLoss: 4.5206e-01   valLoss:4.9935e-01  time: 1.14e+00\n",
      "epoch: 11   trainLoss: 4.2324e-01   valLoss:4.5414e-01  time: 1.13e+00\n",
      "epoch: 12   trainLoss: 4.0892e-01   valLoss:4.2691e-01  time: 1.12e+00\n",
      "epoch: 13   trainLoss: 3.9189e-01   valLoss:4.0037e-01  time: 1.14e+00\n",
      "epoch: 14   trainLoss: 3.4671e-01   valLoss:3.6827e-01  time: 1.13e+00\n",
      "epoch: 15   trainLoss: 3.4717e-01   valLoss:3.6283e-01  time: 1.23e+00\n",
      "epoch: 16   trainLoss: 3.2481e-01   valLoss:3.2269e-01  time: 1.14e+00\n",
      "epoch: 17   trainLoss: 3.1155e-01   valLoss:3.0939e-01  time: 1.13e+00\n",
      "epoch: 18   trainLoss: 2.8551e-01   valLoss:3.2534e-01  time: 1.13e+00\n",
      "epoch: 19   trainLoss: 2.7648e-01   valLoss:3.0108e-01  time: 1.13e+00\n",
      "epoch: 20   trainLoss: 2.6661e-01   valLoss:2.7571e-01  time: 1.12e+00\n",
      "epoch: 21   trainLoss: 2.5437e-01   valLoss:2.6265e-01  time: 1.12e+00\n",
      "epoch: 22   trainLoss: 2.3050e-01   valLoss:2.5207e-01  time: 1.12e+00\n",
      "epoch: 23   trainLoss: 2.1131e-01   valLoss:2.6569e-01  time: 1.22e+00\n",
      "epoch: 24   trainLoss: 2.0291e-01   valLoss:3.0853e-01  time: 1.14e+00\n",
      "epoch: 25   trainLoss: 1.9175e-01   valLoss:2.3601e-01  time: 1.21e+00\n",
      "epoch: 26   trainLoss: 1.9039e-01   valLoss:2.6755e-01  time: 1.21e+00\n",
      "epoch: 27   trainLoss: 1.9613e-01   valLoss:2.1825e-01  time: 1.13e+00\n",
      "epoch: 28   trainLoss: 1.8159e-01   valLoss:2.0729e-01  time: 1.13e+00\n",
      "epoch: 29   trainLoss: 1.6660e-01   valLoss:2.1789e-01  time: 1.24e+00\n",
      "epoch: 30   trainLoss: 1.5568e-01   valLoss:2.1240e-01  time: 1.13e+00\n",
      "epoch: 31   trainLoss: 1.4983e-01   valLoss:2.2341e-01  time: 1.14e+00\n",
      "epoch: 32   trainLoss: 1.4584e-01   valLoss:1.7923e-01  time: 1.13e+00\n",
      "epoch: 33   trainLoss: 1.3781e-01   valLoss:2.0111e-01  time: 1.13e+00\n",
      "epoch: 34   trainLoss: 1.3129e-01   valLoss:1.8550e-01  time: 1.13e+00\n",
      "epoch: 35   trainLoss: 1.2272e-01   valLoss:1.7942e-01  time: 1.14e+00\n",
      "epoch: 36   trainLoss: 1.3084e-01   valLoss:1.7622e-01  time: 1.32e+00\n",
      "epoch: 37   trainLoss: 1.2209e-01   valLoss:1.7771e-01  time: 1.22e+00\n",
      "epoch: 38   trainLoss: 1.2150e-01   valLoss:1.6462e-01  time: 1.12e+00\n",
      "epoch: 39   trainLoss: 1.1490e-01   valLoss:1.8673e-01  time: 1.22e+00\n",
      "epoch: 40   trainLoss: 1.1838e-01   valLoss:1.6875e-01  time: 1.13e+00\n",
      "epoch: 41   trainLoss: 1.0612e-01   valLoss:2.3163e-01  time: 1.11e+00\n",
      "epoch: 42   trainLoss: 1.1612e-01   valLoss:2.1007e-01  time: 1.13e+00\n",
      "epoch: 43   trainLoss: 1.0547e-01   valLoss:2.0962e-01  time: 1.14e+00\n",
      "epoch: 44   trainLoss: 9.4487e-02   valLoss:1.4308e-01  time: 1.14e+00\n",
      "epoch: 45   trainLoss: 1.0081e-01   valLoss:1.4468e-01  time: 1.13e+00\n",
      "epoch: 46   trainLoss: 1.1684e-01   valLoss:2.2261e-01  time: 1.14e+00\n",
      "epoch: 47   trainLoss: 1.0920e-01   valLoss:2.4779e-01  time: 1.14e+00\n",
      "epoch: 48   trainLoss: 9.0960e-02   valLoss:2.1516e-01  time: 1.12e+00\n",
      "epoch: 49   trainLoss: 9.3855e-02   valLoss:2.3872e-01  time: 1.14e+00\n",
      "epoch: 50   trainLoss: 9.2768e-02   valLoss:1.5610e-01  time: 1.14e+00\n",
      "epoch: 51   trainLoss: 9.7904e-02   valLoss:1.6037e-01  time: 1.21e+00\n",
      "epoch: 52   trainLoss: 8.7163e-02   valLoss:1.4071e-01  time: 1.13e+00\n",
      "epoch: 53   trainLoss: 9.1196e-02   valLoss:1.4842e-01  time: 1.23e+00\n",
      "epoch: 54   trainLoss: 8.2522e-02   valLoss:1.6617e-01  time: 1.12e+00\n",
      "epoch: 55   trainLoss: 7.4109e-02   valLoss:1.4221e-01  time: 1.22e+00\n",
      "epoch: 56   trainLoss: 8.0309e-02   valLoss:1.8286e-01  time: 1.13e+00\n",
      "epoch: 57   trainLoss: 7.7739e-02   valLoss:1.6798e-01  time: 1.14e+00\n",
      "epoch: 58   trainLoss: 7.9964e-02   valLoss:1.2943e-01  time: 1.14e+00\n",
      "epoch: 59   trainLoss: 7.5183e-02   valLoss:1.5179e-01  time: 1.13e+00\n",
      "epoch: 60   trainLoss: 7.9050e-02   valLoss:1.2586e-01  time: 1.22e+00\n",
      "epoch: 61   trainLoss: 7.1627e-02   valLoss:1.2753e-01  time: 1.14e+00\n",
      "epoch: 62   trainLoss: 6.7618e-02   valLoss:1.4829e-01  time: 1.13e+00\n",
      "epoch: 63   trainLoss: 6.2050e-02   valLoss:1.3028e-01  time: 1.12e+00\n",
      "epoch: 64   trainLoss: 7.1871e-02   valLoss:1.3400e-01  time: 1.14e+00\n",
      "epoch: 65   trainLoss: 7.2350e-02   valLoss:1.3878e-01  time: 1.14e+00\n",
      "epoch: 66   trainLoss: 6.3772e-02   valLoss:1.1353e-01  time: 1.12e+00\n",
      "epoch: 67   trainLoss: 6.4713e-02   valLoss:1.3239e-01  time: 1.13e+00\n",
      "epoch: 68   trainLoss: 7.7783e-02   valLoss:1.0007e-01  time: 1.12e+00\n",
      "epoch: 69   trainLoss: 6.0082e-02   valLoss:1.1205e-01  time: 1.13e+00\n",
      "epoch: 70   trainLoss: 6.9566e-02   valLoss:1.2408e-01  time: 1.13e+00\n",
      "epoch: 71   trainLoss: 6.9534e-02   valLoss:1.9554e-01  time: 1.13e+00\n",
      "epoch: 72   trainLoss: 7.0596e-02   valLoss:1.1157e-01  time: 1.22e+00\n",
      "epoch: 73   trainLoss: 7.6280e-02   valLoss:1.4728e-01  time: 1.12e+00\n",
      "epoch: 74   trainLoss: 6.4999e-02   valLoss:1.0569e-01  time: 1.13e+00\n",
      "epoch: 75   trainLoss: 6.2871e-02   valLoss:9.6307e-02  time: 1.14e+00\n",
      "epoch: 76   trainLoss: 5.8384e-02   valLoss:1.1690e-01  time: 1.13e+00\n",
      "epoch: 77   trainLoss: 5.8564e-02   valLoss:1.1322e-01  time: 1.12e+00\n",
      "epoch: 78   trainLoss: 6.7606e-02   valLoss:1.1317e-01  time: 1.12e+00\n",
      "epoch: 79   trainLoss: 5.8316e-02   valLoss:1.3092e-01  time: 1.30e+00\n",
      "epoch: 80   trainLoss: 5.4764e-02   valLoss:1.2076e-01  time: 1.14e+00\n",
      "epoch: 81   trainLoss: 5.6209e-02   valLoss:1.1917e-01  time: 1.12e+00\n",
      "epoch: 82   trainLoss: 5.1296e-02   valLoss:1.5491e-01  time: 1.15e+00\n",
      "epoch: 83   trainLoss: 5.2060e-02   valLoss:9.6915e-02  time: 1.14e+00\n",
      "epoch: 84   trainLoss: 4.9514e-02   valLoss:9.3092e-02  time: 1.12e+00\n",
      "epoch: 85   trainLoss: 5.6731e-02   valLoss:9.0748e-02  time: 1.13e+00\n",
      "epoch: 86   trainLoss: 4.7434e-02   valLoss:9.6801e-02  time: 1.12e+00\n",
      "epoch: 87   trainLoss: 4.4165e-02   valLoss:9.2951e-02  time: 1.15e+00\n",
      "epoch: 88   trainLoss: 5.6713e-02   valLoss:1.0339e-01  time: 1.12e+00\n",
      "epoch: 89   trainLoss: 4.6209e-02   valLoss:8.4584e-02  time: 1.13e+00\n",
      "epoch: 90   trainLoss: 4.5829e-02   valLoss:9.2669e-02  time: 1.13e+00\n",
      "epoch: 91   trainLoss: 4.5692e-02   valLoss:9.0932e-02  time: 1.15e+00\n",
      "epoch: 92   trainLoss: 4.6241e-02   valLoss:1.0697e-01  time: 1.13e+00\n",
      "epoch: 93   trainLoss: 5.4229e-02   valLoss:8.3393e-02  time: 1.14e+00\n",
      "epoch: 94   trainLoss: 5.7720e-02   valLoss:9.0295e-02  time: 1.32e+00\n",
      "epoch: 95   trainLoss: 5.0589e-02   valLoss:1.0876e-01  time: 1.21e+00\n",
      "epoch: 96   trainLoss: 4.3898e-02   valLoss:8.4397e-02  time: 1.15e+00\n",
      "epoch: 97   trainLoss: 5.3271e-02   valLoss:8.6561e-02  time: 1.14e+00\n",
      "epoch: 98   trainLoss: 4.5392e-02   valLoss:8.3995e-02  time: 1.13e+00\n",
      "epoch: 99   trainLoss: 4.3545e-02   valLoss:9.6977e-02  time: 1.31e+00\n",
      "loading checkpoint 93\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-01bdfa7f39e24a7db98b4fbf7f436c01\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-01bdfa7f39e24a7db98b4fbf7f436c01\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-01bdfa7f39e24a7db98b4fbf7f436c01\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0d82c19e1536ddd515ed34181a5932c9\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0d82c19e1536ddd515ed34181a5932c9\": [{\"index\": 0, \"train\": 1.0742321411768596, \"val\": 1.3587678826448542}, {\"index\": 1, \"train\": 0.9324424266815186, \"val\": 1.3201879574917257}, {\"index\": 2, \"train\": 0.8319845994313558, \"val\": 1.2607923433512311}, {\"index\": 3, \"train\": 0.7970049579938253, \"val\": 1.1973403135349077}, {\"index\": 4, \"train\": 0.7250081698099772, \"val\": 1.2067427749538586}, {\"index\": 5, \"train\": 0.647004763285319, \"val\": 1.176441507862398}, {\"index\": 6, \"train\": 0.6224245230356852, \"val\": 1.134179166138724}, {\"index\": 7, \"train\": 0.566707988580068, \"val\": 1.0048772289797112}, {\"index\": 8, \"train\": 0.5433804392814636, \"val\": 0.7899757152492249}, {\"index\": 9, \"train\": 0.4897124568621318, \"val\": 0.6039806883437214}, {\"index\": 10, \"train\": 0.4520644545555115, \"val\": 0.49935157715204964}, {\"index\": 11, \"train\": 0.4232385853926341, \"val\": 0.4541447141587182}, {\"index\": 12, \"train\": 0.40891799330711365, \"val\": 0.42691305494453347}, {\"index\": 13, \"train\": 0.3918871184190114, \"val\": 0.40037183021195233}, {\"index\": 14, \"train\": 0.346713791290919, \"val\": 0.3682690217608103}, {\"index\": 15, \"train\": 0.34716888268788654, \"val\": 0.3628276359738299}, {\"index\": 16, \"train\": 0.3248069187005361, \"val\": 0.322689362646391}, {\"index\": 17, \"train\": 0.311548391977946, \"val\": 0.30939106049050613}, {\"index\": 18, \"train\": 0.2855131725470225, \"val\": 0.3253426923658009}, {\"index\": 19, \"train\": 0.27648205558458966, \"val\": 0.301082081247673}, {\"index\": 20, \"train\": 0.26660555601119995, \"val\": 0.2757117921934911}, {\"index\": 21, \"train\": 0.2543746729691823, \"val\": 0.2626482910062704}, {\"index\": 22, \"train\": 0.2305014580488205, \"val\": 0.25207036255030996}, {\"index\": 23, \"train\": 0.21130705873171488, \"val\": 0.2656901839051258}, {\"index\": 24, \"train\": 0.20290767649809519, \"val\": 0.3085333593620884}, {\"index\": 25, \"train\": 0.1917510281006495, \"val\": 0.23601104889961858}, {\"index\": 26, \"train\": 0.19038652877012888, \"val\": 0.26754859477040116}, {\"index\": 27, \"train\": 0.19613404075304666, \"val\": 0.21824987634533533}, {\"index\": 28, \"train\": 0.18158803383509317, \"val\": 0.20728951345059882}, {\"index\": 29, \"train\": 0.16660031179587045, \"val\": 0.2178856939749999}, {\"index\": 30, \"train\": 0.15567933519681296, \"val\": 0.21240249176992587}, {\"index\": 31, \"train\": 0.14982669055461884, \"val\": 0.2234065381085707}, {\"index\": 32, \"train\": 0.14584122598171234, \"val\": 0.179226899140135}, {\"index\": 33, \"train\": 0.13781442244847616, \"val\": 0.20110778899483936}, {\"index\": 34, \"train\": 0.1312883123755455, \"val\": 0.18549974478298314}, {\"index\": 35, \"train\": 0.12271888305743535, \"val\": 0.17941574052976514}, {\"index\": 36, \"train\": 0.130843386054039, \"val\": 0.1762164174751551}, {\"index\": 37, \"train\": 0.12209493418534596, \"val\": 0.17771152277580565}, {\"index\": 38, \"train\": 0.1215033233165741, \"val\": 0.16462473632526342}, {\"index\": 39, \"train\": 0.1149019996325175, \"val\": 0.18672848293661243}, {\"index\": 40, \"train\": 0.1183817982673645, \"val\": 0.16874907455941732}, {\"index\": 41, \"train\": 0.1061150108774503, \"val\": 0.23163266492041726}, {\"index\": 42, \"train\": 0.11611797660589218, \"val\": 0.2100702975962863}, {\"index\": 43, \"train\": 0.10547122359275818, \"val\": 0.20961684647395654}, {\"index\": 44, \"train\": 0.09448690960804622, \"val\": 0.1430790318797032}, {\"index\": 45, \"train\": 0.10080587863922119, \"val\": 0.1446824245310078}, {\"index\": 46, \"train\": 0.11683654288450877, \"val\": 0.2226143101385484}, {\"index\": 47, \"train\": 0.10920160760482152, \"val\": 0.24778966517705056}, {\"index\": 48, \"train\": 0.09095970044533412, \"val\": 0.21516036462142235}, {\"index\": 49, \"train\": 0.09385451426108678, \"val\": 0.2387161696916101}, {\"index\": 50, \"train\": 0.09276783714691798, \"val\": 0.15609816410061386}, {\"index\": 51, \"train\": 0.09790367136398952, \"val\": 0.16037071070254402}, {\"index\": 52, \"train\": 0.08716297149658203, \"val\": 0.1407087567883233}, {\"index\": 53, \"train\": 0.0911958043773969, \"val\": 0.14841715023956364}, {\"index\": 54, \"train\": 0.08252156525850296, \"val\": 0.16617364962412803}, {\"index\": 55, \"train\": 0.07410905510187149, \"val\": 0.14221009126588427}, {\"index\": 56, \"train\": 0.08030868818362553, \"val\": 0.1828582416874943}, {\"index\": 57, \"train\": 0.07773924618959427, \"val\": 0.16797782735968078}, {\"index\": 58, \"train\": 0.07996366918087006, \"val\": 0.12942807590675162}, {\"index\": 59, \"train\": 0.07518342013160388, \"val\": 0.1517881011866309}, {\"index\": 60, \"train\": 0.07905022675792377, \"val\": 0.12586065007511665}, {\"index\": 61, \"train\": 0.07162673274676006, \"val\": 0.12752594603053122}, {\"index\": 62, \"train\": 0.06761814281344414, \"val\": 0.14828563904113792}, {\"index\": 63, \"train\": 0.062049860755602516, \"val\": 0.13028104609326907}, {\"index\": 64, \"train\": 0.071870772788922, \"val\": 0.13400429156522645}, {\"index\": 65, \"train\": 0.07234966134031613, \"val\": 0.13877672936629365}, {\"index\": 66, \"train\": 0.06377200037240982, \"val\": 0.1135315482582276}, {\"index\": 67, \"train\": 0.06471302236119907, \"val\": 0.1323892610811594}, {\"index\": 68, \"train\": 0.07778278241554896, \"val\": 0.10007342139983343}, {\"index\": 69, \"train\": 0.06008200099070867, \"val\": 0.11204566844928733}, {\"index\": 70, \"train\": 0.06956571713089943, \"val\": 0.12407595822070208}, {\"index\": 71, \"train\": 0.06953368956844012, \"val\": 0.19554002973664966}, {\"index\": 72, \"train\": 0.07059557363390923, \"val\": 0.11156562035817101}, {\"index\": 73, \"train\": 0.07627968614300092, \"val\": 0.14727552062659352}, {\"index\": 74, \"train\": 0.0649988129734993, \"val\": 0.10569443350174913}, {\"index\": 75, \"train\": 0.0628706340988477, \"val\": 0.09630719139413149}, {\"index\": 76, \"train\": 0.05838441972931226, \"val\": 0.11689502667394225}, {\"index\": 77, \"train\": 0.058564422031243644, \"val\": 0.11321527546668356}, {\"index\": 78, \"train\": 0.06760645657777786, \"val\": 0.11316883044006924}, {\"index\": 79, \"train\": 0.058315590023994446, \"val\": 0.13091822811488615}, {\"index\": 80, \"train\": 0.05476399511098862, \"val\": 0.12075939093699196}, {\"index\": 81, \"train\": 0.05620924259225527, \"val\": 0.11917289618523447}, {\"index\": 82, \"train\": 0.051295759777228035, \"val\": 0.15491201002495708}, {\"index\": 83, \"train\": 0.0520604153474172, \"val\": 0.09691543595141007}, {\"index\": 84, \"train\": 0.0495143656929334, \"val\": 0.09309221972513047}, {\"index\": 85, \"train\": 0.05673141529162725, \"val\": 0.09074843202769342}, {\"index\": 86, \"train\": 0.04743423064549764, \"val\": 0.09680061715361835}, {\"index\": 87, \"train\": 0.04416494940718015, \"val\": 0.09295144416736784}, {\"index\": 88, \"train\": 0.05671316385269165, \"val\": 0.10339212956131194}, {\"index\": 89, \"train\": 0.04620945453643799, \"val\": 0.08458423645114871}, {\"index\": 90, \"train\": 0.045828851560751595, \"val\": 0.0926694888147284}, {\"index\": 91, \"train\": 0.04569215948383013, \"val\": 0.09093234358631351}, {\"index\": 92, \"train\": 0.046241141855716705, \"val\": 0.10696996538037504}, {\"index\": 93, \"train\": 0.05422905584176382, \"val\": 0.08339343986611951}, {\"index\": 94, \"train\": 0.057720174392064415, \"val\": 0.09029485230092649}, {\"index\": 95, \"train\": 0.05058889463543892, \"val\": 0.10875952383934485}, {\"index\": 96, \"train\": 0.04389769832293192, \"val\": 0.08439680010606362}, {\"index\": 97, \"train\": 0.05327099561691284, \"val\": 0.08656091489656656}, {\"index\": 98, \"train\": 0.045392035196224846, \"val\": 0.08399489020100898}, {\"index\": 99, \"train\": 0.04354460289080938, \"val\": 0.09697739377238408}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>maxAE</th>\n",
       "      <th>mae/peak</th>\n",
       "      <th>maxAE/peak</th>\n",
       "      <th>relEAtPeak</th>\n",
       "      <th>Trained on</th>\n",
       "      <th>Tested on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.118980</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.109546</td>\n",
       "      <td>0.344441</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.088504</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.105254</td>\n",
       "      <td>0.047008</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.098149</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.046424</td>\n",
       "      <td>0.164063</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.080626</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.037601</td>\n",
       "      <td>0.104297</td>\n",
       "      <td>0.104297</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.156502</td>\n",
       "      <td>0.397689</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.170756</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>0.459868</td>\n",
       "      <td>0.418951</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>0.141644</td>\n",
       "      <td>0.020024</td>\n",
       "      <td>0.055678</td>\n",
       "      <td>0.124842</td>\n",
       "      <td>0.101105</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.159829</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.120447</td>\n",
       "      <td>0.356676</td>\n",
       "      <td>0.168230</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.157918</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>0.096798</td>\n",
       "      <td>0.398298</td>\n",
       "      <td>0.336077</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.097360</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.055240</td>\n",
       "      <td>0.255657</td>\n",
       "      <td>0.039475</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mse       mae       mre     maxAE  mae/peak  maxAE/peak  relEAtPeak  \\\n",
       "0    0.000005  0.001795  0.118980  0.005643  0.109546    0.344441    0.038204   \n",
       "1    0.000004  0.001591  0.088504  0.003511  0.047683    0.105254    0.047008   \n",
       "2    0.000014  0.002641  0.098149  0.009333  0.046424    0.164063    0.140409   \n",
       "3    0.000021  0.003645  0.080626  0.010110  0.037601    0.104297    0.104297   \n",
       "4    0.000007  0.002221  0.145833  0.005644  0.156502    0.397689    0.015627   \n",
       "..        ...       ...       ...       ...       ...         ...         ...   \n",
       "895  0.000035  0.004708  0.170756  0.017067  0.126866    0.459868    0.418951   \n",
       "896  0.000109  0.008930  0.141644  0.020024  0.055678    0.124842    0.101105   \n",
       "897  0.000026  0.003981  0.159829  0.011790  0.120447    0.356676    0.168230   \n",
       "898  0.000045  0.005326  0.157918  0.021916  0.096798    0.398298    0.336077   \n",
       "899  0.000020  0.003380  0.097360  0.015644  0.055240    0.255657    0.039475   \n",
       "\n",
       "     Trained on Tested on  \n",
       "0    test group  design_5  \n",
       "1    test group  design_5  \n",
       "2    test group  design_5  \n",
       "3    test group  design_5  \n",
       "4    test group  design_5  \n",
       "..          ...       ...  \n",
       "895  test group  design_9  \n",
       "896  test group  design_9  \n",
       "897  test group  design_9  \n",
       "898  test group  design_9  \n",
       "899  test group  design_9  \n",
       "\n",
       "[900 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for trainName, trainSet in trainSets.items():\n",
    "    print('training on '+trainName)\n",
    "\n",
    "    # train\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainSet, valSets[trainName], epochs=epochs, batch_size=256, flatten=True, logTrans=False, \n",
    "                             ssTrans=True, saveDir=saveDir+trainName)\n",
    "\n",
    "    display(plotHistory(history))\n",
    "\n",
    "    # test\n",
    "    print('testing on '+trainName+'\\n')\n",
    "    resultsDict = gcn.testModel(testSets[trainName], level='field')\n",
    "    resultsDict['Trained on'] = ['test group']*len(resultsDict['mse'])\n",
    "    resultsDict['Tested on'] = [trainName]*len(resultsDict['mse'])\n",
    "    results = pivotDict(resultsDict)\n",
    "    resultsList.extend(results)\n",
    "        \n",
    "pd.DataFrame(resultsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train on all groups at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on all groups\n",
      "epoch: 0   trainLoss: 9.2671e-01   valLoss:1.1365e+00  time: 5.43e+00\n",
      "epoch: 1   trainLoss: 7.2841e-01   valLoss:1.0169e+00  time: 5.54e+00\n",
      "epoch: 2   trainLoss: 5.9773e-01   valLoss:5.9983e-01  time: 5.71e+00\n",
      "epoch: 3   trainLoss: 5.0316e-01   valLoss:5.0962e-01  time: 5.60e+00\n",
      "epoch: 4   trainLoss: 4.3783e-01   valLoss:4.4042e-01  time: 5.47e+00\n",
      "epoch: 5   trainLoss: 3.8774e-01   valLoss:4.0421e-01  time: 5.61e+00\n",
      "epoch: 6   trainLoss: 3.5105e-01   valLoss:3.6468e-01  time: 5.60e+00\n",
      "epoch: 7   trainLoss: 3.1636e-01   valLoss:3.6055e-01  time: 5.47e+00\n",
      "epoch: 8   trainLoss: 2.8824e-01   valLoss:3.2132e-01  time: 5.70e+00\n",
      "epoch: 9   trainLoss: 2.6415e-01   valLoss:3.3713e-01  time: 5.32e+00\n",
      "epoch: 10   trainLoss: 2.4084e-01   valLoss:2.8333e-01  time: 5.61e+00\n",
      "epoch: 11   trainLoss: 2.2124e-01   valLoss:2.6678e-01  time: 5.33e+00\n",
      "epoch: 12   trainLoss: 2.1076e-01   valLoss:2.9544e-01  time: 5.64e+00\n",
      "epoch: 13   trainLoss: 1.9571e-01   valLoss:2.6086e-01  time: 5.46e+00\n",
      "epoch: 14   trainLoss: 1.8050e-01   valLoss:2.6758e-01  time: 5.59e+00\n",
      "epoch: 15   trainLoss: 1.7908e-01   valLoss:2.4193e-01  time: 5.56e+00\n",
      "epoch: 16   trainLoss: 1.6886e-01   valLoss:1.9181e-01  time: 5.36e+00\n",
      "epoch: 17   trainLoss: 1.5625e-01   valLoss:1.9121e-01  time: 5.48e+00\n",
      "epoch: 18   trainLoss: 1.4566e-01   valLoss:1.8195e-01  time: 5.66e+00\n",
      "epoch: 19   trainLoss: 1.3687e-01   valLoss:2.0990e-01  time: 5.43e+00\n",
      "epoch: 20   trainLoss: 1.3618e-01   valLoss:2.2445e-01  time: 5.55e+00\n",
      "epoch: 21   trainLoss: 1.2963e-01   valLoss:1.6181e-01  time: 5.60e+00\n",
      "epoch: 22   trainLoss: 1.2482e-01   valLoss:1.5464e-01  time: 5.67e+00\n",
      "epoch: 23   trainLoss: 1.2218e-01   valLoss:1.7730e-01  time: 5.54e+00\n",
      "epoch: 24   trainLoss: 1.1220e-01   valLoss:2.0733e-01  time: 5.58e+00\n",
      "epoch: 25   trainLoss: 1.0888e-01   valLoss:1.9211e-01  time: 5.44e+00\n",
      "epoch: 26   trainLoss: 1.1508e-01   valLoss:1.6325e-01  time: 5.46e+00\n",
      "epoch: 27   trainLoss: 1.0889e-01   valLoss:1.9729e-01  time: 5.42e+00\n",
      "epoch: 28   trainLoss: 1.0639e-01   valLoss:1.9245e-01  time: 5.64e+00\n",
      "epoch: 29   trainLoss: 1.0273e-01   valLoss:1.7242e-01  time: 5.57e+00\n",
      "epoch: 30   trainLoss: 9.8619e-02   valLoss:1.5584e-01  time: 5.68e+00\n",
      "epoch: 31   trainLoss: 9.8794e-02   valLoss:1.7172e-01  time: 5.61e+00\n",
      "epoch: 32   trainLoss: 1.0093e-01   valLoss:1.6605e-01  time: 5.70e+00\n",
      "epoch: 33   trainLoss: 9.2690e-02   valLoss:1.3004e-01  time: 5.40e+00\n",
      "epoch: 34   trainLoss: 8.9919e-02   valLoss:1.3076e-01  time: 5.66e+00\n",
      "epoch: 35   trainLoss: 8.6854e-02   valLoss:1.5756e-01  time: 5.56e+00\n",
      "epoch: 36   trainLoss: 9.1185e-02   valLoss:1.2255e-01  time: 5.79e+00\n",
      "epoch: 37   trainLoss: 8.1799e-02   valLoss:1.3619e-01  time: 5.61e+00\n",
      "epoch: 38   trainLoss: 8.1609e-02   valLoss:1.3280e-01  time: 5.62e+00\n",
      "epoch: 39   trainLoss: 8.2601e-02   valLoss:1.3453e-01  time: 5.21e+00\n",
      "epoch: 40   trainLoss: 8.2300e-02   valLoss:1.0894e-01  time: 5.41e+00\n",
      "epoch: 41   trainLoss: 7.4828e-02   valLoss:1.3794e-01  time: 5.71e+00\n",
      "epoch: 42   trainLoss: 7.5732e-02   valLoss:1.4830e-01  time: 5.11e+00\n",
      "epoch: 43   trainLoss: 7.6480e-02   valLoss:1.4433e-01  time: 5.22e+00\n",
      "epoch: 44   trainLoss: 8.3061e-02   valLoss:1.2721e-01  time: 5.42e+00\n",
      "epoch: 45   trainLoss: 8.0192e-02   valLoss:1.2621e-01  time: 5.16e+00\n",
      "epoch: 46   trainLoss: 7.3942e-02   valLoss:1.2826e-01  time: 5.72e+00\n",
      "epoch: 47   trainLoss: 7.1691e-02   valLoss:1.3235e-01  time: 5.40e+00\n",
      "epoch: 48   trainLoss: 6.8317e-02   valLoss:1.2601e-01  time: 5.56e+00\n",
      "epoch: 49   trainLoss: 7.1239e-02   valLoss:1.3357e-01  time: 5.36e+00\n",
      "epoch: 50   trainLoss: 6.5809e-02   valLoss:1.5398e-01  time: 5.45e+00\n",
      "epoch: 51   trainLoss: 6.5804e-02   valLoss:1.0523e-01  time: 5.72e+00\n",
      "epoch: 52   trainLoss: 7.1708e-02   valLoss:1.2542e-01  time: 5.68e+00\n",
      "epoch: 53   trainLoss: 7.0663e-02   valLoss:1.5410e-01  time: 5.46e+00\n",
      "epoch: 54   trainLoss: 6.6899e-02   valLoss:1.1192e-01  time: 5.41e+00\n",
      "epoch: 55   trainLoss: 7.1818e-02   valLoss:1.2387e-01  time: 5.53e+00\n",
      "epoch: 56   trainLoss: 6.6855e-02   valLoss:1.0154e-01  time: 5.42e+00\n",
      "epoch: 57   trainLoss: 6.0980e-02   valLoss:1.1022e-01  time: 5.46e+00\n",
      "epoch: 58   trainLoss: 6.0878e-02   valLoss:1.1267e-01  time: 5.28e+00\n",
      "epoch: 59   trainLoss: 6.8582e-02   valLoss:8.5885e-02  time: 5.42e+00\n",
      "epoch: 60   trainLoss: 6.4659e-02   valLoss:1.1294e-01  time: 5.27e+00\n",
      "epoch: 61   trainLoss: 6.6043e-02   valLoss:9.9217e-02  time: 5.39e+00\n",
      "epoch: 62   trainLoss: 5.8701e-02   valLoss:9.8126e-02  time: 5.09e+00\n",
      "epoch: 63   trainLoss: 5.8126e-02   valLoss:1.1095e-01  time: 5.25e+00\n",
      "epoch: 64   trainLoss: 5.8993e-02   valLoss:1.0518e-01  time: 5.29e+00\n",
      "epoch: 65   trainLoss: 6.1760e-02   valLoss:9.4699e-02  time: 5.36e+00\n",
      "epoch: 66   trainLoss: 6.3214e-02   valLoss:9.1473e-02  time: 5.61e+00\n",
      "epoch: 67   trainLoss: 6.3589e-02   valLoss:8.7998e-02  time: 5.41e+00\n",
      "epoch: 68   trainLoss: 6.2021e-02   valLoss:9.2437e-02  time: 5.52e+00\n",
      "epoch: 69   trainLoss: 6.4785e-02   valLoss:9.6263e-02  time: 5.23e+00\n",
      "epoch: 70   trainLoss: 5.3690e-02   valLoss:8.6779e-02  time: 5.13e+00\n",
      "epoch: 71   trainLoss: 5.4668e-02   valLoss:1.0255e-01  time: 5.16e+00\n",
      "epoch: 72   trainLoss: 5.2072e-02   valLoss:9.9186e-02  time: 5.51e+00\n",
      "epoch: 73   trainLoss: 5.0762e-02   valLoss:9.0648e-02  time: 5.12e+00\n",
      "epoch: 74   trainLoss: 5.3407e-02   valLoss:8.6963e-02  time: 5.16e+00\n",
      "epoch: 75   trainLoss: 5.8634e-02   valLoss:1.0960e-01  time: 5.30e+00\n",
      "epoch: 76   trainLoss: 5.7945e-02   valLoss:9.1087e-02  time: 5.49e+00\n",
      "epoch: 77   trainLoss: 5.8361e-02   valLoss:8.6162e-02  time: 5.41e+00\n",
      "epoch: 78   trainLoss: 5.0375e-02   valLoss:7.3116e-02  time: 5.42e+00\n",
      "epoch: 79   trainLoss: 5.3139e-02   valLoss:7.5914e-02  time: 5.42e+00\n",
      "epoch: 80   trainLoss: 5.0073e-02   valLoss:8.7054e-02  time: 5.44e+00\n",
      "epoch: 81   trainLoss: 4.8443e-02   valLoss:9.7863e-02  time: 5.61e+00\n",
      "epoch: 82   trainLoss: 5.1141e-02   valLoss:6.9180e-02  time: 5.29e+00\n",
      "epoch: 83   trainLoss: 5.0981e-02   valLoss:8.3262e-02  time: 5.38e+00\n",
      "epoch: 84   trainLoss: 4.8781e-02   valLoss:8.3137e-02  time: 5.43e+00\n",
      "epoch: 85   trainLoss: 5.5594e-02   valLoss:1.0762e-01  time: 5.62e+00\n",
      "epoch: 86   trainLoss: 4.8839e-02   valLoss:6.6682e-02  time: 5.57e+00\n",
      "epoch: 87   trainLoss: 5.2783e-02   valLoss:7.8733e-02  time: 5.41e+00\n",
      "epoch: 88   trainLoss: 5.0631e-02   valLoss:7.2594e-02  time: 5.44e+00\n",
      "epoch: 89   trainLoss: 4.8834e-02   valLoss:7.8959e-02  time: 5.30e+00\n",
      "epoch: 90   trainLoss: 4.9669e-02   valLoss:7.2691e-02  time: 5.37e+00\n",
      "epoch: 91   trainLoss: 4.8616e-02   valLoss:8.0533e-02  time: 5.24e+00\n",
      "epoch: 92   trainLoss: 4.6028e-02   valLoss:6.6727e-02  time: 5.66e+00\n",
      "epoch: 93   trainLoss: 4.4859e-02   valLoss:6.7165e-02  time: 5.55e+00\n",
      "epoch: 94   trainLoss: 4.3177e-02   valLoss:6.9802e-02  time: 5.64e+00\n",
      "epoch: 95   trainLoss: 4.8099e-02   valLoss:6.9822e-02  time: 5.13e+00\n",
      "epoch: 96   trainLoss: 5.0859e-02   valLoss:6.7098e-02  time: 5.52e+00\n",
      "epoch: 97   trainLoss: 4.8654e-02   valLoss:7.7908e-02  time: 5.43e+00\n",
      "epoch: 98   trainLoss: 5.5383e-02   valLoss:9.0735e-02  time: 5.35e+00\n",
      "epoch: 99   trainLoss: 4.7259e-02   valLoss:7.1431e-02  time: 5.43e+00\n",
      "loading checkpoint 86\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6dfc7c544098432fbe563ff968a392a7\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6dfc7c544098432fbe563ff968a392a7\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6dfc7c544098432fbe563ff968a392a7\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-96e6261f9d506ac98f4ba52ff4422d2a\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-96e6261f9d506ac98f4ba52ff4422d2a\": [{\"index\": 0, \"train\": 0.9267131239175797, \"val\": 1.1364789186183502}, {\"index\": 1, \"train\": 0.728412926197052, \"val\": 1.0168615214457666}, {\"index\": 2, \"train\": 0.5977262407541275, \"val\": 0.59983329259687}, {\"index\": 3, \"train\": 0.5031624883413315, \"val\": 0.5096209435413281}, {\"index\": 4, \"train\": 0.4378274629513423, \"val\": 0.4404173014909719}, {\"index\": 5, \"train\": 0.3877421095967293, \"val\": 0.40420860163695005}, {\"index\": 6, \"train\": 0.35104505469401676, \"val\": 0.3646803170373594}, {\"index\": 7, \"train\": 0.3163635656237602, \"val\": 0.3605541522787125}, {\"index\": 8, \"train\": 0.28824374203880626, \"val\": 0.321324460822399}, {\"index\": 9, \"train\": 0.26414594799280167, \"val\": 0.3371286958835467}, {\"index\": 10, \"train\": 0.24084130177895227, \"val\": 0.28333380056148466}, {\"index\": 11, \"train\": 0.22123875468969345, \"val\": 0.26677579376415383}, {\"index\": 12, \"train\": 0.2107623666524887, \"val\": 0.29543881900522306}, {\"index\": 13, \"train\": 0.19571383918325105, \"val\": 0.2608565849269292}, {\"index\": 14, \"train\": 0.1804963524142901, \"val\": 0.2675752039334771}, {\"index\": 15, \"train\": 0.17907781774799028, \"val\": 0.24192797185646164}, {\"index\": 16, \"train\": 0.1688592011729876, \"val\": 0.19181397689799606}, {\"index\": 17, \"train\": 0.1562513578683138, \"val\": 0.1912120751614027}, {\"index\": 18, \"train\": 0.14565549915035567, \"val\": 0.1819530513913681}, {\"index\": 19, \"train\": 0.13686668127775192, \"val\": 0.20990317884435947}, {\"index\": 20, \"train\": 0.13618322586019835, \"val\": 0.2244485803267539}, {\"index\": 21, \"train\": 0.12963259654740492, \"val\": 0.16181243642629986}, {\"index\": 22, \"train\": 0.12481797051926453, \"val\": 0.15464083655927055}, {\"index\": 23, \"train\": 0.1221781490991513, \"val\": 0.17730079047399125}, {\"index\": 24, \"train\": 0.11220223332444827, \"val\": 0.20732703782204126}, {\"index\": 25, \"train\": 0.10888221052785714, \"val\": 0.192107828496093}, {\"index\": 26, \"train\": 0.1150757900128762, \"val\": 0.16325368180232883}, {\"index\": 27, \"train\": 0.10889246438940366, \"val\": 0.1972852562334285}, {\"index\": 28, \"train\": 0.10639018441239993, \"val\": 0.19245353817629318}, {\"index\": 29, \"train\": 0.10272757957379024, \"val\": 0.1724201385923489}, {\"index\": 30, \"train\": 0.09861902085443337, \"val\": 0.15583804499754614}, {\"index\": 31, \"train\": 0.09879439572493236, \"val\": 0.17172483922051335}, {\"index\": 32, \"train\": 0.10093130730092525, \"val\": 0.16605189862505843}, {\"index\": 33, \"train\": 0.0926902877787749, \"val\": 0.13003994296520466}, {\"index\": 34, \"train\": 0.08991912628213565, \"val\": 0.13076090732227183}, {\"index\": 35, \"train\": 0.08685429766774178, \"val\": 0.1575617720372975}, {\"index\": 36, \"train\": 0.0911852450420459, \"val\": 0.12254504710392751}, {\"index\": 37, \"train\": 0.08179870496193568, \"val\": 0.13618877473744323}, {\"index\": 38, \"train\": 0.08160893991589546, \"val\": 0.1327981753899992}, {\"index\": 39, \"train\": 0.08260132496555646, \"val\": 0.13453410284343623}, {\"index\": 40, \"train\": 0.08229967144628365, \"val\": 0.10894440098889861}, {\"index\": 41, \"train\": 0.07482833322137594, \"val\": 0.137939824989169}, {\"index\": 42, \"train\": 0.07573210882643859, \"val\": 0.14829700930664938}, {\"index\": 43, \"train\": 0.07648005771140258, \"val\": 0.14433195401647095}, {\"index\": 44, \"train\": 0.08306136913597584, \"val\": 0.1272145663086256}, {\"index\": 45, \"train\": 0.0801915880292654, \"val\": 0.12620587546973386}, {\"index\": 46, \"train\": 0.0739424005150795, \"val\": 0.12825573672825058}, {\"index\": 47, \"train\": 0.0716905277222395, \"val\": 0.13234778081963736}, {\"index\": 48, \"train\": 0.06831708271056414, \"val\": 0.1260111598961952}, {\"index\": 49, \"train\": 0.07123924015710752, \"val\": 0.1335669369648073}, {\"index\": 50, \"train\": 0.06580943738420804, \"val\": 0.15398188083027317}, {\"index\": 51, \"train\": 0.06580403322974841, \"val\": 0.10523343650253351}, {\"index\": 52, \"train\": 0.07170801640798648, \"val\": 0.12542360109208084}, {\"index\": 53, \"train\": 0.0706625518699487, \"val\": 0.15409706582022073}, {\"index\": 54, \"train\": 0.06689884389440219, \"val\": 0.11191987840036206}, {\"index\": 55, \"train\": 0.07181787087271611, \"val\": 0.12386577176663127}, {\"index\": 56, \"train\": 0.06685467064380646, \"val\": 0.10154397518425766}, {\"index\": 57, \"train\": 0.060979786018530525, \"val\": 0.11021815874415485}, {\"index\": 58, \"train\": 0.06087766091028849, \"val\": 0.1126698721424435}, {\"index\": 59, \"train\": 0.0685816112284859, \"val\": 0.08588491115348276}, {\"index\": 60, \"train\": 0.06465861139198144, \"val\": 0.11293574670346937}, {\"index\": 61, \"train\": 0.0660425228998065, \"val\": 0.09921691273339092}, {\"index\": 62, \"train\": 0.05870119544366995, \"val\": 0.0981259623408766}, {\"index\": 63, \"train\": 0.05812631951024135, \"val\": 0.1109451471025952}, {\"index\": 64, \"train\": 0.058992754047115646, \"val\": 0.1051758014872946}, {\"index\": 65, \"train\": 0.061759828279415764, \"val\": 0.09469945392079858}, {\"index\": 66, \"train\": 0.06321412324905396, \"val\": 0.0914727386800762}, {\"index\": 67, \"train\": 0.06358919944614172, \"val\": 0.08799798831093573}, {\"index\": 68, \"train\": 0.06202078269173702, \"val\": 0.09243717009193023}, {\"index\": 69, \"train\": 0.06478454886625211, \"val\": 0.09626341695009076}, {\"index\": 70, \"train\": 0.053690034275253616, \"val\": 0.08677948045746975}, {\"index\": 71, \"train\": 0.0546675402050217, \"val\": 0.10254514108722408}, {\"index\": 72, \"train\": 0.0520722217236956, \"val\": 0.09918619005820127}, {\"index\": 73, \"train\": 0.05076182012756666, \"val\": 0.0906478743763054}, {\"index\": 74, \"train\": 0.053406948844591774, \"val\": 0.086963093106816}, {\"index\": 75, \"train\": 0.05863438670833906, \"val\": 0.10960062848602387}, {\"index\": 76, \"train\": 0.05794536819060644, \"val\": 0.09108743338966398}, {\"index\": 77, \"train\": 0.05836109103014072, \"val\": 0.0861618842670901}, {\"index\": 78, \"train\": 0.05037525606652101, \"val\": 0.07311577683021486}, {\"index\": 79, \"train\": 0.05313890582571427, \"val\": 0.07591356526396272}, {\"index\": 80, \"train\": 0.05007305461913347, \"val\": 0.08705395144643262}, {\"index\": 81, \"train\": 0.048443328589200974, \"val\": 0.09786297489621643}, {\"index\": 82, \"train\": 0.05114145763218403, \"val\": 0.06917968639711573}, {\"index\": 83, \"train\": 0.05098116584122181, \"val\": 0.08326197414641717}, {\"index\": 84, \"train\": 0.048780642449855804, \"val\": 0.08313706106424366}, {\"index\": 85, \"train\": 0.055593815011282764, \"val\": 0.10761991699885888}, {\"index\": 86, \"train\": 0.04883896249035994, \"val\": 0.06668241434596928}, {\"index\": 87, \"train\": 0.05278282115856806, \"val\": 0.07873333594115037}, {\"index\": 88, \"train\": 0.05063147625575463, \"val\": 0.07259384366571559}, {\"index\": 89, \"train\": 0.048834023686746754, \"val\": 0.07895870827897279}, {\"index\": 90, \"train\": 0.0496693504974246, \"val\": 0.0726907907150841}, {\"index\": 91, \"train\": 0.048615782211224236, \"val\": 0.0805334604748404}, {\"index\": 92, \"train\": 0.04602847248315811, \"val\": 0.06672681135974859}, {\"index\": 93, \"train\": 0.044859420818587147, \"val\": 0.06716488907544839}, {\"index\": 94, \"train\": 0.04317673575133085, \"val\": 0.06980235101116165}, {\"index\": 95, \"train\": 0.04809851354608933, \"val\": 0.06982199931754386}, {\"index\": 96, \"train\": 0.05085937399417162, \"val\": 0.06709822478867998}, {\"index\": 97, \"train\": 0.04865388541171948, \"val\": 0.07790765987247815}, {\"index\": 98, \"train\": 0.05538311849037806, \"val\": 0.09073523101202834}, {\"index\": 99, \"train\": 0.04725913299868504, \"val\": 0.07143080860173709}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_5\n",
      "\n",
      "testing on design_6\n",
      "\n",
      "testing on design_7\n",
      "\n",
      "testing on design_8\n",
      "\n",
      "testing on design_9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>maxAE</th>\n",
       "      <th>mae/peak</th>\n",
       "      <th>maxAE/peak</th>\n",
       "      <th>relEAtPeak</th>\n",
       "      <th>Trained on</th>\n",
       "      <th>Tested on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.118980</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.109546</td>\n",
       "      <td>0.344441</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.088504</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.105254</td>\n",
       "      <td>0.047008</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.098149</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.046424</td>\n",
       "      <td>0.164063</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.080626</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.037601</td>\n",
       "      <td>0.104297</td>\n",
       "      <td>0.104297</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.156502</td>\n",
       "      <td>0.397689</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>test group</td>\n",
       "      <td>design_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>0.191664</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>all groups</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.099831</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>0.041346</td>\n",
       "      <td>0.111605</td>\n",
       "      <td>0.064973</td>\n",
       "      <td>all groups</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.192846</td>\n",
       "      <td>0.015018</td>\n",
       "      <td>0.138644</td>\n",
       "      <td>0.454345</td>\n",
       "      <td>0.085863</td>\n",
       "      <td>all groups</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.051679</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.029405</td>\n",
       "      <td>0.090646</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>all groups</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.016002</td>\n",
       "      <td>0.108049</td>\n",
       "      <td>0.261501</td>\n",
       "      <td>0.221651</td>\n",
       "      <td>all groups</td>\n",
       "      <td>design_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           mse       mae       mre     maxAE  mae/peak  maxAE/peak  \\\n",
       "0     0.000005  0.001795  0.118980  0.005643  0.109546    0.344441   \n",
       "1     0.000004  0.001591  0.088504  0.003511  0.047683    0.105254   \n",
       "2     0.000014  0.002641  0.098149  0.009333  0.046424    0.164063   \n",
       "3     0.000021  0.003645  0.080626  0.010110  0.037601    0.104297   \n",
       "4     0.000007  0.002221  0.145833  0.005644  0.156502    0.397689   \n",
       "...        ...       ...       ...       ...       ...         ...   \n",
       "1795  0.000009  0.002165  0.082089  0.007113  0.058327    0.191664   \n",
       "1796  0.000071  0.006632  0.099831  0.017901  0.041346    0.111605   \n",
       "1797  0.000038  0.004583  0.192846  0.015018  0.138644    0.454345   \n",
       "1798  0.000005  0.001618  0.051679  0.004988  0.029405    0.090646   \n",
       "1799  0.000068  0.006612  0.152818  0.016002  0.108049    0.261501   \n",
       "\n",
       "      relEAtPeak  Trained on Tested on  \n",
       "0       0.038204  test group  design_5  \n",
       "1       0.047008  test group  design_5  \n",
       "2       0.140409  test group  design_5  \n",
       "3       0.104297  test group  design_5  \n",
       "4       0.015627  test group  design_5  \n",
       "...          ...         ...       ...  \n",
       "1795    0.116318  all groups  design_9  \n",
       "1796    0.064973  all groups  design_9  \n",
       "1797    0.085863  all groups  design_9  \n",
       "1798    0.015784  all groups  design_9  \n",
       "1799    0.221651  all groups  design_9  \n",
       "\n",
       "[1800 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrainData, allValData = [], []\n",
    "print('training on all groups')\n",
    "for name, data in trainSets.items():\n",
    "    allTrainData = allTrainData + data\n",
    "    allValData = allValData + valSets[name]\n",
    "\n",
    "gcn = FeaStNet()\n",
    "history = gcn.trainModel(allTrainData, allValData, epochs=epochs, batch_size=256, flatten=True, logTrans=False, \n",
    "                         ssTrans=True, saveDir=saveDir+'allGroups')\n",
    "\n",
    "display(plotHistory(history))\n",
    "\n",
    "# test\n",
    "for testName, testSet in testSets.items():\n",
    "    print('testing on '+testName+'\\n')\n",
    "    resultsDict = gcn.testModel(testSet, level='field')\n",
    "    resultsDict['Trained on'] = ['all groups']*len(resultsDict['mse'])\n",
    "    resultsDict['Tested on'] = [testName]*len(resultsDict['mse'])\n",
    "    results = pivotDict(resultsDict)\n",
    "    resultsList.extend(results)\n",
    "    \n",
    "pd.DataFrame(resultsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on all but design_5\n",
      "epoch: 0   trainLoss: 9.6930e-01   valLoss:1.0775e+00  time: 4.29e+00\n",
      "epoch: 1   trainLoss: 7.8108e-01   valLoss:1.0078e+00  time: 4.15e+00\n",
      "epoch: 2   trainLoss: 6.6328e-01   valLoss:6.9700e-01  time: 4.36e+00\n",
      "epoch: 3   trainLoss: 5.3653e-01   valLoss:5.3344e-01  time: 4.40e+00\n",
      "epoch: 4   trainLoss: 4.5843e-01   valLoss:4.5802e-01  time: 4.40e+00\n",
      "epoch: 5   trainLoss: 4.0087e-01   valLoss:4.1856e-01  time: 4.28e+00\n",
      "epoch: 6   trainLoss: 3.6536e-01   valLoss:3.8926e-01  time: 4.54e+00\n",
      "epoch: 7   trainLoss: 3.3237e-01   valLoss:3.8573e-01  time: 4.32e+00\n",
      "epoch: 8   trainLoss: 3.0631e-01   valLoss:4.0324e-01  time: 4.41e+00\n",
      "epoch: 9   trainLoss: 2.8531e-01   valLoss:3.0220e-01  time: 4.42e+00\n",
      "epoch: 10   trainLoss: 2.5992e-01   valLoss:2.8157e-01  time: 4.37e+00\n",
      "epoch: 11   trainLoss: 2.4162e-01   valLoss:2.8961e-01  time: 4.49e+00\n",
      "epoch: 12   trainLoss: 2.3087e-01   valLoss:2.8926e-01  time: 4.56e+00\n",
      "epoch: 13   trainLoss: 2.1378e-01   valLoss:2.8547e-01  time: 4.50e+00\n",
      "epoch: 14   trainLoss: 1.9667e-01   valLoss:2.6620e-01  time: 4.34e+00\n",
      "epoch: 15   trainLoss: 1.8765e-01   valLoss:2.3478e-01  time: 4.40e+00\n",
      "epoch: 16   trainLoss: 1.7474e-01   valLoss:2.1806e-01  time: 4.22e+00\n",
      "epoch: 17   trainLoss: 1.6814e-01   valLoss:2.0128e-01  time: 4.13e+00\n",
      "epoch: 18   trainLoss: 1.6639e-01   valLoss:2.0252e-01  time: 4.27e+00\n",
      "epoch: 19   trainLoss: 1.5156e-01   valLoss:2.4444e-01  time: 4.55e+00\n",
      "epoch: 20   trainLoss: 1.4032e-01   valLoss:1.9730e-01  time: 4.37e+00\n",
      "epoch: 21   trainLoss: 1.3757e-01   valLoss:2.2154e-01  time: 4.27e+00\n",
      "epoch: 22   trainLoss: 1.3602e-01   valLoss:2.1951e-01  time: 4.31e+00\n",
      "epoch: 23   trainLoss: 1.3203e-01   valLoss:2.7912e-01  time: 4.26e+00\n",
      "epoch: 24   trainLoss: 1.2289e-01   valLoss:1.9018e-01  time: 4.41e+00\n",
      "epoch: 25   trainLoss: 1.1529e-01   valLoss:1.6414e-01  time: 4.11e+00\n",
      "epoch: 26   trainLoss: 1.1896e-01   valLoss:3.0285e-01  time: 4.38e+00\n",
      "epoch: 27   trainLoss: 1.1401e-01   valLoss:2.3660e-01  time: 4.46e+00\n",
      "epoch: 28   trainLoss: 1.1193e-01   valLoss:1.7787e-01  time: 4.36e+00\n",
      "epoch: 29   trainLoss: 1.0677e-01   valLoss:1.6894e-01  time: 4.36e+00\n",
      "epoch: 30   trainLoss: 1.0286e-01   valLoss:1.3789e-01  time: 4.12e+00\n",
      "epoch: 31   trainLoss: 1.0433e-01   valLoss:2.3923e-01  time: 4.39e+00\n",
      "epoch: 32   trainLoss: 9.7156e-02   valLoss:1.3744e-01  time: 4.09e+00\n",
      "epoch: 33   trainLoss: 9.4763e-02   valLoss:1.6191e-01  time: 4.30e+00\n",
      "epoch: 34   trainLoss: 9.3857e-02   valLoss:1.3507e-01  time: 4.36e+00\n",
      "epoch: 35   trainLoss: 8.6033e-02   valLoss:1.7078e-01  time: 4.40e+00\n",
      "epoch: 36   trainLoss: 8.7718e-02   valLoss:2.0974e-01  time: 4.10e+00\n",
      "epoch: 37   trainLoss: 8.3865e-02   valLoss:1.4654e-01  time: 4.39e+00\n",
      "epoch: 38   trainLoss: 8.0989e-02   valLoss:2.0644e-01  time: 4.39e+00\n",
      "epoch: 39   trainLoss: 8.3823e-02   valLoss:2.1096e-01  time: 4.39e+00\n",
      "epoch: 40   trainLoss: 8.0940e-02   valLoss:1.9547e-01  time: 4.21e+00\n",
      "epoch: 41   trainLoss: 7.6458e-02   valLoss:1.7885e-01  time: 4.32e+00\n",
      "epoch: 42   trainLoss: 8.0990e-02   valLoss:1.6780e-01  time: 4.22e+00\n",
      "epoch: 43   trainLoss: 8.7295e-02   valLoss:9.8982e-02  time: 4.39e+00\n",
      "epoch: 44   trainLoss: 7.9353e-02   valLoss:1.0590e-01  time: 4.31e+00\n",
      "epoch: 45   trainLoss: 7.3915e-02   valLoss:1.4871e-01  time: 4.02e+00\n",
      "epoch: 46   trainLoss: 7.0193e-02   valLoss:1.2284e-01  time: 4.33e+00\n",
      "epoch: 47   trainLoss: 7.0975e-02   valLoss:1.9408e-01  time: 4.48e+00\n",
      "epoch: 48   trainLoss: 6.7587e-02   valLoss:1.0421e-01  time: 4.47e+00\n",
      "epoch: 49   trainLoss: 7.0848e-02   valLoss:1.1306e-01  time: 4.32e+00\n",
      "epoch: 50   trainLoss: 7.1928e-02   valLoss:9.8760e-02  time: 4.53e+00\n",
      "epoch: 51   trainLoss: 6.9953e-02   valLoss:1.0322e-01  time: 4.28e+00\n",
      "epoch: 52   trainLoss: 7.4855e-02   valLoss:1.6723e-01  time: 4.32e+00\n",
      "epoch: 53   trainLoss: 7.5976e-02   valLoss:1.2143e-01  time: 4.22e+00\n",
      "epoch: 54   trainLoss: 7.1623e-02   valLoss:9.5647e-02  time: 4.58e+00\n",
      "epoch: 55   trainLoss: 7.0000e-02   valLoss:1.9615e-01  time: 4.20e+00\n",
      "epoch: 56   trainLoss: 6.6872e-02   valLoss:1.2384e-01  time: 4.52e+00\n",
      "epoch: 57   trainLoss: 5.9595e-02   valLoss:1.3900e-01  time: 4.11e+00\n",
      "epoch: 58   trainLoss: 5.9282e-02   valLoss:1.4085e-01  time: 4.38e+00\n",
      "epoch: 59   trainLoss: 6.2727e-02   valLoss:1.0908e-01  time: 4.36e+00\n",
      "epoch: 60   trainLoss: 6.4818e-02   valLoss:1.0355e-01  time: 4.30e+00\n",
      "epoch: 61   trainLoss: 6.8561e-02   valLoss:1.2216e-01  time: 4.11e+00\n",
      "epoch: 62   trainLoss: 6.8385e-02   valLoss:9.5927e-02  time: 4.20e+00\n",
      "epoch: 63   trainLoss: 5.9585e-02   valLoss:1.6458e-01  time: 4.19e+00\n",
      "epoch: 64   trainLoss: 5.8592e-02   valLoss:9.8260e-02  time: 4.37e+00\n",
      "epoch: 65   trainLoss: 5.6011e-02   valLoss:9.1944e-02  time: 4.53e+00\n",
      "epoch: 66   trainLoss: 5.9696e-02   valLoss:7.6785e-02  time: 4.46e+00\n",
      "epoch: 67   trainLoss: 6.0231e-02   valLoss:1.7654e-01  time: 4.30e+00\n",
      "epoch: 68   trainLoss: 5.5705e-02   valLoss:1.5418e-01  time: 4.61e+00\n",
      "epoch: 69   trainLoss: 5.1383e-02   valLoss:1.0985e-01  time: 4.24e+00\n",
      "epoch: 70   trainLoss: 5.3327e-02   valLoss:1.1300e-01  time: 4.25e+00\n",
      "epoch: 71   trainLoss: 5.0656e-02   valLoss:1.2085e-01  time: 4.14e+00\n",
      "epoch: 72   trainLoss: 5.1980e-02   valLoss:7.3110e-02  time: 4.38e+00\n",
      "epoch: 73   trainLoss: 6.1616e-02   valLoss:1.0801e-01  time: 4.25e+00\n",
      "epoch: 74   trainLoss: 5.9251e-02   valLoss:8.3862e-02  time: 4.39e+00\n",
      "epoch: 75   trainLoss: 5.0323e-02   valLoss:1.5151e-01  time: 4.10e+00\n",
      "epoch: 76   trainLoss: 5.6117e-02   valLoss:8.2062e-02  time: 4.49e+00\n",
      "epoch: 77   trainLoss: 5.2438e-02   valLoss:8.4172e-02  time: 4.19e+00\n",
      "epoch: 78   trainLoss: 4.9829e-02   valLoss:9.9795e-02  time: 4.34e+00\n",
      "epoch: 79   trainLoss: 5.0920e-02   valLoss:9.0996e-02  time: 4.11e+00\n",
      "epoch: 80   trainLoss: 4.8677e-02   valLoss:7.3864e-02  time: 4.57e+00\n",
      "epoch: 81   trainLoss: 5.1864e-02   valLoss:7.6787e-02  time: 4.27e+00\n",
      "epoch: 82   trainLoss: 4.5821e-02   valLoss:1.0484e-01  time: 4.41e+00\n",
      "epoch: 83   trainLoss: 5.2804e-02   valLoss:1.1291e-01  time: 4.51e+00\n",
      "epoch: 84   trainLoss: 4.5252e-02   valLoss:7.9193e-02  time: 4.21e+00\n",
      "epoch: 85   trainLoss: 4.8972e-02   valLoss:6.5270e-02  time: 4.33e+00\n",
      "epoch: 86   trainLoss: 4.7388e-02   valLoss:9.7502e-02  time: 4.40e+00\n",
      "epoch: 87   trainLoss: 5.1751e-02   valLoss:1.0122e-01  time: 4.14e+00\n",
      "epoch: 88   trainLoss: 4.9656e-02   valLoss:9.8865e-02  time: 4.17e+00\n",
      "epoch: 89   trainLoss: 5.1873e-02   valLoss:1.3077e-01  time: 4.46e+00\n",
      "epoch: 90   trainLoss: 5.5643e-02   valLoss:8.6699e-02  time: 4.48e+00\n",
      "epoch: 91   trainLoss: 5.0799e-02   valLoss:7.2482e-02  time: 4.37e+00\n",
      "epoch: 92   trainLoss: 5.2064e-02   valLoss:1.0596e-01  time: 4.24e+00\n",
      "epoch: 93   trainLoss: 4.5640e-02   valLoss:7.5593e-02  time: 4.41e+00\n",
      "epoch: 94   trainLoss: 4.0630e-02   valLoss:9.9026e-02  time: 4.05e+00\n",
      "epoch: 95   trainLoss: 4.3489e-02   valLoss:7.8047e-02  time: 4.25e+00\n",
      "epoch: 96   trainLoss: 4.1125e-02   valLoss:7.7466e-02  time: 4.26e+00\n",
      "epoch: 97   trainLoss: 4.1817e-02   valLoss:1.1700e-01  time: 4.10e+00\n",
      "epoch: 98   trainLoss: 4.3383e-02   valLoss:6.7182e-02  time: 4.18e+00\n",
      "epoch: 99   trainLoss: 4.0740e-02   valLoss:1.2767e-01  time: 4.09e+00\n",
      "loading checkpoint 85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-921138e4966c4d169190067bfc0ca438\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-921138e4966c4d169190067bfc0ca438\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-921138e4966c4d169190067bfc0ca438\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-34584b325a7af0355e0d69b4c88b1116\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-34584b325a7af0355e0d69b4c88b1116\": [{\"index\": 0, \"train\": 0.9693046331405639, \"val\": 1.0775158662279998}, {\"index\": 1, \"train\": 0.7810843229293823, \"val\": 1.0077715424596574}, {\"index\": 2, \"train\": 0.6632802724838257, \"val\": 0.6969970712929757}, {\"index\": 3, \"train\": 0.5365300416946411, \"val\": 0.5334445001402249}, {\"index\": 4, \"train\": 0.45842825472354887, \"val\": 0.45801627032751974}, {\"index\": 5, \"train\": 0.40087081789970397, \"val\": 0.4185631985310465}, {\"index\": 6, \"train\": 0.36536018550395966, \"val\": 0.3892632572894433}, {\"index\": 7, \"train\": 0.3323693826794624, \"val\": 0.3857317693721227}, {\"index\": 8, \"train\": 0.3063072547316551, \"val\": 0.40323598552883294}, {\"index\": 9, \"train\": 0.2853063777089119, \"val\": 0.3022005957684963}, {\"index\": 10, \"train\": 0.2599198594689369, \"val\": 0.2815678593771601}, {\"index\": 11, \"train\": 0.24162048250436782, \"val\": 0.28960669113331716}, {\"index\": 12, \"train\": 0.23087040036916734, \"val\": 0.28925838842190355}, {\"index\": 13, \"train\": 0.2137764111161232, \"val\": 0.2854663805563986}, {\"index\": 14, \"train\": 0.19667397439479828, \"val\": 0.26619556373744097}, {\"index\": 15, \"train\": 0.1876460999250412, \"val\": 0.23478285221844325}, {\"index\": 16, \"train\": 0.17473964318633078, \"val\": 0.21805564144354625}, {\"index\": 17, \"train\": 0.16814340874552727, \"val\": 0.20128196369774673}, {\"index\": 18, \"train\": 0.16639137417078018, \"val\": 0.20252370498147448}, {\"index\": 19, \"train\": 0.1515640437602997, \"val\": 0.2444355431620756}, {\"index\": 20, \"train\": 0.1403222881257534, \"val\": 0.19729579998939126}, {\"index\": 21, \"train\": 0.13756919652223587, \"val\": 0.22154128374704332}, {\"index\": 22, \"train\": 0.13602113649249076, \"val\": 0.21951116411946714}, {\"index\": 23, \"train\": 0.13202981203794478, \"val\": 0.27912460135606426}, {\"index\": 24, \"train\": 0.12289492785930634, \"val\": 0.1901756245610563}, {\"index\": 25, \"train\": 0.11529159992933273, \"val\": 0.1641371287847438}, {\"index\": 26, \"train\": 0.11895680278539658, \"val\": 0.30285031803241175}, {\"index\": 27, \"train\": 0.11400813609361649, \"val\": 0.2366022899179909}, {\"index\": 28, \"train\": 0.11192606836557388, \"val\": 0.17786983321463964}, {\"index\": 29, \"train\": 0.10676617249846458, \"val\": 0.1689365306778604}, {\"index\": 30, \"train\": 0.10286398157477379, \"val\": 0.13788863621798722}, {\"index\": 31, \"train\": 0.10432631373405457, \"val\": 0.23922740824356745}, {\"index\": 32, \"train\": 0.09715600982308388, \"val\": 0.13743950007483363}, {\"index\": 33, \"train\": 0.09476331025362014, \"val\": 0.16190746955948676}, {\"index\": 34, \"train\": 0.09385724738240242, \"val\": 0.13507271207614663}, {\"index\": 35, \"train\": 0.08603278622031212, \"val\": 0.17078486829457148}, {\"index\": 36, \"train\": 0.08771779611706734, \"val\": 0.20974224172338532}, {\"index\": 37, \"train\": 0.08386512920260429, \"val\": 0.1465402599381007}, {\"index\": 38, \"train\": 0.08098919354379178, \"val\": 0.2064377166335126}, {\"index\": 39, \"train\": 0.08382289633154869, \"val\": 0.21095589732350264}, {\"index\": 40, \"train\": 0.08094036541879177, \"val\": 0.19546900476712767}, {\"index\": 41, \"train\": 0.07645817399024964, \"val\": 0.17884769745783327}, {\"index\": 42, \"train\": 0.08099029436707497, \"val\": 0.16779632180933496}, {\"index\": 43, \"train\": 0.08729465417563916, \"val\": 0.09898220869729778}, {\"index\": 44, \"train\": 0.07935285121202469, \"val\": 0.10589755770235099}, {\"index\": 45, \"train\": 0.07391483150422573, \"val\": 0.14871015266355783}, {\"index\": 46, \"train\": 0.0701932568103075, \"val\": 0.12284356202818109}, {\"index\": 47, \"train\": 0.07097486406564713, \"val\": 0.19408249804163696}, {\"index\": 48, \"train\": 0.06758653037250043, \"val\": 0.10421077012951072}, {\"index\": 49, \"train\": 0.07084847949445247, \"val\": 0.11305856355681533}, {\"index\": 50, \"train\": 0.07192830257117748, \"val\": 0.09876009285303385}, {\"index\": 51, \"train\": 0.06995272040367126, \"val\": 0.10321635910400397}, {\"index\": 52, \"train\": 0.07485496550798416, \"val\": 0.16722937794542372}, {\"index\": 53, \"train\": 0.0759756613522768, \"val\": 0.12143373504255174}, {\"index\": 54, \"train\": 0.07162263281643391, \"val\": 0.09564656215931151}, {\"index\": 55, \"train\": 0.06999990306794643, \"val\": 0.19615020590707125}, {\"index\": 56, \"train\": 0.06687151864171029, \"val\": 0.12384000869146634}, {\"index\": 57, \"train\": 0.05959505848586559, \"val\": 0.13900143533828668}, {\"index\": 58, \"train\": 0.05928219072520733, \"val\": 0.1408521579174299}, {\"index\": 59, \"train\": 0.062727340310812, \"val\": 0.10908389038943117}, {\"index\": 60, \"train\": 0.06481800600886345, \"val\": 0.10354735016072583}, {\"index\": 61, \"train\": 0.06856079958379269, \"val\": 0.1221580487003343}, {\"index\": 62, \"train\": 0.0683853380382061, \"val\": 0.09592712182051467}, {\"index\": 63, \"train\": 0.05958477035164833, \"val\": 0.16457798862797468}, {\"index\": 64, \"train\": 0.058592277392745015, \"val\": 0.09826005054411427}, {\"index\": 65, \"train\": 0.05601133406162262, \"val\": 0.09194375843960895}, {\"index\": 66, \"train\": 0.05969623774290085, \"val\": 0.07678495623070004}, {\"index\": 67, \"train\": 0.060230696201324464, \"val\": 0.17653800509180184}, {\"index\": 68, \"train\": 0.05570521540939808, \"val\": 0.15418393063440886}, {\"index\": 69, \"train\": 0.05138296596705914, \"val\": 0.10984897246170375}, {\"index\": 70, \"train\": 0.053326615691185, \"val\": 0.11299718508369685}, {\"index\": 71, \"train\": 0.05065639577805996, \"val\": 0.12084569674496176}, {\"index\": 72, \"train\": 0.051980025693774225, \"val\": 0.07310965720939243}, {\"index\": 73, \"train\": 0.06161593534052372, \"val\": 0.10800583487326987}, {\"index\": 74, \"train\": 0.05925113409757614, \"val\": 0.08386230479367508}, {\"index\": 75, \"train\": 0.0503228634595871, \"val\": 0.15150554356956422}, {\"index\": 76, \"train\": 0.05611732266843319, \"val\": 0.08206152623191613}, {\"index\": 77, \"train\": 0.052438008785247806, \"val\": 0.08417235365238783}, {\"index\": 78, \"train\": 0.0498292189091444, \"val\": 0.09979463961132785}, {\"index\": 79, \"train\": 0.050919640064239505, \"val\": 0.09099571926883611}, {\"index\": 80, \"train\": 0.04867701567709446, \"val\": 0.07386408750230826}, {\"index\": 81, \"train\": 0.05186397023499012, \"val\": 0.07678675573077742}, {\"index\": 82, \"train\": 0.04582063741981983, \"val\": 0.10483801324315006}, {\"index\": 83, \"train\": 0.05280385576188564, \"val\": 0.1129059297036966}, {\"index\": 84, \"train\": 0.045252369716763496, \"val\": 0.07919335968445348}, {\"index\": 85, \"train\": 0.04897228814661503, \"val\": 0.06526986875515259}, {\"index\": 86, \"train\": 0.04738777317106724, \"val\": 0.09750245038540689}, {\"index\": 87, \"train\": 0.05175071954727173, \"val\": 0.10122331760659883}, {\"index\": 88, \"train\": 0.049655796587467195, \"val\": 0.09886537274158197}, {\"index\": 89, \"train\": 0.05187278725206852, \"val\": 0.13076728209907068}, {\"index\": 90, \"train\": 0.05564264394342899, \"val\": 0.08669900689144722}, {\"index\": 91, \"train\": 0.05079935751855373, \"val\": 0.07248212607023392}, {\"index\": 92, \"train\": 0.05206392742693424, \"val\": 0.10595893829735427}, {\"index\": 93, \"train\": 0.04564025439321995, \"val\": 0.0755928037084501}, {\"index\": 94, \"train\": 0.04062968883663416, \"val\": 0.09902636763365956}, {\"index\": 95, \"train\": 0.04348868727684021, \"val\": 0.07804747288065739}, {\"index\": 96, \"train\": 0.041125408932566644, \"val\": 0.07746590785218696}, {\"index\": 97, \"train\": 0.041817137971520424, \"val\": 0.11700296001853766}, {\"index\": 98, \"train\": 0.043383185379207136, \"val\": 0.06718177854296907}, {\"index\": 99, \"train\": 0.040740443393588066, \"val\": 0.12766709756289385}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on design_5\n",
      "\n",
      "training on all but design_6\n",
      "epoch: 0   trainLoss: 9.0773e-01   valLoss:1.0125e+00  time: 4.60e+00\n",
      "epoch: 1   trainLoss: 7.5854e-01   valLoss:9.3181e-01  time: 4.58e+00\n",
      "epoch: 2   trainLoss: 6.4352e-01   valLoss:6.9308e-01  time: 4.46e+00\n",
      "epoch: 3   trainLoss: 5.5340e-01   valLoss:5.6763e-01  time: 4.24e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-75d558d7a59a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeaStNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     history = gcn.trainModel(allTrainData, allValData, epochs=epochs, batch_size=256, flatten=True, logTrans=False, \n\u001b[0;32m---> 12\u001b[0;31m                              ssTrans=True, saveDir=saveDir+'allBut_'+ trainName)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ewhalen/projects/gcnSurrogate/models/feastnetSurrogateModel.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(self, trainGraphs, valGraphs, epochs, saveDir, batch_size, flatten, logTrans, ssTrans, restartFile)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalLoader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mbatchHist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/ptgeom/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/ewhalen/projects/gcnSurrogate/models/feastnetSurrogateModel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkptFiles = {}\n",
    "for trainName, trainSet in trainSets.items():\n",
    "    allTrainData, allValData = [], []\n",
    "    print('training on all but '+ trainName)\n",
    "    for name, data in trainSets.items():\n",
    "        if name != trainName:\n",
    "            allTrainData = allTrainData + data\n",
    "            allValData = allValData + valSets[name]\n",
    "\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(allTrainData, allValData, epochs=epochs, batch_size=256, flatten=True, logTrans=False, \n",
    "                             ssTrans=True, saveDir=saveDir+'allBut_'+ trainName)\n",
    "\n",
    "    display(plotHistory(history))\n",
    "\n",
    "    # test\n",
    "    print('testing on '+trainName+'\\n')\n",
    "    resultsDict = gcn.testModel(testSets[trainName], level='field')\n",
    "    resultsDict['Trained on'] = ['all groups but test group']*len(resultsDict['mse'])\n",
    "    resultsDict['Tested on'] = [trainName]*len(resultsDict['mse'])\n",
    "    results = pivotDict(resultsDict)\n",
    "    resultsList.extend(results)\n",
    "    \n",
    "    # note checkpoint files for use in transfer learning\n",
    "    checkptFiles[trainName] = gcn.checkptFile\n",
    "    \n",
    "pd.DataFrame(resultsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allIds = list(range(len(trainSet)))\n",
    "_, trainIds = train_test_split(allIds, test_size=0.2, shuffle=True, random_state=1234) # train on only 25%\n",
    "\n",
    "for trainName, trainSet in trainSets.items():\n",
    "    # load pre-trained model and train on new data\n",
    "    print('transfer learning on '+trainName)\n",
    "    gcn = FeaStNet()\n",
    "    smallTrainSet = [trainSet[i] for i in trainIds]\n",
    "    history = gcn.trainModel(smallTrainSet, valSets[trainName], restartFile=checkptFiles[trainName], \n",
    "                             epochs=epochs, batch_size=256, saveDir=saveDir+'transferLearn_'+ trainName)\n",
    "\n",
    "    display(plotHistory(history))\n",
    "\n",
    "    # test\n",
    "    print('testing on '+trainName+'\\n')\n",
    "    resultsDict = gcn.testModel(testSets[trainName], level='field')\n",
    "    resultsDict['Trained on'] = ['transfer learning (20%)']*len(resultsDict['mse'])\n",
    "    resultsDict['Tested on'] = [trainName]*len(resultsDict['mse'])\n",
    "    results = pivotDict(resultsDict)\n",
    "    resultsList.extend(results)\n",
    "        \n",
    "pd.DataFrame(resultsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test results to file\n",
    "df = pd.DataFrame(resultsList)\n",
    "df.to_csv(saveDir+'testResults.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test results from file\n",
    "df = pd.read_csv(saveDir+'testResults.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['test group', 'all groups']\n",
    "barChart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('Trained on:N', sort=order, title='', axis=alt.Axis(ticks=False, labels=False)),\n",
    "    y=alt.Y('mean(mse):Q', scale=alt.Scale(type='log'), axis=alt.Axis(tickCount=8, format=\".0e\"), title='MSE'),\n",
    "    color=alt.Color('Trained on:N', sort=order),\n",
    "    opacity = alt.OpacityValue(0.8),\n",
    "    tooltip='mean(mse):Q'\n",
    ").properties(width=75, height=200)\n",
    "\n",
    "scatter = alt.Chart(df).mark_circle(size=20).encode(\n",
    "    x=alt.X('Trained on:N', title='', sort=order),\n",
    "    y=alt.Y('mse:Q', scale=alt.Scale(type='log')),\n",
    "    color=alt.Color('Trained on:N', sort=order),\n",
    "    opacity = alt.OpacityValue(0.3),\n",
    "    tooltip='mse:Q'\n",
    ")\n",
    "\n",
    "alt.layer(barChart, scatter, data=df).facet(\n",
    "    column=alt.Column('Tested on:N'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['test group', 'all groups']\n",
    "barChart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('Trained on:N', sort=order, title='', axis=alt.Axis(ticks=False, labels=False)),\n",
    "    y=alt.Y('mean(mse):Q', axis=alt.Axis(tickCount=8, format=\".0e\"), title='MSE'),\n",
    "    color=alt.Color('Trained on:N', sort=order),\n",
    "    opacity = alt.OpacityValue(0.8),\n",
    "    tooltip='mean(mse):Q'\n",
    ").properties(width=75, height=200)\n",
    "\n",
    "scatter = alt.Chart(df).mark_circle(size=20).encode(\n",
    "    x=alt.X('Trained on:N', title='', sort=order),\n",
    "    y=alt.Y('mse:Q'),\n",
    "    color=alt.Color('Trained on:N', sort=order),\n",
    "    opacity = alt.OpacityValue(0.3),\n",
    "    tooltip='mse:Q'\n",
    ")\n",
    "\n",
    "alt.layer(barChart, scatter, data=df).facet(\n",
    "    column=alt.Column('Tested on:N'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['test group', 'all groups']\n",
    "barChart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('Trained on:N', sort=order, title='', axis=alt.Axis(ticks=False, labels=False)),\n",
    "    y=alt.Y('mean(mse):Q', axis=alt.Axis(tickCount=8, format=\".0e\"), title='MSE'),\n",
    "    color=alt.Color('Trained on:N', sort=order),\n",
    "    opacity = alt.OpacityValue(0.8),\n",
    "    tooltip='mean(mse):Q'\n",
    ").properties(width=75, height=200)\n",
    "\n",
    "alt.layer(barChart, data=df).facet(\n",
    "    column=alt.Column('Tested on:N'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['test group', 'all groups']\n",
    "barChart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('Trained on:N', sort=order, title='', axis=alt.Axis(ticks=False, labels=False)),\n",
    "    y=alt.Y('mean(mse):Q', axis=alt.Axis(tickCount=8, format=\".0e\"), title='MSE'),\n",
    "    color=alt.Color('Trained on:N', sort=order, legend=alt.Legend(orient='bottom')),\n",
    "    opacity = alt.OpacityValue(0.8),\n",
    "    tooltip='mean(mse):Q'\n",
    ").properties(width=75, height=200)\n",
    "\n",
    "alt.layer(barChart, data=df).facet(\n",
    "    column=alt.Column('Tested on:N'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
