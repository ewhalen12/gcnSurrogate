{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "sys.path.append('./models')\n",
    "from feastnetSurrogateModel import FeaStNet\n",
    "from pointRegressorSurrogateModel import PointRegressor\n",
    "\n",
    "sys.path.append('./readers')\n",
    "from loadGhGraphs import loadGhGraphs\n",
    "\n",
    "sys.path.append('./visualization')\n",
    "from altTrussViz import plotTruss, interactiveErrorPlot\n",
    "\n",
    "sys.path.append('./util')\n",
    "from gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.574372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.021285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.035028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.072772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.701946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.141483\n",
       "std       1.574372\n",
       "min       0.006253\n",
       "25%       0.021285\n",
       "50%       0.035028\n",
       "75%       0.072772\n",
       "max      48.701946"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "testFile = os.path.join(dataDir, 'design_9_N_1000.csv')\n",
    "allGraphsUnfiltered = loadGhGraphs(testFile, NUM_DV=5)\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.042389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.020052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.031303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.055108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.140049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.042389\n",
       "std      0.030990\n",
       "min      0.006253\n",
       "25%      0.020052\n",
       "50%      0.031303\n",
       "75%      0.055108\n",
       "max      0.140049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max().item()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_7\n",
      "loading design_6\n",
      "loading design_8\n",
      "loading design_5\n",
      "loaded 3600 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainFiles = glob.glob(os.path.join(dataDir, '*1000.csv'))\n",
    "pretrainFiles.remove(testFile)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainFile in pretrainFiles:\n",
    "    designName = pretrainFile.split('/')[-1].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    graphsUnfiltered = loadGhGraphs(pretrainFile, NUM_DV=5)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 7.2725e-01   valLoss:1.4253e+00  time: 7.37e+00\n",
      "epoch: 1   trainLoss: 3.5379e-01   valLoss:6.7384e+00  time: 7.13e+00\n",
      "epoch: 2   trainLoss: 2.4041e-01   valLoss:6.1685e-01  time: 7.00e+00\n",
      "epoch: 3   trainLoss: 1.8522e-01   valLoss:4.6739e-01  time: 7.06e+00\n",
      "epoch: 4   trainLoss: 1.4396e-01   valLoss:3.6726e-01  time: 7.04e+00\n",
      "epoch: 5   trainLoss: 1.2312e-01   valLoss:3.2846e-01  time: 7.17e+00\n",
      "epoch: 6   trainLoss: 1.1689e-01   valLoss:4.5505e-01  time: 6.97e+00\n",
      "epoch: 7   trainLoss: 1.0619e-01   valLoss:2.7603e-01  time: 6.87e+00\n",
      "epoch: 8   trainLoss: 1.0080e-01   valLoss:2.8771e-01  time: 6.91e+00\n",
      "epoch: 9   trainLoss: 9.5196e-02   valLoss:4.2475e-01  time: 6.94e+00\n",
      "epoch: 10   trainLoss: 9.9184e-02   valLoss:2.6734e-01  time: 6.96e+00\n",
      "epoch: 11   trainLoss: 1.0495e-01   valLoss:1.6180e-01  time: 6.94e+00\n",
      "epoch: 12   trainLoss: 1.1795e-01   valLoss:2.8249e-01  time: 7.04e+00\n",
      "epoch: 13   trainLoss: 9.9028e-02   valLoss:1.3849e-01  time: 6.95e+00\n",
      "epoch: 14   trainLoss: 8.8296e-02   valLoss:1.2698e-01  time: 6.90e+00\n",
      "epoch: 15   trainLoss: 7.8106e-02   valLoss:1.2135e-01  time: 6.93e+00\n",
      "epoch: 16   trainLoss: 7.0544e-02   valLoss:1.4664e-01  time: 6.93e+00\n",
      "epoch: 17   trainLoss: 6.1789e-02   valLoss:7.2890e-02  time: 7.10e+00\n",
      "epoch: 18   trainLoss: 6.6516e-02   valLoss:1.5836e-01  time: 7.13e+00\n",
      "epoch: 19   trainLoss: 6.5063e-02   valLoss:1.2744e-01  time: 7.20e+00\n",
      "epoch: 20   trainLoss: 6.1741e-02   valLoss:1.2491e-01  time: 7.08e+00\n",
      "epoch: 21   trainLoss: 7.1749e-02   valLoss:1.4698e-01  time: 7.08e+00\n",
      "epoch: 22   trainLoss: 8.2515e-02   valLoss:1.9930e-01  time: 7.03e+00\n",
      "epoch: 23   trainLoss: 7.7904e-02   valLoss:1.2593e-01  time: 7.06e+00\n",
      "epoch: 24   trainLoss: 7.0076e-02   valLoss:1.0159e-01  time: 7.07e+00\n",
      "epoch: 25   trainLoss: 5.3604e-02   valLoss:1.9515e-01  time: 7.07e+00\n",
      "epoch: 26   trainLoss: 6.2608e-02   valLoss:1.0294e-01  time: 7.10e+00\n",
      "epoch: 27   trainLoss: 6.3133e-02   valLoss:1.6777e-01  time: 7.14e+00\n",
      "epoch: 28   trainLoss: 6.6503e-02   valLoss:1.3036e-01  time: 7.08e+00\n",
      "epoch: 29   trainLoss: 6.5382e-02   valLoss:1.4564e-01  time: 7.10e+00\n",
      "epoch: 30   trainLoss: 6.6401e-02   valLoss:1.1109e-01  time: 7.22e+00\n",
      "epoch: 31   trainLoss: 6.5727e-02   valLoss:8.1313e-02  time: 7.11e+00\n",
      "epoch: 32   trainLoss: 5.7721e-02   valLoss:9.5341e-02  time: 7.21e+00\n",
      "epoch: 33   trainLoss: 6.0990e-02   valLoss:6.3300e-02  time: 7.01e+00\n",
      "epoch: 34   trainLoss: 4.9068e-02   valLoss:1.4466e-01  time: 7.06e+00\n",
      "epoch: 35   trainLoss: 7.5746e-02   valLoss:1.1705e-01  time: 7.04e+00\n",
      "epoch: 36   trainLoss: 6.1153e-02   valLoss:8.7947e-02  time: 6.98e+00\n",
      "epoch: 37   trainLoss: 5.3671e-02   valLoss:1.0604e-01  time: 7.02e+00\n",
      "epoch: 38   trainLoss: 4.6553e-02   valLoss:6.7320e-02  time: 6.99e+00\n",
      "epoch: 39   trainLoss: 5.3844e-02   valLoss:6.2494e-02  time: 7.03e+00\n",
      "epoch: 40   trainLoss: 4.2991e-02   valLoss:8.7891e-02  time: 7.10e+00\n",
      "epoch: 41   trainLoss: 4.8903e-02   valLoss:7.0811e-02  time: 7.05e+00\n",
      "epoch: 42   trainLoss: 5.7116e-02   valLoss:4.5320e-02  time: 7.08e+00\n",
      "epoch: 43   trainLoss: 5.9665e-02   valLoss:1.4134e-01  time: 6.96e+00\n",
      "epoch: 44   trainLoss: 6.3082e-02   valLoss:9.6401e-02  time: 7.08e+00\n",
      "epoch: 45   trainLoss: 6.0291e-02   valLoss:1.8387e-01  time: 6.96e+00\n",
      "epoch: 46   trainLoss: 5.5100e-02   valLoss:6.7989e-02  time: 7.04e+00\n",
      "epoch: 47   trainLoss: 7.0870e-02   valLoss:1.5935e-01  time: 7.10e+00\n",
      "epoch: 48   trainLoss: 6.3768e-02   valLoss:1.3516e-01  time: 7.07e+00\n",
      "epoch: 49   trainLoss: 6.4311e-02   valLoss:3.8423e-01  time: 7.06e+00\n",
      "epoch: 50   trainLoss: 6.0807e-02   valLoss:6.1698e-02  time: 6.95e+00\n",
      "epoch: 51   trainLoss: 4.3948e-02   valLoss:1.0505e-01  time: 6.93e+00\n",
      "epoch: 52   trainLoss: 5.4955e-02   valLoss:1.1194e-01  time: 6.89e+00\n",
      "epoch: 53   trainLoss: 4.3546e-02   valLoss:4.4684e-02  time: 6.93e+00\n",
      "epoch: 54   trainLoss: 3.7503e-02   valLoss:4.6884e-02  time: 7.22e+00\n",
      "epoch: 55   trainLoss: 4.1276e-02   valLoss:4.0469e-02  time: 7.06e+00\n",
      "epoch: 56   trainLoss: 3.7839e-02   valLoss:4.7132e-02  time: 7.18e+00\n",
      "epoch: 57   trainLoss: 4.6015e-02   valLoss:2.0082e-01  time: 7.16e+00\n",
      "epoch: 58   trainLoss: 6.1729e-02   valLoss:1.5693e-01  time: 7.14e+00\n",
      "epoch: 59   trainLoss: 5.1405e-02   valLoss:1.2151e-01  time: 7.19e+00\n",
      "epoch: 60   trainLoss: 4.3503e-02   valLoss:6.5568e-02  time: 7.14e+00\n",
      "epoch: 61   trainLoss: 5.2954e-02   valLoss:6.9620e-02  time: 7.18e+00\n",
      "epoch: 62   trainLoss: 5.0430e-02   valLoss:9.0034e-02  time: 7.12e+00\n",
      "epoch: 63   trainLoss: 5.0674e-02   valLoss:1.1513e-01  time: 7.15e+00\n",
      "epoch: 64   trainLoss: 4.5477e-02   valLoss:9.1098e-02  time: 7.14e+00\n",
      "epoch: 65   trainLoss: 5.2249e-02   valLoss:9.1418e-02  time: 7.02e+00\n",
      "epoch: 66   trainLoss: 4.6539e-02   valLoss:1.6335e-01  time: 7.04e+00\n",
      "epoch: 67   trainLoss: 4.6621e-02   valLoss:1.0364e-01  time: 7.02e+00\n",
      "epoch: 68   trainLoss: 5.3920e-02   valLoss:5.8225e-02  time: 7.09e+00\n",
      "epoch: 69   trainLoss: 5.1277e-02   valLoss:7.2042e-02  time: 7.07e+00\n",
      "epoch: 70   trainLoss: 4.7244e-02   valLoss:7.1639e-02  time: 7.07e+00\n",
      "epoch: 71   trainLoss: 4.9129e-02   valLoss:1.0499e-01  time: 6.99e+00\n",
      "epoch: 72   trainLoss: 4.5624e-02   valLoss:5.7415e-02  time: 7.03e+00\n",
      "epoch: 73   trainLoss: 6.0224e-02   valLoss:9.7484e-02  time: 7.09e+00\n",
      "epoch: 74   trainLoss: 4.4463e-02   valLoss:8.7243e-02  time: 7.06e+00\n",
      "epoch: 75   trainLoss: 4.7614e-02   valLoss:5.3050e-02  time: 7.09e+00\n",
      "epoch: 76   trainLoss: 4.2917e-02   valLoss:6.4814e-02  time: 7.07e+00\n",
      "epoch: 77   trainLoss: 4.5099e-02   valLoss:6.4673e-02  time: 7.04e+00\n",
      "epoch: 78   trainLoss: 5.1336e-02   valLoss:7.1902e-02  time: 7.05e+00\n",
      "epoch: 79   trainLoss: 5.0256e-02   valLoss:8.7084e-02  time: 7.02e+00\n",
      "epoch: 80   trainLoss: 4.8739e-02   valLoss:5.5394e-02  time: 7.07e+00\n",
      "epoch: 81   trainLoss: 4.2922e-02   valLoss:7.2848e-02  time: 7.07e+00\n",
      "epoch: 82   trainLoss: 3.9652e-02   valLoss:9.9727e-02  time: 7.11e+00\n",
      "epoch: 83   trainLoss: 4.5973e-02   valLoss:7.7561e-02  time: 7.14e+00\n",
      "epoch: 84   trainLoss: 4.3144e-02   valLoss:7.2836e-02  time: 7.09e+00\n",
      "epoch: 85   trainLoss: 5.7801e-02   valLoss:6.8403e-02  time: 7.02e+00\n",
      "epoch: 86   trainLoss: 4.9051e-02   valLoss:6.9428e-02  time: 7.06e+00\n",
      "epoch: 87   trainLoss: 4.5111e-02   valLoss:9.3935e-02  time: 7.07e+00\n",
      "epoch: 88   trainLoss: 4.0970e-02   valLoss:7.5578e-02  time: 7.09e+00\n",
      "epoch: 89   trainLoss: 4.0647e-02   valLoss:4.6648e-02  time: 7.08e+00\n",
      "epoch: 90   trainLoss: 4.5648e-02   valLoss:8.5210e-02  time: 7.07e+00\n",
      "epoch: 91   trainLoss: 4.7523e-02   valLoss:1.4863e-01  time: 7.11e+00\n",
      "epoch: 92   trainLoss: 4.6749e-02   valLoss:7.9895e-02  time: 7.02e+00\n",
      "epoch: 93   trainLoss: 4.5763e-02   valLoss:1.7586e-01  time: 7.00e+00\n",
      "epoch: 94   trainLoss: 4.2770e-02   valLoss:6.7487e-02  time: 7.01e+00\n",
      "epoch: 95   trainLoss: 4.3428e-02   valLoss:4.6985e-02  time: 6.97e+00\n",
      "epoch: 96   trainLoss: 3.5771e-02   valLoss:5.0332e-02  time: 7.00e+00\n",
      "epoch: 97   trainLoss: 3.7103e-02   valLoss:5.7780e-02  time: 6.98e+00\n",
      "epoch: 98   trainLoss: 4.9687e-02   valLoss:1.3084e-01  time: 7.01e+00\n",
      "epoch: 99   trainLoss: 4.1451e-02   valLoss:1.0465e-01  time: 6.98e+00\n",
      "loading checkpoint 55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-92219323794f4f41b47680df4a512ce5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-92219323794f4f41b47680df4a512ce5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-92219323794f4f41b47680df4a512ce5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-f696ceb40f1c4fb18dfb3c037b150b00\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-f696ceb40f1c4fb18dfb3c037b150b00\": [{\"train\": 0.7272462397813797, \"val\": 1.4253151725011843, \"epoch\": 0}, {\"train\": 0.3537882740298907, \"val\": 6.738354231003258, \"epoch\": 1}, {\"train\": 0.2404067205886046, \"val\": 0.6168511074001867, \"epoch\": 2}, {\"train\": 0.1852193921804428, \"val\": 0.4673878643989425, \"epoch\": 3}, {\"train\": 0.1439620809008678, \"val\": 0.36726411004654236, \"epoch\": 4}, {\"train\": 0.1231226809322834, \"val\": 0.3284625232064476, \"epoch\": 5}, {\"train\": 0.11689206523199876, \"val\": 0.45505326065769486, \"epoch\": 6}, {\"train\": 0.10618990535537402, \"val\": 0.27603006545557746, \"epoch\": 7}, {\"train\": 0.10079925631483395, \"val\": 0.2877099801311959, \"epoch\": 8}, {\"train\": 0.09519618873794873, \"val\": 0.4247469771127389, \"epoch\": 9}, {\"train\": 0.09918376617133617, \"val\": 0.2673409665632924, \"epoch\": 10}, {\"train\": 0.1049475980301698, \"val\": 0.161801748479613, \"epoch\": 11}, {\"train\": 0.1179543814311425, \"val\": 0.28248551376508896, \"epoch\": 12}, {\"train\": 0.09902819556494553, \"val\": 0.13848831050563604, \"epoch\": 13}, {\"train\": 0.08829642335573833, \"val\": 0.1269794802233163, \"epoch\": 14}, {\"train\": 0.07810589919487636, \"val\": 0.12134801273630863, \"epoch\": 15}, {\"train\": 0.07054370082914829, \"val\": 0.14663652766245866, \"epoch\": 16}, {\"train\": 0.06178869307041168, \"val\": 0.07289000384302603, \"epoch\": 17}, {\"train\": 0.06651624261091153, \"val\": 0.15836476178762193, \"epoch\": 18}, {\"train\": 0.0650631207972765, \"val\": 0.12744316907720296, \"epoch\": 19}, {\"train\": 0.061741454216341175, \"val\": 0.1249129490560369, \"epoch\": 20}, {\"train\": 0.07174942673494418, \"val\": 0.14697975217821765, \"epoch\": 21}, {\"train\": 0.08251543405155341, \"val\": 0.19930101563912575, \"epoch\": 22}, {\"train\": 0.07790447150667508, \"val\": 0.12592532731359823, \"epoch\": 23}, {\"train\": 0.0700756786391139, \"val\": 0.10158715483848937, \"epoch\": 24}, {\"train\": 0.05360417906194925, \"val\": 0.19515441983476867, \"epoch\": 25}, {\"train\": 0.06260833330452442, \"val\": 0.1029414869273616, \"epoch\": 26}, {\"train\": 0.06313268623004357, \"val\": 0.1677745537862561, \"epoch\": 27}, {\"train\": 0.06650341550509135, \"val\": 0.13035861635753127, \"epoch\": 28}, {\"train\": 0.065382136652867, \"val\": 0.1456369174954792, \"epoch\": 29}, {\"train\": 0.06640060773740213, \"val\": 0.11109232850269311, \"epoch\": 30}, {\"train\": 0.06572710163891315, \"val\": 0.08131253510170305, \"epoch\": 31}, {\"train\": 0.057721057596306004, \"val\": 0.09534131697016872, \"epoch\": 32}, {\"train\": 0.06098951461414496, \"val\": 0.0633000903655085, \"epoch\": 33}, {\"train\": 0.04906810230265061, \"val\": 0.14466173787842745, \"epoch\": 34}, {\"train\": 0.07574568223208189, \"val\": 0.11704767230031494, \"epoch\": 35}, {\"train\": 0.06115338454643885, \"val\": 0.08794662818545476, \"epoch\": 36}, {\"train\": 0.0536713395267725, \"val\": 0.10604137096562664, \"epoch\": 37}, {\"train\": 0.046552919782698154, \"val\": 0.06731996617551582, \"epoch\": 38}, {\"train\": 0.053844439797103405, \"val\": 0.06249432139146073, \"epoch\": 39}, {\"train\": 0.0429910138870279, \"val\": 0.08789067275399619, \"epoch\": 40}, {\"train\": 0.04890337431182464, \"val\": 0.07081121625389507, \"epoch\": 41}, {\"train\": 0.057115974525610604, \"val\": 0.04531960496398689, \"epoch\": 42}, {\"train\": 0.05966537414739529, \"val\": 0.14133980058702744, \"epoch\": 43}, {\"train\": 0.06308154948055744, \"val\": 0.09640124445755241, \"epoch\": 44}, {\"train\": 0.06029075011610985, \"val\": 0.18386765610819028, \"epoch\": 45}, {\"train\": 0.05509966115156809, \"val\": 0.06798884620420048, \"epoch\": 46}, {\"train\": 0.07087028492242098, \"val\": 0.1593539545058135, \"epoch\": 47}, {\"train\": 0.06376757224400838, \"val\": 0.1351612599605384, \"epoch\": 48}, {\"train\": 0.06431079717973869, \"val\": 0.38423039661957537, \"epoch\": 49}, {\"train\": 0.06080716382712126, \"val\": 0.06169840014082621, \"epoch\": 50}, {\"train\": 0.04394784724960724, \"val\": 0.10504907188156654, \"epoch\": 51}, {\"train\": 0.05495473726963004, \"val\": 0.11193958458326826, \"epoch\": 52}, {\"train\": 0.043546347257991634, \"val\": 0.04468445814876489, \"epoch\": 53}, {\"train\": 0.037502977841844164, \"val\": 0.0468838649760949, \"epoch\": 54}, {\"train\": 0.04127588076516986, \"val\": 0.04046927878947894, \"epoch\": 55}, {\"train\": 0.03783946127320329, \"val\": 0.04713223096397188, \"epoch\": 56}, {\"train\": 0.04601494502276182, \"val\": 0.20081575225059944, \"epoch\": 57}, {\"train\": 0.06172864666829506, \"val\": 0.15692878213141942, \"epoch\": 58}, {\"train\": 0.05140470682332913, \"val\": 0.12150749756601052, \"epoch\": 59}, {\"train\": 0.043502915340165295, \"val\": 0.06556814535245023, \"epoch\": 60}, {\"train\": 0.052954442178209625, \"val\": 0.06962024049975703, \"epoch\": 61}, {\"train\": 0.05043032951653004, \"val\": 0.09003391663060972, \"epoch\": 62}, {\"train\": 0.05067424631367127, \"val\": 0.11513486410926647, \"epoch\": 63}, {\"train\": 0.04547690708811084, \"val\": 0.09109774869758877, \"epoch\": 64}, {\"train\": 0.052249216474592686, \"val\": 0.0914179301141606, \"epoch\": 65}, {\"train\": 0.046539184637367725, \"val\": 0.16335447077958465, \"epoch\": 66}, {\"train\": 0.04662065083781878, \"val\": 0.10364130529816504, \"epoch\": 67}, {\"train\": 0.0539201640834411, \"val\": 0.058225211666905564, \"epoch\": 68}, {\"train\": 0.051276713609695435, \"val\": 0.07204244041910257, \"epoch\": 69}, {\"train\": 0.047244361291329064, \"val\": 0.07163930664127865, \"epoch\": 70}, {\"train\": 0.04912896112849315, \"val\": 0.1049924438715809, \"epoch\": 71}, {\"train\": 0.04562352752933899, \"val\": 0.05741473032890267, \"epoch\": 72}, {\"train\": 0.06022383691743016, \"val\": 0.09748421381572607, \"epoch\": 73}, {\"train\": 0.04446318776657184, \"val\": 0.08724284503841773, \"epoch\": 74}, {\"train\": 0.047614481610556446, \"val\": 0.05305005528584675, \"epoch\": 75}, {\"train\": 0.04291732034956416, \"val\": 0.06481414686120977, \"epoch\": 76}, {\"train\": 0.04509904173513254, \"val\": 0.06467283385529839, \"epoch\": 77}, {\"train\": 0.051336303198089205, \"val\": 0.07190225221479692, \"epoch\": 78}, {\"train\": 0.05025631934404373, \"val\": 0.08708446151490702, \"epoch\": 79}, {\"train\": 0.04873883972565333, \"val\": 0.05539397559111455, \"epoch\": 80}, {\"train\": 0.04292160148421923, \"val\": 0.07284839385861738, \"epoch\": 81}, {\"train\": 0.03965183130155007, \"val\": 0.09972680740994057, \"epoch\": 82}, {\"train\": 0.04597285917649666, \"val\": 0.07756127590352359, \"epoch\": 83}, {\"train\": 0.04314385587349534, \"val\": 0.07283635238073422, \"epoch\": 84}, {\"train\": 0.05780146364122629, \"val\": 0.06840342271909186, \"epoch\": 85}, {\"train\": 0.049050714199741684, \"val\": 0.069428440748231, \"epoch\": 86}, {\"train\": 0.045110589979837336, \"val\": 0.09393514247257607, \"epoch\": 87}, {\"train\": 0.04096950264647603, \"val\": 0.07557837787909968, \"epoch\": 88}, {\"train\": 0.04064675342912475, \"val\": 0.0466482994978799, \"epoch\": 89}, {\"train\": 0.04564835596829653, \"val\": 0.08520984070513535, \"epoch\": 90}, {\"train\": 0.047523492171118654, \"val\": 0.14863007260816327, \"epoch\": 91}, {\"train\": 0.046748686426629625, \"val\": 0.07989453038252477, \"epoch\": 92}, {\"train\": 0.045762967163076006, \"val\": 0.1758599292262699, \"epoch\": 93}, {\"train\": 0.0427702342470487, \"val\": 0.06748659696421344, \"epoch\": 94}, {\"train\": 0.0434280790699025, \"val\": 0.046984640498110956, \"epoch\": 95}, {\"train\": 0.0357706593349576, \"val\": 0.05033246142556891, \"epoch\": 96}, {\"train\": 0.037102681739876665, \"val\": 0.05778045848796696, \"epoch\": 97}, {\"train\": 0.049687228786448635, \"val\": 0.1308387047007542, \"epoch\": 98}, {\"train\": 0.04145055326322714, \"val\": 0.10464951311134629, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = './results/transferLrn_des9_01/'\n",
    "epochs=100\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                            epochs=epochs,\n",
    "                            saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.090497</td>\n",
       "      <td>0.958374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.172388</td>\n",
       "      <td>0.741564</td>\n",
       "      <td>0.933792</td>\n",
       "      <td>0.760291</td>\n",
       "      <td>-0.184816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre    peakR2  maxAggR2  meanAggR2  minAggR2\n",
       "train  0.000013  0.002188  0.090497  0.958374       NaN        NaN       NaN\n",
       "test   0.000062  0.005307  0.172388  0.741564  0.933792   0.760291 -0.184816"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train set of size 48\n",
      "epoch: 0   trainLoss: 9.4186e-01   valLoss:9.5560e-01  time: 4.32e-01\n",
      "epoch: 1   trainLoss: 7.7727e-01   valLoss:9.4003e-01  time: 4.42e-01\n",
      "epoch: 2   trainLoss: 6.6229e-01   valLoss:9.1578e-01  time: 4.50e-01\n",
      "epoch: 3   trainLoss: 5.6774e-01   valLoss:8.8743e-01  time: 4.48e-01\n",
      "epoch: 4   trainLoss: 4.6349e-01   valLoss:8.5494e-01  time: 4.59e-01\n",
      "epoch: 5   trainLoss: 3.9376e-01   valLoss:8.2469e-01  time: 4.30e-01\n",
      "epoch: 6   trainLoss: 3.2349e-01   valLoss:8.1825e-01  time: 4.33e-01\n",
      "epoch: 7   trainLoss: 2.6142e-01   valLoss:8.3949e-01  time: 4.34e-01\n",
      "epoch: 8   trainLoss: 2.1795e-01   valLoss:8.8210e-01  time: 4.33e-01\n",
      "epoch: 9   trainLoss: 1.9015e-01   valLoss:9.3346e-01  time: 4.39e-01\n",
      "epoch: 10   trainLoss: 1.7563e-01   valLoss:9.5297e-01  time: 4.31e-01\n",
      "epoch: 11   trainLoss: 1.5534e-01   valLoss:9.5408e-01  time: 4.28e-01\n",
      "epoch: 12   trainLoss: 1.4607e-01   valLoss:9.2513e-01  time: 4.29e-01\n",
      "epoch: 13   trainLoss: 1.3889e-01   valLoss:9.3186e-01  time: 4.34e-01\n",
      "epoch: 14   trainLoss: 1.3338e-01   valLoss:9.4710e-01  time: 4.32e-01\n",
      "epoch: 15   trainLoss: 1.1505e-01   valLoss:9.3627e-01  time: 4.22e-01\n",
      "epoch: 16   trainLoss: 1.0358e-01   valLoss:9.1492e-01  time: 4.24e-01\n",
      "epoch: 17   trainLoss: 9.1390e-02   valLoss:8.9815e-01  time: 4.21e-01\n",
      "epoch: 18   trainLoss: 8.4334e-02   valLoss:8.7727e-01  time: 4.41e-01\n",
      "epoch: 19   trainLoss: 7.7507e-02   valLoss:8.4771e-01  time: 4.26e-01\n",
      "epoch: 20   trainLoss: 6.9008e-02   valLoss:8.1386e-01  time: 4.23e-01\n",
      "epoch: 21   trainLoss: 6.3745e-02   valLoss:7.7548e-01  time: 4.22e-01\n",
      "epoch: 22   trainLoss: 5.6617e-02   valLoss:7.2118e-01  time: 4.39e-01\n",
      "epoch: 23   trainLoss: 5.0338e-02   valLoss:6.4328e-01  time: 4.26e-01\n",
      "epoch: 24   trainLoss: 4.5509e-02   valLoss:5.6207e-01  time: 4.28e-01\n",
      "epoch: 25   trainLoss: 4.1964e-02   valLoss:4.8644e-01  time: 4.26e-01\n",
      "epoch: 26   trainLoss: 3.9257e-02   valLoss:4.3633e-01  time: 4.26e-01\n",
      "epoch: 27   trainLoss: 3.7866e-02   valLoss:3.8976e-01  time: 4.26e-01\n",
      "epoch: 28   trainLoss: 3.7472e-02   valLoss:3.6949e-01  time: 4.33e-01\n",
      "epoch: 29   trainLoss: 3.2338e-02   valLoss:3.2458e-01  time: 4.28e-01\n",
      "epoch: 30   trainLoss: 2.9803e-02   valLoss:2.9891e-01  time: 4.25e-01\n",
      "epoch: 31   trainLoss: 2.8537e-02   valLoss:3.0849e-01  time: 4.22e-01\n",
      "epoch: 32   trainLoss: 2.6851e-02   valLoss:2.5893e-01  time: 4.21e-01\n",
      "epoch: 33   trainLoss: 2.5943e-02   valLoss:2.8093e-01  time: 4.23e-01\n",
      "epoch: 34   trainLoss: 2.2909e-02   valLoss:2.4832e-01  time: 4.36e-01\n",
      "epoch: 35   trainLoss: 2.0403e-02   valLoss:2.4089e-01  time: 4.33e-01\n",
      "epoch: 36   trainLoss: 1.9796e-02   valLoss:2.6366e-01  time: 4.40e-01\n",
      "epoch: 37   trainLoss: 1.9002e-02   valLoss:2.1322e-01  time: 4.27e-01\n",
      "epoch: 38   trainLoss: 1.7873e-02   valLoss:2.2283e-01  time: 4.27e-01\n",
      "epoch: 39   trainLoss: 1.6060e-02   valLoss:2.1286e-01  time: 4.21e-01\n",
      "epoch: 40   trainLoss: 1.4901e-02   valLoss:1.9312e-01  time: 4.23e-01\n",
      "epoch: 41   trainLoss: 1.4204e-02   valLoss:1.9069e-01  time: 4.24e-01\n",
      "epoch: 42   trainLoss: 1.3410e-02   valLoss:1.6270e-01  time: 4.28e-01\n",
      "epoch: 43   trainLoss: 1.3382e-02   valLoss:1.8051e-01  time: 4.22e-01\n",
      "epoch: 44   trainLoss: 1.3174e-02   valLoss:1.4919e-01  time: 4.22e-01\n",
      "epoch: 45   trainLoss: 1.2698e-02   valLoss:1.7393e-01  time: 4.30e-01\n",
      "epoch: 46   trainLoss: 1.1665e-02   valLoss:1.5200e-01  time: 4.28e-01\n",
      "epoch: 47   trainLoss: 1.0304e-02   valLoss:1.7088e-01  time: 4.22e-01\n",
      "epoch: 48   trainLoss: 9.3598e-03   valLoss:1.5791e-01  time: 4.18e-01\n",
      "epoch: 49   trainLoss: 8.4104e-03   valLoss:1.4551e-01  time: 4.22e-01\n",
      "epoch: 50   trainLoss: 8.2832e-03   valLoss:1.5632e-01  time: 4.21e-01\n",
      "epoch: 51   trainLoss: 8.0913e-03   valLoss:1.3312e-01  time: 4.20e-01\n",
      "epoch: 52   trainLoss: 8.0207e-03   valLoss:1.3028e-01  time: 4.27e-01\n",
      "epoch: 53   trainLoss: 7.4358e-03   valLoss:1.1576e-01  time: 4.16e-01\n",
      "epoch: 54   trainLoss: 6.5858e-03   valLoss:1.1225e-01  time: 4.20e-01\n",
      "epoch: 55   trainLoss: 5.5830e-03   valLoss:1.0794e-01  time: 4.19e-01\n",
      "epoch: 56   trainLoss: 5.2183e-03   valLoss:1.1110e-01  time: 4.23e-01\n",
      "epoch: 57   trainLoss: 5.1853e-03   valLoss:1.0355e-01  time: 4.17e-01\n",
      "epoch: 58   trainLoss: 5.5365e-03   valLoss:1.1049e-01  time: 4.38e-01\n",
      "epoch: 59   trainLoss: 6.1581e-03   valLoss:9.5550e-02  time: 4.22e-01\n",
      "epoch: 60   trainLoss: 7.2681e-03   valLoss:1.2745e-01  time: 4.25e-01\n",
      "epoch: 61   trainLoss: 1.0006e-02   valLoss:8.6296e-02  time: 4.23e-01\n",
      "epoch: 62   trainLoss: 1.3404e-02   valLoss:1.1760e-01  time: 4.23e-01\n",
      "epoch: 63   trainLoss: 1.4031e-02   valLoss:1.1091e-01  time: 4.25e-01\n",
      "epoch: 64   trainLoss: 1.4673e-02   valLoss:6.6665e-02  time: 4.21e-01\n",
      "epoch: 65   trainLoss: 1.2613e-02   valLoss:1.2387e-01  time: 4.29e-01\n",
      "epoch: 66   trainLoss: 8.1284e-03   valLoss:1.1520e-01  time: 4.25e-01\n",
      "epoch: 67   trainLoss: 8.1366e-03   valLoss:8.6543e-02  time: 4.26e-01\n",
      "epoch: 68   trainLoss: 8.9779e-03   valLoss:1.1770e-01  time: 4.24e-01\n",
      "epoch: 69   trainLoss: 5.7744e-03   valLoss:1.2337e-01  time: 4.34e-01\n",
      "epoch: 70   trainLoss: 6.6461e-03   valLoss:8.5381e-02  time: 4.32e-01\n",
      "epoch: 71   trainLoss: 5.9206e-03   valLoss:8.5340e-02  time: 4.22e-01\n",
      "epoch: 72   trainLoss: 4.3597e-03   valLoss:1.0233e-01  time: 4.31e-01\n",
      "epoch: 73   trainLoss: 5.7474e-03   valLoss:7.8731e-02  time: 4.26e-01\n",
      "epoch: 74   trainLoss: 3.5365e-03   valLoss:7.2509e-02  time: 4.40e-01\n",
      "epoch: 75   trainLoss: 4.5112e-03   valLoss:7.6636e-02  time: 4.43e-01\n",
      "epoch: 76   trainLoss: 3.6150e-03   valLoss:7.5507e-02  time: 4.28e-01\n",
      "epoch: 77   trainLoss: 3.1307e-03   valLoss:7.6708e-02  time: 4.33e-01\n",
      "epoch: 78   trainLoss: 3.5012e-03   valLoss:7.7399e-02  time: 4.26e-01\n",
      "epoch: 79   trainLoss: 2.3833e-03   valLoss:7.3091e-02  time: 4.26e-01\n",
      "epoch: 80   trainLoss: 3.0301e-03   valLoss:7.7029e-02  time: 4.33e-01\n",
      "epoch: 81   trainLoss: 2.1294e-03   valLoss:8.3764e-02  time: 4.24e-01\n",
      "epoch: 82   trainLoss: 2.2085e-03   valLoss:7.8211e-02  time: 4.27e-01\n",
      "epoch: 83   trainLoss: 2.3475e-03   valLoss:7.7437e-02  time: 4.24e-01\n",
      "epoch: 84   trainLoss: 1.4841e-03   valLoss:8.1259e-02  time: 4.25e-01\n",
      "epoch: 85   trainLoss: 2.0725e-03   valLoss:7.6545e-02  time: 4.23e-01\n",
      "epoch: 86   trainLoss: 1.6524e-03   valLoss:8.1876e-02  time: 4.22e-01\n",
      "epoch: 87   trainLoss: 1.4393e-03   valLoss:8.3800e-02  time: 4.20e-01\n",
      "epoch: 88   trainLoss: 1.4485e-03   valLoss:7.8190e-02  time: 4.21e-01\n",
      "epoch: 89   trainLoss: 1.4228e-03   valLoss:8.2121e-02  time: 4.24e-01\n",
      "epoch: 90   trainLoss: 1.2724e-03   valLoss:7.9874e-02  time: 4.30e-01\n",
      "epoch: 91   trainLoss: 1.0985e-03   valLoss:7.4711e-02  time: 4.28e-01\n",
      "epoch: 92   trainLoss: 1.1794e-03   valLoss:7.4153e-02  time: 4.25e-01\n",
      "epoch: 93   trainLoss: 1.1087e-03   valLoss:7.4052e-02  time: 4.24e-01\n",
      "epoch: 94   trainLoss: 9.7959e-04   valLoss:7.1425e-02  time: 4.23e-01\n",
      "epoch: 95   trainLoss: 1.0139e-03   valLoss:7.4406e-02  time: 4.24e-01\n",
      "epoch: 96   trainLoss: 1.1511e-03   valLoss:7.0647e-02  time: 4.30e-01\n",
      "epoch: 97   trainLoss: 1.2068e-03   valLoss:7.6949e-02  time: 4.20e-01\n",
      "epoch: 98   trainLoss: 1.8211e-03   valLoss:6.3902e-02  time: 4.25e-01\n",
      "epoch: 99   trainLoss: 4.7788e-03   valLoss:1.0131e-01  time: 4.22e-01\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2276e+00   valLoss:9.3326e-01  time: 4.44e-01\n",
      "epoch: 1   trainLoss: 1.0614e+00   valLoss:1.1156e+00  time: 4.60e-01\n",
      "epoch: 2   trainLoss: 6.8450e-01   valLoss:1.0287e+00  time: 4.39e-01\n",
      "epoch: 3   trainLoss: 4.8891e-01   valLoss:9.0258e-01  time: 4.24e-01\n",
      "epoch: 4   trainLoss: 3.7340e-01   valLoss:7.6792e-01  time: 4.24e-01\n",
      "epoch: 5   trainLoss: 2.9884e-01   valLoss:8.7062e-01  time: 4.26e-01\n",
      "epoch: 6   trainLoss: 2.1112e-01   valLoss:5.1619e-01  time: 4.24e-01\n",
      "epoch: 7   trainLoss: 1.7485e-01   valLoss:8.5603e-01  time: 4.31e-01\n",
      "epoch: 8   trainLoss: 1.6386e-01   valLoss:4.0405e-01  time: 4.28e-01\n",
      "epoch: 9   trainLoss: 1.9939e-01   valLoss:4.6397e-01  time: 4.18e-01\n",
      "epoch: 10   trainLoss: 1.7115e-01   valLoss:5.2355e-01  time: 4.21e-01\n",
      "epoch: 11   trainLoss: 1.3381e-01   valLoss:3.7968e-01  time: 4.32e-01\n",
      "epoch: 12   trainLoss: 1.3360e-01   valLoss:4.4935e-01  time: 4.27e-01\n",
      "epoch: 13   trainLoss: 1.2662e-01   valLoss:4.4455e-01  time: 4.26e-01\n",
      "epoch: 14   trainLoss: 1.0527e-01   valLoss:4.2985e-01  time: 4.24e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15   trainLoss: 9.7771e-02   valLoss:3.1456e-01  time: 4.24e-01\n",
      "epoch: 16   trainLoss: 8.9025e-02   valLoss:2.6532e-01  time: 4.23e-01\n",
      "epoch: 17   trainLoss: 7.8687e-02   valLoss:2.2505e-01  time: 4.21e-01\n",
      "epoch: 18   trainLoss: 7.8339e-02   valLoss:1.7222e-01  time: 4.24e-01\n",
      "epoch: 19   trainLoss: 6.8303e-02   valLoss:2.1605e-01  time: 4.24e-01\n",
      "epoch: 20   trainLoss: 6.7263e-02   valLoss:1.8227e-01  time: 4.25e-01\n",
      "epoch: 21   trainLoss: 5.8265e-02   valLoss:9.0417e-02  time: 4.26e-01\n",
      "epoch: 22   trainLoss: 5.0787e-02   valLoss:6.4769e-02  time: 4.27e-01\n",
      "epoch: 23   trainLoss: 5.2088e-02   valLoss:9.8280e-02  time: 4.20e-01\n",
      "epoch: 24   trainLoss: 4.7897e-02   valLoss:1.2745e-01  time: 4.25e-01\n",
      "epoch: 25   trainLoss: 4.3389e-02   valLoss:1.1500e-01  time: 4.16e-01\n",
      "epoch: 26   trainLoss: 4.2176e-02   valLoss:1.0468e-01  time: 4.20e-01\n",
      "epoch: 27   trainLoss: 3.7702e-02   valLoss:1.1417e-01  time: 4.26e-01\n",
      "epoch: 28   trainLoss: 3.7316e-02   valLoss:1.1364e-01  time: 4.24e-01\n",
      "epoch: 29   trainLoss: 3.5454e-02   valLoss:1.1150e-01  time: 4.25e-01\n",
      "epoch: 30   trainLoss: 3.1920e-02   valLoss:1.1129e-01  time: 4.18e-01\n",
      "epoch: 31   trainLoss: 3.2090e-02   valLoss:9.2855e-02  time: 4.20e-01\n",
      "epoch: 32   trainLoss: 3.0420e-02   valLoss:7.4292e-02  time: 4.22e-01\n",
      "epoch: 33   trainLoss: 2.8312e-02   valLoss:6.6410e-02  time: 4.19e-01\n",
      "epoch: 34   trainLoss: 2.7604e-02   valLoss:6.2433e-02  time: 4.24e-01\n",
      "epoch: 35   trainLoss: 2.5909e-02   valLoss:5.8918e-02  time: 4.21e-01\n",
      "epoch: 36   trainLoss: 2.4421e-02   valLoss:5.6432e-02  time: 4.19e-01\n",
      "epoch: 37   trainLoss: 2.3922e-02   valLoss:5.0098e-02  time: 4.22e-01\n",
      "epoch: 38   trainLoss: 2.2202e-02   valLoss:4.5983e-02  time: 4.32e-01\n",
      "epoch: 39   trainLoss: 2.1552e-02   valLoss:4.3484e-02  time: 4.32e-01\n",
      "epoch: 40   trainLoss: 2.0948e-02   valLoss:4.5377e-02  time: 4.32e-01\n",
      "epoch: 41   trainLoss: 1.9868e-02   valLoss:4.1229e-02  time: 4.29e-01\n",
      "epoch: 42   trainLoss: 1.9317e-02   valLoss:3.4807e-02  time: 4.22e-01\n",
      "epoch: 43   trainLoss: 1.8217e-02   valLoss:3.1109e-02  time: 4.26e-01\n",
      "epoch: 44   trainLoss: 1.7756e-02   valLoss:3.0282e-02  time: 4.26e-01\n",
      "epoch: 45   trainLoss: 1.7045e-02   valLoss:3.4837e-02  time: 4.25e-01\n",
      "epoch: 46   trainLoss: 1.6147e-02   valLoss:3.3018e-02  time: 4.22e-01\n",
      "epoch: 47   trainLoss: 1.5970e-02   valLoss:3.0332e-02  time: 4.28e-01\n",
      "epoch: 48   trainLoss: 1.5009e-02   valLoss:3.0524e-02  time: 4.23e-01\n",
      "epoch: 49   trainLoss: 1.4670e-02   valLoss:2.9674e-02  time: 4.20e-01\n",
      "epoch: 50   trainLoss: 1.4111e-02   valLoss:3.4039e-02  time: 4.22e-01\n",
      "epoch: 51   trainLoss: 1.3534e-02   valLoss:3.8976e-02  time: 4.21e-01\n",
      "epoch: 52   trainLoss: 1.2936e-02   valLoss:3.9326e-02  time: 4.21e-01\n",
      "epoch: 53   trainLoss: 1.2432e-02   valLoss:3.9800e-02  time: 4.19e-01\n",
      "epoch: 54   trainLoss: 1.2148e-02   valLoss:3.5757e-02  time: 4.35e-01\n",
      "epoch: 55   trainLoss: 1.1737e-02   valLoss:3.2222e-02  time: 4.31e-01\n",
      "epoch: 56   trainLoss: 1.1455e-02   valLoss:3.3738e-02  time: 4.25e-01\n",
      "epoch: 57   trainLoss: 1.1314e-02   valLoss:2.3800e-02  time: 4.38e-01\n",
      "epoch: 58   trainLoss: 1.1798e-02   valLoss:3.8485e-02  time: 4.29e-01\n",
      "epoch: 59   trainLoss: 1.3956e-02   valLoss:4.5320e-02  time: 4.22e-01\n",
      "epoch: 60   trainLoss: 2.0271e-02   valLoss:5.0285e-02  time: 4.23e-01\n",
      "epoch: 61   trainLoss: 2.2384e-02   valLoss:2.7547e-02  time: 4.29e-01\n",
      "epoch: 62   trainLoss: 1.9001e-02   valLoss:2.3472e-02  time: 4.24e-01\n",
      "epoch: 63   trainLoss: 1.6427e-02   valLoss:2.9239e-02  time: 4.25e-01\n",
      "epoch: 64   trainLoss: 1.0748e-02   valLoss:2.1626e-02  time: 4.25e-01\n",
      "epoch: 65   trainLoss: 1.3445e-02   valLoss:2.3820e-02  time: 4.25e-01\n",
      "epoch: 66   trainLoss: 1.7703e-02   valLoss:2.5630e-02  time: 4.21e-01\n",
      "epoch: 67   trainLoss: 9.9604e-03   valLoss:2.6730e-02  time: 4.23e-01\n",
      "epoch: 68   trainLoss: 1.1422e-02   valLoss:3.0100e-02  time: 4.24e-01\n",
      "epoch: 69   trainLoss: 1.5564e-02   valLoss:2.3630e-02  time: 4.23e-01\n",
      "epoch: 70   trainLoss: 9.0793e-03   valLoss:1.6449e-02  time: 4.26e-01\n",
      "epoch: 71   trainLoss: 1.0395e-02   valLoss:2.1610e-02  time: 4.30e-01\n",
      "epoch: 72   trainLoss: 1.2507e-02   valLoss:1.7762e-02  time: 4.25e-01\n",
      "epoch: 73   trainLoss: 8.0851e-03   valLoss:3.4726e-02  time: 4.25e-01\n",
      "epoch: 74   trainLoss: 9.7674e-03   valLoss:3.9148e-02  time: 4.23e-01\n",
      "epoch: 75   trainLoss: 1.1025e-02   valLoss:2.4464e-02  time: 4.22e-01\n",
      "epoch: 76   trainLoss: 7.6819e-03   valLoss:4.1878e-02  time: 4.21e-01\n",
      "epoch: 77   trainLoss: 8.7655e-03   valLoss:5.5736e-02  time: 4.23e-01\n",
      "epoch: 78   trainLoss: 1.0032e-02   valLoss:4.4216e-02  time: 4.29e-01\n",
      "epoch: 79   trainLoss: 7.2474e-03   valLoss:4.9493e-02  time: 4.19e-01\n",
      "epoch: 80   trainLoss: 7.8380e-03   valLoss:5.7198e-02  time: 4.34e-01\n",
      "epoch: 81   trainLoss: 8.9153e-03   valLoss:4.1750e-02  time: 4.23e-01\n",
      "epoch: 82   trainLoss: 6.7602e-03   valLoss:4.6217e-02  time: 4.23e-01\n",
      "epoch: 83   trainLoss: 6.8805e-03   valLoss:5.3445e-02  time: 4.30e-01\n",
      "epoch: 84   trainLoss: 7.8877e-03   valLoss:5.2033e-02  time: 4.22e-01\n",
      "epoch: 85   trainLoss: 6.3080e-03   valLoss:4.6869e-02  time: 4.20e-01\n",
      "epoch: 86   trainLoss: 5.9148e-03   valLoss:4.6013e-02  time: 4.19e-01\n",
      "epoch: 87   trainLoss: 7.0267e-03   valLoss:3.6519e-02  time: 4.33e-01\n",
      "epoch: 88   trainLoss: 5.9060e-03   valLoss:2.5597e-02  time: 4.23e-01\n",
      "epoch: 89   trainLoss: 4.9476e-03   valLoss:2.7923e-02  time: 4.31e-01\n",
      "epoch: 90   trainLoss: 6.3631e-03   valLoss:2.0962e-02  time: 4.30e-01\n",
      "epoch: 91   trainLoss: 5.4468e-03   valLoss:1.6686e-02  time: 4.25e-01\n",
      "epoch: 92   trainLoss: 4.1105e-03   valLoss:2.4254e-02  time: 4.23e-01\n",
      "epoch: 93   trainLoss: 5.4849e-03   valLoss:1.6349e-02  time: 4.24e-01\n",
      "epoch: 94   trainLoss: 5.2240e-03   valLoss:1.6817e-02  time: 4.25e-01\n",
      "epoch: 95   trainLoss: 3.8447e-03   valLoss:2.4646e-02  time: 4.20e-01\n",
      "epoch: 96   trainLoss: 4.4458e-03   valLoss:1.7208e-02  time: 4.36e-01\n",
      "epoch: 97   trainLoss: 4.7671e-03   valLoss:2.4507e-02  time: 4.20e-01\n",
      "epoch: 98   trainLoss: 4.5699e-03   valLoss:3.5186e-02  time: 4.29e-01\n",
      "epoch: 99   trainLoss: 6.5772e-03   valLoss:4.0611e-02  time: 4.19e-01\n",
      "loading checkpoint 93\n",
      "trained 38 random forest models in 5.13 seconds\n",
      "loaded train set of size 18\n",
      "epoch: 0   trainLoss: 1.1425e+00   valLoss:1.0196e+00  time: 1.74e-01\n",
      "epoch: 1   trainLoss: 9.5801e-01   valLoss:9.9986e-01  time: 1.74e-01\n",
      "epoch: 2   trainLoss: 8.2499e-01   valLoss:9.8414e-01  time: 1.75e-01\n",
      "epoch: 3   trainLoss: 7.0742e-01   valLoss:9.5560e-01  time: 1.72e-01\n",
      "epoch: 4   trainLoss: 6.1542e-01   valLoss:9.1905e-01  time: 1.74e-01\n",
      "epoch: 5   trainLoss: 5.2637e-01   valLoss:8.7612e-01  time: 1.74e-01\n",
      "epoch: 6   trainLoss: 4.5803e-01   valLoss:8.4068e-01  time: 1.72e-01\n",
      "epoch: 7   trainLoss: 3.9643e-01   valLoss:8.3207e-01  time: 1.75e-01\n",
      "epoch: 8   trainLoss: 3.4627e-01   valLoss:8.6548e-01  time: 1.75e-01\n",
      "epoch: 9   trainLoss: 2.9634e-01   valLoss:9.3791e-01  time: 1.73e-01\n",
      "epoch: 10   trainLoss: 2.7132e-01   valLoss:1.0031e+00  time: 1.78e-01\n",
      "epoch: 11   trainLoss: 2.4380e-01   valLoss:1.0227e+00  time: 1.73e-01\n",
      "epoch: 12   trainLoss: 2.1304e-01   valLoss:9.9294e-01  time: 1.73e-01\n",
      "epoch: 13   trainLoss: 1.8174e-01   valLoss:9.4659e-01  time: 1.76e-01\n",
      "epoch: 14   trainLoss: 1.7046e-01   valLoss:8.9909e-01  time: 1.73e-01\n",
      "epoch: 15   trainLoss: 1.5635e-01   valLoss:8.6541e-01  time: 1.72e-01\n",
      "epoch: 16   trainLoss: 1.3529e-01   valLoss:8.5971e-01  time: 1.74e-01\n",
      "epoch: 17   trainLoss: 1.1919e-01   valLoss:8.7843e-01  time: 1.86e-01\n",
      "epoch: 18   trainLoss: 9.3228e-02   valLoss:9.0845e-01  time: 1.78e-01\n",
      "epoch: 19   trainLoss: 9.0987e-02   valLoss:8.9495e-01  time: 1.74e-01\n",
      "epoch: 20   trainLoss: 8.4889e-02   valLoss:8.5793e-01  time: 1.76e-01\n",
      "epoch: 21   trainLoss: 6.8275e-02   valLoss:8.0848e-01  time: 1.81e-01\n",
      "epoch: 22   trainLoss: 6.0964e-02   valLoss:7.2023e-01  time: 1.73e-01\n",
      "epoch: 23   trainLoss: 5.2336e-02   valLoss:6.0915e-01  time: 1.73e-01\n",
      "epoch: 24   trainLoss: 4.4878e-02   valLoss:4.7181e-01  time: 1.76e-01\n",
      "epoch: 25   trainLoss: 3.7640e-02   valLoss:3.4561e-01  time: 1.77e-01\n",
      "epoch: 26   trainLoss: 3.1415e-02   valLoss:2.7695e-01  time: 1.76e-01\n",
      "epoch: 27   trainLoss: 2.7245e-02   valLoss:2.3682e-01  time: 1.73e-01\n",
      "epoch: 28   trainLoss: 2.3121e-02   valLoss:2.0008e-01  time: 1.73e-01\n",
      "epoch: 29   trainLoss: 2.2453e-02   valLoss:2.1582e-01  time: 1.79e-01\n",
      "epoch: 30   trainLoss: 1.9116e-02   valLoss:2.1343e-01  time: 1.77e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31   trainLoss: 1.4972e-02   valLoss:2.6096e-01  time: 1.71e-01\n",
      "epoch: 32   trainLoss: 1.5520e-02   valLoss:2.6135e-01  time: 1.73e-01\n",
      "epoch: 33   trainLoss: 1.4271e-02   valLoss:2.8526e-01  time: 1.72e-01\n",
      "epoch: 34   trainLoss: 1.1651e-02   valLoss:2.7952e-01  time: 1.72e-01\n",
      "epoch: 35   trainLoss: 1.0301e-02   valLoss:2.8700e-01  time: 1.76e-01\n",
      "epoch: 36   trainLoss: 8.4198e-03   valLoss:2.7419e-01  time: 1.76e-01\n",
      "epoch: 37   trainLoss: 6.9203e-03   valLoss:2.6867e-01  time: 1.78e-01\n",
      "epoch: 38   trainLoss: 5.7935e-03   valLoss:2.5334e-01  time: 1.72e-01\n",
      "epoch: 39   trainLoss: 6.1147e-03   valLoss:2.2850e-01  time: 1.73e-01\n",
      "epoch: 40   trainLoss: 5.3042e-03   valLoss:2.3038e-01  time: 1.74e-01\n",
      "epoch: 41   trainLoss: 5.3936e-03   valLoss:2.2788e-01  time: 1.74e-01\n",
      "epoch: 42   trainLoss: 4.4071e-03   valLoss:2.3003e-01  time: 1.73e-01\n",
      "epoch: 43   trainLoss: 3.7351e-03   valLoss:2.1957e-01  time: 1.75e-01\n",
      "epoch: 44   trainLoss: 3.3481e-03   valLoss:2.0760e-01  time: 1.72e-01\n",
      "epoch: 45   trainLoss: 3.2061e-03   valLoss:2.1331e-01  time: 1.74e-01\n",
      "epoch: 46   trainLoss: 3.1350e-03   valLoss:2.1097e-01  time: 1.76e-01\n",
      "epoch: 47   trainLoss: 2.5648e-03   valLoss:2.0119e-01  time: 1.72e-01\n",
      "epoch: 48   trainLoss: 2.2418e-03   valLoss:1.9426e-01  time: 1.73e-01\n",
      "epoch: 49   trainLoss: 2.2465e-03   valLoss:1.9083e-01  time: 1.74e-01\n",
      "epoch: 50   trainLoss: 2.1722e-03   valLoss:1.9173e-01  time: 1.74e-01\n",
      "epoch: 51   trainLoss: 2.0758e-03   valLoss:1.7858e-01  time: 1.75e-01\n",
      "epoch: 52   trainLoss: 1.8275e-03   valLoss:1.7620e-01  time: 1.76e-01\n",
      "epoch: 53   trainLoss: 1.4627e-03   valLoss:1.7588e-01  time: 1.75e-01\n",
      "epoch: 54   trainLoss: 1.4362e-03   valLoss:1.7230e-01  time: 1.76e-01\n",
      "epoch: 55   trainLoss: 1.3869e-03   valLoss:1.7298e-01  time: 1.73e-01\n",
      "epoch: 56   trainLoss: 1.4479e-03   valLoss:1.6433e-01  time: 1.73e-01\n",
      "epoch: 57   trainLoss: 1.3065e-03   valLoss:1.6421e-01  time: 1.74e-01\n",
      "epoch: 58   trainLoss: 1.1882e-03   valLoss:1.5711e-01  time: 1.73e-01\n",
      "epoch: 59   trainLoss: 9.5927e-04   valLoss:1.5520e-01  time: 1.75e-01\n",
      "epoch: 60   trainLoss: 8.5691e-04   valLoss:1.4943e-01  time: 1.76e-01\n",
      "epoch: 61   trainLoss: 8.4925e-04   valLoss:1.3962e-01  time: 1.76e-01\n",
      "epoch: 62   trainLoss: 8.3514e-04   valLoss:1.4118e-01  time: 1.73e-01\n",
      "epoch: 63   trainLoss: 8.4036e-04   valLoss:1.3160e-01  time: 1.71e-01\n",
      "epoch: 64   trainLoss: 9.7261e-04   valLoss:1.3390e-01  time: 1.75e-01\n",
      "epoch: 65   trainLoss: 1.1468e-03   valLoss:1.2227e-01  time: 1.73e-01\n",
      "epoch: 66   trainLoss: 1.4899e-03   valLoss:1.3207e-01  time: 1.77e-01\n",
      "epoch: 67   trainLoss: 2.2530e-03   valLoss:1.1183e-01  time: 1.72e-01\n",
      "epoch: 68   trainLoss: 3.4749e-03   valLoss:1.3990e-01  time: 1.73e-01\n",
      "epoch: 69   trainLoss: 4.8208e-03   valLoss:1.1072e-01  time: 1.77e-01\n",
      "epoch: 70   trainLoss: 4.0268e-03   valLoss:1.1751e-01  time: 1.72e-01\n",
      "epoch: 71   trainLoss: 1.3410e-03   valLoss:1.1351e-01  time: 1.73e-01\n",
      "epoch: 72   trainLoss: 1.0559e-03   valLoss:9.9712e-02  time: 1.74e-01\n",
      "epoch: 73   trainLoss: 2.4719e-03   valLoss:1.0263e-01  time: 1.74e-01\n",
      "epoch: 74   trainLoss: 9.7983e-04   valLoss:9.8672e-02  time: 1.75e-01\n",
      "epoch: 75   trainLoss: 9.0081e-04   valLoss:8.6599e-02  time: 1.73e-01\n",
      "epoch: 76   trainLoss: 1.5875e-03   valLoss:9.0387e-02  time: 1.72e-01\n",
      "epoch: 77   trainLoss: 5.7988e-04   valLoss:9.1538e-02  time: 1.75e-01\n",
      "epoch: 78   trainLoss: 1.0174e-03   valLoss:7.8644e-02  time: 1.72e-01\n",
      "epoch: 79   trainLoss: 1.0034e-03   valLoss:8.1668e-02  time: 1.74e-01\n",
      "epoch: 80   trainLoss: 5.7105e-04   valLoss:8.0622e-02  time: 1.77e-01\n",
      "epoch: 81   trainLoss: 9.4133e-04   valLoss:7.1097e-02  time: 1.74e-01\n",
      "epoch: 82   trainLoss: 5.6538e-04   valLoss:7.2468e-02  time: 1.77e-01\n",
      "epoch: 83   trainLoss: 7.0766e-04   valLoss:7.0390e-02  time: 1.78e-01\n",
      "epoch: 84   trainLoss: 5.7560e-04   valLoss:6.7084e-02  time: 1.79e-01\n",
      "epoch: 85   trainLoss: 3.9937e-04   valLoss:6.8067e-02  time: 1.77e-01\n",
      "epoch: 86   trainLoss: 6.5102e-04   valLoss:6.3542e-02  time: 1.74e-01\n",
      "epoch: 87   trainLoss: 3.1403e-04   valLoss:6.4757e-02  time: 1.74e-01\n",
      "epoch: 88   trainLoss: 4.4152e-04   valLoss:6.2971e-02  time: 1.75e-01\n",
      "epoch: 89   trainLoss: 4.7547e-04   valLoss:6.0846e-02  time: 1.76e-01\n",
      "epoch: 90   trainLoss: 2.8880e-04   valLoss:6.1647e-02  time: 1.73e-01\n",
      "epoch: 91   trainLoss: 5.1359e-04   valLoss:5.8916e-02  time: 1.73e-01\n",
      "epoch: 92   trainLoss: 9.3786e-04   valLoss:5.6816e-02  time: 1.73e-01\n",
      "epoch: 93   trainLoss: 2.8187e-03   valLoss:6.9590e-02  time: 1.76e-01\n",
      "epoch: 94   trainLoss: 1.0749e-02   valLoss:5.1496e-02  time: 1.70e-01\n",
      "epoch: 95   trainLoss: 2.5206e-02   valLoss:1.0812e-01  time: 1.76e-01\n",
      "epoch: 96   trainLoss: 5.0190e-02   valLoss:7.8899e-02  time: 1.77e-01\n",
      "epoch: 97   trainLoss: 4.0673e-02   valLoss:9.6814e-02  time: 1.79e-01\n",
      "epoch: 98   trainLoss: 4.3715e-02   valLoss:8.8851e-02  time: 1.75e-01\n",
      "epoch: 99   trainLoss: 4.5301e-02   valLoss:1.6703e-01  time: 1.74e-01\n",
      "loading checkpoint 94\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 8.2807e-01   valLoss:9.4810e-01  time: 1.77e-01\n",
      "epoch: 1   trainLoss: 5.8897e-01   valLoss:1.1165e+00  time: 1.78e-01\n",
      "epoch: 2   trainLoss: 3.5219e-01   valLoss:7.7536e+00  time: 1.79e-01\n",
      "epoch: 3   trainLoss: 3.2146e-01   valLoss:2.6048e+00  time: 1.77e-01\n",
      "epoch: 4   trainLoss: 2.1524e-01   valLoss:8.1349e-01  time: 1.76e-01\n",
      "epoch: 5   trainLoss: 1.4255e-01   valLoss:9.0637e-01  time: 1.81e-01\n",
      "epoch: 6   trainLoss: 9.3193e-02   valLoss:8.9038e-01  time: 1.78e-01\n",
      "epoch: 7   trainLoss: 8.0424e-02   valLoss:5.4113e-01  time: 1.79e-01\n",
      "epoch: 8   trainLoss: 7.6579e-02   valLoss:3.8041e-01  time: 1.78e-01\n",
      "epoch: 9   trainLoss: 6.2672e-02   valLoss:3.3712e-01  time: 1.77e-01\n",
      "epoch: 10   trainLoss: 6.6632e-02   valLoss:2.8754e-01  time: 1.77e-01\n",
      "epoch: 11   trainLoss: 6.2320e-02   valLoss:2.9299e-01  time: 1.80e-01\n",
      "epoch: 12   trainLoss: 5.0872e-02   valLoss:2.2055e-01  time: 1.82e-01\n",
      "epoch: 13   trainLoss: 4.5013e-02   valLoss:1.0102e-01  time: 1.87e-01\n",
      "epoch: 14   trainLoss: 3.6984e-02   valLoss:8.0451e-02  time: 1.75e-01\n",
      "epoch: 15   trainLoss: 3.3269e-02   valLoss:9.7904e-02  time: 1.81e-01\n",
      "epoch: 16   trainLoss: 3.3735e-02   valLoss:8.8289e-02  time: 1.75e-01\n",
      "epoch: 17   trainLoss: 2.6854e-02   valLoss:7.3763e-02  time: 1.75e-01\n",
      "epoch: 18   trainLoss: 2.4784e-02   valLoss:6.6733e-02  time: 1.75e-01\n",
      "epoch: 19   trainLoss: 2.2725e-02   valLoss:6.4863e-02  time: 1.74e-01\n",
      "epoch: 20   trainLoss: 2.1077e-02   valLoss:5.7998e-02  time: 1.75e-01\n",
      "epoch: 21   trainLoss: 1.9389e-02   valLoss:4.7874e-02  time: 1.74e-01\n",
      "epoch: 22   trainLoss: 1.7498e-02   valLoss:3.4064e-02  time: 1.75e-01\n",
      "epoch: 23   trainLoss: 1.5751e-02   valLoss:3.7590e-02  time: 1.80e-01\n",
      "epoch: 24   trainLoss: 1.4974e-02   valLoss:7.1844e-02  time: 1.74e-01\n",
      "epoch: 25   trainLoss: 1.4760e-02   valLoss:7.6586e-02  time: 1.75e-01\n",
      "epoch: 26   trainLoss: 1.3056e-02   valLoss:5.5678e-02  time: 1.93e-01\n",
      "epoch: 27   trainLoss: 1.2058e-02   valLoss:4.4350e-02  time: 1.80e-01\n",
      "epoch: 28   trainLoss: 1.1524e-02   valLoss:3.9165e-02  time: 1.90e-01\n",
      "epoch: 29   trainLoss: 1.0537e-02   valLoss:3.2635e-02  time: 1.74e-01\n",
      "epoch: 30   trainLoss: 9.7501e-03   valLoss:2.5764e-02  time: 1.76e-01\n",
      "epoch: 31   trainLoss: 9.1594e-03   valLoss:1.6124e-02  time: 1.79e-01\n",
      "epoch: 32   trainLoss: 8.1941e-03   valLoss:1.3398e-02  time: 1.77e-01\n",
      "epoch: 33   trainLoss: 7.5475e-03   valLoss:1.4056e-02  time: 1.83e-01\n",
      "epoch: 34   trainLoss: 7.0991e-03   valLoss:1.3483e-02  time: 1.75e-01\n",
      "epoch: 35   trainLoss: 6.6449e-03   valLoss:2.0257e-02  time: 1.73e-01\n",
      "epoch: 36   trainLoss: 6.3892e-03   valLoss:2.9588e-02  time: 1.77e-01\n",
      "epoch: 37   trainLoss: 5.8871e-03   valLoss:3.4918e-02  time: 1.75e-01\n",
      "epoch: 38   trainLoss: 5.4050e-03   valLoss:4.1678e-02  time: 1.75e-01\n",
      "epoch: 39   trainLoss: 5.2659e-03   valLoss:4.5263e-02  time: 2.04e-01\n",
      "epoch: 40   trainLoss: 4.9124e-03   valLoss:5.5155e-02  time: 1.76e-01\n",
      "epoch: 41   trainLoss: 4.6285e-03   valLoss:5.9219e-02  time: 1.77e-01\n",
      "epoch: 42   trainLoss: 4.4149e-03   valLoss:5.0725e-02  time: 1.79e-01\n",
      "epoch: 43   trainLoss: 4.2510e-03   valLoss:5.2152e-02  time: 1.76e-01\n",
      "epoch: 44   trainLoss: 4.2307e-03   valLoss:4.3867e-02  time: 1.77e-01\n",
      "epoch: 45   trainLoss: 4.0198e-03   valLoss:3.9638e-02  time: 1.78e-01\n",
      "epoch: 46   trainLoss: 3.7590e-03   valLoss:3.2469e-02  time: 1.77e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47   trainLoss: 3.6137e-03   valLoss:2.5914e-02  time: 1.79e-01\n",
      "epoch: 48   trainLoss: 3.4810e-03   valLoss:2.0141e-02  time: 1.74e-01\n",
      "epoch: 49   trainLoss: 3.3515e-03   valLoss:1.6957e-02  time: 1.76e-01\n",
      "epoch: 50   trainLoss: 3.2261e-03   valLoss:1.9338e-02  time: 1.76e-01\n",
      "epoch: 51   trainLoss: 3.2332e-03   valLoss:2.0490e-02  time: 1.75e-01\n",
      "epoch: 52   trainLoss: 3.2244e-03   valLoss:2.7332e-02  time: 1.75e-01\n",
      "epoch: 53   trainLoss: 3.2978e-03   valLoss:2.4031e-02  time: 1.78e-01\n",
      "epoch: 54   trainLoss: 3.6553e-03   valLoss:3.4863e-02  time: 1.89e-01\n",
      "epoch: 55   trainLoss: 4.5812e-03   valLoss:2.8094e-02  time: 1.77e-01\n",
      "epoch: 56   trainLoss: 6.6294e-03   valLoss:4.0125e-02  time: 1.77e-01\n",
      "epoch: 57   trainLoss: 9.1923e-03   valLoss:2.7894e-02  time: 1.78e-01\n",
      "epoch: 58   trainLoss: 9.2887e-03   valLoss:2.5544e-02  time: 1.74e-01\n",
      "epoch: 59   trainLoss: 4.1530e-03   valLoss:1.5860e-02  time: 1.75e-01\n",
      "epoch: 60   trainLoss: 3.0343e-03   valLoss:2.2992e-02  time: 1.76e-01\n",
      "epoch: 61   trainLoss: 5.9610e-03   valLoss:1.6917e-02  time: 1.74e-01\n",
      "epoch: 62   trainLoss: 3.7117e-03   valLoss:1.6613e-02  time: 1.74e-01\n",
      "epoch: 63   trainLoss: 2.8514e-03   valLoss:2.4218e-02  time: 1.79e-01\n",
      "epoch: 64   trainLoss: 4.3792e-03   valLoss:2.3906e-02  time: 1.78e-01\n",
      "epoch: 65   trainLoss: 2.6993e-03   valLoss:2.7607e-02  time: 1.77e-01\n",
      "epoch: 66   trainLoss: 3.0301e-03   valLoss:2.9792e-02  time: 1.76e-01\n",
      "epoch: 67   trainLoss: 3.1995e-03   valLoss:3.5627e-02  time: 1.87e-01\n",
      "epoch: 68   trainLoss: 2.2933e-03   valLoss:4.0322e-02  time: 1.75e-01\n",
      "epoch: 69   trainLoss: 2.8815e-03   valLoss:3.1422e-02  time: 1.77e-01\n",
      "epoch: 70   trainLoss: 2.3526e-03   valLoss:3.0206e-02  time: 1.83e-01\n",
      "epoch: 71   trainLoss: 2.3138e-03   valLoss:3.3096e-02  time: 1.77e-01\n",
      "epoch: 72   trainLoss: 2.3869e-03   valLoss:2.7011e-02  time: 1.77e-01\n",
      "epoch: 73   trainLoss: 1.9647e-03   valLoss:2.6049e-02  time: 1.86e-01\n",
      "epoch: 74   trainLoss: 2.1811e-03   valLoss:2.8632e-02  time: 1.77e-01\n",
      "epoch: 75   trainLoss: 2.0018e-03   valLoss:2.8271e-02  time: 1.76e-01\n",
      "epoch: 76   trainLoss: 1.7487e-03   valLoss:3.3131e-02  time: 1.77e-01\n",
      "epoch: 77   trainLoss: 1.9732e-03   valLoss:3.9771e-02  time: 1.74e-01\n",
      "epoch: 78   trainLoss: 1.7346e-03   valLoss:4.4761e-02  time: 1.80e-01\n",
      "epoch: 79   trainLoss: 1.6005e-03   valLoss:4.8134e-02  time: 1.75e-01\n",
      "epoch: 80   trainLoss: 1.7675e-03   valLoss:5.7798e-02  time: 1.77e-01\n",
      "epoch: 81   trainLoss: 1.5633e-03   valLoss:6.0170e-02  time: 1.77e-01\n",
      "epoch: 82   trainLoss: 1.5283e-03   valLoss:5.8560e-02  time: 1.77e-01\n",
      "epoch: 83   trainLoss: 1.5983e-03   valLoss:5.7718e-02  time: 1.80e-01\n",
      "epoch: 84   trainLoss: 1.3959e-03   valLoss:5.4425e-02  time: 1.80e-01\n",
      "epoch: 85   trainLoss: 1.4999e-03   valLoss:4.9134e-02  time: 1.74e-01\n",
      "epoch: 86   trainLoss: 1.9832e-03   valLoss:5.2948e-02  time: 1.78e-01\n",
      "epoch: 87   trainLoss: 2.4241e-03   valLoss:4.8690e-02  time: 1.82e-01\n",
      "epoch: 88   trainLoss: 5.2166e-03   valLoss:6.1124e-02  time: 1.77e-01\n",
      "epoch: 89   trainLoss: 1.3434e-02   valLoss:6.2708e-02  time: 1.76e-01\n",
      "epoch: 90   trainLoss: 2.0340e-02   valLoss:5.3193e-02  time: 1.82e-01\n",
      "epoch: 91   trainLoss: 1.1553e-02   valLoss:3.2734e-02  time: 1.77e-01\n",
      "epoch: 92   trainLoss: 6.6358e-03   valLoss:4.0431e-02  time: 1.77e-01\n",
      "epoch: 93   trainLoss: 1.1258e-02   valLoss:4.1388e-02  time: 1.80e-01\n",
      "epoch: 94   trainLoss: 4.1207e-03   valLoss:4.6172e-02  time: 1.77e-01\n",
      "epoch: 95   trainLoss: 7.3335e-03   valLoss:3.1390e-02  time: 1.84e-01\n",
      "epoch: 96   trainLoss: 4.6696e-03   valLoss:3.8595e-02  time: 1.75e-01\n",
      "epoch: 97   trainLoss: 5.4695e-03   valLoss:4.2895e-02  time: 1.79e-01\n",
      "epoch: 98   trainLoss: 4.0398e-03   valLoss:4.3638e-02  time: 1.76e-01\n",
      "epoch: 99   trainLoss: 4.2757e-03   valLoss:3.2191e-02  time: 1.74e-01\n",
      "loading checkpoint 32\n",
      "trained 38 random forest models in 4.56 seconds\n",
      "loaded train set of size 452\n",
      "epoch: 0   trainLoss: 9.2763e-01   valLoss:1.0179e+00  time: 3.88e+00\n",
      "epoch: 1   trainLoss: 6.1881e-01   valLoss:9.4535e-01  time: 4.01e+00\n",
      "epoch: 2   trainLoss: 4.0427e-01   valLoss:8.7305e-01  time: 3.88e+00\n",
      "epoch: 3   trainLoss: 2.9548e-01   valLoss:8.5616e-01  time: 3.85e+00\n",
      "epoch: 4   trainLoss: 2.7101e-01   valLoss:8.9913e-01  time: 3.84e+00\n",
      "epoch: 5   trainLoss: 2.3508e-01   valLoss:9.5700e-01  time: 3.85e+00\n",
      "epoch: 6   trainLoss: 1.9899e-01   valLoss:9.7810e-01  time: 3.82e+00\n",
      "epoch: 7   trainLoss: 1.7665e-01   valLoss:9.1809e-01  time: 3.83e+00\n",
      "epoch: 8   trainLoss: 1.5452e-01   valLoss:8.4710e-01  time: 3.85e+00\n",
      "epoch: 9   trainLoss: 1.4670e-01   valLoss:7.7734e-01  time: 3.85e+00\n",
      "epoch: 10   trainLoss: 1.2957e-01   valLoss:6.8195e-01  time: 3.84e+00\n",
      "epoch: 11   trainLoss: 1.2211e-01   valLoss:5.9552e-01  time: 3.81e+00\n",
      "epoch: 12   trainLoss: 1.0370e-01   valLoss:4.8426e-01  time: 3.87e+00\n",
      "epoch: 13   trainLoss: 1.0418e-01   valLoss:4.1794e-01  time: 3.88e+00\n",
      "epoch: 14   trainLoss: 1.0801e-01   valLoss:4.0280e-01  time: 3.84e+00\n",
      "epoch: 15   trainLoss: 8.9348e-02   valLoss:4.0053e-01  time: 3.83e+00\n",
      "epoch: 16   trainLoss: 8.4073e-02   valLoss:3.7268e-01  time: 3.84e+00\n",
      "epoch: 17   trainLoss: 7.5243e-02   valLoss:3.7685e-01  time: 3.84e+00\n",
      "epoch: 18   trainLoss: 7.8885e-02   valLoss:3.7648e-01  time: 3.87e+00\n",
      "epoch: 19   trainLoss: 6.6496e-02   valLoss:3.9285e-01  time: 3.85e+00\n",
      "epoch: 20   trainLoss: 6.2305e-02   valLoss:3.9491e-01  time: 3.86e+00\n",
      "epoch: 21   trainLoss: 6.2100e-02   valLoss:4.6669e-01  time: 3.83e+00\n",
      "epoch: 22   trainLoss: 5.7616e-02   valLoss:4.0780e-01  time: 3.81e+00\n",
      "epoch: 23   trainLoss: 5.9636e-02   valLoss:4.5269e-01  time: 3.85e+00\n",
      "epoch: 24   trainLoss: 6.0421e-02   valLoss:4.2117e-01  time: 3.85e+00\n",
      "epoch: 25   trainLoss: 6.1805e-02   valLoss:3.9482e-01  time: 3.90e+00\n",
      "epoch: 26   trainLoss: 6.3161e-02   valLoss:4.6725e-01  time: 3.85e+00\n",
      "epoch: 27   trainLoss: 5.2609e-02   valLoss:4.5371e-01  time: 3.88e+00\n",
      "epoch: 28   trainLoss: 5.2934e-02   valLoss:3.7904e-01  time: 3.86e+00\n",
      "epoch: 29   trainLoss: 5.6228e-02   valLoss:3.7185e-01  time: 3.84e+00\n",
      "epoch: 30   trainLoss: 5.5315e-02   valLoss:3.3876e-01  time: 3.87e+00\n",
      "epoch: 31   trainLoss: 4.3022e-02   valLoss:4.0729e-01  time: 3.83e+00\n",
      "epoch: 32   trainLoss: 5.0509e-02   valLoss:4.0937e-01  time: 3.84e+00\n",
      "epoch: 33   trainLoss: 5.7157e-02   valLoss:4.0982e-01  time: 3.85e+00\n",
      "epoch: 34   trainLoss: 4.7322e-02   valLoss:4.5250e-01  time: 3.82e+00\n",
      "epoch: 35   trainLoss: 4.8058e-02   valLoss:4.4575e-01  time: 3.85e+00\n",
      "epoch: 36   trainLoss: 4.1334e-02   valLoss:4.7198e-01  time: 3.82e+00\n",
      "epoch: 37   trainLoss: 4.4878e-02   valLoss:3.3741e-01  time: 3.90e+00\n",
      "epoch: 38   trainLoss: 4.8805e-02   valLoss:3.4680e-01  time: 3.82e+00\n",
      "epoch: 39   trainLoss: 3.5374e-02   valLoss:3.6808e-01  time: 3.81e+00\n",
      "epoch: 40   trainLoss: 4.0151e-02   valLoss:3.6126e-01  time: 3.84e+00\n",
      "epoch: 41   trainLoss: 3.7384e-02   valLoss:3.8230e-01  time: 3.84e+00\n",
      "epoch: 42   trainLoss: 4.1195e-02   valLoss:3.4263e-01  time: 3.80e+00\n",
      "epoch: 43   trainLoss: 3.3326e-02   valLoss:3.6805e-01  time: 3.82e+00\n",
      "epoch: 44   trainLoss: 3.1036e-02   valLoss:3.5355e-01  time: 3.79e+00\n",
      "epoch: 45   trainLoss: 3.6073e-02   valLoss:3.6701e-01  time: 3.86e+00\n",
      "epoch: 46   trainLoss: 3.5524e-02   valLoss:2.9647e-01  time: 3.87e+00\n",
      "epoch: 47   trainLoss: 2.7386e-02   valLoss:2.7814e-01  time: 3.80e+00\n",
      "epoch: 48   trainLoss: 3.2924e-02   valLoss:2.8359e-01  time: 3.81e+00\n",
      "epoch: 49   trainLoss: 2.9885e-02   valLoss:2.8852e-01  time: 3.87e+00\n",
      "epoch: 50   trainLoss: 2.6824e-02   valLoss:3.3622e-01  time: 3.79e+00\n",
      "epoch: 51   trainLoss: 2.4944e-02   valLoss:3.3639e-01  time: 3.79e+00\n",
      "epoch: 52   trainLoss: 2.2956e-02   valLoss:2.9353e-01  time: 3.76e+00\n",
      "epoch: 53   trainLoss: 2.2319e-02   valLoss:2.7108e-01  time: 3.82e+00\n",
      "epoch: 54   trainLoss: 2.7336e-02   valLoss:3.0829e-01  time: 3.83e+00\n",
      "epoch: 55   trainLoss: 2.1219e-02   valLoss:3.1656e-01  time: 3.80e+00\n",
      "epoch: 56   trainLoss: 2.3473e-02   valLoss:2.6677e-01  time: 3.86e+00\n",
      "epoch: 57   trainLoss: 3.9232e-02   valLoss:2.5573e-01  time: 3.86e+00\n",
      "epoch: 58   trainLoss: 3.3937e-02   valLoss:2.7302e-01  time: 3.78e+00\n",
      "epoch: 59   trainLoss: 2.8340e-02   valLoss:2.2911e-01  time: 3.81e+00\n",
      "epoch: 60   trainLoss: 3.2551e-02   valLoss:2.9631e-01  time: 3.78e+00\n",
      "epoch: 61   trainLoss: 3.1432e-02   valLoss:1.9095e-01  time: 3.84e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 62   trainLoss: 4.1508e-02   valLoss:2.2562e-01  time: 3.86e+00\n",
      "epoch: 63   trainLoss: 3.4572e-02   valLoss:3.5142e-01  time: 3.80e+00\n",
      "epoch: 64   trainLoss: 3.6134e-02   valLoss:2.4152e-01  time: 3.81e+00\n",
      "epoch: 65   trainLoss: 3.2443e-02   valLoss:1.6048e-01  time: 3.86e+00\n",
      "epoch: 66   trainLoss: 3.4776e-02   valLoss:1.9285e-01  time: 3.89e+00\n",
      "epoch: 67   trainLoss: 2.5899e-02   valLoss:1.8871e-01  time: 3.81e+00\n",
      "epoch: 68   trainLoss: 2.7614e-02   valLoss:2.0747e-01  time: 3.84e+00\n",
      "epoch: 69   trainLoss: 2.2523e-02   valLoss:2.0548e-01  time: 3.87e+00\n",
      "epoch: 70   trainLoss: 2.1115e-02   valLoss:2.2073e-01  time: 3.84e+00\n",
      "epoch: 71   trainLoss: 4.1177e-02   valLoss:1.5243e-01  time: 3.89e+00\n",
      "epoch: 72   trainLoss: 3.0718e-02   valLoss:1.4279e-01  time: 3.85e+00\n",
      "epoch: 73   trainLoss: 2.8420e-02   valLoss:1.4602e-01  time: 3.84e+00\n",
      "epoch: 74   trainLoss: 3.3931e-02   valLoss:1.5412e-01  time: 3.86e+00\n",
      "epoch: 75   trainLoss: 2.7623e-02   valLoss:1.7420e-01  time: 3.86e+00\n",
      "epoch: 76   trainLoss: 3.4653e-02   valLoss:1.5982e-01  time: 3.78e+00\n",
      "epoch: 77   trainLoss: 3.2659e-02   valLoss:1.5986e-01  time: 3.78e+00\n",
      "epoch: 78   trainLoss: 2.3824e-02   valLoss:1.3671e-01  time: 3.82e+00\n",
      "epoch: 79   trainLoss: 3.5712e-02   valLoss:1.5226e-01  time: 3.79e+00\n",
      "epoch: 80   trainLoss: 2.1449e-02   valLoss:1.0756e-01  time: 3.77e+00\n",
      "epoch: 81   trainLoss: 2.1614e-02   valLoss:9.6823e-02  time: 3.83e+00\n",
      "epoch: 82   trainLoss: 2.1042e-02   valLoss:1.5406e-01  time: 3.80e+00\n",
      "epoch: 83   trainLoss: 1.7928e-02   valLoss:1.0334e-01  time: 3.81e+00\n",
      "epoch: 84   trainLoss: 2.3089e-02   valLoss:1.3920e-01  time: 3.82e+00\n",
      "epoch: 85   trainLoss: 1.9714e-02   valLoss:9.7022e-02  time: 3.82e+00\n",
      "epoch: 86   trainLoss: 1.7482e-02   valLoss:8.4522e-02  time: 3.82e+00\n",
      "epoch: 87   trainLoss: 1.9917e-02   valLoss:1.3730e-01  time: 3.86e+00\n",
      "epoch: 88   trainLoss: 2.1549e-02   valLoss:1.2587e-01  time: 3.86e+00\n",
      "epoch: 89   trainLoss: 1.6254e-02   valLoss:1.1306e-01  time: 3.89e+00\n",
      "epoch: 90   trainLoss: 2.5420e-02   valLoss:1.2430e-01  time: 3.79e+00\n",
      "epoch: 91   trainLoss: 2.4806e-02   valLoss:7.4589e-02  time: 3.78e+00\n",
      "epoch: 92   trainLoss: 2.2424e-02   valLoss:8.4207e-02  time: 3.77e+00\n",
      "epoch: 93   trainLoss: 2.1109e-02   valLoss:7.5242e-02  time: 3.84e+00\n",
      "epoch: 94   trainLoss: 1.8863e-02   valLoss:6.2649e-02  time: 3.80e+00\n",
      "epoch: 95   trainLoss: 1.9183e-02   valLoss:7.3283e-02  time: 3.83e+00\n",
      "epoch: 96   trainLoss: 2.1630e-02   valLoss:1.0003e-01  time: 3.73e+00\n",
      "epoch: 97   trainLoss: 1.5348e-02   valLoss:8.0454e-02  time: 3.89e+00\n",
      "epoch: 98   trainLoss: 1.8871e-02   valLoss:8.8607e-02  time: 3.85e+00\n",
      "epoch: 99   trainLoss: 2.3515e-02   valLoss:1.3847e-01  time: 3.79e+00\n",
      "loading checkpoint 94\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 7.2980e-01   valLoss:1.0976e+00  time: 3.88e+00\n",
      "epoch: 1   trainLoss: 3.9519e-01   valLoss:4.9939e-01  time: 3.87e+00\n",
      "epoch: 2   trainLoss: 2.1965e-01   valLoss:4.9902e-01  time: 3.87e+00\n",
      "epoch: 3   trainLoss: 1.7566e-01   valLoss:2.6340e-01  time: 3.88e+00\n",
      "epoch: 4   trainLoss: 1.4582e-01   valLoss:2.2137e-01  time: 3.94e+00\n",
      "epoch: 5   trainLoss: 1.3897e-01   valLoss:2.1406e-01  time: 3.86e+00\n",
      "epoch: 6   trainLoss: 1.1957e-01   valLoss:1.9999e-01  time: 3.90e+00\n",
      "epoch: 7   trainLoss: 1.1569e-01   valLoss:1.2144e-01  time: 3.84e+00\n",
      "epoch: 8   trainLoss: 1.0487e-01   valLoss:3.1501e-01  time: 3.85e+00\n",
      "epoch: 9   trainLoss: 9.9507e-02   valLoss:1.4413e-01  time: 3.86e+00\n",
      "epoch: 10   trainLoss: 9.8092e-02   valLoss:2.2245e-01  time: 3.85e+00\n",
      "epoch: 11   trainLoss: 7.8830e-02   valLoss:2.8166e-01  time: 3.83e+00\n",
      "epoch: 12   trainLoss: 7.8264e-02   valLoss:1.5595e-01  time: 3.85e+00\n",
      "epoch: 13   trainLoss: 7.0340e-02   valLoss:1.6519e-01  time: 3.87e+00\n",
      "epoch: 14   trainLoss: 6.8412e-02   valLoss:1.3414e-01  time: 3.83e+00\n",
      "epoch: 15   trainLoss: 6.2939e-02   valLoss:7.6240e-02  time: 3.86e+00\n",
      "epoch: 16   trainLoss: 5.7896e-02   valLoss:6.3390e-02  time: 3.86e+00\n",
      "epoch: 17   trainLoss: 5.7606e-02   valLoss:5.2883e-02  time: 3.88e+00\n",
      "epoch: 18   trainLoss: 5.8348e-02   valLoss:1.0399e-01  time: 3.91e+00\n",
      "epoch: 19   trainLoss: 5.2485e-02   valLoss:6.9755e-02  time: 3.86e+00\n",
      "epoch: 20   trainLoss: 4.8625e-02   valLoss:9.2368e-02  time: 3.88e+00\n",
      "epoch: 21   trainLoss: 4.5895e-02   valLoss:6.4667e-02  time: 3.85e+00\n",
      "epoch: 22   trainLoss: 5.8723e-02   valLoss:7.2876e-02  time: 3.90e+00\n",
      "epoch: 23   trainLoss: 4.3194e-02   valLoss:5.5537e-02  time: 3.94e+00\n",
      "epoch: 24   trainLoss: 4.0686e-02   valLoss:5.5191e-02  time: 3.92e+00\n",
      "epoch: 25   trainLoss: 3.7845e-02   valLoss:4.7562e-02  time: 3.85e+00\n",
      "epoch: 26   trainLoss: 4.3205e-02   valLoss:6.6272e-02  time: 3.89e+00\n",
      "epoch: 27   trainLoss: 4.7071e-02   valLoss:5.0169e-02  time: 3.98e+00\n",
      "epoch: 28   trainLoss: 4.2065e-02   valLoss:6.3695e-02  time: 3.92e+00\n",
      "epoch: 29   trainLoss: 3.4979e-02   valLoss:7.7459e-02  time: 3.93e+00\n",
      "epoch: 30   trainLoss: 3.5071e-02   valLoss:8.9911e-02  time: 3.97e+00\n",
      "epoch: 31   trainLoss: 3.7190e-02   valLoss:9.2834e-02  time: 3.92e+00\n",
      "epoch: 32   trainLoss: 6.5614e-02   valLoss:6.7775e-02  time: 3.96e+00\n",
      "epoch: 33   trainLoss: 6.3230e-02   valLoss:7.5438e-02  time: 3.93e+00\n",
      "epoch: 34   trainLoss: 4.7422e-02   valLoss:1.6697e-01  time: 3.93e+00\n",
      "epoch: 35   trainLoss: 4.4912e-02   valLoss:9.1047e-02  time: 3.82e+00\n",
      "epoch: 36   trainLoss: 4.3205e-02   valLoss:9.8286e-02  time: 3.88e+00\n",
      "epoch: 37   trainLoss: 4.6075e-02   valLoss:1.3435e-01  time: 3.92e+00\n",
      "epoch: 38   trainLoss: 3.6787e-02   valLoss:1.3299e-01  time: 3.90e+00\n",
      "epoch: 39   trainLoss: 3.9179e-02   valLoss:1.3228e-01  time: 3.84e+00\n",
      "epoch: 40   trainLoss: 3.5654e-02   valLoss:9.0676e-02  time: 3.82e+00\n",
      "epoch: 41   trainLoss: 3.5212e-02   valLoss:6.9747e-02  time: 3.92e+00\n",
      "epoch: 42   trainLoss: 3.3311e-02   valLoss:4.6194e-02  time: 3.94e+00\n",
      "epoch: 43   trainLoss: 2.8173e-02   valLoss:4.4713e-02  time: 3.86e+00\n",
      "epoch: 44   trainLoss: 4.6610e-02   valLoss:8.1377e-02  time: 3.85e+00\n",
      "epoch: 45   trainLoss: 2.8471e-02   valLoss:7.1207e-02  time: 3.81e+00\n",
      "epoch: 46   trainLoss: 2.5918e-02   valLoss:7.0651e-02  time: 3.85e+00\n",
      "epoch: 47   trainLoss: 2.4191e-02   valLoss:6.3873e-02  time: 3.89e+00\n",
      "epoch: 48   trainLoss: 2.4271e-02   valLoss:4.7006e-02  time: 3.97e+00\n",
      "epoch: 49   trainLoss: 2.6261e-02   valLoss:4.8368e-02  time: 3.94e+00\n",
      "epoch: 50   trainLoss: 2.3815e-02   valLoss:5.7001e-02  time: 3.95e+00\n",
      "epoch: 51   trainLoss: 2.4780e-02   valLoss:5.6729e-02  time: 3.94e+00\n",
      "epoch: 52   trainLoss: 2.6564e-02   valLoss:7.2860e-02  time: 3.88e+00\n",
      "epoch: 53   trainLoss: 2.3925e-02   valLoss:5.7881e-02  time: 3.88e+00\n",
      "epoch: 54   trainLoss: 2.1193e-02   valLoss:4.3469e-02  time: 3.91e+00\n",
      "epoch: 55   trainLoss: 1.9540e-02   valLoss:4.7357e-02  time: 3.74e+00\n",
      "epoch: 56   trainLoss: 3.6834e-02   valLoss:5.2803e-02  time: 3.75e+00\n",
      "epoch: 57   trainLoss: 2.3942e-02   valLoss:5.7705e-02  time: 3.73e+00\n",
      "epoch: 58   trainLoss: 3.4832e-02   valLoss:1.0049e-01  time: 3.75e+00\n",
      "epoch: 59   trainLoss: 2.6011e-02   valLoss:9.4917e-02  time: 3.76e+00\n",
      "epoch: 60   trainLoss: 4.3102e-02   valLoss:6.8854e-02  time: 3.79e+00\n",
      "epoch: 61   trainLoss: 2.4070e-02   valLoss:6.1913e-02  time: 3.88e+00\n",
      "epoch: 62   trainLoss: 2.5017e-02   valLoss:9.8253e-02  time: 3.84e+00\n",
      "epoch: 63   trainLoss: 2.4475e-02   valLoss:8.3853e-02  time: 3.86e+00\n",
      "epoch: 64   trainLoss: 3.2045e-02   valLoss:1.1545e-01  time: 3.86e+00\n",
      "epoch: 65   trainLoss: 2.7206e-02   valLoss:6.5686e-02  time: 3.91e+00\n",
      "epoch: 66   trainLoss: 2.5350e-02   valLoss:6.3817e-02  time: 3.87e+00\n",
      "epoch: 67   trainLoss: 2.2576e-02   valLoss:4.6420e-02  time: 3.80e+00\n",
      "epoch: 68   trainLoss: 2.0975e-02   valLoss:5.1488e-02  time: 3.77e+00\n",
      "epoch: 69   trainLoss: 2.1716e-02   valLoss:6.7874e-02  time: 3.83e+00\n",
      "epoch: 70   trainLoss: 1.7996e-02   valLoss:8.9862e-02  time: 3.85e+00\n",
      "epoch: 71   trainLoss: 1.8775e-02   valLoss:9.0014e-02  time: 3.81e+00\n",
      "epoch: 72   trainLoss: 1.8454e-02   valLoss:6.5767e-02  time: 3.85e+00\n",
      "epoch: 73   trainLoss: 1.7010e-02   valLoss:5.8516e-02  time: 3.81e+00\n",
      "epoch: 74   trainLoss: 1.7199e-02   valLoss:7.4875e-02  time: 3.82e+00\n",
      "epoch: 75   trainLoss: 2.0650e-02   valLoss:5.5155e-02  time: 3.83e+00\n",
      "epoch: 76   trainLoss: 1.7393e-02   valLoss:1.2344e-01  time: 3.90e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77   trainLoss: 1.9084e-02   valLoss:6.3373e-02  time: 3.90e+00\n",
      "epoch: 78   trainLoss: 2.7065e-02   valLoss:3.9371e-02  time: 3.91e+00\n",
      "epoch: 79   trainLoss: 3.3987e-02   valLoss:5.5758e-02  time: 3.86e+00\n",
      "epoch: 80   trainLoss: 2.7938e-02   valLoss:3.5886e-02  time: 3.89e+00\n",
      "epoch: 81   trainLoss: 3.6085e-02   valLoss:6.7476e-02  time: 3.92e+00\n",
      "epoch: 82   trainLoss: 2.7715e-02   valLoss:7.3077e-02  time: 3.85e+00\n",
      "epoch: 83   trainLoss: 3.8074e-02   valLoss:5.6913e-02  time: 3.94e+00\n",
      "epoch: 84   trainLoss: 3.6775e-02   valLoss:8.4913e-02  time: 3.91e+00\n",
      "epoch: 85   trainLoss: 2.4699e-02   valLoss:4.7103e-02  time: 3.92e+00\n",
      "epoch: 86   trainLoss: 2.8929e-02   valLoss:4.0646e-02  time: 3.86e+00\n",
      "epoch: 87   trainLoss: 2.4545e-02   valLoss:6.2950e-02  time: 3.94e+00\n",
      "epoch: 88   trainLoss: 4.3821e-02   valLoss:4.8617e-02  time: 3.92e+00\n",
      "epoch: 89   trainLoss: 2.2633e-02   valLoss:6.2796e-02  time: 3.94e+00\n",
      "epoch: 90   trainLoss: 2.9155e-02   valLoss:5.1755e-02  time: 4.04e+00\n",
      "epoch: 91   trainLoss: 1.8234e-02   valLoss:6.1651e-02  time: 3.97e+00\n",
      "epoch: 92   trainLoss: 2.2493e-02   valLoss:4.9960e-02  time: 3.86e+00\n",
      "epoch: 93   trainLoss: 2.1037e-02   valLoss:3.6790e-02  time: 3.85e+00\n",
      "epoch: 94   trainLoss: 2.4496e-02   valLoss:2.1220e-01  time: 3.95e+00\n",
      "epoch: 95   trainLoss: 2.5389e-02   valLoss:6.1597e-02  time: 3.87e+00\n",
      "epoch: 96   trainLoss: 3.1778e-02   valLoss:6.0522e-02  time: 3.92e+00\n",
      "epoch: 97   trainLoss: 2.6436e-02   valLoss:5.3397e-02  time: 3.85e+00\n",
      "epoch: 98   trainLoss: 2.9359e-02   valLoss:8.2445e-02  time: 3.88e+00\n",
      "epoch: 99   trainLoss: 2.9478e-02   valLoss:8.1476e-02  time: 3.95e+00\n",
      "loading checkpoint 80\n",
      "trained 38 random forest models in 16.29 seconds\n",
      "loaded train set of size 178\n",
      "epoch: 0   trainLoss: 9.8479e-01   valLoss:9.6198e-01  time: 1.55e+00\n",
      "epoch: 1   trainLoss: 8.0022e-01   valLoss:9.3535e-01  time: 1.54e+00\n",
      "epoch: 2   trainLoss: 6.7347e-01   valLoss:9.1599e-01  time: 1.52e+00\n",
      "epoch: 3   trainLoss: 5.6947e-01   valLoss:8.9748e-01  time: 1.50e+00\n",
      "epoch: 4   trainLoss: 4.7944e-01   valLoss:8.8055e-01  time: 1.51e+00\n",
      "epoch: 5   trainLoss: 4.1516e-01   valLoss:8.7330e-01  time: 1.52e+00\n",
      "epoch: 6   trainLoss: 3.6357e-01   valLoss:8.7931e-01  time: 1.50e+00\n",
      "epoch: 7   trainLoss: 3.1857e-01   valLoss:8.9464e-01  time: 1.52e+00\n",
      "epoch: 8   trainLoss: 2.8505e-01   valLoss:9.0766e-01  time: 1.50e+00\n",
      "epoch: 9   trainLoss: 2.6235e-01   valLoss:9.1731e-01  time: 1.53e+00\n",
      "epoch: 10   trainLoss: 2.4234e-01   valLoss:9.1273e-01  time: 1.52e+00\n",
      "epoch: 11   trainLoss: 2.2051e-01   valLoss:9.0256e-01  time: 1.53e+00\n",
      "epoch: 12   trainLoss: 2.0313e-01   valLoss:9.0771e-01  time: 1.52e+00\n",
      "epoch: 13   trainLoss: 1.8597e-01   valLoss:9.1673e-01  time: 1.49e+00\n",
      "epoch: 14   trainLoss: 1.7541e-01   valLoss:9.6261e-01  time: 1.52e+00\n",
      "epoch: 15   trainLoss: 1.7372e-01   valLoss:1.0775e+00  time: 1.51e+00\n",
      "epoch: 16   trainLoss: 1.7084e-01   valLoss:1.0860e+00  time: 1.52e+00\n",
      "epoch: 17   trainLoss: 1.4879e-01   valLoss:1.1027e+00  time: 1.50e+00\n",
      "epoch: 18   trainLoss: 1.4770e-01   valLoss:1.1331e+00  time: 1.53e+00\n",
      "epoch: 19   trainLoss: 1.3312e-01   valLoss:1.0219e+00  time: 1.56e+00\n",
      "epoch: 20   trainLoss: 1.3329e-01   valLoss:1.0130e+00  time: 1.49e+00\n",
      "epoch: 21   trainLoss: 1.1925e-01   valLoss:1.0279e+00  time: 1.52e+00\n",
      "epoch: 22   trainLoss: 1.1908e-01   valLoss:9.3010e-01  time: 1.51e+00\n",
      "epoch: 23   trainLoss: 1.1000e-01   valLoss:9.7432e-01  time: 1.52e+00\n",
      "epoch: 24   trainLoss: 1.1039e-01   valLoss:7.8411e-01  time: 1.51e+00\n",
      "epoch: 25   trainLoss: 1.0091e-01   valLoss:7.9081e-01  time: 1.51e+00\n",
      "epoch: 26   trainLoss: 9.3992e-02   valLoss:8.8799e-01  time: 1.57e+00\n",
      "epoch: 27   trainLoss: 8.6424e-02   valLoss:8.7221e-01  time: 1.50e+00\n",
      "epoch: 28   trainLoss: 8.5852e-02   valLoss:9.0943e-01  time: 1.50e+00\n",
      "epoch: 29   trainLoss: 8.0231e-02   valLoss:1.2294e+00  time: 1.50e+00\n",
      "epoch: 30   trainLoss: 8.2654e-02   valLoss:9.5971e-01  time: 1.51e+00\n",
      "epoch: 31   trainLoss: 7.2668e-02   valLoss:1.0942e+00  time: 1.52e+00\n",
      "epoch: 32   trainLoss: 6.9333e-02   valLoss:9.6301e-01  time: 1.51e+00\n",
      "epoch: 33   trainLoss: 6.5881e-02   valLoss:1.1156e+00  time: 1.51e+00\n",
      "epoch: 34   trainLoss: 7.3666e-02   valLoss:7.3591e-01  time: 1.51e+00\n",
      "epoch: 35   trainLoss: 9.4668e-02   valLoss:1.1865e+00  time: 1.51e+00\n",
      "epoch: 36   trainLoss: 7.7008e-02   valLoss:7.6123e-01  time: 1.53e+00\n",
      "epoch: 37   trainLoss: 7.3634e-02   valLoss:9.5338e-01  time: 1.52e+00\n",
      "epoch: 38   trainLoss: 6.7083e-02   valLoss:1.0417e+00  time: 1.51e+00\n",
      "epoch: 39   trainLoss: 6.2315e-02   valLoss:7.8964e-01  time: 1.51e+00\n",
      "epoch: 40   trainLoss: 6.0169e-02   valLoss:9.0432e-01  time: 1.51e+00\n",
      "epoch: 41   trainLoss: 5.7740e-02   valLoss:7.5806e-01  time: 1.52e+00\n",
      "epoch: 42   trainLoss: 5.3768e-02   valLoss:7.0242e-01  time: 1.52e+00\n",
      "epoch: 43   trainLoss: 4.8593e-02   valLoss:9.4160e-01  time: 1.51e+00\n",
      "epoch: 44   trainLoss: 4.8708e-02   valLoss:8.2118e-01  time: 1.52e+00\n",
      "epoch: 45   trainLoss: 4.4239e-02   valLoss:7.1094e-01  time: 1.52e+00\n",
      "epoch: 46   trainLoss: 4.1637e-02   valLoss:1.0441e+00  time: 1.52e+00\n",
      "epoch: 47   trainLoss: 4.0729e-02   valLoss:1.0762e+00  time: 1.52e+00\n",
      "epoch: 48   trainLoss: 3.7752e-02   valLoss:1.1612e+00  time: 1.54e+00\n",
      "epoch: 49   trainLoss: 3.6941e-02   valLoss:1.0800e+00  time: 1.54e+00\n",
      "epoch: 50   trainLoss: 3.2217e-02   valLoss:9.8280e-01  time: 1.51e+00\n",
      "epoch: 51   trainLoss: 2.9981e-02   valLoss:1.1428e+00  time: 1.52e+00\n",
      "epoch: 52   trainLoss: 3.0205e-02   valLoss:8.9805e-01  time: 1.52e+00\n",
      "epoch: 53   trainLoss: 3.1948e-02   valLoss:1.2503e+00  time: 1.51e+00\n",
      "epoch: 54   trainLoss: 3.4694e-02   valLoss:8.7122e-01  time: 1.52e+00\n",
      "epoch: 55   trainLoss: 3.1843e-02   valLoss:1.3255e+00  time: 1.53e+00\n",
      "epoch: 56   trainLoss: 3.4613e-02   valLoss:7.7169e-01  time: 1.52e+00\n",
      "epoch: 57   trainLoss: 3.7099e-02   valLoss:1.0506e+00  time: 1.52e+00\n",
      "epoch: 58   trainLoss: 3.9087e-02   valLoss:7.5479e-01  time: 1.50e+00\n",
      "epoch: 59   trainLoss: 2.2438e-02   valLoss:7.1448e-01  time: 1.52e+00\n",
      "epoch: 60   trainLoss: 3.0729e-02   valLoss:8.9081e-01  time: 1.54e+00\n",
      "epoch: 61   trainLoss: 2.6622e-02   valLoss:8.0860e-01  time: 1.51e+00\n",
      "epoch: 62   trainLoss: 2.2529e-02   valLoss:7.2997e-01  time: 1.50e+00\n",
      "epoch: 63   trainLoss: 2.4727e-02   valLoss:7.8268e-01  time: 1.55e+00\n",
      "epoch: 64   trainLoss: 1.8222e-02   valLoss:8.3428e-01  time: 1.52e+00\n",
      "epoch: 65   trainLoss: 2.0944e-02   valLoss:7.4949e-01  time: 1.52e+00\n",
      "epoch: 66   trainLoss: 1.6501e-02   valLoss:7.3363e-01  time: 1.51e+00\n",
      "epoch: 67   trainLoss: 1.8688e-02   valLoss:7.5393e-01  time: 1.50e+00\n",
      "epoch: 68   trainLoss: 1.4455e-02   valLoss:6.9340e-01  time: 1.51e+00\n",
      "epoch: 69   trainLoss: 1.6296e-02   valLoss:7.5499e-01  time: 1.51e+00\n",
      "epoch: 70   trainLoss: 1.3583e-02   valLoss:6.1176e-01  time: 1.51e+00\n",
      "epoch: 71   trainLoss: 1.4048e-02   valLoss:6.6792e-01  time: 1.52e+00\n",
      "epoch: 72   trainLoss: 1.3137e-02   valLoss:6.2862e-01  time: 1.51e+00\n",
      "epoch: 73   trainLoss: 1.2651e-02   valLoss:6.6344e-01  time: 1.51e+00\n",
      "epoch: 74   trainLoss: 1.5116e-02   valLoss:5.9152e-01  time: 1.51e+00\n",
      "epoch: 75   trainLoss: 2.1892e-02   valLoss:6.3293e-01  time: 1.51e+00\n",
      "epoch: 76   trainLoss: 3.0615e-02   valLoss:7.2226e-01  time: 1.53e+00\n",
      "epoch: 77   trainLoss: 3.2603e-02   valLoss:6.3974e-01  time: 1.54e+00\n",
      "epoch: 78   trainLoss: 1.7371e-02   valLoss:5.8075e-01  time: 1.51e+00\n",
      "epoch: 79   trainLoss: 2.2763e-02   valLoss:6.9346e-01  time: 1.53e+00\n",
      "epoch: 80   trainLoss: 1.8698e-02   valLoss:5.5523e-01  time: 1.52e+00\n",
      "epoch: 81   trainLoss: 1.8173e-02   valLoss:4.8683e-01  time: 1.51e+00\n",
      "epoch: 82   trainLoss: 1.5472e-02   valLoss:5.4058e-01  time: 1.52e+00\n",
      "epoch: 83   trainLoss: 1.7644e-02   valLoss:5.2887e-01  time: 1.52e+00\n",
      "epoch: 84   trainLoss: 1.2569e-02   valLoss:5.1206e-01  time: 1.50e+00\n",
      "epoch: 85   trainLoss: 1.5362e-02   valLoss:5.2045e-01  time: 1.51e+00\n",
      "epoch: 86   trainLoss: 1.2356e-02   valLoss:5.0215e-01  time: 1.50e+00\n",
      "epoch: 87   trainLoss: 1.3969e-02   valLoss:5.0830e-01  time: 1.50e+00\n",
      "epoch: 88   trainLoss: 1.0690e-02   valLoss:4.4887e-01  time: 1.52e+00\n",
      "epoch: 89   trainLoss: 1.1812e-02   valLoss:5.2406e-01  time: 1.50e+00\n",
      "epoch: 90   trainLoss: 1.1504e-02   valLoss:3.7179e-01  time: 1.51e+00\n",
      "epoch: 91   trainLoss: 1.5847e-02   valLoss:6.9773e-01  time: 1.51e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92   trainLoss: 2.3409e-02   valLoss:3.4602e-01  time: 1.51e+00\n",
      "epoch: 93   trainLoss: 2.3145e-02   valLoss:5.8872e-01  time: 1.52e+00\n",
      "epoch: 94   trainLoss: 1.4544e-02   valLoss:5.3725e-01  time: 1.52e+00\n",
      "epoch: 95   trainLoss: 1.1796e-02   valLoss:3.4199e-01  time: 1.51e+00\n",
      "epoch: 96   trainLoss: 1.4689e-02   valLoss:4.0156e-01  time: 1.50e+00\n",
      "epoch: 97   trainLoss: 1.0245e-02   valLoss:4.5825e-01  time: 1.52e+00\n",
      "epoch: 98   trainLoss: 1.0865e-02   valLoss:3.6752e-01  time: 1.50e+00\n",
      "epoch: 99   trainLoss: 9.0527e-03   valLoss:3.9646e-01  time: 1.49e+00\n",
      "loading checkpoint 95\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 8.6573e-01   valLoss:1.1886e+00  time: 1.57e+00\n",
      "epoch: 1   trainLoss: 6.9615e-01   valLoss:1.9085e+00  time: 1.55e+00\n",
      "epoch: 2   trainLoss: 4.7003e-01   valLoss:1.6476e+00  time: 1.53e+00\n",
      "epoch: 3   trainLoss: 3.4381e-01   valLoss:9.6824e-01  time: 1.52e+00\n",
      "epoch: 4   trainLoss: 2.5748e-01   valLoss:8.7376e-01  time: 1.52e+00\n",
      "epoch: 5   trainLoss: 2.0224e-01   valLoss:7.7009e-01  time: 1.51e+00\n",
      "epoch: 6   trainLoss: 1.6818e-01   valLoss:4.9917e-01  time: 1.58e+00\n",
      "epoch: 7   trainLoss: 1.5848e-01   valLoss:4.7800e-01  time: 1.58e+00\n",
      "epoch: 8   trainLoss: 1.5261e-01   valLoss:3.2710e-01  time: 1.53e+00\n",
      "epoch: 9   trainLoss: 1.3668e-01   valLoss:4.0732e-01  time: 1.54e+00\n",
      "epoch: 10   trainLoss: 1.3374e-01   valLoss:3.2664e-01  time: 1.51e+00\n",
      "epoch: 11   trainLoss: 1.2401e-01   valLoss:1.9264e-01  time: 1.54e+00\n",
      "epoch: 12   trainLoss: 1.1311e-01   valLoss:1.7131e-01  time: 1.52e+00\n",
      "epoch: 13   trainLoss: 1.0264e-01   valLoss:1.6299e-01  time: 1.51e+00\n",
      "epoch: 14   trainLoss: 9.5468e-02   valLoss:1.4186e-01  time: 1.51e+00\n",
      "epoch: 15   trainLoss: 8.5556e-02   valLoss:1.3202e-01  time: 1.54e+00\n",
      "epoch: 16   trainLoss: 8.2616e-02   valLoss:1.2264e-01  time: 1.52e+00\n",
      "epoch: 17   trainLoss: 7.7770e-02   valLoss:9.4517e-02  time: 1.55e+00\n",
      "epoch: 18   trainLoss: 7.3489e-02   valLoss:8.6877e-02  time: 1.54e+00\n",
      "epoch: 19   trainLoss: 7.1145e-02   valLoss:7.6930e-02  time: 1.54e+00\n",
      "epoch: 20   trainLoss: 6.5792e-02   valLoss:6.9368e-02  time: 1.54e+00\n",
      "epoch: 21   trainLoss: 6.3801e-02   valLoss:8.0231e-02  time: 1.54e+00\n",
      "epoch: 22   trainLoss: 6.0272e-02   valLoss:8.5515e-02  time: 1.51e+00\n",
      "epoch: 23   trainLoss: 5.7696e-02   valLoss:7.9186e-02  time: 1.56e+00\n",
      "epoch: 24   trainLoss: 5.5557e-02   valLoss:8.0513e-02  time: 1.55e+00\n",
      "epoch: 25   trainLoss: 5.2790e-02   valLoss:7.3531e-02  time: 1.50e+00\n",
      "epoch: 26   trainLoss: 5.0237e-02   valLoss:7.3777e-02  time: 1.51e+00\n",
      "epoch: 27   trainLoss: 4.8005e-02   valLoss:7.3380e-02  time: 1.52e+00\n",
      "epoch: 28   trainLoss: 4.5865e-02   valLoss:5.9645e-02  time: 1.51e+00\n",
      "epoch: 29   trainLoss: 4.3550e-02   valLoss:6.2995e-02  time: 1.50e+00\n",
      "epoch: 30   trainLoss: 4.2128e-02   valLoss:6.8490e-02  time: 1.50e+00\n",
      "epoch: 31   trainLoss: 4.1786e-02   valLoss:8.3427e-02  time: 1.52e+00\n",
      "epoch: 32   trainLoss: 4.6360e-02   valLoss:1.4651e-01  time: 1.52e+00\n",
      "epoch: 33   trainLoss: 5.8452e-02   valLoss:3.0871e-01  time: 1.52e+00\n",
      "epoch: 34   trainLoss: 6.2331e-02   valLoss:1.0224e-01  time: 1.53e+00\n",
      "epoch: 35   trainLoss: 3.9920e-02   valLoss:1.2755e-01  time: 1.51e+00\n",
      "epoch: 36   trainLoss: 4.9834e-02   valLoss:1.5963e-01  time: 1.52e+00\n",
      "epoch: 37   trainLoss: 3.4924e-02   valLoss:2.0163e-01  time: 1.50e+00\n",
      "epoch: 38   trainLoss: 4.4996e-02   valLoss:1.0625e-01  time: 1.53e+00\n",
      "epoch: 39   trainLoss: 3.4712e-02   valLoss:6.8336e-02  time: 1.51e+00\n",
      "epoch: 40   trainLoss: 3.7283e-02   valLoss:4.6643e-02  time: 1.51e+00\n",
      "epoch: 41   trainLoss: 3.2264e-02   valLoss:5.9908e-02  time: 1.51e+00\n",
      "epoch: 42   trainLoss: 3.1565e-02   valLoss:4.9344e-02  time: 1.54e+00\n",
      "epoch: 43   trainLoss: 3.2362e-02   valLoss:4.7900e-02  time: 1.50e+00\n",
      "epoch: 44   trainLoss: 2.9363e-02   valLoss:7.6945e-02  time: 1.50e+00\n",
      "epoch: 45   trainLoss: 2.8175e-02   valLoss:6.7446e-02  time: 1.53e+00\n",
      "epoch: 46   trainLoss: 2.5930e-02   valLoss:5.0461e-02  time: 1.52e+00\n",
      "epoch: 47   trainLoss: 2.4964e-02   valLoss:6.8552e-02  time: 1.51e+00\n",
      "epoch: 48   trainLoss: 2.3501e-02   valLoss:7.5819e-02  time: 1.53e+00\n",
      "epoch: 49   trainLoss: 2.3644e-02   valLoss:7.5720e-02  time: 1.51e+00\n",
      "epoch: 50   trainLoss: 2.3309e-02   valLoss:7.3494e-02  time: 1.51e+00\n",
      "epoch: 51   trainLoss: 2.4890e-02   valLoss:7.0157e-02  time: 1.52e+00\n",
      "epoch: 52   trainLoss: 2.6878e-02   valLoss:6.7698e-02  time: 1.52e+00\n",
      "epoch: 53   trainLoss: 2.8549e-02   valLoss:6.2058e-02  time: 1.51e+00\n",
      "epoch: 54   trainLoss: 2.2828e-02   valLoss:5.5164e-02  time: 1.52e+00\n",
      "epoch: 55   trainLoss: 1.7729e-02   valLoss:6.3702e-02  time: 1.51e+00\n",
      "epoch: 56   trainLoss: 1.7393e-02   valLoss:8.0566e-02  time: 1.51e+00\n",
      "epoch: 57   trainLoss: 2.0462e-02   valLoss:1.2587e-01  time: 1.51e+00\n",
      "epoch: 58   trainLoss: 1.9481e-02   valLoss:1.3544e-01  time: 1.50e+00\n",
      "epoch: 59   trainLoss: 1.5075e-02   valLoss:1.3460e-01  time: 1.50e+00\n",
      "epoch: 60   trainLoss: 1.4647e-02   valLoss:1.0670e-01  time: 1.52e+00\n",
      "epoch: 61   trainLoss: 1.5671e-02   valLoss:1.0002e-01  time: 1.53e+00\n",
      "epoch: 62   trainLoss: 1.6137e-02   valLoss:1.1023e-01  time: 1.52e+00\n",
      "epoch: 63   trainLoss: 1.3849e-02   valLoss:7.3231e-02  time: 1.54e+00\n",
      "epoch: 64   trainLoss: 1.2523e-02   valLoss:5.9257e-02  time: 1.50e+00\n",
      "epoch: 65   trainLoss: 1.5940e-02   valLoss:4.2041e-02  time: 1.50e+00\n",
      "epoch: 66   trainLoss: 1.9071e-02   valLoss:1.2234e-01  time: 1.53e+00\n",
      "epoch: 67   trainLoss: 2.1883e-02   valLoss:5.6489e-02  time: 1.51e+00\n",
      "epoch: 68   trainLoss: 1.8314e-02   valLoss:3.6405e-02  time: 1.52e+00\n",
      "epoch: 69   trainLoss: 2.2900e-02   valLoss:8.4047e-02  time: 1.52e+00\n",
      "epoch: 70   trainLoss: 2.8176e-02   valLoss:6.1096e-02  time: 1.51e+00\n",
      "epoch: 71   trainLoss: 1.6671e-02   valLoss:3.6006e-02  time: 1.50e+00\n",
      "epoch: 72   trainLoss: 1.6811e-02   valLoss:2.7696e-02  time: 1.51e+00\n",
      "epoch: 73   trainLoss: 1.6633e-02   valLoss:6.6129e-02  time: 1.51e+00\n",
      "epoch: 74   trainLoss: 1.5942e-02   valLoss:2.5896e-02  time: 1.51e+00\n",
      "epoch: 75   trainLoss: 1.2439e-02   valLoss:6.2912e-02  time: 1.52e+00\n",
      "epoch: 76   trainLoss: 1.4892e-02   valLoss:2.9566e-02  time: 1.53e+00\n",
      "epoch: 77   trainLoss: 1.0929e-02   valLoss:4.3066e-02  time: 1.54e+00\n",
      "epoch: 78   trainLoss: 1.2944e-02   valLoss:4.7836e-02  time: 1.52e+00\n",
      "epoch: 79   trainLoss: 1.0126e-02   valLoss:8.0772e-02  time: 1.53e+00\n",
      "epoch: 80   trainLoss: 1.0705e-02   valLoss:7.6104e-02  time: 1.52e+00\n",
      "epoch: 81   trainLoss: 9.3719e-03   valLoss:7.5101e-02  time: 1.52e+00\n",
      "epoch: 82   trainLoss: 9.2603e-03   valLoss:9.1696e-02  time: 1.52e+00\n",
      "epoch: 83   trainLoss: 8.9198e-03   valLoss:1.0104e-01  time: 1.51e+00\n",
      "epoch: 84   trainLoss: 7.8463e-03   valLoss:1.1114e-01  time: 1.54e+00\n",
      "epoch: 85   trainLoss: 8.4454e-03   valLoss:1.0954e-01  time: 1.50e+00\n",
      "epoch: 86   trainLoss: 8.5869e-03   valLoss:1.1752e-01  time: 1.51e+00\n",
      "epoch: 87   trainLoss: 9.6349e-03   valLoss:1.0008e-01  time: 1.60e+00\n",
      "epoch: 88   trainLoss: 1.4262e-02   valLoss:1.2038e-01  time: 1.50e+00\n",
      "epoch: 89   trainLoss: 2.1719e-02   valLoss:4.5613e-02  time: 1.51e+00\n",
      "epoch: 90   trainLoss: 2.9969e-02   valLoss:5.0265e-02  time: 1.50e+00\n",
      "epoch: 91   trainLoss: 1.8173e-02   valLoss:3.9619e-02  time: 1.50e+00\n",
      "epoch: 92   trainLoss: 1.0236e-02   valLoss:4.5661e-02  time: 1.53e+00\n",
      "epoch: 93   trainLoss: 1.9973e-02   valLoss:4.9284e-02  time: 1.54e+00\n",
      "epoch: 94   trainLoss: 1.2465e-02   valLoss:1.9472e-02  time: 1.52e+00\n",
      "epoch: 95   trainLoss: 1.2168e-02   valLoss:4.5724e-02  time: 1.52e+00\n",
      "epoch: 96   trainLoss: 1.3477e-02   valLoss:2.6427e-02  time: 1.56e+00\n",
      "epoch: 97   trainLoss: 9.1585e-03   valLoss:2.8637e-02  time: 1.51e+00\n",
      "epoch: 98   trainLoss: 1.2783e-02   valLoss:5.9548e-02  time: 1.53e+00\n",
      "epoch: 99   trainLoss: 9.4818e-03   valLoss:4.7036e-02  time: 1.52e+00\n",
      "loading checkpoint 94\n",
      "trained 38 random forest models in 8.17 seconds\n",
      "loaded train set of size 5\n",
      "epoch: 0   trainLoss: 9.6347e-01   valLoss:9.7190e-01  time: 6.26e-02\n",
      "epoch: 1   trainLoss: 7.2968e-01   valLoss:9.5760e-01  time: 6.59e-02\n",
      "epoch: 2   trainLoss: 5.8496e-01   valLoss:9.2972e-01  time: 6.47e-02\n",
      "epoch: 3   trainLoss: 4.6791e-01   valLoss:8.8812e-01  time: 6.33e-02\n",
      "epoch: 4   trainLoss: 3.3606e-01   valLoss:8.4009e-01  time: 6.31e-02\n",
      "epoch: 5   trainLoss: 2.6857e-01   valLoss:7.9888e-01  time: 6.32e-02\n",
      "epoch: 6   trainLoss: 1.9784e-01   valLoss:7.8736e-01  time: 6.47e-02\n",
      "epoch: 7   trainLoss: 1.6832e-01   valLoss:8.9387e-01  time: 6.34e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8   trainLoss: 1.2473e-01   valLoss:1.1330e+00  time: 6.34e-02\n",
      "epoch: 9   trainLoss: 1.0059e-01   valLoss:1.3576e+00  time: 6.19e-02\n",
      "epoch: 10   trainLoss: 9.3947e-02   valLoss:1.4548e+00  time: 6.23e-02\n",
      "epoch: 11   trainLoss: 1.0423e-01   valLoss:1.4007e+00  time: 6.22e-02\n",
      "epoch: 12   trainLoss: 9.9066e-02   valLoss:1.2570e+00  time: 6.18e-02\n",
      "epoch: 13   trainLoss: 7.7285e-02   valLoss:1.1431e+00  time: 6.30e-02\n",
      "epoch: 14   trainLoss: 6.9176e-02   valLoss:1.0864e+00  time: 6.29e-02\n",
      "epoch: 15   trainLoss: 5.8136e-02   valLoss:1.0767e+00  time: 6.24e-02\n",
      "epoch: 16   trainLoss: 4.9737e-02   valLoss:1.0706e+00  time: 6.33e-02\n",
      "epoch: 17   trainLoss: 4.4405e-02   valLoss:1.0262e+00  time: 6.31e-02\n",
      "epoch: 18   trainLoss: 3.7106e-02   valLoss:9.3793e-01  time: 6.11e-02\n",
      "epoch: 19   trainLoss: 3.2820e-02   valLoss:8.2016e-01  time: 6.16e-02\n",
      "epoch: 20   trainLoss: 2.8681e-02   valLoss:6.6678e-01  time: 6.11e-02\n",
      "epoch: 21   trainLoss: 2.4817e-02   valLoss:5.0845e-01  time: 6.46e-02\n",
      "epoch: 22   trainLoss: 2.1743e-02   valLoss:3.6539e-01  time: 6.16e-02\n",
      "epoch: 23   trainLoss: 1.8982e-02   valLoss:2.6485e-01  time: 6.23e-02\n",
      "epoch: 24   trainLoss: 1.5902e-02   valLoss:1.9337e-01  time: 6.39e-02\n",
      "epoch: 25   trainLoss: 1.3200e-02   valLoss:1.4757e-01  time: 6.15e-02\n",
      "epoch: 26   trainLoss: 1.0728e-02   valLoss:1.2379e-01  time: 6.58e-02\n",
      "epoch: 27   trainLoss: 8.5909e-03   valLoss:1.1117e-01  time: 6.44e-02\n",
      "epoch: 28   trainLoss: 6.9617e-03   valLoss:1.0150e-01  time: 6.48e-02\n",
      "epoch: 29   trainLoss: 5.6315e-03   valLoss:9.5908e-02  time: 6.27e-02\n",
      "epoch: 30   trainLoss: 5.1824e-03   valLoss:9.3691e-02  time: 6.14e-02\n",
      "epoch: 31   trainLoss: 4.5560e-03   valLoss:9.0888e-02  time: 6.29e-02\n",
      "epoch: 32   trainLoss: 3.8382e-03   valLoss:8.6835e-02  time: 6.27e-02\n",
      "epoch: 33   trainLoss: 2.9801e-03   valLoss:8.3238e-02  time: 6.19e-02\n",
      "epoch: 34   trainLoss: 2.3888e-03   valLoss:8.1667e-02  time: 6.34e-02\n",
      "epoch: 35   trainLoss: 2.0788e-03   valLoss:8.1045e-02  time: 6.14e-02\n",
      "epoch: 36   trainLoss: 2.0065e-03   valLoss:7.9092e-02  time: 6.18e-02\n",
      "epoch: 37   trainLoss: 1.8196e-03   valLoss:7.6616e-02  time: 6.21e-02\n",
      "epoch: 38   trainLoss: 1.3499e-03   valLoss:7.5910e-02  time: 6.20e-02\n",
      "epoch: 39   trainLoss: 1.2072e-03   valLoss:7.6504e-02  time: 6.17e-02\n",
      "epoch: 40   trainLoss: 1.1235e-03   valLoss:7.6839e-02  time: 6.15e-02\n",
      "epoch: 41   trainLoss: 1.0539e-03   valLoss:7.6790e-02  time: 6.06e-02\n",
      "epoch: 42   trainLoss: 9.4731e-04   valLoss:7.6812e-02  time: 6.21e-02\n",
      "epoch: 43   trainLoss: 8.5487e-04   valLoss:7.7251e-02  time: 6.33e-02\n",
      "epoch: 44   trainLoss: 7.3526e-04   valLoss:7.7311e-02  time: 6.26e-02\n",
      "epoch: 45   trainLoss: 6.7000e-04   valLoss:7.6232e-02  time: 6.15e-02\n",
      "epoch: 46   trainLoss: 5.7811e-04   valLoss:7.5107e-02  time: 6.16e-02\n",
      "epoch: 47   trainLoss: 5.2655e-04   valLoss:7.4360e-02  time: 6.46e-02\n",
      "epoch: 48   trainLoss: 4.6965e-04   valLoss:7.4044e-02  time: 6.27e-02\n",
      "epoch: 49   trainLoss: 4.2194e-04   valLoss:7.3267e-02  time: 6.28e-02\n",
      "epoch: 50   trainLoss: 3.8471e-04   valLoss:7.2958e-02  time: 6.26e-02\n",
      "epoch: 51   trainLoss: 3.5163e-04   valLoss:7.3445e-02  time: 6.51e-02\n",
      "epoch: 52   trainLoss: 3.2835e-04   valLoss:7.3819e-02  time: 6.24e-02\n",
      "epoch: 53   trainLoss: 2.7812e-04   valLoss:7.3923e-02  time: 6.20e-02\n",
      "epoch: 54   trainLoss: 2.2620e-04   valLoss:7.4090e-02  time: 6.06e-02\n",
      "epoch: 55   trainLoss: 2.3186e-04   valLoss:7.4962e-02  time: 6.12e-02\n",
      "epoch: 56   trainLoss: 2.7119e-04   valLoss:7.4574e-02  time: 6.10e-02\n",
      "epoch: 57   trainLoss: 5.4814e-04   valLoss:7.6675e-02  time: 6.27e-02\n",
      "epoch: 58   trainLoss: 1.8736e-03   valLoss:7.4017e-02  time: 6.09e-02\n",
      "epoch: 59   trainLoss: 1.3000e-03   valLoss:7.1781e-02  time: 6.20e-02\n",
      "epoch: 60   trainLoss: 4.1577e-04   valLoss:7.1316e-02  time: 6.52e-02\n",
      "epoch: 61   trainLoss: 5.5152e-04   valLoss:7.1407e-02  time: 6.25e-02\n",
      "epoch: 62   trainLoss: 5.8078e-04   valLoss:6.9465e-02  time: 6.25e-02\n",
      "epoch: 63   trainLoss: 5.1926e-04   valLoss:6.8681e-02  time: 6.33e-02\n",
      "epoch: 64   trainLoss: 4.0247e-04   valLoss:6.9201e-02  time: 6.14e-02\n",
      "epoch: 65   trainLoss: 3.3067e-04   valLoss:6.9134e-02  time: 6.11e-02\n",
      "epoch: 66   trainLoss: 2.8407e-04   valLoss:6.8753e-02  time: 6.25e-02\n",
      "epoch: 67   trainLoss: 3.8661e-04   valLoss:6.8460e-02  time: 6.15e-02\n",
      "epoch: 68   trainLoss: 1.9624e-04   valLoss:6.7594e-02  time: 6.17e-02\n",
      "epoch: 69   trainLoss: 2.5006e-04   valLoss:6.7582e-02  time: 6.26e-02\n",
      "epoch: 70   trainLoss: 2.4237e-04   valLoss:6.8621e-02  time: 6.33e-02\n",
      "epoch: 71   trainLoss: 1.7832e-04   valLoss:6.7950e-02  time: 6.14e-02\n",
      "epoch: 72   trainLoss: 1.5300e-04   valLoss:6.6215e-02  time: 6.16e-02\n",
      "epoch: 73   trainLoss: 2.0203e-04   valLoss:6.5253e-02  time: 6.50e-02\n",
      "epoch: 74   trainLoss: 1.4940e-04   valLoss:6.5520e-02  time: 6.21e-02\n",
      "epoch: 75   trainLoss: 1.2215e-04   valLoss:6.5111e-02  time: 6.15e-02\n",
      "epoch: 76   trainLoss: 1.2699e-04   valLoss:6.4435e-02  time: 6.40e-02\n",
      "epoch: 77   trainLoss: 1.2844e-04   valLoss:6.3701e-02  time: 6.19e-02\n",
      "epoch: 78   trainLoss: 8.6593e-05   valLoss:6.2350e-02  time: 6.17e-02\n",
      "epoch: 79   trainLoss: 1.1471e-04   valLoss:6.2545e-02  time: 6.26e-02\n",
      "epoch: 80   trainLoss: 1.0196e-04   valLoss:6.1550e-02  time: 6.25e-02\n",
      "epoch: 81   trainLoss: 9.9068e-05   valLoss:6.2583e-02  time: 6.34e-02\n",
      "epoch: 82   trainLoss: 1.0241e-04   valLoss:6.0602e-02  time: 6.12e-02\n",
      "epoch: 83   trainLoss: 1.6510e-04   valLoss:6.2508e-02  time: 6.17e-02\n",
      "epoch: 84   trainLoss: 3.7215e-04   valLoss:5.8768e-02  time: 6.07e-02\n",
      "epoch: 85   trainLoss: 1.0929e-03   valLoss:6.7290e-02  time: 6.14e-02\n",
      "epoch: 86   trainLoss: 2.8642e-03   valLoss:5.6448e-02  time: 6.13e-02\n",
      "epoch: 87   trainLoss: 5.2305e-03   valLoss:6.4770e-02  time: 6.22e-02\n",
      "epoch: 88   trainLoss: 2.5195e-03   valLoss:5.5990e-02  time: 6.18e-02\n",
      "epoch: 89   trainLoss: 1.0274e-03   valLoss:4.8045e-02  time: 6.17e-02\n",
      "epoch: 90   trainLoss: 1.9651e-03   valLoss:4.9820e-02  time: 6.19e-02\n",
      "epoch: 91   trainLoss: 7.9750e-04   valLoss:5.3906e-02  time: 6.09e-02\n",
      "epoch: 92   trainLoss: 1.2729e-03   valLoss:4.8708e-02  time: 6.04e-02\n",
      "epoch: 93   trainLoss: 8.5021e-04   valLoss:4.6932e-02  time: 6.13e-02\n",
      "epoch: 94   trainLoss: 8.9159e-04   valLoss:4.6952e-02  time: 6.37e-02\n",
      "epoch: 95   trainLoss: 1.4680e-03   valLoss:4.7301e-02  time: 6.10e-02\n",
      "epoch: 96   trainLoss: 7.4198e-04   valLoss:4.5667e-02  time: 6.30e-02\n",
      "epoch: 97   trainLoss: 8.1538e-04   valLoss:4.6476e-02  time: 6.40e-02\n",
      "epoch: 98   trainLoss: 5.3061e-04   valLoss:4.8539e-02  time: 6.03e-02\n",
      "epoch: 99   trainLoss: 6.0690e-04   valLoss:4.7920e-02  time: 6.19e-02\n",
      "loading checkpoint 96\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.6054e+00   valLoss:1.4171e+00  time: 6.31e-02\n",
      "epoch: 1   trainLoss: 1.1759e+00   valLoss:2.6077e+00  time: 6.41e-02\n",
      "epoch: 2   trainLoss: 9.5366e-01   valLoss:2.6935e+00  time: 6.21e-02\n",
      "epoch: 3   trainLoss: 7.4254e-01   valLoss:1.7465e+00  time: 6.22e-02\n",
      "epoch: 4   trainLoss: 5.1320e-01   valLoss:1.1312e+00  time: 6.47e-02\n",
      "epoch: 5   trainLoss: 3.7148e-01   valLoss:8.2887e-01  time: 6.43e-02\n",
      "epoch: 6   trainLoss: 2.7769e-01   valLoss:5.8683e-01  time: 6.44e-02\n",
      "epoch: 7   trainLoss: 2.1377e-01   valLoss:4.0929e-01  time: 6.44e-02\n",
      "epoch: 8   trainLoss: 1.3630e-01   valLoss:4.4170e-01  time: 6.54e-02\n",
      "epoch: 9   trainLoss: 9.7492e-02   valLoss:5.3288e-01  time: 6.34e-02\n",
      "epoch: 10   trainLoss: 1.0134e-01   valLoss:4.6186e-01  time: 6.23e-02\n",
      "epoch: 11   trainLoss: 9.3877e-02   valLoss:3.2441e-01  time: 6.25e-02\n",
      "epoch: 12   trainLoss: 8.8218e-02   valLoss:2.5169e-01  time: 6.45e-02\n",
      "epoch: 13   trainLoss: 8.5526e-02   valLoss:1.9947e-01  time: 6.37e-02\n",
      "epoch: 14   trainLoss: 7.4037e-02   valLoss:1.6933e-01  time: 6.39e-02\n",
      "epoch: 15   trainLoss: 7.2390e-02   valLoss:1.7057e-01  time: 6.31e-02\n",
      "epoch: 16   trainLoss: 5.9821e-02   valLoss:1.7634e-01  time: 6.19e-02\n",
      "epoch: 17   trainLoss: 6.1256e-02   valLoss:1.0094e-01  time: 6.18e-02\n",
      "epoch: 18   trainLoss: 4.5982e-02   valLoss:8.6129e-02  time: 6.28e-02\n",
      "epoch: 19   trainLoss: 4.5105e-02   valLoss:1.0141e-01  time: 6.29e-02\n",
      "epoch: 20   trainLoss: 4.3015e-02   valLoss:7.1579e-02  time: 6.27e-02\n",
      "epoch: 21   trainLoss: 3.4106e-02   valLoss:4.2198e-02  time: 6.47e-02\n",
      "epoch: 22   trainLoss: 3.0164e-02   valLoss:3.6389e-02  time: 6.24e-02\n",
      "epoch: 23   trainLoss: 2.8204e-02   valLoss:3.4946e-02  time: 6.46e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24   trainLoss: 2.3995e-02   valLoss:3.3784e-02  time: 6.23e-02\n",
      "epoch: 25   trainLoss: 2.0035e-02   valLoss:3.3574e-02  time: 6.25e-02\n",
      "epoch: 26   trainLoss: 1.8282e-02   valLoss:3.0007e-02  time: 6.21e-02\n",
      "epoch: 27   trainLoss: 1.6858e-02   valLoss:2.6776e-02  time: 6.26e-02\n",
      "epoch: 28   trainLoss: 1.4954e-02   valLoss:2.1598e-02  time: 6.43e-02\n",
      "epoch: 29   trainLoss: 1.1459e-02   valLoss:2.0945e-02  time: 6.42e-02\n",
      "epoch: 30   trainLoss: 1.1647e-02   valLoss:1.6619e-02  time: 6.56e-02\n",
      "epoch: 31   trainLoss: 1.0298e-02   valLoss:1.8563e-02  time: 6.29e-02\n",
      "epoch: 32   trainLoss: 9.1056e-03   valLoss:2.1805e-02  time: 6.17e-02\n",
      "epoch: 33   trainLoss: 8.5003e-03   valLoss:2.0651e-02  time: 6.53e-02\n",
      "epoch: 34   trainLoss: 8.6007e-03   valLoss:1.7038e-02  time: 7.38e-02\n",
      "epoch: 35   trainLoss: 7.8222e-03   valLoss:1.3267e-02  time: 6.35e-02\n",
      "epoch: 36   trainLoss: 7.1244e-03   valLoss:1.2390e-02  time: 6.32e-02\n",
      "epoch: 37   trainLoss: 6.8321e-03   valLoss:1.1452e-02  time: 6.50e-02\n",
      "epoch: 38   trainLoss: 6.1054e-03   valLoss:1.1076e-02  time: 6.28e-02\n",
      "epoch: 39   trainLoss: 5.4501e-03   valLoss:1.1399e-02  time: 6.19e-02\n",
      "epoch: 40   trainLoss: 5.0965e-03   valLoss:1.3698e-02  time: 6.25e-02\n",
      "epoch: 41   trainLoss: 4.8835e-03   valLoss:1.7002e-02  time: 6.47e-02\n",
      "epoch: 42   trainLoss: 4.0524e-03   valLoss:1.8973e-02  time: 6.16e-02\n",
      "epoch: 43   trainLoss: 3.8200e-03   valLoss:1.9052e-02  time: 6.17e-02\n",
      "epoch: 44   trainLoss: 3.8111e-03   valLoss:2.0724e-02  time: 6.11e-02\n",
      "epoch: 45   trainLoss: 3.4764e-03   valLoss:2.4394e-02  time: 6.20e-02\n",
      "epoch: 46   trainLoss: 3.1756e-03   valLoss:2.6815e-02  time: 6.21e-02\n",
      "epoch: 47   trainLoss: 3.0140e-03   valLoss:2.7017e-02  time: 6.17e-02\n",
      "epoch: 48   trainLoss: 3.0367e-03   valLoss:2.4827e-02  time: 6.20e-02\n",
      "epoch: 49   trainLoss: 2.6948e-03   valLoss:2.2589e-02  time: 6.21e-02\n",
      "epoch: 50   trainLoss: 2.6044e-03   valLoss:2.1856e-02  time: 6.13e-02\n",
      "epoch: 51   trainLoss: 2.4755e-03   valLoss:2.2043e-02  time: 6.21e-02\n",
      "epoch: 52   trainLoss: 2.3349e-03   valLoss:2.2863e-02  time: 6.52e-02\n",
      "epoch: 53   trainLoss: 2.2351e-03   valLoss:2.3781e-02  time: 6.45e-02\n",
      "epoch: 54   trainLoss: 2.1170e-03   valLoss:2.3381e-02  time: 6.26e-02\n",
      "epoch: 55   trainLoss: 1.9969e-03   valLoss:2.3279e-02  time: 6.59e-02\n",
      "epoch: 56   trainLoss: 1.8605e-03   valLoss:2.4848e-02  time: 6.20e-02\n",
      "epoch: 57   trainLoss: 1.7906e-03   valLoss:2.5912e-02  time: 6.14e-02\n",
      "epoch: 58   trainLoss: 1.7158e-03   valLoss:2.6252e-02  time: 6.51e-02\n",
      "epoch: 59   trainLoss: 1.5880e-03   valLoss:2.7608e-02  time: 6.28e-02\n",
      "epoch: 60   trainLoss: 1.5451e-03   valLoss:2.8719e-02  time: 6.31e-02\n",
      "epoch: 61   trainLoss: 1.4725e-03   valLoss:2.8155e-02  time: 6.28e-02\n",
      "epoch: 62   trainLoss: 1.3686e-03   valLoss:2.7722e-02  time: 6.15e-02\n",
      "epoch: 63   trainLoss: 1.3453e-03   valLoss:2.8977e-02  time: 6.17e-02\n",
      "epoch: 64   trainLoss: 1.2776e-03   valLoss:3.0427e-02  time: 6.43e-02\n",
      "epoch: 65   trainLoss: 1.1967e-03   valLoss:3.1671e-02  time: 6.30e-02\n",
      "epoch: 66   trainLoss: 1.1741e-03   valLoss:3.2861e-02  time: 6.35e-02\n",
      "epoch: 67   trainLoss: 1.1169e-03   valLoss:3.3309e-02  time: 6.36e-02\n",
      "epoch: 68   trainLoss: 1.0502e-03   valLoss:3.3650e-02  time: 6.32e-02\n",
      "epoch: 69   trainLoss: 1.0146e-03   valLoss:3.5022e-02  time: 6.18e-02\n",
      "epoch: 70   trainLoss: 9.6898e-04   valLoss:3.5934e-02  time: 6.18e-02\n",
      "epoch: 71   trainLoss: 9.2089e-04   valLoss:3.5459e-02  time: 6.17e-02\n",
      "epoch: 72   trainLoss: 8.8920e-04   valLoss:3.5193e-02  time: 6.35e-02\n",
      "epoch: 73   trainLoss: 8.5564e-04   valLoss:3.5160e-02  time: 6.17e-02\n",
      "epoch: 74   trainLoss: 8.2569e-04   valLoss:3.5117e-02  time: 6.43e-02\n",
      "epoch: 75   trainLoss: 7.9081e-04   valLoss:3.5468e-02  time: 6.22e-02\n",
      "epoch: 76   trainLoss: 7.5580e-04   valLoss:3.6412e-02  time: 6.20e-02\n",
      "epoch: 77   trainLoss: 7.3249e-04   valLoss:3.6703e-02  time: 6.15e-02\n",
      "epoch: 78   trainLoss: 7.0837e-04   valLoss:3.6926e-02  time: 6.23e-02\n",
      "epoch: 79   trainLoss: 6.8416e-04   valLoss:3.7403e-02  time: 6.28e-02\n",
      "epoch: 80   trainLoss: 6.6243e-04   valLoss:3.7151e-02  time: 6.32e-02\n",
      "epoch: 81   trainLoss: 6.3516e-04   valLoss:3.7765e-02  time: 6.29e-02\n",
      "epoch: 82   trainLoss: 6.1573e-04   valLoss:3.7462e-02  time: 6.23e-02\n",
      "epoch: 83   trainLoss: 5.8982e-04   valLoss:3.7473e-02  time: 6.20e-02\n",
      "epoch: 84   trainLoss: 5.7236e-04   valLoss:3.7806e-02  time: 6.41e-02\n",
      "epoch: 85   trainLoss: 5.5678e-04   valLoss:3.7680e-02  time: 6.33e-02\n",
      "epoch: 86   trainLoss: 5.4009e-04   valLoss:3.8337e-02  time: 6.19e-02\n",
      "epoch: 87   trainLoss: 5.3461e-04   valLoss:3.8567e-02  time: 6.37e-02\n",
      "epoch: 88   trainLoss: 5.4027e-04   valLoss:3.9602e-02  time: 6.45e-02\n",
      "epoch: 89   trainLoss: 6.1614e-04   valLoss:4.0460e-02  time: 6.25e-02\n",
      "epoch: 90   trainLoss: 9.7645e-04   valLoss:4.2132e-02  time: 6.34e-02\n",
      "epoch: 91   trainLoss: 2.6147e-03   valLoss:5.1395e-02  time: 6.27e-02\n",
      "epoch: 92   trainLoss: 8.9007e-03   valLoss:7.3006e-02  time: 6.12e-02\n",
      "epoch: 93   trainLoss: 2.7800e-02   valLoss:6.7224e-02  time: 6.15e-02\n",
      "epoch: 94   trainLoss: 2.8044e-02   valLoss:1.9598e-02  time: 6.17e-02\n",
      "epoch: 95   trainLoss: 7.1040e-03   valLoss:4.0446e-02  time: 6.18e-02\n",
      "epoch: 96   trainLoss: 1.3853e-02   valLoss:2.4843e-02  time: 6.16e-02\n",
      "epoch: 97   trainLoss: 6.2474e-03   valLoss:2.5874e-02  time: 6.39e-02\n",
      "epoch: 98   trainLoss: 8.2309e-03   valLoss:2.5545e-02  time: 6.34e-02\n",
      "epoch: 99   trainLoss: 5.9086e-03   valLoss:2.4682e-02  time: 6.28e-02\n",
      "loading checkpoint 38\n",
      "trained 38 random forest models in 4.34 seconds\n",
      "loaded train set of size 93\n",
      "epoch: 0   trainLoss: 1.0644e+00   valLoss:1.0114e+00  time: 8.44e-01\n",
      "epoch: 1   trainLoss: 9.2769e-01   valLoss:9.9639e-01  time: 8.15e-01\n",
      "epoch: 2   trainLoss: 8.0389e-01   valLoss:9.7082e-01  time: 7.99e-01\n",
      "epoch: 3   trainLoss: 6.8497e-01   valLoss:9.3588e-01  time: 7.98e-01\n",
      "epoch: 4   trainLoss: 5.9216e-01   valLoss:8.9579e-01  time: 7.97e-01\n",
      "epoch: 5   trainLoss: 5.0505e-01   valLoss:8.5946e-01  time: 7.89e-01\n",
      "epoch: 6   trainLoss: 4.3566e-01   valLoss:8.2599e-01  time: 7.83e-01\n",
      "epoch: 7   trainLoss: 3.9087e-01   valLoss:8.0040e-01  time: 7.85e-01\n",
      "epoch: 8   trainLoss: 3.4690e-01   valLoss:8.1800e-01  time: 7.76e-01\n",
      "epoch: 9   trainLoss: 3.1471e-01   valLoss:9.2120e-01  time: 7.83e-01\n",
      "epoch: 10   trainLoss: 2.9446e-01   valLoss:1.1591e+00  time: 7.76e-01\n",
      "epoch: 11   trainLoss: 2.7811e-01   valLoss:1.5825e+00  time: 7.78e-01\n",
      "epoch: 12   trainLoss: 2.5987e-01   valLoss:2.1412e+00  time: 7.86e-01\n",
      "epoch: 13   trainLoss: 2.4338e-01   valLoss:2.6631e+00  time: 7.85e-01\n",
      "epoch: 14   trainLoss: 2.2304e-01   valLoss:2.9689e+00  time: 7.74e-01\n",
      "epoch: 15   trainLoss: 1.9586e-01   valLoss:2.9342e+00  time: 7.85e-01\n",
      "epoch: 16   trainLoss: 1.8178e-01   valLoss:2.7142e+00  time: 7.86e-01\n",
      "epoch: 17   trainLoss: 1.6734e-01   valLoss:2.5258e+00  time: 8.14e-01\n",
      "epoch: 18   trainLoss: 1.5349e-01   valLoss:2.2889e+00  time: 7.87e-01\n",
      "epoch: 19   trainLoss: 1.3735e-01   valLoss:2.8114e+00  time: 8.05e-01\n",
      "epoch: 20   trainLoss: 1.1974e-01   valLoss:3.2111e+00  time: 8.09e-01\n",
      "epoch: 21   trainLoss: 1.0608e-01   valLoss:4.3519e+00  time: 8.01e-01\n",
      "epoch: 22   trainLoss: 9.5113e-02   valLoss:5.0866e+00  time: 8.27e-01\n",
      "epoch: 23   trainLoss: 8.4794e-02   valLoss:4.0585e+00  time: 8.22e-01\n",
      "epoch: 24   trainLoss: 8.2365e-02   valLoss:5.7626e+00  time: 8.08e-01\n",
      "epoch: 25   trainLoss: 8.0255e-02   valLoss:3.9812e+00  time: 8.03e-01\n",
      "epoch: 26   trainLoss: 7.2883e-02   valLoss:3.7865e+00  time: 8.12e-01\n",
      "epoch: 27   trainLoss: 6.0000e-02   valLoss:3.8537e+00  time: 7.91e-01\n",
      "epoch: 28   trainLoss: 5.9616e-02   valLoss:1.3854e+00  time: 7.95e-01\n",
      "epoch: 29   trainLoss: 5.7076e-02   valLoss:1.2887e+00  time: 7.85e-01\n",
      "epoch: 30   trainLoss: 4.8201e-02   valLoss:1.8715e+00  time: 7.96e-01\n",
      "epoch: 31   trainLoss: 4.9795e-02   valLoss:5.6409e-01  time: 7.87e-01\n",
      "epoch: 32   trainLoss: 4.2256e-02   valLoss:4.9451e-01  time: 7.93e-01\n",
      "epoch: 33   trainLoss: 4.2527e-02   valLoss:3.8998e-01  time: 7.93e-01\n",
      "epoch: 34   trainLoss: 3.3128e-02   valLoss:3.1007e-01  time: 8.10e-01\n",
      "epoch: 35   trainLoss: 3.5415e-02   valLoss:3.8986e-01  time: 7.99e-01\n",
      "epoch: 36   trainLoss: 3.0954e-02   valLoss:2.9597e-01  time: 8.06e-01\n",
      "epoch: 37   trainLoss: 3.0629e-02   valLoss:2.6841e-01  time: 7.95e-01\n",
      "epoch: 38   trainLoss: 2.7855e-02   valLoss:3.6293e-01  time: 8.03e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39   trainLoss: 3.0235e-02   valLoss:2.1835e-01  time: 7.93e-01\n",
      "epoch: 40   trainLoss: 2.7899e-02   valLoss:2.6263e-01  time: 7.92e-01\n",
      "epoch: 41   trainLoss: 2.0343e-02   valLoss:3.3062e-01  time: 7.88e-01\n",
      "epoch: 42   trainLoss: 2.1922e-02   valLoss:2.4941e-01  time: 7.94e-01\n",
      "epoch: 43   trainLoss: 1.9933e-02   valLoss:2.7546e-01  time: 7.93e-01\n",
      "epoch: 44   trainLoss: 1.8652e-02   valLoss:2.9851e-01  time: 8.18e-01\n",
      "epoch: 45   trainLoss: 1.8330e-02   valLoss:3.1254e-01  time: 8.00e-01\n",
      "epoch: 46   trainLoss: 2.3085e-02   valLoss:2.9900e-01  time: 8.24e-01\n",
      "epoch: 47   trainLoss: 2.6764e-02   valLoss:3.3843e-01  time: 8.03e-01\n",
      "epoch: 48   trainLoss: 2.7503e-02   valLoss:2.3780e-01  time: 8.03e-01\n",
      "epoch: 49   trainLoss: 1.9080e-02   valLoss:2.9635e-01  time: 8.17e-01\n",
      "epoch: 50   trainLoss: 1.4896e-02   valLoss:3.0935e-01  time: 8.15e-01\n",
      "epoch: 51   trainLoss: 1.7711e-02   valLoss:2.1122e-01  time: 8.35e-01\n",
      "epoch: 52   trainLoss: 1.3760e-02   valLoss:2.4450e-01  time: 7.93e-01\n",
      "epoch: 53   trainLoss: 1.3409e-02   valLoss:3.1514e-01  time: 7.89e-01\n",
      "epoch: 54   trainLoss: 1.3136e-02   valLoss:2.3688e-01  time: 7.87e-01\n",
      "epoch: 55   trainLoss: 1.1041e-02   valLoss:2.3555e-01  time: 8.22e-01\n",
      "epoch: 56   trainLoss: 1.1052e-02   valLoss:2.8988e-01  time: 7.99e-01\n",
      "epoch: 57   trainLoss: 1.0077e-02   valLoss:2.7535e-01  time: 7.99e-01\n",
      "epoch: 58   trainLoss: 8.8322e-03   valLoss:2.4216e-01  time: 7.98e-01\n",
      "epoch: 59   trainLoss: 9.0744e-03   valLoss:2.4567e-01  time: 8.01e-01\n",
      "epoch: 60   trainLoss: 7.4753e-03   valLoss:2.6284e-01  time: 8.13e-01\n",
      "epoch: 61   trainLoss: 7.5810e-03   valLoss:2.5736e-01  time: 7.95e-01\n",
      "epoch: 62   trainLoss: 7.2555e-03   valLoss:2.5137e-01  time: 8.13e-01\n",
      "epoch: 63   trainLoss: 6.1789e-03   valLoss:2.6196e-01  time: 8.03e-01\n",
      "epoch: 64   trainLoss: 6.5063e-03   valLoss:2.5572e-01  time: 8.04e-01\n",
      "epoch: 65   trainLoss: 5.7740e-03   valLoss:2.9081e-01  time: 8.35e-01\n",
      "epoch: 66   trainLoss: 5.1188e-03   valLoss:2.9851e-01  time: 8.12e-01\n",
      "epoch: 67   trainLoss: 5.5995e-03   valLoss:2.4224e-01  time: 8.15e-01\n",
      "epoch: 68   trainLoss: 5.5183e-03   valLoss:2.4814e-01  time: 8.52e-01\n",
      "epoch: 69   trainLoss: 6.4773e-03   valLoss:2.8792e-01  time: 8.03e-01\n",
      "epoch: 70   trainLoss: 1.2136e-02   valLoss:1.7672e-01  time: 8.02e-01\n",
      "epoch: 71   trainLoss: 2.3284e-02   valLoss:2.7556e-01  time: 8.11e-01\n",
      "epoch: 72   trainLoss: 2.3762e-02   valLoss:2.0183e-01  time: 8.06e-01\n",
      "epoch: 73   trainLoss: 1.0680e-02   valLoss:1.8378e-01  time: 8.04e-01\n",
      "epoch: 74   trainLoss: 1.3343e-02   valLoss:1.7477e-01  time: 8.13e-01\n",
      "epoch: 75   trainLoss: 1.0137e-02   valLoss:1.9010e-01  time: 8.18e-01\n",
      "epoch: 76   trainLoss: 1.0844e-02   valLoss:1.6691e-01  time: 8.09e-01\n",
      "epoch: 77   trainLoss: 9.5525e-03   valLoss:1.4301e-01  time: 8.03e-01\n",
      "epoch: 78   trainLoss: 8.5797e-03   valLoss:1.3718e-01  time: 8.20e-01\n",
      "epoch: 79   trainLoss: 8.0335e-03   valLoss:1.7325e-01  time: 8.01e-01\n",
      "epoch: 80   trainLoss: 7.3336e-03   valLoss:1.6211e-01  time: 8.34e-01\n",
      "epoch: 81   trainLoss: 7.0574e-03   valLoss:1.4815e-01  time: 8.09e-01\n",
      "epoch: 82   trainLoss: 6.5993e-03   valLoss:1.3127e-01  time: 8.02e-01\n",
      "epoch: 83   trainLoss: 6.3573e-03   valLoss:1.3827e-01  time: 8.10e-01\n",
      "epoch: 84   trainLoss: 5.8517e-03   valLoss:1.5757e-01  time: 7.99e-01\n",
      "epoch: 85   trainLoss: 5.6096e-03   valLoss:1.7097e-01  time: 7.94e-01\n",
      "epoch: 86   trainLoss: 6.0907e-03   valLoss:1.3181e-01  time: 7.87e-01\n",
      "epoch: 87   trainLoss: 5.8867e-03   valLoss:1.3874e-01  time: 8.05e-01\n",
      "epoch: 88   trainLoss: 6.7816e-03   valLoss:1.5776e-01  time: 7.97e-01\n",
      "epoch: 89   trainLoss: 6.0158e-03   valLoss:1.5463e-01  time: 8.03e-01\n",
      "epoch: 90   trainLoss: 5.4528e-03   valLoss:1.3221e-01  time: 7.88e-01\n",
      "epoch: 91   trainLoss: 4.0700e-03   valLoss:1.3286e-01  time: 7.90e-01\n",
      "epoch: 92   trainLoss: 3.5959e-03   valLoss:1.3141e-01  time: 7.94e-01\n",
      "epoch: 93   trainLoss: 4.3305e-03   valLoss:1.2967e-01  time: 7.93e-01\n",
      "epoch: 94   trainLoss: 3.6711e-03   valLoss:1.2939e-01  time: 8.08e-01\n",
      "epoch: 95   trainLoss: 3.2309e-03   valLoss:1.3426e-01  time: 7.87e-01\n",
      "epoch: 96   trainLoss: 3.2954e-03   valLoss:1.1680e-01  time: 7.88e-01\n",
      "epoch: 97   trainLoss: 3.2714e-03   valLoss:1.2086e-01  time: 7.93e-01\n",
      "epoch: 98   trainLoss: 4.0800e-03   valLoss:1.0651e-01  time: 7.90e-01\n",
      "epoch: 99   trainLoss: 6.0727e-03   valLoss:1.5907e-01  time: 7.93e-01\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 6.9693e-01   valLoss:1.0828e+00  time: 8.20e-01\n",
      "epoch: 1   trainLoss: 6.3838e-01   valLoss:9.3964e-01  time: 8.10e-01\n",
      "epoch: 2   trainLoss: 4.3725e-01   valLoss:5.3067e-01  time: 8.01e-01\n",
      "epoch: 3   trainLoss: 2.4556e-01   valLoss:5.8162e-01  time: 8.11e-01\n",
      "epoch: 4   trainLoss: 2.0604e-01   valLoss:5.2971e-01  time: 8.03e-01\n",
      "epoch: 5   trainLoss: 1.6121e-01   valLoss:3.6672e-01  time: 8.01e-01\n",
      "epoch: 6   trainLoss: 1.4224e-01   valLoss:2.9166e-01  time: 7.84e-01\n",
      "epoch: 7   trainLoss: 1.2579e-01   valLoss:2.8162e-01  time: 7.95e-01\n",
      "epoch: 8   trainLoss: 1.1368e-01   valLoss:3.0798e-01  time: 7.84e-01\n",
      "epoch: 9   trainLoss: 1.0452e-01   valLoss:4.1664e-01  time: 7.89e-01\n",
      "epoch: 10   trainLoss: 1.0082e-01   valLoss:2.7548e-01  time: 7.93e-01\n",
      "epoch: 11   trainLoss: 8.5183e-02   valLoss:1.7804e-01  time: 7.93e-01\n",
      "epoch: 12   trainLoss: 7.8773e-02   valLoss:1.6858e-01  time: 7.88e-01\n",
      "epoch: 13   trainLoss: 6.8884e-02   valLoss:1.4995e-01  time: 8.06e-01\n",
      "epoch: 14   trainLoss: 6.0900e-02   valLoss:1.1315e-01  time: 8.07e-01\n",
      "epoch: 15   trainLoss: 5.9118e-02   valLoss:1.0767e-01  time: 7.99e-01\n",
      "epoch: 16   trainLoss: 5.1785e-02   valLoss:1.3092e-01  time: 7.89e-01\n",
      "epoch: 17   trainLoss: 4.9850e-02   valLoss:7.9966e-02  time: 7.99e-01\n",
      "epoch: 18   trainLoss: 4.6022e-02   valLoss:1.2204e-01  time: 8.01e-01\n",
      "epoch: 19   trainLoss: 4.3822e-02   valLoss:9.2664e-02  time: 8.07e-01\n",
      "epoch: 20   trainLoss: 4.4636e-02   valLoss:1.4145e-01  time: 7.89e-01\n",
      "epoch: 21   trainLoss: 4.1625e-02   valLoss:8.5261e-02  time: 7.87e-01\n",
      "epoch: 22   trainLoss: 3.4802e-02   valLoss:5.8955e-02  time: 7.94e-01\n",
      "epoch: 23   trainLoss: 3.2994e-02   valLoss:7.4493e-02  time: 8.09e-01\n",
      "epoch: 24   trainLoss: 3.1961e-02   valLoss:4.0242e-02  time: 7.99e-01\n",
      "epoch: 25   trainLoss: 2.9168e-02   valLoss:3.7058e-02  time: 7.96e-01\n",
      "epoch: 26   trainLoss: 2.4948e-02   valLoss:6.2915e-02  time: 8.01e-01\n",
      "epoch: 27   trainLoss: 2.5040e-02   valLoss:4.0217e-02  time: 8.00e-01\n",
      "epoch: 28   trainLoss: 2.4705e-02   valLoss:5.4598e-02  time: 7.92e-01\n",
      "epoch: 29   trainLoss: 2.1527e-02   valLoss:5.3085e-02  time: 7.85e-01\n",
      "epoch: 30   trainLoss: 1.9739e-02   valLoss:3.9865e-02  time: 7.89e-01\n",
      "epoch: 31   trainLoss: 2.0347e-02   valLoss:5.3038e-02  time: 7.98e-01\n",
      "epoch: 32   trainLoss: 1.8841e-02   valLoss:3.5037e-02  time: 8.01e-01\n",
      "epoch: 33   trainLoss: 1.7064e-02   valLoss:2.8660e-02  time: 7.91e-01\n",
      "epoch: 34   trainLoss: 1.5747e-02   valLoss:3.9470e-02  time: 7.92e-01\n",
      "epoch: 35   trainLoss: 1.6426e-02   valLoss:3.1852e-02  time: 7.94e-01\n",
      "epoch: 36   trainLoss: 1.6676e-02   valLoss:4.0715e-02  time: 7.97e-01\n",
      "epoch: 37   trainLoss: 1.6107e-02   valLoss:3.4588e-02  time: 8.01e-01\n",
      "epoch: 38   trainLoss: 1.5387e-02   valLoss:5.1259e-02  time: 7.92e-01\n",
      "epoch: 39   trainLoss: 1.5712e-02   valLoss:4.6600e-02  time: 7.99e-01\n",
      "epoch: 40   trainLoss: 1.6550e-02   valLoss:7.5957e-02  time: 7.92e-01\n",
      "epoch: 41   trainLoss: 1.4304e-02   valLoss:4.5672e-02  time: 8.01e-01\n",
      "epoch: 42   trainLoss: 1.1628e-02   valLoss:3.1934e-02  time: 7.89e-01\n",
      "epoch: 43   trainLoss: 1.0901e-02   valLoss:4.3446e-02  time: 7.96e-01\n",
      "epoch: 44   trainLoss: 1.1671e-02   valLoss:2.8986e-02  time: 8.06e-01\n",
      "epoch: 45   trainLoss: 1.0759e-02   valLoss:2.8534e-02  time: 7.90e-01\n",
      "epoch: 46   trainLoss: 9.7935e-03   valLoss:3.3680e-02  time: 7.93e-01\n",
      "epoch: 47   trainLoss: 1.0877e-02   valLoss:2.7663e-02  time: 8.03e-01\n",
      "epoch: 48   trainLoss: 1.1011e-02   valLoss:3.9693e-02  time: 7.99e-01\n",
      "epoch: 49   trainLoss: 1.3101e-02   valLoss:4.7318e-02  time: 7.89e-01\n",
      "epoch: 50   trainLoss: 1.9971e-02   valLoss:6.2703e-02  time: 8.02e-01\n",
      "epoch: 51   trainLoss: 2.1519e-02   valLoss:4.0068e-02  time: 7.95e-01\n",
      "epoch: 52   trainLoss: 1.0334e-02   valLoss:4.7639e-02  time: 7.89e-01\n",
      "epoch: 53   trainLoss: 1.3127e-02   valLoss:6.0890e-02  time: 7.99e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54   trainLoss: 1.5001e-02   valLoss:4.2860e-02  time: 8.11e-01\n",
      "epoch: 55   trainLoss: 9.2667e-03   valLoss:4.4487e-02  time: 8.02e-01\n",
      "epoch: 56   trainLoss: 1.1861e-02   valLoss:3.5914e-02  time: 7.84e-01\n",
      "epoch: 57   trainLoss: 9.3267e-03   valLoss:3.1037e-02  time: 8.14e-01\n",
      "epoch: 58   trainLoss: 9.9017e-03   valLoss:2.8121e-02  time: 7.88e-01\n",
      "epoch: 59   trainLoss: 8.5214e-03   valLoss:2.6441e-02  time: 7.91e-01\n",
      "epoch: 60   trainLoss: 8.3902e-03   valLoss:3.1398e-02  time: 8.01e-01\n",
      "epoch: 61   trainLoss: 8.0871e-03   valLoss:2.5511e-02  time: 8.17e-01\n",
      "epoch: 62   trainLoss: 6.9744e-03   valLoss:1.8661e-02  time: 7.95e-01\n",
      "epoch: 63   trainLoss: 7.7141e-03   valLoss:1.9832e-02  time: 7.91e-01\n",
      "epoch: 64   trainLoss: 6.1881e-03   valLoss:1.7499e-02  time: 7.88e-01\n",
      "epoch: 65   trainLoss: 6.6149e-03   valLoss:1.5491e-02  time: 8.24e-01\n",
      "epoch: 66   trainLoss: 6.1326e-03   valLoss:1.6207e-02  time: 8.04e-01\n",
      "epoch: 67   trainLoss: 5.6334e-03   valLoss:1.8924e-02  time: 7.98e-01\n",
      "epoch: 68   trainLoss: 5.8846e-03   valLoss:1.2971e-02  time: 7.86e-01\n",
      "epoch: 69   trainLoss: 5.8258e-03   valLoss:2.7013e-02  time: 7.91e-01\n",
      "epoch: 70   trainLoss: 7.4218e-03   valLoss:4.3360e-02  time: 7.97e-01\n",
      "epoch: 71   trainLoss: 1.7788e-02   valLoss:6.3259e-02  time: 7.82e-01\n",
      "epoch: 72   trainLoss: 3.4800e-02   valLoss:6.8882e-02  time: 7.89e-01\n",
      "epoch: 73   trainLoss: 1.8869e-02   valLoss:1.1428e-01  time: 8.13e-01\n",
      "epoch: 74   trainLoss: 2.0195e-02   valLoss:3.4368e-02  time: 7.98e-01\n",
      "epoch: 75   trainLoss: 1.4219e-02   valLoss:3.1507e-02  time: 7.91e-01\n",
      "epoch: 76   trainLoss: 1.8420e-02   valLoss:5.0501e-02  time: 7.88e-01\n",
      "epoch: 77   trainLoss: 1.1794e-02   valLoss:1.0267e-01  time: 7.93e-01\n",
      "epoch: 78   trainLoss: 1.4658e-02   valLoss:8.5868e-02  time: 7.88e-01\n",
      "epoch: 79   trainLoss: 1.1988e-02   valLoss:8.2685e-02  time: 7.90e-01\n",
      "epoch: 80   trainLoss: 1.0939e-02   valLoss:8.5849e-02  time: 7.88e-01\n",
      "epoch: 81   trainLoss: 1.1032e-02   valLoss:8.4520e-02  time: 7.89e-01\n",
      "epoch: 82   trainLoss: 1.0130e-02   valLoss:7.1011e-02  time: 7.93e-01\n",
      "epoch: 83   trainLoss: 8.6948e-03   valLoss:5.1145e-02  time: 7.89e-01\n",
      "epoch: 84   trainLoss: 8.8004e-03   valLoss:4.1224e-02  time: 7.93e-01\n",
      "epoch: 85   trainLoss: 8.2617e-03   valLoss:5.1367e-02  time: 7.92e-01\n",
      "epoch: 86   trainLoss: 7.3954e-03   valLoss:5.0649e-02  time: 7.95e-01\n",
      "epoch: 87   trainLoss: 6.7267e-03   valLoss:4.1422e-02  time: 7.96e-01\n",
      "epoch: 88   trainLoss: 6.8994e-03   valLoss:3.5656e-02  time: 8.04e-01\n",
      "epoch: 89   trainLoss: 6.0396e-03   valLoss:3.3253e-02  time: 7.90e-01\n",
      "epoch: 90   trainLoss: 5.9630e-03   valLoss:2.6579e-02  time: 7.96e-01\n",
      "epoch: 91   trainLoss: 5.2914e-03   valLoss:2.1327e-02  time: 7.95e-01\n",
      "epoch: 92   trainLoss: 5.2643e-03   valLoss:2.4188e-02  time: 8.07e-01\n",
      "epoch: 93   trainLoss: 4.9393e-03   valLoss:2.4574e-02  time: 8.08e-01\n",
      "epoch: 94   trainLoss: 4.7542e-03   valLoss:2.6275e-02  time: 7.99e-01\n",
      "epoch: 95   trainLoss: 4.4586e-03   valLoss:2.7492e-02  time: 7.93e-01\n",
      "epoch: 96   trainLoss: 4.1150e-03   valLoss:3.0278e-02  time: 8.01e-01\n",
      "epoch: 97   trainLoss: 4.4299e-03   valLoss:3.3428e-02  time: 7.90e-01\n",
      "epoch: 98   trainLoss: 5.9182e-03   valLoss:3.3639e-02  time: 8.01e-01\n",
      "epoch: 99   trainLoss: 1.3409e-02   valLoss:1.0549e-01  time: 7.90e-01\n",
      "loading checkpoint 68\n",
      "trained 38 random forest models in 6.29 seconds\n",
      "loaded train set of size 914\n",
      "epoch: 0   trainLoss: 7.2961e-01   valLoss:9.0999e-01  time: 7.84e+00\n",
      "epoch: 1   trainLoss: 4.0049e-01   valLoss:8.8734e-01  time: 7.78e+00\n",
      "epoch: 2   trainLoss: 2.8314e-01   valLoss:8.8488e-01  time: 7.79e+00\n",
      "epoch: 3   trainLoss: 2.1856e-01   valLoss:9.5323e-01  time: 7.84e+00\n",
      "epoch: 4   trainLoss: 1.7686e-01   valLoss:9.5652e-01  time: 7.89e+00\n",
      "epoch: 5   trainLoss: 1.6986e-01   valLoss:6.3799e-01  time: 7.81e+00\n",
      "epoch: 6   trainLoss: 1.3840e-01   valLoss:3.3258e-01  time: 7.84e+00\n",
      "epoch: 7   trainLoss: 1.2148e-01   valLoss:2.0475e-01  time: 7.80e+00\n",
      "epoch: 8   trainLoss: 1.0936e-01   valLoss:2.1215e-01  time: 7.75e+00\n",
      "epoch: 9   trainLoss: 1.1696e-01   valLoss:2.3444e-01  time: 7.79e+00\n",
      "epoch: 10   trainLoss: 1.0812e-01   valLoss:2.1604e-01  time: 7.81e+00\n",
      "epoch: 11   trainLoss: 1.0143e-01   valLoss:1.9389e-01  time: 7.79e+00\n",
      "epoch: 12   trainLoss: 9.1671e-02   valLoss:2.4197e-01  time: 7.82e+00\n",
      "epoch: 13   trainLoss: 8.8147e-02   valLoss:2.6981e-01  time: 7.74e+00\n",
      "epoch: 14   trainLoss: 8.8183e-02   valLoss:3.0855e-01  time: 7.93e+00\n",
      "epoch: 15   trainLoss: 8.7940e-02   valLoss:1.6257e-01  time: 7.86e+00\n",
      "epoch: 16   trainLoss: 7.6228e-02   valLoss:2.3629e-01  time: 7.88e+00\n",
      "epoch: 17   trainLoss: 7.5474e-02   valLoss:3.3161e-01  time: 7.81e+00\n",
      "epoch: 18   trainLoss: 7.3322e-02   valLoss:1.3651e-01  time: 7.85e+00\n",
      "epoch: 19   trainLoss: 6.5066e-02   valLoss:1.4287e-01  time: 7.77e+00\n",
      "epoch: 20   trainLoss: 6.2324e-02   valLoss:1.4272e-01  time: 7.80e+00\n",
      "epoch: 21   trainLoss: 6.2458e-02   valLoss:1.2778e-01  time: 7.85e+00\n",
      "epoch: 22   trainLoss: 5.4308e-02   valLoss:1.6380e-01  time: 7.86e+00\n",
      "epoch: 23   trainLoss: 5.7477e-02   valLoss:1.6019e-01  time: 7.82e+00\n",
      "epoch: 24   trainLoss: 5.6612e-02   valLoss:1.1242e-01  time: 7.74e+00\n",
      "epoch: 25   trainLoss: 5.3713e-02   valLoss:1.0717e-01  time: 7.83e+00\n",
      "epoch: 26   trainLoss: 5.7278e-02   valLoss:1.6804e-01  time: 7.85e+00\n",
      "epoch: 27   trainLoss: 6.7174e-02   valLoss:1.5974e-01  time: 7.86e+00\n",
      "epoch: 28   trainLoss: 6.4696e-02   valLoss:1.8510e-01  time: 7.84e+00\n",
      "epoch: 29   trainLoss: 5.9058e-02   valLoss:1.2145e-01  time: 7.87e+00\n",
      "epoch: 30   trainLoss: 5.5808e-02   valLoss:8.6227e-02  time: 7.85e+00\n",
      "epoch: 31   trainLoss: 5.2781e-02   valLoss:1.7902e-01  time: 7.79e+00\n",
      "epoch: 32   trainLoss: 5.0042e-02   valLoss:1.2657e-01  time: 7.76e+00\n",
      "epoch: 33   trainLoss: 4.7190e-02   valLoss:8.8866e-02  time: 7.90e+00\n",
      "epoch: 34   trainLoss: 4.2645e-02   valLoss:1.6162e-01  time: 7.76e+00\n",
      "epoch: 35   trainLoss: 4.1136e-02   valLoss:8.0183e-02  time: 7.80e+00\n",
      "epoch: 36   trainLoss: 4.6835e-02   valLoss:8.6880e-02  time: 7.80e+00\n",
      "epoch: 37   trainLoss: 4.4903e-02   valLoss:9.2401e-02  time: 7.77e+00\n",
      "epoch: 38   trainLoss: 4.6589e-02   valLoss:1.3921e-01  time: 7.81e+00\n",
      "epoch: 39   trainLoss: 4.7719e-02   valLoss:9.5943e-02  time: 7.73e+00\n",
      "epoch: 40   trainLoss: 4.6460e-02   valLoss:1.0098e-01  time: 7.76e+00\n",
      "epoch: 41   trainLoss: 4.4310e-02   valLoss:7.4945e-02  time: 7.81e+00\n",
      "epoch: 42   trainLoss: 4.7249e-02   valLoss:6.9357e-02  time: 7.80e+00\n",
      "epoch: 43   trainLoss: 4.4791e-02   valLoss:1.1569e-01  time: 7.81e+00\n",
      "epoch: 44   trainLoss: 5.6370e-02   valLoss:9.4112e-02  time: 7.82e+00\n",
      "epoch: 45   trainLoss: 5.5313e-02   valLoss:9.1925e-02  time: 7.85e+00\n",
      "epoch: 46   trainLoss: 5.1447e-02   valLoss:7.4121e-02  time: 7.83e+00\n",
      "epoch: 47   trainLoss: 4.7874e-02   valLoss:1.5990e-01  time: 7.77e+00\n",
      "epoch: 48   trainLoss: 4.2940e-02   valLoss:6.7284e-02  time: 7.78e+00\n",
      "epoch: 49   trainLoss: 4.9478e-02   valLoss:7.2185e-02  time: 7.77e+00\n",
      "epoch: 50   trainLoss: 3.6048e-02   valLoss:8.4465e-02  time: 7.79e+00\n",
      "epoch: 51   trainLoss: 3.1529e-02   valLoss:7.1412e-02  time: 7.85e+00\n",
      "epoch: 52   trainLoss: 3.9187e-02   valLoss:6.1952e-02  time: 7.90e+00\n",
      "epoch: 53   trainLoss: 3.9605e-02   valLoss:4.0041e-02  time: 7.83e+00\n",
      "epoch: 54   trainLoss: 3.4573e-02   valLoss:7.3527e-02  time: 7.82e+00\n",
      "epoch: 55   trainLoss: 3.1067e-02   valLoss:4.9990e-02  time: 7.79e+00\n",
      "epoch: 56   trainLoss: 3.6169e-02   valLoss:6.1273e-02  time: 7.84e+00\n",
      "epoch: 57   trainLoss: 3.3120e-02   valLoss:4.9734e-02  time: 7.80e+00\n",
      "epoch: 58   trainLoss: 3.7218e-02   valLoss:8.2700e-02  time: 7.90e+00\n",
      "epoch: 59   trainLoss: 3.1435e-02   valLoss:4.5324e-02  time: 7.79e+00\n",
      "epoch: 60   trainLoss: 3.7801e-02   valLoss:1.1537e-01  time: 7.79e+00\n",
      "epoch: 61   trainLoss: 4.7076e-02   valLoss:7.6738e-02  time: 7.83e+00\n",
      "epoch: 62   trainLoss: 3.7655e-02   valLoss:9.2942e-02  time: 7.83e+00\n",
      "epoch: 63   trainLoss: 3.3879e-02   valLoss:4.8449e-02  time: 7.79e+00\n",
      "epoch: 64   trainLoss: 3.2388e-02   valLoss:5.6830e-02  time: 7.77e+00\n",
      "epoch: 65   trainLoss: 3.5369e-02   valLoss:8.0466e-02  time: 7.79e+00\n",
      "epoch: 66   trainLoss: 3.4239e-02   valLoss:1.5496e-01  time: 7.80e+00\n",
      "epoch: 67   trainLoss: 3.3205e-02   valLoss:9.7067e-02  time: 7.79e+00\n",
      "epoch: 68   trainLoss: 3.0710e-02   valLoss:4.5670e-02  time: 7.81e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69   trainLoss: 2.8861e-02   valLoss:4.3609e-02  time: 7.88e+00\n",
      "epoch: 70   trainLoss: 4.5674e-02   valLoss:5.5100e-02  time: 7.85e+00\n",
      "epoch: 71   trainLoss: 3.5261e-02   valLoss:4.7679e-02  time: 7.78e+00\n",
      "epoch: 72   trainLoss: 3.4419e-02   valLoss:4.9637e-02  time: 7.82e+00\n",
      "epoch: 73   trainLoss: 3.9823e-02   valLoss:8.7582e-02  time: 7.82e+00\n",
      "epoch: 74   trainLoss: 4.1319e-02   valLoss:5.1229e-02  time: 7.73e+00\n",
      "epoch: 75   trainLoss: 4.0186e-02   valLoss:7.7679e-02  time: 7.84e+00\n",
      "epoch: 76   trainLoss: 3.4270e-02   valLoss:5.4962e-02  time: 7.84e+00\n",
      "epoch: 77   trainLoss: 4.8045e-02   valLoss:4.7933e-02  time: 7.81e+00\n",
      "epoch: 78   trainLoss: 4.6611e-02   valLoss:4.2696e-02  time: 7.82e+00\n",
      "epoch: 79   trainLoss: 4.0320e-02   valLoss:5.8746e-02  time: 7.85e+00\n",
      "epoch: 80   trainLoss: 3.4100e-02   valLoss:5.7013e-02  time: 7.77e+00\n",
      "epoch: 81   trainLoss: 3.4612e-02   valLoss:7.3466e-02  time: 7.85e+00\n",
      "epoch: 82   trainLoss: 3.6770e-02   valLoss:8.4578e-02  time: 7.89e+00\n",
      "epoch: 83   trainLoss: 4.1696e-02   valLoss:6.5051e-02  time: 7.87e+00\n",
      "epoch: 84   trainLoss: 3.7770e-02   valLoss:5.6536e-02  time: 7.86e+00\n",
      "epoch: 85   trainLoss: 3.8235e-02   valLoss:8.4619e-02  time: 7.77e+00\n",
      "epoch: 86   trainLoss: 4.0434e-02   valLoss:5.4850e-02  time: 7.83e+00\n",
      "epoch: 87   trainLoss: 2.9241e-02   valLoss:4.7358e-02  time: 7.92e+00\n",
      "epoch: 88   trainLoss: 3.9209e-02   valLoss:3.7245e-02  time: 7.87e+00\n",
      "epoch: 89   trainLoss: 3.5231e-02   valLoss:3.2886e-02  time: 7.78e+00\n",
      "epoch: 90   trainLoss: 3.1069e-02   valLoss:3.9202e-02  time: 7.79e+00\n",
      "epoch: 91   trainLoss: 2.7121e-02   valLoss:7.0418e-02  time: 7.88e+00\n",
      "epoch: 92   trainLoss: 2.7381e-02   valLoss:1.0136e-01  time: 7.79e+00\n",
      "epoch: 93   trainLoss: 3.3308e-02   valLoss:3.8339e-02  time: 7.83e+00\n",
      "epoch: 94   trainLoss: 4.2951e-02   valLoss:4.0035e-02  time: 7.87e+00\n",
      "epoch: 95   trainLoss: 4.0068e-02   valLoss:4.1287e-02  time: 7.80e+00\n",
      "epoch: 96   trainLoss: 3.5752e-02   valLoss:3.5167e-02  time: 7.82e+00\n",
      "epoch: 97   trainLoss: 3.4307e-02   valLoss:4.4038e-02  time: 7.83e+00\n",
      "epoch: 98   trainLoss: 3.5505e-02   valLoss:7.9934e-02  time: 7.82e+00\n",
      "epoch: 99   trainLoss: 4.3927e-02   valLoss:4.6452e-02  time: 7.82e+00\n",
      "loading checkpoint 89\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 6.6768e-01   valLoss:1.0587e+00  time: 7.91e+00\n",
      "epoch: 1   trainLoss: 2.4945e-01   valLoss:5.8079e-01  time: 7.79e+00\n",
      "epoch: 2   trainLoss: 1.9435e-01   valLoss:2.6789e-01  time: 7.81e+00\n",
      "epoch: 3   trainLoss: 1.4601e-01   valLoss:1.6569e-01  time: 7.91e+00\n",
      "epoch: 4   trainLoss: 1.2161e-01   valLoss:1.4965e-01  time: 7.91e+00\n",
      "epoch: 5   trainLoss: 1.0279e-01   valLoss:1.1443e-01  time: 7.86e+00\n",
      "epoch: 6   trainLoss: 9.5195e-02   valLoss:1.0224e-01  time: 7.69e+00\n",
      "epoch: 7   trainLoss: 8.2819e-02   valLoss:9.5115e-02  time: 7.76e+00\n",
      "epoch: 8   trainLoss: 7.8139e-02   valLoss:1.1770e-01  time: 7.67e+00\n",
      "epoch: 9   trainLoss: 6.8654e-02   valLoss:7.1849e-02  time: 7.67e+00\n",
      "epoch: 10   trainLoss: 8.0202e-02   valLoss:8.1861e-02  time: 7.65e+00\n",
      "epoch: 11   trainLoss: 6.9741e-02   valLoss:2.1238e-01  time: 7.62e+00\n",
      "epoch: 12   trainLoss: 7.7673e-02   valLoss:1.2437e-01  time: 7.70e+00\n",
      "epoch: 13   trainLoss: 8.5107e-02   valLoss:9.8382e-02  time: 7.62e+00\n",
      "epoch: 14   trainLoss: 7.8813e-02   valLoss:8.7870e-02  time: 7.84e+00\n",
      "epoch: 15   trainLoss: 6.7623e-02   valLoss:1.1174e-01  time: 7.80e+00\n",
      "epoch: 16   trainLoss: 6.4636e-02   valLoss:9.5569e-02  time: 7.90e+00\n",
      "epoch: 17   trainLoss: 6.5677e-02   valLoss:8.4534e-02  time: 7.78e+00\n",
      "epoch: 18   trainLoss: 5.8099e-02   valLoss:8.4722e-02  time: 7.82e+00\n",
      "epoch: 19   trainLoss: 5.3704e-02   valLoss:9.3231e-02  time: 7.79e+00\n",
      "epoch: 20   trainLoss: 6.6574e-02   valLoss:1.1965e-01  time: 7.87e+00\n",
      "epoch: 21   trainLoss: 5.3399e-02   valLoss:9.0468e-02  time: 7.87e+00\n",
      "epoch: 22   trainLoss: 5.7853e-02   valLoss:8.1782e-02  time: 7.88e+00\n",
      "epoch: 23   trainLoss: 5.0282e-02   valLoss:1.0300e-01  time: 7.77e+00\n",
      "epoch: 24   trainLoss: 4.5322e-02   valLoss:1.3307e-01  time: 7.85e+00\n",
      "epoch: 25   trainLoss: 4.6897e-02   valLoss:9.8100e-02  time: 7.88e+00\n",
      "epoch: 26   trainLoss: 4.8882e-02   valLoss:1.3571e-01  time: 7.88e+00\n",
      "epoch: 27   trainLoss: 5.2584e-02   valLoss:1.1235e-01  time: 7.89e+00\n",
      "epoch: 28   trainLoss: 5.5450e-02   valLoss:9.7503e-02  time: 7.98e+00\n",
      "epoch: 29   trainLoss: 6.3777e-02   valLoss:2.2714e-01  time: 7.84e+00\n",
      "epoch: 30   trainLoss: 5.6944e-02   valLoss:4.4932e-01  time: 7.80e+00\n",
      "epoch: 31   trainLoss: 6.0096e-02   valLoss:1.7089e-01  time: 7.82e+00\n",
      "epoch: 32   trainLoss: 5.4890e-02   valLoss:3.8951e-02  time: 7.86e+00\n",
      "epoch: 33   trainLoss: 4.9736e-02   valLoss:1.3035e-01  time: 7.83e+00\n",
      "epoch: 34   trainLoss: 6.4696e-02   valLoss:2.2121e-01  time: 7.93e+00\n",
      "epoch: 35   trainLoss: 4.6370e-02   valLoss:1.7492e-01  time: 8.07e+00\n",
      "epoch: 36   trainLoss: 4.3000e-02   valLoss:1.5636e-01  time: 8.05e+00\n",
      "epoch: 37   trainLoss: 4.4945e-02   valLoss:1.6296e-01  time: 7.95e+00\n",
      "epoch: 38   trainLoss: 5.0961e-02   valLoss:2.8156e-01  time: 7.77e+00\n",
      "epoch: 39   trainLoss: 4.5367e-02   valLoss:6.3514e-02  time: 8.08e+00\n",
      "epoch: 40   trainLoss: 5.0506e-02   valLoss:8.6261e-02  time: 7.92e+00\n",
      "epoch: 41   trainLoss: 4.5915e-02   valLoss:1.3462e-01  time: 7.78e+00\n",
      "epoch: 42   trainLoss: 4.0405e-02   valLoss:1.4637e-01  time: 7.81e+00\n",
      "epoch: 43   trainLoss: 4.2370e-02   valLoss:7.1090e-02  time: 7.87e+00\n",
      "epoch: 44   trainLoss: 3.6304e-02   valLoss:3.9490e-02  time: 7.80e+00\n",
      "epoch: 45   trainLoss: 4.0385e-02   valLoss:9.5707e-02  time: 7.80e+00\n",
      "epoch: 46   trainLoss: 3.8043e-02   valLoss:8.3187e-02  time: 7.81e+00\n",
      "epoch: 47   trainLoss: 5.5224e-02   valLoss:5.3526e-02  time: 7.72e+00\n",
      "epoch: 48   trainLoss: 3.3652e-02   valLoss:8.3308e-02  time: 7.80e+00\n",
      "epoch: 49   trainLoss: 3.7342e-02   valLoss:1.1181e-01  time: 7.80e+00\n",
      "epoch: 50   trainLoss: 4.2067e-02   valLoss:1.3572e-01  time: 7.88e+00\n",
      "epoch: 51   trainLoss: 3.3467e-02   valLoss:7.4068e-02  time: 7.82e+00\n",
      "epoch: 52   trainLoss: 4.0910e-02   valLoss:5.7640e-02  time: 7.94e+00\n",
      "epoch: 53   trainLoss: 4.4696e-02   valLoss:1.3438e-01  time: 7.86e+00\n",
      "epoch: 54   trainLoss: 7.5903e-02   valLoss:5.9291e-02  time: 7.88e+00\n",
      "epoch: 55   trainLoss: 6.2324e-02   valLoss:9.3587e-02  time: 7.83e+00\n",
      "epoch: 56   trainLoss: 5.9348e-02   valLoss:1.1404e-01  time: 7.85e+00\n",
      "epoch: 57   trainLoss: 5.8051e-02   valLoss:7.5861e-02  time: 7.91e+00\n",
      "epoch: 58   trainLoss: 3.9519e-02   valLoss:4.8961e-02  time: 7.93e+00\n",
      "epoch: 59   trainLoss: 3.6832e-02   valLoss:7.4324e-02  time: 7.83e+00\n",
      "epoch: 60   trainLoss: 4.6810e-02   valLoss:9.9640e-02  time: 7.86e+00\n",
      "epoch: 61   trainLoss: 5.8514e-02   valLoss:5.2857e-02  time: 7.84e+00\n",
      "epoch: 62   trainLoss: 4.7436e-02   valLoss:9.9146e-02  time: 7.83e+00\n",
      "epoch: 63   trainLoss: 4.7663e-02   valLoss:2.0414e-01  time: 7.78e+00\n",
      "epoch: 64   trainLoss: 4.2605e-02   valLoss:3.1001e-01  time: 7.84e+00\n",
      "epoch: 65   trainLoss: 4.8125e-02   valLoss:7.2298e-02  time: 7.82e+00\n",
      "epoch: 66   trainLoss: 4.6120e-02   valLoss:9.3085e-02  time: 7.94e+00\n",
      "epoch: 67   trainLoss: 4.0228e-02   valLoss:2.5857e-01  time: 7.83e+00\n",
      "epoch: 68   trainLoss: 4.2642e-02   valLoss:8.9769e-02  time: 7.82e+00\n",
      "epoch: 69   trainLoss: 5.4735e-02   valLoss:1.9338e-01  time: 7.80e+00\n",
      "epoch: 70   trainLoss: 3.6029e-02   valLoss:6.0522e-02  time: 7.85e+00\n",
      "epoch: 71   trainLoss: 3.7098e-02   valLoss:1.2162e-01  time: 7.83e+00\n",
      "epoch: 72   trainLoss: 3.9331e-02   valLoss:1.0505e-01  time: 7.80e+00\n",
      "epoch: 73   trainLoss: 4.1751e-02   valLoss:5.8905e-02  time: 7.78e+00\n",
      "epoch: 74   trainLoss: 3.8932e-02   valLoss:1.0423e-01  time: 7.84e+00\n",
      "epoch: 75   trainLoss: 3.3817e-02   valLoss:1.2877e-01  time: 7.83e+00\n",
      "epoch: 76   trainLoss: 3.8175e-02   valLoss:9.4586e-02  time: 7.82e+00\n",
      "epoch: 77   trainLoss: 3.0441e-02   valLoss:1.4556e-01  time: 7.86e+00\n",
      "epoch: 78   trainLoss: 3.1685e-02   valLoss:5.7217e-02  time: 7.75e+00\n",
      "epoch: 79   trainLoss: 3.3111e-02   valLoss:5.7503e-02  time: 7.71e+00\n",
      "epoch: 80   trainLoss: 3.5817e-02   valLoss:7.0733e-02  time: 7.78e+00\n",
      "epoch: 81   trainLoss: 3.9342e-02   valLoss:9.7761e-02  time: 7.83e+00\n",
      "epoch: 82   trainLoss: 3.9811e-02   valLoss:1.2841e-01  time: 7.77e+00\n",
      "epoch: 83   trainLoss: 3.4902e-02   valLoss:9.8228e-02  time: 7.82e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 84   trainLoss: 4.0260e-02   valLoss:1.2618e-01  time: 7.69e+00\n",
      "epoch: 85   trainLoss: 4.3327e-02   valLoss:9.6933e-02  time: 7.81e+00\n",
      "epoch: 86   trainLoss: 4.7483e-02   valLoss:1.9842e-01  time: 7.77e+00\n",
      "epoch: 87   trainLoss: 5.0297e-02   valLoss:6.3483e-02  time: 7.83e+00\n",
      "epoch: 88   trainLoss: 5.1200e-02   valLoss:6.6966e-02  time: 7.81e+00\n",
      "epoch: 89   trainLoss: 3.6984e-02   valLoss:8.2331e-02  time: 7.77e+00\n",
      "epoch: 90   trainLoss: 4.6055e-02   valLoss:2.6859e-01  time: 7.79e+00\n",
      "epoch: 91   trainLoss: 5.2479e-02   valLoss:2.3410e-01  time: 7.75e+00\n",
      "epoch: 92   trainLoss: 4.3168e-02   valLoss:1.1824e-01  time: 7.82e+00\n",
      "epoch: 93   trainLoss: 3.9220e-02   valLoss:7.1859e-02  time: 7.82e+00\n",
      "epoch: 94   trainLoss: 3.8190e-02   valLoss:1.4837e-01  time: 7.82e+00\n",
      "epoch: 95   trainLoss: 3.8246e-02   valLoss:6.0340e-02  time: 7.74e+00\n",
      "epoch: 96   trainLoss: 4.2598e-02   valLoss:5.6555e-02  time: 7.75e+00\n",
      "epoch: 97   trainLoss: 3.2448e-02   valLoss:8.1798e-02  time: 7.77e+00\n",
      "epoch: 98   trainLoss: 3.6530e-02   valLoss:6.7963e-02  time: 7.73e+00\n",
      "epoch: 99   trainLoss: 3.5046e-02   valLoss:7.2364e-02  time: 7.74e+00\n",
      "loading checkpoint 32\n",
      "trained 38 random forest models in 31.99 seconds\n",
      "loaded train set of size 8\n",
      "epoch: 0   trainLoss: 1.1363e+00   valLoss:9.9451e-01  time: 9.00e-02\n",
      "epoch: 1   trainLoss: 8.6948e-01   valLoss:9.9063e-01  time: 9.14e-02\n",
      "epoch: 2   trainLoss: 7.5722e-01   valLoss:9.8364e-01  time: 8.96e-02\n",
      "epoch: 3   trainLoss: 6.4754e-01   valLoss:9.7406e-01  time: 9.12e-02\n",
      "epoch: 4   trainLoss: 5.5798e-01   valLoss:9.6064e-01  time: 9.67e-02\n",
      "epoch: 5   trainLoss: 4.7022e-01   valLoss:9.4880e-01  time: 9.40e-02\n",
      "epoch: 6   trainLoss: 3.8875e-01   valLoss:9.3468e-01  time: 8.96e-02\n",
      "epoch: 7   trainLoss: 3.1346e-01   valLoss:9.2617e-01  time: 9.30e-02\n",
      "epoch: 8   trainLoss: 2.5309e-01   valLoss:9.3212e-01  time: 9.26e-02\n",
      "epoch: 9   trainLoss: 1.8762e-01   valLoss:9.7094e-01  time: 9.03e-02\n",
      "epoch: 10   trainLoss: 1.4301e-01   valLoss:1.0790e+00  time: 9.12e-02\n",
      "epoch: 11   trainLoss: 1.1377e-01   valLoss:1.2507e+00  time: 8.80e-02\n",
      "epoch: 12   trainLoss: 9.3275e-02   valLoss:1.4548e+00  time: 8.60e-02\n",
      "epoch: 13   trainLoss: 8.1281e-02   valLoss:1.6628e+00  time: 8.64e-02\n",
      "epoch: 14   trainLoss: 7.2521e-02   valLoss:1.7889e+00  time: 8.60e-02\n",
      "epoch: 15   trainLoss: 6.4312e-02   valLoss:1.8257e+00  time: 8.90e-02\n",
      "epoch: 16   trainLoss: 5.4704e-02   valLoss:1.8181e+00  time: 8.78e-02\n",
      "epoch: 17   trainLoss: 4.6699e-02   valLoss:1.7536e+00  time: 8.61e-02\n",
      "epoch: 18   trainLoss: 3.8938e-02   valLoss:1.6148e+00  time: 8.60e-02\n",
      "epoch: 19   trainLoss: 3.4106e-02   valLoss:1.4391e+00  time: 8.75e-02\n",
      "epoch: 20   trainLoss: 2.9043e-02   valLoss:1.2200e+00  time: 8.61e-02\n",
      "epoch: 21   trainLoss: 2.5895e-02   valLoss:9.8880e-01  time: 8.59e-02\n",
      "epoch: 22   trainLoss: 2.2695e-02   valLoss:7.4533e-01  time: 8.85e-02\n",
      "epoch: 23   trainLoss: 2.0032e-02   valLoss:5.5673e-01  time: 9.02e-02\n",
      "epoch: 24   trainLoss: 1.7646e-02   valLoss:4.1488e-01  time: 9.32e-02\n",
      "epoch: 25   trainLoss: 1.5774e-02   valLoss:3.2163e-01  time: 8.88e-02\n",
      "epoch: 26   trainLoss: 1.3897e-02   valLoss:2.5441e-01  time: 9.04e-02\n",
      "epoch: 27   trainLoss: 1.3251e-02   valLoss:2.1185e-01  time: 8.81e-02\n",
      "epoch: 28   trainLoss: 1.0445e-02   valLoss:1.8521e-01  time: 8.83e-02\n",
      "epoch: 29   trainLoss: 9.4429e-03   valLoss:1.7154e-01  time: 8.87e-02\n",
      "epoch: 30   trainLoss: 8.6099e-03   valLoss:1.6142e-01  time: 9.05e-02\n",
      "epoch: 31   trainLoss: 7.9501e-03   valLoss:1.5191e-01  time: 8.96e-02\n",
      "epoch: 32   trainLoss: 7.1915e-03   valLoss:1.4533e-01  time: 8.95e-02\n",
      "epoch: 33   trainLoss: 5.9817e-03   valLoss:1.3770e-01  time: 9.07e-02\n",
      "epoch: 34   trainLoss: 5.7684e-03   valLoss:1.2500e-01  time: 9.04e-02\n",
      "epoch: 35   trainLoss: 5.4637e-03   valLoss:1.2115e-01  time: 9.04e-02\n",
      "epoch: 36   trainLoss: 4.5733e-03   valLoss:1.2282e-01  time: 8.86e-02\n",
      "epoch: 37   trainLoss: 4.5125e-03   valLoss:1.2300e-01  time: 8.58e-02\n",
      "epoch: 38   trainLoss: 3.9043e-03   valLoss:1.2401e-01  time: 8.58e-02\n",
      "epoch: 39   trainLoss: 3.4811e-03   valLoss:1.2420e-01  time: 8.62e-02\n",
      "epoch: 40   trainLoss: 3.2609e-03   valLoss:1.2349e-01  time: 8.71e-02\n",
      "epoch: 41   trainLoss: 2.6586e-03   valLoss:1.2382e-01  time: 8.61e-02\n",
      "epoch: 42   trainLoss: 2.4959e-03   valLoss:1.2402e-01  time: 8.55e-02\n",
      "epoch: 43   trainLoss: 2.3105e-03   valLoss:1.2440e-01  time: 8.58e-02\n",
      "epoch: 44   trainLoss: 1.8802e-03   valLoss:1.1899e-01  time: 8.65e-02\n",
      "epoch: 45   trainLoss: 1.7085e-03   valLoss:1.1426e-01  time: 8.80e-02\n",
      "epoch: 46   trainLoss: 1.5418e-03   valLoss:1.1914e-01  time: 8.66e-02\n",
      "epoch: 47   trainLoss: 1.2365e-03   valLoss:1.2325e-01  time: 8.74e-02\n",
      "epoch: 48   trainLoss: 1.1699e-03   valLoss:1.1986e-01  time: 8.59e-02\n",
      "epoch: 49   trainLoss: 1.0539e-03   valLoss:1.2068e-01  time: 8.59e-02\n",
      "epoch: 50   trainLoss: 9.6500e-04   valLoss:1.2517e-01  time: 8.73e-02\n",
      "epoch: 51   trainLoss: 8.3512e-04   valLoss:1.3050e-01  time: 8.59e-02\n",
      "epoch: 52   trainLoss: 7.5027e-04   valLoss:1.3410e-01  time: 8.58e-02\n",
      "epoch: 53   trainLoss: 6.4763e-04   valLoss:1.3311e-01  time: 9.18e-02\n",
      "epoch: 54   trainLoss: 6.1115e-04   valLoss:1.3261e-01  time: 8.55e-02\n",
      "epoch: 55   trainLoss: 6.1715e-04   valLoss:1.3661e-01  time: 8.61e-02\n",
      "epoch: 56   trainLoss: 5.6385e-04   valLoss:1.3582e-01  time: 8.71e-02\n",
      "epoch: 57   trainLoss: 4.9304e-04   valLoss:1.3128e-01  time: 8.52e-02\n",
      "epoch: 58   trainLoss: 6.3973e-04   valLoss:1.3416e-01  time: 8.69e-02\n",
      "epoch: 59   trainLoss: 7.2257e-04   valLoss:1.3031e-01  time: 8.61e-02\n",
      "epoch: 60   trainLoss: 1.0853e-03   valLoss:1.3336e-01  time: 8.69e-02\n",
      "epoch: 61   trainLoss: 6.8462e-04   valLoss:1.3348e-01  time: 8.54e-02\n",
      "epoch: 62   trainLoss: 6.2697e-04   valLoss:1.2804e-01  time: 8.66e-02\n",
      "epoch: 63   trainLoss: 6.7707e-04   valLoss:1.3052e-01  time: 8.60e-02\n",
      "epoch: 64   trainLoss: 4.2268e-04   valLoss:1.3127e-01  time: 8.54e-02\n",
      "epoch: 65   trainLoss: 4.2924e-04   valLoss:1.2795e-01  time: 8.53e-02\n",
      "epoch: 66   trainLoss: 4.3107e-04   valLoss:1.3027e-01  time: 8.61e-02\n",
      "epoch: 67   trainLoss: 4.9219e-04   valLoss:1.2954e-01  time: 9.12e-02\n",
      "epoch: 68   trainLoss: 3.7972e-04   valLoss:1.2989e-01  time: 9.04e-02\n",
      "epoch: 69   trainLoss: 4.9729e-04   valLoss:1.3440e-01  time: 8.66e-02\n",
      "epoch: 70   trainLoss: 6.6322e-04   valLoss:1.3507e-01  time: 8.60e-02\n",
      "epoch: 71   trainLoss: 4.7438e-04   valLoss:1.3144e-01  time: 8.51e-02\n",
      "epoch: 72   trainLoss: 8.3890e-04   valLoss:1.3537e-01  time: 8.61e-02\n",
      "epoch: 73   trainLoss: 1.1399e-03   valLoss:1.3538e-01  time: 8.52e-02\n",
      "epoch: 74   trainLoss: 2.0306e-03   valLoss:1.3796e-01  time: 8.67e-02\n",
      "epoch: 75   trainLoss: 3.3544e-03   valLoss:1.3703e-01  time: 8.62e-02\n",
      "epoch: 76   trainLoss: 4.1688e-03   valLoss:1.2629e-01  time: 8.66e-02\n",
      "epoch: 77   trainLoss: 4.0222e-03   valLoss:1.2127e-01  time: 8.59e-02\n",
      "epoch: 78   trainLoss: 4.0230e-03   valLoss:1.1440e-01  time: 8.58e-02\n",
      "epoch: 79   trainLoss: 2.7431e-03   valLoss:9.8397e-02  time: 8.74e-02\n",
      "epoch: 80   trainLoss: 3.1145e-03   valLoss:1.0551e-01  time: 9.47e-02\n",
      "epoch: 81   trainLoss: 5.8544e-03   valLoss:1.0910e-01  time: 8.82e-02\n",
      "epoch: 82   trainLoss: 4.4256e-03   valLoss:9.5867e-02  time: 8.53e-02\n",
      "epoch: 83   trainLoss: 2.4036e-03   valLoss:1.0126e-01  time: 8.70e-02\n",
      "epoch: 84   trainLoss: 2.3656e-03   valLoss:1.0528e-01  time: 8.70e-02\n",
      "epoch: 85   trainLoss: 2.1975e-03   valLoss:9.5537e-02  time: 8.66e-02\n",
      "epoch: 86   trainLoss: 2.5103e-03   valLoss:1.0078e-01  time: 8.65e-02\n",
      "epoch: 87   trainLoss: 1.6753e-03   valLoss:1.0546e-01  time: 8.53e-02\n",
      "epoch: 88   trainLoss: 1.5806e-03   valLoss:1.0600e-01  time: 8.60e-02\n",
      "epoch: 89   trainLoss: 1.2300e-03   valLoss:1.0916e-01  time: 8.53e-02\n",
      "epoch: 90   trainLoss: 1.3608e-03   valLoss:1.1239e-01  time: 8.73e-02\n",
      "epoch: 91   trainLoss: 1.0878e-03   valLoss:1.1245e-01  time: 8.59e-02\n",
      "epoch: 92   trainLoss: 9.2148e-04   valLoss:1.0936e-01  time: 8.62e-02\n",
      "epoch: 93   trainLoss: 9.7542e-04   valLoss:1.0628e-01  time: 8.65e-02\n",
      "epoch: 94   trainLoss: 7.9499e-04   valLoss:1.0734e-01  time: 8.54e-02\n",
      "epoch: 95   trainLoss: 6.5304e-04   valLoss:1.1113e-01  time: 8.62e-02\n",
      "epoch: 96   trainLoss: 7.2270e-04   valLoss:1.1284e-01  time: 8.68e-02\n",
      "epoch: 97   trainLoss: 5.3675e-04   valLoss:1.1460e-01  time: 8.51e-02\n",
      "epoch: 98   trainLoss: 4.3139e-04   valLoss:1.1469e-01  time: 8.72e-02\n",
      "epoch: 99   trainLoss: 4.8736e-04   valLoss:1.1575e-01  time: 8.56e-02\n",
      "loading checkpoint 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.0883e-01   valLoss:1.3243e+00  time: 8.52e-02\n",
      "epoch: 1   trainLoss: 3.6212e-01   valLoss:7.8721e+00  time: 8.64e-02\n",
      "epoch: 2   trainLoss: 1.7104e-01   valLoss:6.0023e-01  time: 8.88e-02\n",
      "epoch: 3   trainLoss: 9.3814e-02   valLoss:8.1151e-01  time: 8.99e-02\n",
      "epoch: 4   trainLoss: 6.5106e-02   valLoss:6.3800e-01  time: 8.92e-02\n",
      "epoch: 5   trainLoss: 4.9501e-02   valLoss:3.7689e-01  time: 9.09e-02\n",
      "epoch: 6   trainLoss: 4.4609e-02   valLoss:2.8836e-01  time: 9.14e-02\n",
      "epoch: 7   trainLoss: 3.9954e-02   valLoss:2.5355e-01  time: 9.46e-02\n",
      "epoch: 8   trainLoss: 3.7229e-02   valLoss:1.7444e-01  time: 8.78e-02\n",
      "epoch: 9   trainLoss: 2.8229e-02   valLoss:1.3386e-01  time: 8.58e-02\n",
      "epoch: 10   trainLoss: 2.6884e-02   valLoss:1.2538e-01  time: 8.82e-02\n",
      "epoch: 11   trainLoss: 2.0044e-02   valLoss:1.2681e-01  time: 8.81e-02\n",
      "epoch: 12   trainLoss: 1.5721e-02   valLoss:1.1222e-01  time: 8.63e-02\n",
      "epoch: 13   trainLoss: 1.4419e-02   valLoss:8.1013e-02  time: 8.63e-02\n",
      "epoch: 14   trainLoss: 1.2956e-02   valLoss:5.6383e-02  time: 8.93e-02\n",
      "epoch: 15   trainLoss: 1.0406e-02   valLoss:5.5902e-02  time: 9.55e-02\n",
      "epoch: 16   trainLoss: 1.0149e-02   valLoss:4.8168e-02  time: 9.21e-02\n",
      "epoch: 17   trainLoss: 9.3669e-03   valLoss:4.1335e-02  time: 8.97e-02\n",
      "epoch: 18   trainLoss: 8.7152e-03   valLoss:3.8207e-02  time: 8.76e-02\n",
      "epoch: 19   trainLoss: 7.7727e-03   valLoss:3.3480e-02  time: 8.83e-02\n",
      "epoch: 20   trainLoss: 7.5428e-03   valLoss:2.5891e-02  time: 8.72e-02\n",
      "epoch: 21   trainLoss: 6.7223e-03   valLoss:2.2549e-02  time: 8.63e-02\n",
      "epoch: 22   trainLoss: 6.3545e-03   valLoss:2.2580e-02  time: 8.85e-02\n",
      "epoch: 23   trainLoss: 5.7966e-03   valLoss:1.8264e-02  time: 8.62e-02\n",
      "epoch: 24   trainLoss: 5.4735e-03   valLoss:1.0666e-02  time: 8.78e-02\n",
      "epoch: 25   trainLoss: 4.9054e-03   valLoss:1.1113e-02  time: 8.88e-02\n",
      "epoch: 26   trainLoss: 4.4809e-03   valLoss:1.4113e-02  time: 8.67e-02\n",
      "epoch: 27   trainLoss: 4.0897e-03   valLoss:1.3965e-02  time: 8.55e-02\n",
      "epoch: 28   trainLoss: 3.9565e-03   valLoss:1.2371e-02  time: 8.61e-02\n",
      "epoch: 29   trainLoss: 3.7085e-03   valLoss:1.2709e-02  time: 8.84e-02\n",
      "epoch: 30   trainLoss: 3.4543e-03   valLoss:1.5359e-02  time: 8.67e-02\n",
      "epoch: 31   trainLoss: 3.1430e-03   valLoss:1.8604e-02  time: 8.59e-02\n",
      "epoch: 32   trainLoss: 2.9947e-03   valLoss:1.7839e-02  time: 8.65e-02\n",
      "epoch: 33   trainLoss: 2.9107e-03   valLoss:1.6692e-02  time: 8.75e-02\n",
      "epoch: 34   trainLoss: 2.7852e-03   valLoss:1.8071e-02  time: 8.57e-02\n",
      "epoch: 35   trainLoss: 2.6338e-03   valLoss:1.9722e-02  time: 8.53e-02\n",
      "epoch: 36   trainLoss: 2.4640e-03   valLoss:1.9248e-02  time: 8.53e-02\n",
      "epoch: 37   trainLoss: 2.3654e-03   valLoss:1.9193e-02  time: 8.62e-02\n",
      "epoch: 38   trainLoss: 2.1038e-03   valLoss:2.0763e-02  time: 8.55e-02\n",
      "epoch: 39   trainLoss: 1.9758e-03   valLoss:2.1852e-02  time: 8.57e-02\n",
      "epoch: 40   trainLoss: 1.8814e-03   valLoss:2.3047e-02  time: 9.29e-02\n",
      "epoch: 41   trainLoss: 1.7383e-03   valLoss:2.4068e-02  time: 8.58e-02\n",
      "epoch: 42   trainLoss: 1.6546e-03   valLoss:2.4787e-02  time: 8.58e-02\n",
      "epoch: 43   trainLoss: 1.5534e-03   valLoss:2.6221e-02  time: 9.06e-02\n",
      "epoch: 44   trainLoss: 1.4583e-03   valLoss:2.8307e-02  time: 9.33e-02\n",
      "epoch: 45   trainLoss: 1.3887e-03   valLoss:3.0405e-02  time: 9.17e-02\n",
      "epoch: 46   trainLoss: 1.3134e-03   valLoss:3.2885e-02  time: 9.90e-02\n",
      "epoch: 47   trainLoss: 1.2354e-03   valLoss:3.5227e-02  time: 9.34e-02\n",
      "epoch: 48   trainLoss: 1.1676e-03   valLoss:3.7970e-02  time: 9.04e-02\n",
      "epoch: 49   trainLoss: 1.0971e-03   valLoss:4.0747e-02  time: 8.89e-02\n",
      "epoch: 50   trainLoss: 1.0446e-03   valLoss:4.2056e-02  time: 9.15e-02\n",
      "epoch: 51   trainLoss: 1.0009e-03   valLoss:4.2433e-02  time: 9.12e-02\n",
      "epoch: 52   trainLoss: 9.4505e-04   valLoss:4.0004e-02  time: 9.37e-02\n",
      "epoch: 53   trainLoss: 8.9889e-04   valLoss:4.0250e-02  time: 8.87e-02\n",
      "epoch: 54   trainLoss: 9.1127e-04   valLoss:3.5787e-02  time: 8.89e-02\n",
      "epoch: 55   trainLoss: 1.0946e-03   valLoss:4.2257e-02  time: 8.95e-02\n",
      "epoch: 56   trainLoss: 1.9528e-03   valLoss:3.7598e-02  time: 8.82e-02\n",
      "epoch: 57   trainLoss: 5.4118e-03   valLoss:6.5143e-02  time: 8.96e-02\n",
      "epoch: 58   trainLoss: 1.1700e-02   valLoss:3.4278e-02  time: 8.90e-02\n",
      "epoch: 59   trainLoss: 7.9067e-03   valLoss:1.6022e-02  time: 9.32e-02\n",
      "epoch: 60   trainLoss: 2.4931e-03   valLoss:3.7279e-02  time: 8.66e-02\n",
      "epoch: 61   trainLoss: 5.3196e-03   valLoss:1.9280e-02  time: 8.92e-02\n",
      "epoch: 62   trainLoss: 2.3825e-03   valLoss:2.4445e-02  time: 9.44e-02\n",
      "epoch: 63   trainLoss: 3.5293e-03   valLoss:2.0998e-02  time: 8.99e-02\n",
      "epoch: 64   trainLoss: 2.6235e-03   valLoss:1.8843e-02  time: 8.66e-02\n",
      "epoch: 65   trainLoss: 2.2203e-03   valLoss:2.3649e-02  time: 8.72e-02\n",
      "epoch: 66   trainLoss: 2.5426e-03   valLoss:1.9464e-02  time: 8.72e-02\n",
      "epoch: 67   trainLoss: 1.6432e-03   valLoss:1.9428e-02  time: 8.78e-02\n",
      "epoch: 68   trainLoss: 2.1702e-03   valLoss:2.2300e-02  time: 8.93e-02\n",
      "epoch: 69   trainLoss: 1.5175e-03   valLoss:2.8446e-02  time: 8.93e-02\n",
      "epoch: 70   trainLoss: 1.4467e-03   valLoss:3.2675e-02  time: 8.64e-02\n",
      "epoch: 71   trainLoss: 1.5025e-03   valLoss:3.2669e-02  time: 8.81e-02\n",
      "epoch: 72   trainLoss: 1.1079e-03   valLoss:3.2141e-02  time: 8.82e-02\n",
      "epoch: 73   trainLoss: 1.1996e-03   valLoss:2.8410e-02  time: 8.89e-02\n",
      "epoch: 74   trainLoss: 1.0508e-03   valLoss:2.8585e-02  time: 9.09e-02\n",
      "epoch: 75   trainLoss: 8.6063e-04   valLoss:3.1699e-02  time: 8.72e-02\n",
      "epoch: 76   trainLoss: 9.3457e-04   valLoss:3.2530e-02  time: 8.60e-02\n",
      "epoch: 77   trainLoss: 6.9449e-04   valLoss:3.2760e-02  time: 9.05e-02\n",
      "epoch: 78   trainLoss: 7.2870e-04   valLoss:3.6169e-02  time: 9.11e-02\n",
      "epoch: 79   trainLoss: 6.5792e-04   valLoss:3.6649e-02  time: 9.00e-02\n",
      "epoch: 80   trainLoss: 5.5942e-04   valLoss:3.4700e-02  time: 8.91e-02\n",
      "epoch: 81   trainLoss: 6.2454e-04   valLoss:3.9316e-02  time: 8.61e-02\n",
      "epoch: 82   trainLoss: 4.8563e-04   valLoss:4.1068e-02  time: 8.73e-02\n",
      "epoch: 83   trainLoss: 5.0127e-04   valLoss:4.2597e-02  time: 8.91e-02\n",
      "epoch: 84   trainLoss: 5.4364e-04   valLoss:4.2134e-02  time: 8.64e-02\n",
      "epoch: 85   trainLoss: 5.8868e-04   valLoss:4.7468e-02  time: 8.87e-02\n",
      "epoch: 86   trainLoss: 1.1011e-03   valLoss:3.9809e-02  time: 8.79e-02\n",
      "epoch: 87   trainLoss: 2.9715e-03   valLoss:6.1377e-02  time: 8.66e-02\n",
      "epoch: 88   trainLoss: 5.9232e-03   valLoss:5.1077e-02  time: 8.96e-02\n",
      "epoch: 89   trainLoss: 1.2753e-02   valLoss:4.2840e-02  time: 8.99e-02\n",
      "epoch: 90   trainLoss: 4.1256e-03   valLoss:4.0155e-02  time: 8.64e-02\n",
      "epoch: 91   trainLoss: 3.0854e-03   valLoss:3.8489e-02  time: 8.92e-02\n",
      "epoch: 92   trainLoss: 4.2123e-03   valLoss:2.8932e-02  time: 8.73e-02\n",
      "epoch: 93   trainLoss: 2.3864e-03   valLoss:3.7174e-02  time: 8.73e-02\n",
      "epoch: 94   trainLoss: 2.8008e-03   valLoss:4.1653e-02  time: 8.74e-02\n",
      "epoch: 95   trainLoss: 2.1063e-03   valLoss:3.7463e-02  time: 8.73e-02\n",
      "epoch: 96   trainLoss: 1.9662e-03   valLoss:4.3177e-02  time: 8.72e-02\n",
      "epoch: 97   trainLoss: 2.1174e-03   valLoss:4.7044e-02  time: 8.85e-02\n",
      "epoch: 98   trainLoss: 1.1096e-03   valLoss:5.1849e-02  time: 8.67e-02\n",
      "epoch: 99   trainLoss: 2.0465e-03   valLoss:4.4168e-02  time: 8.66e-02\n",
      "loading checkpoint 24\n",
      "trained 38 random forest models in 4.34 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.163317</td>\n",
       "      <td>0.882675</td>\n",
       "      <td>0.968082</td>\n",
       "      <td>0.493723</td>\n",
       "      <td>-11.255813</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.236134</td>\n",
       "      <td>0.478429</td>\n",
       "      <td>0.766847</td>\n",
       "      <td>0.359519</td>\n",
       "      <td>-6.779388</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.061272</td>\n",
       "      <td>0.976042</td>\n",
       "      <td>0.994296</td>\n",
       "      <td>0.961505</td>\n",
       "      <td>0.407564</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.148968</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.883682</td>\n",
       "      <td>0.762376</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.090335</td>\n",
       "      <td>0.916350</td>\n",
       "      <td>0.977951</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.206737</td>\n",
       "      <td>0.536838</td>\n",
       "      <td>0.777957</td>\n",
       "      <td>0.673963</td>\n",
       "      <td>0.530883</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.120591</td>\n",
       "      <td>0.930699</td>\n",
       "      <td>0.993558</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>-0.819952</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.009086</td>\n",
       "      <td>0.269193</td>\n",
       "      <td>0.188515</td>\n",
       "      <td>0.660763</td>\n",
       "      <td>0.109233</td>\n",
       "      <td>-9.802806</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.067048</td>\n",
       "      <td>0.980933</td>\n",
       "      <td>0.995591</td>\n",
       "      <td>0.967842</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.005995</td>\n",
       "      <td>0.180815</td>\n",
       "      <td>0.466764</td>\n",
       "      <td>0.792074</td>\n",
       "      <td>0.655240</td>\n",
       "      <td>0.169769</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.099611</td>\n",
       "      <td>0.883029</td>\n",
       "      <td>0.964017</td>\n",
       "      <td>0.911467</td>\n",
       "      <td>0.826741</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.267617</td>\n",
       "      <td>0.284085</td>\n",
       "      <td>0.613989</td>\n",
       "      <td>0.458731</td>\n",
       "      <td>0.213384</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.130085</td>\n",
       "      <td>0.878527</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>0.854355</td>\n",
       "      <td>0.231846</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.765645</td>\n",
       "      <td>0.931321</td>\n",
       "      <td>0.799106</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.092179</td>\n",
       "      <td>0.944927</td>\n",
       "      <td>0.982321</td>\n",
       "      <td>0.935492</td>\n",
       "      <td>0.618484</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.105142</td>\n",
       "      <td>0.871055</td>\n",
       "      <td>0.956827</td>\n",
       "      <td>0.894622</td>\n",
       "      <td>0.639359</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.050026</td>\n",
       "      <td>0.967902</td>\n",
       "      <td>0.990846</td>\n",
       "      <td>0.982109</td>\n",
       "      <td>0.973711</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.898826</td>\n",
       "      <td>0.845482</td>\n",
       "      <td>0.755097</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>0.269056</td>\n",
       "      <td>0.669925</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.211388</td>\n",
       "      <td>-4.734536</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>0.574274</td>\n",
       "      <td>0.830944</td>\n",
       "      <td>0.257639</td>\n",
       "      <td>-1.904599</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.071612</td>\n",
       "      <td>0.974403</td>\n",
       "      <td>0.992626</td>\n",
       "      <td>0.930093</td>\n",
       "      <td>-0.214424</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.105127</td>\n",
       "      <td>0.876431</td>\n",
       "      <td>0.951051</td>\n",
       "      <td>0.889585</td>\n",
       "      <td>0.395513</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>0.955039</td>\n",
       "      <td>0.988258</td>\n",
       "      <td>0.973573</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.666519</td>\n",
       "      <td>0.858249</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.657565</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.148245</td>\n",
       "      <td>0.853139</td>\n",
       "      <td>0.984640</td>\n",
       "      <td>0.826602</td>\n",
       "      <td>-2.020204</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>0.386732</td>\n",
       "      <td>0.022562</td>\n",
       "      <td>0.356597</td>\n",
       "      <td>-0.279365</td>\n",
       "      <td>-6.062881</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.061334</td>\n",
       "      <td>0.991055</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.946374</td>\n",
       "      <td>-0.316462</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.292241</td>\n",
       "      <td>0.220035</td>\n",
       "      <td>0.682006</td>\n",
       "      <td>0.243933</td>\n",
       "      <td>-2.204085</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>0.338059</td>\n",
       "      <td>0.533530</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.759495</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>0.371222</td>\n",
       "      <td>0.081765</td>\n",
       "      <td>0.306189</td>\n",
       "      <td>0.111947</td>\n",
       "      <td>-0.817344</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.183926</td>\n",
       "      <td>0.930554</td>\n",
       "      <td>0.987653</td>\n",
       "      <td>0.454273</td>\n",
       "      <td>-8.752284</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.235867</td>\n",
       "      <td>0.556162</td>\n",
       "      <td>0.822242</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>-7.279881</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.069872</td>\n",
       "      <td>0.979109</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.962104</td>\n",
       "      <td>0.695135</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.123586</td>\n",
       "      <td>0.794344</td>\n",
       "      <td>0.922382</td>\n",
       "      <td>0.836401</td>\n",
       "      <td>0.225206</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.072637</td>\n",
       "      <td>0.906248</td>\n",
       "      <td>0.973866</td>\n",
       "      <td>0.957845</td>\n",
       "      <td>0.931509</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.192283</td>\n",
       "      <td>0.438279</td>\n",
       "      <td>0.820735</td>\n",
       "      <td>0.672316</td>\n",
       "      <td>0.529418</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.102952</td>\n",
       "      <td>0.949463</td>\n",
       "      <td>0.976699</td>\n",
       "      <td>0.909234</td>\n",
       "      <td>0.178548</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.111115</td>\n",
       "      <td>0.912283</td>\n",
       "      <td>0.965734</td>\n",
       "      <td>0.893551</td>\n",
       "      <td>0.297245</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.089927</td>\n",
       "      <td>0.947361</td>\n",
       "      <td>0.977165</td>\n",
       "      <td>0.930539</td>\n",
       "      <td>0.568394</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.096692</td>\n",
       "      <td>0.924715</td>\n",
       "      <td>0.970727</td>\n",
       "      <td>0.920148</td>\n",
       "      <td>0.647018</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.043705</td>\n",
       "      <td>0.969916</td>\n",
       "      <td>0.994499</td>\n",
       "      <td>0.983727</td>\n",
       "      <td>0.974182</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.104842</td>\n",
       "      <td>0.816190</td>\n",
       "      <td>0.946874</td>\n",
       "      <td>0.890920</td>\n",
       "      <td>0.817685</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.162339</td>\n",
       "      <td>0.703440</td>\n",
       "      <td>0.961249</td>\n",
       "      <td>0.830408</td>\n",
       "      <td>-0.317098</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.318608</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.507507</td>\n",
       "      <td>-0.083851</td>\n",
       "      <td>-7.047225</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.065396</td>\n",
       "      <td>0.951726</td>\n",
       "      <td>0.998528</td>\n",
       "      <td>0.966271</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.268351</td>\n",
       "      <td>0.307749</td>\n",
       "      <td>0.682917</td>\n",
       "      <td>0.309362</td>\n",
       "      <td>-1.452218</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.130953</td>\n",
       "      <td>0.692747</td>\n",
       "      <td>0.921199</td>\n",
       "      <td>0.879252</td>\n",
       "      <td>0.797198</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>0.289489</td>\n",
       "      <td>-0.088285</td>\n",
       "      <td>0.567222</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2   minAggR2  \\\n",
       "0   0.000045  0.004630  0.163317  0.882675  0.968082   0.493723 -11.255813   \n",
       "1   0.000132  0.007395  0.236134  0.478429  0.766847   0.359519  -6.779388   \n",
       "2   0.000006  0.001685  0.061272  0.976042  0.994296   0.961505   0.407564   \n",
       "3   0.000067  0.004643  0.148968  0.690041  0.883682   0.762376  -0.028446   \n",
       "4   0.000022  0.002845  0.090335  0.916350  0.977951   0.949572   0.850276   \n",
       "5   0.000112  0.006616  0.206737  0.536838  0.777957   0.673963   0.530883   \n",
       "6   0.000030  0.003575  0.120591  0.930699  0.993558   0.841346  -0.819952   \n",
       "7   0.000218  0.009086  0.269193  0.188515  0.660763   0.109233  -9.802806   \n",
       "8   0.000005  0.001649  0.067048  0.980933  0.995591   0.967842   0.736369   \n",
       "9   0.000111  0.005995  0.180815  0.466764  0.792074   0.655240   0.169769   \n",
       "10  0.000029  0.003105  0.099611  0.883029  0.964017   0.911467   0.826741   \n",
       "11  0.000202  0.008956  0.267617  0.284085  0.613989   0.458731   0.213384   \n",
       "12  0.000031  0.003602  0.130085  0.878527  0.963094   0.854355   0.231846   \n",
       "13  0.000054  0.004353  0.144494  0.765645  0.931321   0.799106   0.123201   \n",
       "14  0.000013  0.002424  0.092179  0.944927  0.982321   0.935492   0.618484   \n",
       "15  0.000030  0.003129  0.105142  0.871055  0.956827   0.894622   0.639359   \n",
       "16  0.000006  0.001400  0.050026  0.967902  0.990846   0.982109   0.973711   \n",
       "17  0.000057  0.004100  0.123210  0.727944  0.898826   0.845482   0.755097   \n",
       "18  0.000182  0.009304  0.269056  0.669925  0.910799   0.211388  -4.734536   \n",
       "19  0.000230  0.010374  0.290932  0.574274  0.830944   0.257639  -1.904599   \n",
       "20  0.000007  0.001855  0.071612  0.974403  0.992626   0.930093  -0.214424   \n",
       "21  0.000029  0.003134  0.105127  0.876431  0.951051   0.889585   0.395513   \n",
       "22  0.000009  0.001774  0.060270  0.955039  0.988258   0.973573   0.963094   \n",
       "23  0.000073  0.004964  0.154257  0.666519  0.858249   0.789546   0.657565   \n",
       "24  0.000048  0.004631  0.148245  0.853139  0.984640   0.826602  -2.020204   \n",
       "25  0.000364  0.013356  0.386732  0.022562  0.356597  -0.279365  -6.062881   \n",
       "26  0.000004  0.001494  0.061334  0.991055  0.999882   0.946374  -0.316462   \n",
       "27  0.000219  0.009812  0.292241  0.220035  0.682006   0.243933  -2.204085   \n",
       "28  0.000182  0.010910  0.338059  0.533530  0.874250   0.759495   0.680988   \n",
       "29  0.000327  0.012537  0.371222  0.081765  0.306189   0.111947  -0.817344   \n",
       "30  0.000044  0.004528  0.183926  0.930554  0.987653   0.454273  -8.752284   \n",
       "31  0.000121  0.007061  0.235867  0.556162  0.822242   0.366147  -7.279881   \n",
       "32  0.000005  0.001573  0.069872  0.979109  0.993671   0.962104   0.695135   \n",
       "33  0.000045  0.003799  0.123586  0.794344  0.922382   0.836401   0.225206   \n",
       "34  0.000011  0.001993  0.072637  0.906248  0.973866   0.957845   0.931509   \n",
       "35  0.000120  0.006461  0.192283  0.438279  0.820735   0.672316   0.529418   \n",
       "36  0.000018  0.002754  0.102952  0.949463  0.976699   0.909234   0.178548   \n",
       "37  0.000026  0.003084  0.111115  0.912283  0.965734   0.893551   0.297245   \n",
       "38  0.000014  0.002411  0.089927  0.947361  0.977165   0.930539   0.568394   \n",
       "39  0.000020  0.002696  0.096692  0.924715  0.970727   0.920148   0.647018   \n",
       "40  0.000006  0.001282  0.043705  0.969916  0.994499   0.983727   0.974182   \n",
       "41  0.000039  0.003372  0.104842  0.816190  0.946874   0.890920   0.817685   \n",
       "42  0.000030  0.003895  0.162339  0.703440  0.961249   0.830408  -0.317098   \n",
       "43  0.000263  0.010492  0.318608 -0.011265  0.507507  -0.083851  -7.047225   \n",
       "44  0.000004  0.001362  0.065396  0.951726  0.998528   0.966271   0.582016   \n",
       "45  0.000208  0.008952  0.268351  0.307749  0.682917   0.309362  -1.452218   \n",
       "46  0.000027  0.003548  0.130953  0.692747  0.921199   0.879252   0.797198   \n",
       "47  0.000248  0.009684  0.289489 -0.088285  0.567222   0.329571   0.019554   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train          48  \n",
       "1               Fresh   Test          48  \n",
       "2   Transfer learning  Train          48  \n",
       "3   Transfer learning   Test          48  \n",
       "4       Random Forest  Train          48  \n",
       "5       Random Forest   Test          48  \n",
       "6               Fresh  Train          18  \n",
       "7               Fresh   Test          18  \n",
       "8   Transfer learning  Train          18  \n",
       "9   Transfer learning   Test          18  \n",
       "10      Random Forest  Train          18  \n",
       "11      Random Forest   Test          18  \n",
       "12              Fresh  Train         452  \n",
       "13              Fresh   Test         452  \n",
       "14  Transfer learning  Train         452  \n",
       "15  Transfer learning   Test         452  \n",
       "16      Random Forest  Train         452  \n",
       "17      Random Forest   Test         452  \n",
       "18              Fresh  Train         178  \n",
       "19              Fresh   Test         178  \n",
       "20  Transfer learning  Train         178  \n",
       "21  Transfer learning   Test         178  \n",
       "22      Random Forest  Train         178  \n",
       "23      Random Forest   Test         178  \n",
       "24              Fresh  Train           5  \n",
       "25              Fresh   Test           5  \n",
       "26  Transfer learning  Train           5  \n",
       "27  Transfer learning   Test           5  \n",
       "28      Random Forest  Train           5  \n",
       "29      Random Forest   Test           5  \n",
       "30              Fresh  Train          93  \n",
       "31              Fresh   Test          93  \n",
       "32  Transfer learning  Train          93  \n",
       "33  Transfer learning   Test          93  \n",
       "34      Random Forest  Train          93  \n",
       "35      Random Forest   Test          93  \n",
       "36              Fresh  Train         914  \n",
       "37              Fresh   Test         914  \n",
       "38  Transfer learning  Train         914  \n",
       "39  Transfer learning   Test         914  \n",
       "40      Random Forest  Train         914  \n",
       "41      Random Forest   Test         914  \n",
       "42              Fresh  Train           8  \n",
       "43              Fresh   Test           8  \n",
       "44  Transfer learning  Train           8  \n",
       "45  Transfer learning   Test           8  \n",
       "46      Random Forest  Train           8  \n",
       "47      Random Forest   Test           8  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "trainDataFiles = glob.glob(os.path.join(dataDir, 'design_9*.csv'))\n",
    "trainDataFiles.remove(testFile)\n",
    "\n",
    "allResults = []\n",
    "for trainDataFile in trainDataFiles:\n",
    "    trainDataUnfiltered = loadGhGraphs(trainDataFile, NUM_DV=5)\n",
    "    trainData = filterbyDispValue(trainDataUnfiltered, maxDispCutoff)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=epochs, \n",
    "                         saveDir=saveDir+f'{trainSize}/gcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Fresh'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Fresh'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile,\n",
    "                             epochs=epochs, \n",
    "                             saveDir=saveDir+f'{trainSize}/ptrGcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Transfer learning'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Transfer learning'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                     saveDir=saveDir+f'{trainSize}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['Model'] = 'Random Forest'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['Model'] = 'Random Forest'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.163317</td>\n",
       "      <td>0.882675</td>\n",
       "      <td>0.968082</td>\n",
       "      <td>0.493723</td>\n",
       "      <td>-11.255813</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.236134</td>\n",
       "      <td>0.478429</td>\n",
       "      <td>0.766847</td>\n",
       "      <td>0.359519</td>\n",
       "      <td>-6.779388</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.061272</td>\n",
       "      <td>0.976042</td>\n",
       "      <td>0.994296</td>\n",
       "      <td>0.961505</td>\n",
       "      <td>0.407564</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.148968</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.883682</td>\n",
       "      <td>0.762376</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.090335</td>\n",
       "      <td>0.916350</td>\n",
       "      <td>0.977951</td>\n",
       "      <td>0.949572</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.206737</td>\n",
       "      <td>0.536838</td>\n",
       "      <td>0.777957</td>\n",
       "      <td>0.673963</td>\n",
       "      <td>0.530883</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.120591</td>\n",
       "      <td>0.930699</td>\n",
       "      <td>0.993558</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>-0.819952</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.009086</td>\n",
       "      <td>0.269193</td>\n",
       "      <td>0.188515</td>\n",
       "      <td>0.660763</td>\n",
       "      <td>0.109233</td>\n",
       "      <td>-9.802806</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.067048</td>\n",
       "      <td>0.980933</td>\n",
       "      <td>0.995591</td>\n",
       "      <td>0.967842</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.005995</td>\n",
       "      <td>0.180815</td>\n",
       "      <td>0.466764</td>\n",
       "      <td>0.792074</td>\n",
       "      <td>0.655240</td>\n",
       "      <td>0.169769</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.099611</td>\n",
       "      <td>0.883029</td>\n",
       "      <td>0.964017</td>\n",
       "      <td>0.911467</td>\n",
       "      <td>0.826741</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.267617</td>\n",
       "      <td>0.284085</td>\n",
       "      <td>0.613989</td>\n",
       "      <td>0.458731</td>\n",
       "      <td>0.213384</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.130085</td>\n",
       "      <td>0.878527</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>0.854355</td>\n",
       "      <td>0.231846</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.765645</td>\n",
       "      <td>0.931321</td>\n",
       "      <td>0.799106</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.092179</td>\n",
       "      <td>0.944927</td>\n",
       "      <td>0.982321</td>\n",
       "      <td>0.935492</td>\n",
       "      <td>0.618484</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.105142</td>\n",
       "      <td>0.871055</td>\n",
       "      <td>0.956827</td>\n",
       "      <td>0.894622</td>\n",
       "      <td>0.639359</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.050026</td>\n",
       "      <td>0.967902</td>\n",
       "      <td>0.990846</td>\n",
       "      <td>0.982109</td>\n",
       "      <td>0.973711</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.898826</td>\n",
       "      <td>0.845482</td>\n",
       "      <td>0.755097</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>0.269056</td>\n",
       "      <td>0.669925</td>\n",
       "      <td>0.910799</td>\n",
       "      <td>0.211388</td>\n",
       "      <td>-4.734536</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>0.574274</td>\n",
       "      <td>0.830944</td>\n",
       "      <td>0.257639</td>\n",
       "      <td>-1.904599</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.071612</td>\n",
       "      <td>0.974403</td>\n",
       "      <td>0.992626</td>\n",
       "      <td>0.930093</td>\n",
       "      <td>-0.214424</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.105127</td>\n",
       "      <td>0.876431</td>\n",
       "      <td>0.951051</td>\n",
       "      <td>0.889585</td>\n",
       "      <td>0.395513</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>0.955039</td>\n",
       "      <td>0.988258</td>\n",
       "      <td>0.973573</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.666519</td>\n",
       "      <td>0.858249</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.657565</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.148245</td>\n",
       "      <td>0.853139</td>\n",
       "      <td>0.984640</td>\n",
       "      <td>0.826602</td>\n",
       "      <td>-2.020204</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>0.386732</td>\n",
       "      <td>0.022562</td>\n",
       "      <td>0.356597</td>\n",
       "      <td>-0.279365</td>\n",
       "      <td>-6.062881</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.061334</td>\n",
       "      <td>0.991055</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.946374</td>\n",
       "      <td>-0.316462</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.292241</td>\n",
       "      <td>0.220035</td>\n",
       "      <td>0.682006</td>\n",
       "      <td>0.243933</td>\n",
       "      <td>-2.204085</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>0.338059</td>\n",
       "      <td>0.533530</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.759495</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>0.371222</td>\n",
       "      <td>0.081765</td>\n",
       "      <td>0.306189</td>\n",
       "      <td>0.111947</td>\n",
       "      <td>-0.817344</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.183926</td>\n",
       "      <td>0.930554</td>\n",
       "      <td>0.987653</td>\n",
       "      <td>0.454273</td>\n",
       "      <td>-8.752284</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.235867</td>\n",
       "      <td>0.556162</td>\n",
       "      <td>0.822242</td>\n",
       "      <td>0.366147</td>\n",
       "      <td>-7.279881</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.069872</td>\n",
       "      <td>0.979109</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.962104</td>\n",
       "      <td>0.695135</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.123586</td>\n",
       "      <td>0.794344</td>\n",
       "      <td>0.922382</td>\n",
       "      <td>0.836401</td>\n",
       "      <td>0.225206</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.072637</td>\n",
       "      <td>0.906248</td>\n",
       "      <td>0.973866</td>\n",
       "      <td>0.957845</td>\n",
       "      <td>0.931509</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.192283</td>\n",
       "      <td>0.438279</td>\n",
       "      <td>0.820735</td>\n",
       "      <td>0.672316</td>\n",
       "      <td>0.529418</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.102952</td>\n",
       "      <td>0.949463</td>\n",
       "      <td>0.976699</td>\n",
       "      <td>0.909234</td>\n",
       "      <td>0.178548</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.111115</td>\n",
       "      <td>0.912283</td>\n",
       "      <td>0.965734</td>\n",
       "      <td>0.893551</td>\n",
       "      <td>0.297245</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.089927</td>\n",
       "      <td>0.947361</td>\n",
       "      <td>0.977165</td>\n",
       "      <td>0.930539</td>\n",
       "      <td>0.568394</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.096692</td>\n",
       "      <td>0.924715</td>\n",
       "      <td>0.970727</td>\n",
       "      <td>0.920148</td>\n",
       "      <td>0.647018</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.043705</td>\n",
       "      <td>0.969916</td>\n",
       "      <td>0.994499</td>\n",
       "      <td>0.983727</td>\n",
       "      <td>0.974182</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.104842</td>\n",
       "      <td>0.816190</td>\n",
       "      <td>0.946874</td>\n",
       "      <td>0.890920</td>\n",
       "      <td>0.817685</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.162339</td>\n",
       "      <td>0.703440</td>\n",
       "      <td>0.961249</td>\n",
       "      <td>0.830408</td>\n",
       "      <td>-0.317098</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.318608</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.507507</td>\n",
       "      <td>-0.083851</td>\n",
       "      <td>-7.047225</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.065396</td>\n",
       "      <td>0.951726</td>\n",
       "      <td>0.998528</td>\n",
       "      <td>0.966271</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.268351</td>\n",
       "      <td>0.307749</td>\n",
       "      <td>0.682917</td>\n",
       "      <td>0.309362</td>\n",
       "      <td>-1.452218</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.130953</td>\n",
       "      <td>0.692747</td>\n",
       "      <td>0.921199</td>\n",
       "      <td>0.879252</td>\n",
       "      <td>0.797198</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>0.289489</td>\n",
       "      <td>-0.088285</td>\n",
       "      <td>0.567222</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2   minAggR2  \\\n",
       "0   0.000045  0.004630  0.163317  0.882675  0.968082   0.493723 -11.255813   \n",
       "1   0.000132  0.007395  0.236134  0.478429  0.766847   0.359519  -6.779388   \n",
       "2   0.000006  0.001685  0.061272  0.976042  0.994296   0.961505   0.407564   \n",
       "3   0.000067  0.004643  0.148968  0.690041  0.883682   0.762376  -0.028446   \n",
       "4   0.000022  0.002845  0.090335  0.916350  0.977951   0.949572   0.850276   \n",
       "5   0.000112  0.006616  0.206737  0.536838  0.777957   0.673963   0.530883   \n",
       "6   0.000030  0.003575  0.120591  0.930699  0.993558   0.841346  -0.819952   \n",
       "7   0.000218  0.009086  0.269193  0.188515  0.660763   0.109233  -9.802806   \n",
       "8   0.000005  0.001649  0.067048  0.980933  0.995591   0.967842   0.736369   \n",
       "9   0.000111  0.005995  0.180815  0.466764  0.792074   0.655240   0.169769   \n",
       "10  0.000029  0.003105  0.099611  0.883029  0.964017   0.911467   0.826741   \n",
       "11  0.000202  0.008956  0.267617  0.284085  0.613989   0.458731   0.213384   \n",
       "12  0.000031  0.003602  0.130085  0.878527  0.963094   0.854355   0.231846   \n",
       "13  0.000054  0.004353  0.144494  0.765645  0.931321   0.799106   0.123201   \n",
       "14  0.000013  0.002424  0.092179  0.944927  0.982321   0.935492   0.618484   \n",
       "15  0.000030  0.003129  0.105142  0.871055  0.956827   0.894622   0.639359   \n",
       "16  0.000006  0.001400  0.050026  0.967902  0.990846   0.982109   0.973711   \n",
       "17  0.000057  0.004100  0.123210  0.727944  0.898826   0.845482   0.755097   \n",
       "18  0.000182  0.009304  0.269056  0.669925  0.910799   0.211388  -4.734536   \n",
       "19  0.000230  0.010374  0.290932  0.574274  0.830944   0.257639  -1.904599   \n",
       "20  0.000007  0.001855  0.071612  0.974403  0.992626   0.930093  -0.214424   \n",
       "21  0.000029  0.003134  0.105127  0.876431  0.951051   0.889585   0.395513   \n",
       "22  0.000009  0.001774  0.060270  0.955039  0.988258   0.973573   0.963094   \n",
       "23  0.000073  0.004964  0.154257  0.666519  0.858249   0.789546   0.657565   \n",
       "24  0.000048  0.004631  0.148245  0.853139  0.984640   0.826602  -2.020204   \n",
       "25  0.000364  0.013356  0.386732  0.022562  0.356597  -0.279365  -6.062881   \n",
       "26  0.000004  0.001494  0.061334  0.991055  0.999882   0.946374  -0.316462   \n",
       "27  0.000219  0.009812  0.292241  0.220035  0.682006   0.243933  -2.204085   \n",
       "28  0.000182  0.010910  0.338059  0.533530  0.874250   0.759495   0.680988   \n",
       "29  0.000327  0.012537  0.371222  0.081765  0.306189   0.111947  -0.817344   \n",
       "30  0.000044  0.004528  0.183926  0.930554  0.987653   0.454273  -8.752284   \n",
       "31  0.000121  0.007061  0.235867  0.556162  0.822242   0.366147  -7.279881   \n",
       "32  0.000005  0.001573  0.069872  0.979109  0.993671   0.962104   0.695135   \n",
       "33  0.000045  0.003799  0.123586  0.794344  0.922382   0.836401   0.225206   \n",
       "34  0.000011  0.001993  0.072637  0.906248  0.973866   0.957845   0.931509   \n",
       "35  0.000120  0.006461  0.192283  0.438279  0.820735   0.672316   0.529418   \n",
       "36  0.000018  0.002754  0.102952  0.949463  0.976699   0.909234   0.178548   \n",
       "37  0.000026  0.003084  0.111115  0.912283  0.965734   0.893551   0.297245   \n",
       "38  0.000014  0.002411  0.089927  0.947361  0.977165   0.930539   0.568394   \n",
       "39  0.000020  0.002696  0.096692  0.924715  0.970727   0.920148   0.647018   \n",
       "40  0.000006  0.001282  0.043705  0.969916  0.994499   0.983727   0.974182   \n",
       "41  0.000039  0.003372  0.104842  0.816190  0.946874   0.890920   0.817685   \n",
       "42  0.000030  0.003895  0.162339  0.703440  0.961249   0.830408  -0.317098   \n",
       "43  0.000263  0.010492  0.318608 -0.011265  0.507507  -0.083851  -7.047225   \n",
       "44  0.000004  0.001362  0.065396  0.951726  0.998528   0.966271   0.582016   \n",
       "45  0.000208  0.008952  0.268351  0.307749  0.682917   0.309362  -1.452218   \n",
       "46  0.000027  0.003548  0.130953  0.692747  0.921199   0.879252   0.797198   \n",
       "47  0.000248  0.009684  0.289489 -0.088285  0.567222   0.329571   0.019554   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train          48  \n",
       "1               Fresh   Test          48  \n",
       "2   Transfer learning  Train          48  \n",
       "3   Transfer learning   Test          48  \n",
       "4       Random Forest  Train          48  \n",
       "5       Random Forest   Test          48  \n",
       "6               Fresh  Train          18  \n",
       "7               Fresh   Test          18  \n",
       "8   Transfer learning  Train          18  \n",
       "9   Transfer learning   Test          18  \n",
       "10      Random Forest  Train          18  \n",
       "11      Random Forest   Test          18  \n",
       "12              Fresh  Train         452  \n",
       "13              Fresh   Test         452  \n",
       "14  Transfer learning  Train         452  \n",
       "15  Transfer learning   Test         452  \n",
       "16      Random Forest  Train         452  \n",
       "17      Random Forest   Test         452  \n",
       "18              Fresh  Train         178  \n",
       "19              Fresh   Test         178  \n",
       "20  Transfer learning  Train         178  \n",
       "21  Transfer learning   Test         178  \n",
       "22      Random Forest  Train         178  \n",
       "23      Random Forest   Test         178  \n",
       "24              Fresh  Train           5  \n",
       "25              Fresh   Test           5  \n",
       "26  Transfer learning  Train           5  \n",
       "27  Transfer learning   Test           5  \n",
       "28      Random Forest  Train           5  \n",
       "29      Random Forest   Test           5  \n",
       "30              Fresh  Train          93  \n",
       "31              Fresh   Test          93  \n",
       "32  Transfer learning  Train          93  \n",
       "33  Transfer learning   Test          93  \n",
       "34      Random Forest  Train          93  \n",
       "35      Random Forest   Test          93  \n",
       "36              Fresh  Train         914  \n",
       "37              Fresh   Test         914  \n",
       "38  Transfer learning  Train         914  \n",
       "39  Transfer learning   Test         914  \n",
       "40      Random Forest  Train         914  \n",
       "41      Random Forest   Test         914  \n",
       "42              Fresh  Train           8  \n",
       "43              Fresh   Test           8  \n",
       "44  Transfer learning  Train           8  \n",
       "45  Transfer learning   Test           8  \n",
       "46      Random Forest  Train           8  \n",
       "47      Random Forest   Test           8  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(allResults)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-bec2847b00ec4829aa587dc49a4db36d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-bec2847b00ec4829aa587dc49a4db36d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-bec2847b00ec4829aa587dc49a4db36d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9f391405bdd5042a789cfd527b925cf5\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Model\"}, {\"type\": \"quantitative\", \"field\": \"mse\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Train Size\", \"scale\": {\"type\": \"log\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"mse\", \"title\": \"MSE\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-9f391405bdd5042a789cfd527b925cf5\": [{\"mse\": 0.00013239975669421256, \"mae\": 0.0073951613157987595, \"mre\": 0.23613427579402924, \"peakR2\": 0.47842884849229084, \"maxAggR2\": 0.7668469077836286, \"meanAggR2\": 0.3595185652712302, \"minAggR2\": -6.779387857973115, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 6.743849371559918e-05, \"mae\": 0.004642562009394169, \"mre\": 0.14896759390830994, \"peakR2\": 0.6900412308995685, \"maxAggR2\": 0.8836816383759831, \"meanAggR2\": 0.7623756168404656, \"minAggR2\": -0.028445758976652913, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 0.00011246544642070364, \"mae\": 0.006616252006297514, \"mre\": 0.2067372251586526, \"peakR2\": 0.5368375934050054, \"maxAggR2\": 0.7779574603912461, \"meanAggR2\": 0.6739629502637875, \"minAggR2\": 0.5308830196657073, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 0.00021771628235001117, \"mae\": 0.009085983969271183, \"mre\": 0.2691933512687683, \"peakR2\": 0.18851495308899058, \"maxAggR2\": 0.6607632757146799, \"meanAggR2\": 0.10923347360444581, \"minAggR2\": -9.802806097338738, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 0.000110870816570241, \"mae\": 0.005995152983814478, \"mre\": 0.18081489205360413, \"peakR2\": 0.46676352848122393, \"maxAggR2\": 0.7920738283239477, \"meanAggR2\": 0.6552404774135128, \"minAggR2\": 0.16976910703455816, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 0.00020244630332931501, \"mae\": 0.008955885011452999, \"mre\": 0.26761687643623056, \"peakR2\": 0.2840847339150381, \"maxAggR2\": 0.6139889275515114, \"meanAggR2\": 0.45873059729015925, \"minAggR2\": 0.21338414147779994, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 5.416949716163799e-05, \"mae\": 0.004353469703346491, \"mre\": 0.1444936841726303, \"peakR2\": 0.7656445720005688, \"maxAggR2\": 0.9313206944266822, \"meanAggR2\": 0.7991058247977757, \"minAggR2\": 0.12320067904141208, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 2.9880715374019928e-05, \"mae\": 0.00312911719083786, \"mre\": 0.10514210909605026, \"peakR2\": 0.8710545789894386, \"maxAggR2\": 0.9568271176874266, \"meanAggR2\": 0.8946223934965023, \"minAggR2\": 0.6393592266277728, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 5.707159312904131e-05, \"mae\": 0.004100101598023114, \"mre\": 0.12321002909595599, \"peakR2\": 0.7279443364465518, \"maxAggR2\": 0.898826089326938, \"meanAggR2\": 0.8454822851209279, \"minAggR2\": 0.7550971351593285, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 0.00022950535640120506, \"mae\": 0.010374131612479687, \"mre\": 0.29093217849731445, \"peakR2\": 0.5742738128155641, \"maxAggR2\": 0.8309443748503414, \"meanAggR2\": 0.2576394227837426, \"minAggR2\": -1.9045991271012759, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 2.8881295293103904e-05, \"mae\": 0.003134325612336397, \"mre\": 0.10512659698724747, \"peakR2\": 0.8764311717969782, \"maxAggR2\": 0.9510513048783097, \"meanAggR2\": 0.8895854654851437, \"minAggR2\": 0.3955125004200759, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 7.298801986935364e-05, \"mae\": 0.004964239905348007, \"mre\": 0.15425712963738358, \"peakR2\": 0.6665188623965452, \"maxAggR2\": 0.8582492615432595, \"meanAggR2\": 0.7895463791199311, \"minAggR2\": 0.6575649631735613, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 0.0003636475303210318, \"mae\": 0.013355902396142483, \"mre\": 0.3867315351963043, \"peakR2\": 0.022562240218296448, \"maxAggR2\": 0.356597376403047, \"meanAggR2\": -0.2793649513258819, \"minAggR2\": -6.062880659374441, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.00021949855727143586, \"mae\": 0.009811561554670334, \"mre\": 0.29224058985710144, \"peakR2\": 0.22003499231929347, \"maxAggR2\": 0.6820059011926161, \"meanAggR2\": 0.2439333756439543, \"minAggR2\": -2.2040848165786064, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.0003266324415617001, \"mae\": 0.012537363327359934, \"mre\": 0.37122207481320973, \"peakR2\": 0.08176478278216937, \"maxAggR2\": 0.3061888385960987, \"meanAggR2\": 0.1119471228823537, \"minAggR2\": -0.8173438702711409, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.00012083098408766091, \"mae\": 0.007060639094561338, \"mre\": 0.2358672320842743, \"peakR2\": 0.5561616996179963, \"maxAggR2\": 0.8222422850375093, \"meanAggR2\": 0.3661467889817982, \"minAggR2\": -7.279880506200454, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 4.544541297946125e-05, \"mae\": 0.0037988205440342426, \"mre\": 0.12358648329973221, \"peakR2\": 0.7943442048105649, \"maxAggR2\": 0.9223820097416336, \"meanAggR2\": 0.8364005166013656, \"minAggR2\": 0.2252060042200783, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 0.000119914262344142, \"mae\": 0.006460516766633253, \"mre\": 0.19228315293430737, \"peakR2\": 0.4382790685818362, \"maxAggR2\": 0.8207353468848997, \"meanAggR2\": 0.6723159143608466, \"minAggR2\": 0.5294181193681399, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 2.5872723199427128e-05, \"mae\": 0.0030839703977108, \"mre\": 0.1111145094037056, \"peakR2\": 0.9122826093369697, \"maxAggR2\": 0.9657343181510923, \"meanAggR2\": 0.8935507701507021, \"minAggR2\": 0.2972450368055931, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 2.0050940293003805e-05, \"mae\": 0.002695560222491622, \"mre\": 0.09669221937656403, \"peakR2\": 0.9247149789965305, \"maxAggR2\": 0.9707268255964078, \"meanAggR2\": 0.9201480691352086, \"minAggR2\": 0.6470184121125188, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 3.8644817376023864e-05, \"mae\": 0.003371606942862902, \"mre\": 0.10484216920035425, \"peakR2\": 0.8161900206176175, \"maxAggR2\": 0.9468738536717735, \"meanAggR2\": 0.8909197488710563, \"minAggR2\": 0.8176853662229693, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 0.00026348239043727517, \"mae\": 0.010492314584553242, \"mre\": 0.31860819458961487, \"peakR2\": -0.011264838822156564, \"maxAggR2\": 0.5075073976867077, \"meanAggR2\": -0.08385146672955819, \"minAggR2\": -7.047225323562271, \"Model\": \"Fresh\", \"Set\": \"Test\", \"Train Size\": 8}, {\"mse\": 0.00020826903346460313, \"mae\": 0.008952383883297443, \"mre\": 0.2683512568473816, \"peakR2\": 0.3077490537840726, \"maxAggR2\": 0.6829168044439238, \"meanAggR2\": 0.3093624246839384, \"minAggR2\": -1.4522183013411136, \"Model\": \"Transfer learning\", \"Set\": \"Test\", \"Train Size\": 8}, {\"mse\": 0.000248050777170827, \"mae\": 0.00968352100434512, \"mre\": 0.2894886252795136, \"peakR2\": -0.08828492599716276, \"maxAggR2\": 0.5672215387333416, \"meanAggR2\": 0.3295714436292946, \"minAggR2\": 0.01955364515768554, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 8}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df.Set=='Test']).mark_circle().encode(\n",
    "    x=alt.X('Train Size:Q', scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('mse:Q', title='MSE'),\n",
    "    color='Model',\n",
    "    tooltip=['Model', 'mse']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
