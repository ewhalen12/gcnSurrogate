{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from gcnSurrogate.models.feastnetSurrogateModel import FeaStNet\n",
    "from gcnSurrogate.models.pointRegressorSurrogateModel import PointRegressor\n",
    "from gcnSurrogate.readers.loadConmechGraphs import loadConmechGraphs\n",
    "from gcnSurrogate.visualization.altTrussViz import plotTruss, interactiveErrorPlot\n",
    "from gcnSurrogate.util.gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.027434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.022934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.028404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.065064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.027434\n",
       "std       0.005930\n",
       "min       0.022934\n",
       "25%       0.023780\n",
       "50%       0.025250\n",
       "75%       0.028404\n",
       "max       0.065064"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataDir = \"data/2D_Truss_v1.3/conmech/\"\n",
    "# testDir = os.path.join(dataDir, 'design_9_N_1000/')\n",
    "# allGraphsUnfiltered = loadConmechGraphs(testDir)\n",
    "\n",
    "# dataDir = \"data/endLoadsv1.0/conmech/\"\n",
    "# testDir = os.path.join(dataDir, 'design_7_N_1000/')\n",
    "# allGraphsUnfiltered = loadConmechGraphs(testDir)\n",
    "\n",
    "dataDir = \"data/tower1.0/conmech/\"\n",
    "testDir = os.path.join(dataDir, 'design_5_N_1000/')\n",
    "allGraphsUnfiltered = loadConmechGraphs(testDir, loadDims=[0])\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.025836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.022934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.027176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.034477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.025836\n",
       "std      0.002804\n",
       "min      0.022934\n",
       "25%      0.023687\n",
       "50%      0.024775\n",
       "75%      0.027176\n",
       "max      0.034477"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max().item()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from data/2D_Truss_v1.3/conmech/design_7_N_1000/\n",
      "loaded 900 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "# pretrainDirs = glob.glob(os.path.join(dataDir, '*1000/'))\n",
    "# pretrainDirs.remove(testDir)\n",
    "\n",
    "pretrainDirs = ['data/2D_Truss_v1.3/conmech/design_7_N_1000/']\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainDir in pretrainDirs:\n",
    "    designName = pretrainDir.split('/')[-2].split('_N')[0]\n",
    "    print(f'loading from {pretrainDir}')\n",
    "    graphsUnfiltered = loadConmechGraphs(pretrainDir)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 9.1679e-01   valLoss:8.3533e-01  time: 2.15e+00\n",
      "epoch: 1   trainLoss: 7.5296e-01   valLoss:8.4459e-01  time: 1.81e+00\n",
      "epoch: 2   trainLoss: 6.4657e-01   valLoss:9.2376e-01  time: 1.82e+00\n",
      "epoch: 3   trainLoss: 5.7322e-01   valLoss:1.1124e+00  time: 1.83e+00\n",
      "epoch: 4   trainLoss: 5.0579e-01   valLoss:1.3154e+00  time: 1.83e+00\n",
      "epoch: 5   trainLoss: 4.6400e-01   valLoss:2.0401e+00  time: 1.84e+00\n",
      "epoch: 6   trainLoss: 4.0377e-01   valLoss:3.0466e+00  time: 1.81e+00\n",
      "epoch: 7   trainLoss: 3.5834e-01   valLoss:3.6507e+00  time: 1.84e+00\n",
      "epoch: 8   trainLoss: 3.1649e-01   valLoss:3.2505e+00  time: 1.83e+00\n",
      "epoch: 9   trainLoss: 2.8219e-01   valLoss:2.2849e+00  time: 1.83e+00\n",
      "epoch: 10   trainLoss: 2.6543e-01   valLoss:1.4233e+00  time: 1.81e+00\n",
      "epoch: 11   trainLoss: 2.2674e-01   valLoss:1.0754e+00  time: 1.81e+00\n",
      "epoch: 12   trainLoss: 2.3209e-01   valLoss:8.7757e-01  time: 1.80e+00\n",
      "epoch: 13   trainLoss: 2.1179e-01   valLoss:8.9913e-01  time: 1.82e+00\n",
      "epoch: 14   trainLoss: 1.9070e-01   valLoss:1.7224e+00  time: 1.84e+00\n",
      "epoch: 15   trainLoss: 1.7677e-01   valLoss:1.3606e+00  time: 1.86e+00\n",
      "epoch: 16   trainLoss: 1.5807e-01   valLoss:1.6134e+00  time: 1.79e+00\n",
      "epoch: 17   trainLoss: 1.5293e-01   valLoss:1.4399e+00  time: 1.80e+00\n",
      "epoch: 18   trainLoss: 1.4532e-01   valLoss:1.4158e+00  time: 1.80e+00\n",
      "epoch: 19   trainLoss: 1.2709e-01   valLoss:1.3411e+00  time: 1.80e+00\n",
      "epoch: 20   trainLoss: 1.1922e-01   valLoss:1.1162e+00  time: 1.84e+00\n",
      "epoch: 21   trainLoss: 1.1218e-01   valLoss:1.3086e+00  time: 1.87e+00\n",
      "epoch: 22   trainLoss: 1.2342e-01   valLoss:5.5316e-01  time: 1.81e+00\n",
      "epoch: 23   trainLoss: 1.1468e-01   valLoss:4.2835e-01  time: 1.84e+00\n",
      "epoch: 24   trainLoss: 9.8328e-02   valLoss:3.7461e-01  time: 1.85e+00\n",
      "epoch: 25   trainLoss: 1.1504e-01   valLoss:4.1087e-01  time: 1.85e+00\n",
      "epoch: 26   trainLoss: 1.0319e-01   valLoss:3.5631e-01  time: 1.86e+00\n",
      "epoch: 27   trainLoss: 1.0299e-01   valLoss:4.9554e-01  time: 1.85e+00\n",
      "epoch: 28   trainLoss: 1.2393e-01   valLoss:2.8382e-01  time: 1.84e+00\n",
      "epoch: 29   trainLoss: 1.1451e-01   valLoss:4.2327e-01  time: 1.84e+00\n",
      "epoch: 30   trainLoss: 1.1964e-01   valLoss:2.6623e-01  time: 1.82e+00\n",
      "epoch: 31   trainLoss: 1.1577e-01   valLoss:5.4254e-01  time: 1.83e+00\n",
      "epoch: 32   trainLoss: 1.0853e-01   valLoss:5.9603e-01  time: 1.82e+00\n",
      "epoch: 33   trainLoss: 8.2481e-02   valLoss:1.2984e+00  time: 1.82e+00\n",
      "epoch: 34   trainLoss: 7.1177e-02   valLoss:9.2819e-01  time: 1.83e+00\n",
      "epoch: 35   trainLoss: 7.0500e-02   valLoss:9.7059e-01  time: 1.85e+00\n",
      "epoch: 36   trainLoss: 6.1900e-02   valLoss:4.7806e-01  time: 1.83e+00\n",
      "epoch: 37   trainLoss: 5.5808e-02   valLoss:3.6074e-01  time: 1.94e+00\n",
      "epoch: 38   trainLoss: 6.2399e-02   valLoss:2.8703e-01  time: 1.85e+00\n",
      "epoch: 39   trainLoss: 5.8584e-02   valLoss:2.6290e-01  time: 1.84e+00\n",
      "epoch: 40   trainLoss: 5.8379e-02   valLoss:3.1408e-01  time: 1.84e+00\n",
      "epoch: 41   trainLoss: 7.6034e-02   valLoss:5.6538e-01  time: 1.83e+00\n",
      "epoch: 42   trainLoss: 1.0462e-01   valLoss:9.4798e-01  time: 1.84e+00\n",
      "epoch: 43   trainLoss: 1.0893e-01   valLoss:9.3906e-01  time: 1.85e+00\n",
      "epoch: 44   trainLoss: 9.4977e-02   valLoss:7.7217e-01  time: 1.83e+00\n",
      "epoch: 45   trainLoss: 7.9063e-02   valLoss:7.9493e-01  time: 1.86e+00\n",
      "epoch: 46   trainLoss: 7.0040e-02   valLoss:9.3018e-01  time: 1.85e+00\n",
      "epoch: 47   trainLoss: 7.5244e-02   valLoss:6.3424e-01  time: 1.82e+00\n",
      "epoch: 48   trainLoss: 6.6615e-02   valLoss:2.7185e-01  time: 1.83e+00\n",
      "epoch: 49   trainLoss: 6.8974e-02   valLoss:2.6619e-01  time: 1.83e+00\n",
      "epoch: 50   trainLoss: 7.3840e-02   valLoss:3.0308e-01  time: 1.85e+00\n",
      "epoch: 51   trainLoss: 4.5865e-02   valLoss:2.2961e-01  time: 1.84e+00\n",
      "epoch: 52   trainLoss: 4.0546e-02   valLoss:2.5117e-01  time: 1.83e+00\n",
      "epoch: 53   trainLoss: 4.9862e-02   valLoss:3.0980e-01  time: 1.87e+00\n",
      "epoch: 54   trainLoss: 5.0399e-02   valLoss:2.5699e-01  time: 1.84e+00\n",
      "epoch: 55   trainLoss: 5.9505e-02   valLoss:1.0710e+00  time: 1.84e+00\n",
      "epoch: 56   trainLoss: 5.3377e-02   valLoss:1.2213e+00  time: 1.86e+00\n",
      "epoch: 57   trainLoss: 4.4380e-02   valLoss:1.3473e+00  time: 1.84e+00\n",
      "epoch: 58   trainLoss: 4.4820e-02   valLoss:1.2049e+00  time: 1.83e+00\n",
      "epoch: 59   trainLoss: 3.7428e-02   valLoss:7.2305e-01  time: 1.84e+00\n",
      "epoch: 60   trainLoss: 4.0764e-02   valLoss:2.4516e-01  time: 1.85e+00\n",
      "epoch: 61   trainLoss: 4.3789e-02   valLoss:3.1721e-01  time: 1.85e+00\n",
      "epoch: 62   trainLoss: 4.3406e-02   valLoss:1.5098e-01  time: 1.83e+00\n",
      "epoch: 63   trainLoss: 4.9208e-02   valLoss:4.6122e-01  time: 1.84e+00\n",
      "epoch: 64   trainLoss: 4.3632e-02   valLoss:1.7092e-01  time: 1.84e+00\n",
      "epoch: 65   trainLoss: 3.3390e-02   valLoss:1.8130e-01  time: 1.84e+00\n",
      "epoch: 66   trainLoss: 4.0213e-02   valLoss:2.6606e-01  time: 1.84e+00\n",
      "epoch: 67   trainLoss: 3.5770e-02   valLoss:2.9987e-01  time: 1.84e+00\n",
      "epoch: 68   trainLoss: 4.3696e-02   valLoss:2.0189e-01  time: 1.85e+00\n",
      "epoch: 69   trainLoss: 4.8083e-02   valLoss:3.7430e-01  time: 1.84e+00\n",
      "epoch: 70   trainLoss: 5.0321e-02   valLoss:7.4670e-01  time: 1.86e+00\n",
      "epoch: 71   trainLoss: 6.0566e-02   valLoss:8.1161e-01  time: 1.83e+00\n",
      "epoch: 72   trainLoss: 6.1614e-02   valLoss:3.0281e-01  time: 1.82e+00\n",
      "epoch: 73   trainLoss: 5.6272e-02   valLoss:2.6691e-01  time: 1.85e+00\n",
      "epoch: 74   trainLoss: 6.0147e-02   valLoss:4.6931e-01  time: 1.86e+00\n",
      "epoch: 75   trainLoss: 4.8706e-02   valLoss:4.6192e-01  time: 1.89e+00\n",
      "epoch: 76   trainLoss: 4.8841e-02   valLoss:2.1013e-01  time: 1.81e+00\n",
      "epoch: 77   trainLoss: 3.6286e-02   valLoss:1.2944e-01  time: 1.81e+00\n",
      "epoch: 78   trainLoss: 5.0230e-02   valLoss:2.1563e-01  time: 1.88e+00\n",
      "epoch: 79   trainLoss: 4.0600e-02   valLoss:1.9634e-01  time: 1.83e+00\n",
      "epoch: 80   trainLoss: 3.6162e-02   valLoss:2.8904e-01  time: 1.82e+00\n",
      "epoch: 81   trainLoss: 5.1589e-02   valLoss:2.2908e-01  time: 1.86e+00\n",
      "epoch: 82   trainLoss: 5.1060e-02   valLoss:2.3877e-01  time: 1.87e+00\n",
      "epoch: 83   trainLoss: 4.9027e-02   valLoss:2.4302e-01  time: 1.85e+00\n",
      "epoch: 84   trainLoss: 3.9498e-02   valLoss:2.9663e-01  time: 1.86e+00\n",
      "epoch: 85   trainLoss: 4.5236e-02   valLoss:1.9130e-01  time: 1.85e+00\n",
      "epoch: 86   trainLoss: 4.7222e-02   valLoss:3.4681e-01  time: 1.86e+00\n",
      "epoch: 87   trainLoss: 4.0480e-02   valLoss:5.8140e-01  time: 1.86e+00\n",
      "epoch: 88   trainLoss: 3.5878e-02   valLoss:5.9493e-01  time: 1.84e+00\n",
      "epoch: 89   trainLoss: 3.7294e-02   valLoss:3.4218e-01  time: 1.87e+00\n",
      "epoch: 90   trainLoss: 3.5727e-02   valLoss:5.1904e-01  time: 1.84e+00\n",
      "epoch: 91   trainLoss: 3.8718e-02   valLoss:4.1478e-01  time: 1.87e+00\n",
      "epoch: 92   trainLoss: 4.6501e-02   valLoss:4.0544e-01  time: 1.85e+00\n",
      "epoch: 93   trainLoss: 5.3768e-02   valLoss:1.0645e+00  time: 1.82e+00\n",
      "epoch: 94   trainLoss: 5.4920e-02   valLoss:2.0989e-01  time: 1.85e+00\n",
      "epoch: 95   trainLoss: 5.9785e-02   valLoss:2.2860e-01  time: 1.84e+00\n",
      "epoch: 96   trainLoss: 4.6306e-02   valLoss:2.5007e-01  time: 1.86e+00\n",
      "epoch: 97   trainLoss: 3.7980e-02   valLoss:3.6420e-01  time: 1.85e+00\n",
      "epoch: 98   trainLoss: 4.5839e-02   valLoss:2.0483e-01  time: 1.82e+00\n",
      "epoch: 99   trainLoss: 4.8402e-02   valLoss:2.6789e-01  time: 1.81e+00\n",
      "loading checkpoint 77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-a61174c8135d4390aa0aa8a58bb15ab2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-a61174c8135d4390aa0aa8a58bb15ab2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-a61174c8135d4390aa0aa8a58bb15ab2\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8f8db4d517b4bc3027d6bec77348824f\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-8f8db4d517b4bc3027d6bec77348824f\": [{\"train\": 0.9167852203051249, \"val\": 0.8353289900554551, \"epoch\": 0}, {\"train\": 0.7529593308766683, \"val\": 0.8445925413182488, \"epoch\": 1}, {\"train\": 0.646568497021993, \"val\": 0.9237600552263083, \"epoch\": 2}, {\"train\": 0.5732211271921793, \"val\": 1.1124207218488058, \"epoch\": 3}, {\"train\": 0.5057910283406576, \"val\": 1.3153773258129755, \"epoch\": 4}, {\"train\": 0.4640036424001058, \"val\": 2.040081106291877, \"epoch\": 5}, {\"train\": 0.40377066532770794, \"val\": 3.0465512423603625, \"epoch\": 6}, {\"train\": 0.35834093888600665, \"val\": 3.6506961822509765, \"epoch\": 7}, {\"train\": 0.3164867361386617, \"val\": 3.2504536392512144, \"epoch\": 8}, {\"train\": 0.28218773504098255, \"val\": 2.28490344495685, \"epoch\": 9}, {\"train\": 0.2654339522123337, \"val\": 1.4232673967878025, \"epoch\": 10}, {\"train\": 0.22674328088760376, \"val\": 1.0754413008413932, \"epoch\": 11}, {\"train\": 0.23208575944105783, \"val\": 0.8775740441900712, \"epoch\": 12}, {\"train\": 0.21179463962713876, \"val\": 0.8991259983844228, \"epoch\": 13}, {\"train\": 0.19070377945899963, \"val\": 1.7224386280885449, \"epoch\": 14}, {\"train\": 0.17676931619644165, \"val\": 1.3606289059475616, \"epoch\": 15}, {\"train\": 0.15806831419467926, \"val\": 1.6133758809831407, \"epoch\": 16}, {\"train\": 0.1529251585404078, \"val\": 1.4399237128043616, \"epoch\": 17}, {\"train\": 0.14532421032587686, \"val\": 1.4157506824091628, \"epoch\": 18}, {\"train\": 0.127093106508255, \"val\": 1.341056669489653, \"epoch\": 19}, {\"train\": 0.11922406901915868, \"val\": 1.1162107337128233, \"epoch\": 20}, {\"train\": 0.11217947800954182, \"val\": 1.3086190149601964, \"epoch\": 21}, {\"train\": 0.12342387934525807, \"val\": 0.553156108898973, \"epoch\": 22}, {\"train\": 0.11468446006377538, \"val\": 0.42834617782522133, \"epoch\": 23}, {\"train\": 0.09832762678464253, \"val\": 0.37461458177616197, \"epoch\": 24}, {\"train\": 0.11503831048806508, \"val\": 0.410866806704413, \"epoch\": 25}, {\"train\": 0.10319323341051738, \"val\": 0.35630715657715445, \"epoch\": 26}, {\"train\": 0.10299460838238399, \"val\": 0.4955424744808288, \"epoch\": 27}, {\"train\": 0.12392677118380864, \"val\": 0.28381771153459945, \"epoch\": 28}, {\"train\": 0.11451296259959538, \"val\": 0.4232694658071355, \"epoch\": 29}, {\"train\": 0.1196354329586029, \"val\": 0.2662315397074929, \"epoch\": 30}, {\"train\": 0.1157661999265353, \"val\": 0.5425357611918891, \"epoch\": 31}, {\"train\": 0.10853107025225957, \"val\": 0.5960318970355999, \"epoch\": 32}, {\"train\": 0.08248068640629451, \"val\": 1.2984142239998888, \"epoch\": 33}, {\"train\": 0.07117736836274464, \"val\": 0.928192746873807, \"epoch\": 34}, {\"train\": 0.07049961512287457, \"val\": 0.9705907456300876, \"epoch\": 35}, {\"train\": 0.06190032636125883, \"val\": 0.47805681741210043, \"epoch\": 36}, {\"train\": 0.055808222542206444, \"val\": 0.3607417050335142, \"epoch\": 37}, {\"train\": 0.06239859387278557, \"val\": 0.28703000967011405, \"epoch\": 38}, {\"train\": 0.05858388667305311, \"val\": 0.26290312692247053, \"epoch\": 39}, {\"train\": 0.058379169553518295, \"val\": 0.3140806948283204, \"epoch\": 40}, {\"train\": 0.07603422676523526, \"val\": 0.5653824757646632, \"epoch\": 41}, {\"train\": 0.10462083915869395, \"val\": 0.9479828734295788, \"epoch\": 42}, {\"train\": 0.10892845193545024, \"val\": 0.939063954298143, \"epoch\": 43}, {\"train\": 0.09497677038113277, \"val\": 0.7721748914431643, \"epoch\": 44}, {\"train\": 0.07906270772218704, \"val\": 0.7949257521717636, \"epoch\": 45}, {\"train\": 0.07003990933299065, \"val\": 0.9301832365355006, \"epoch\": 46}, {\"train\": 0.07524420072635014, \"val\": 0.6342360128924527, \"epoch\": 47}, {\"train\": 0.06661479671796162, \"val\": 0.271845593555244, \"epoch\": 48}, {\"train\": 0.06897367785374324, \"val\": 0.2661894775306185, \"epoch\": 49}, {\"train\": 0.07384006182352702, \"val\": 0.30308178331427, \"epoch\": 50}, {\"train\": 0.045864954590797424, \"val\": 0.22960567851550878, \"epoch\": 51}, {\"train\": 0.04054618130127589, \"val\": 0.25117063263983086, \"epoch\": 52}, {\"train\": 0.04986209919055303, \"val\": 0.3097959830943081, \"epoch\": 53}, {\"train\": 0.05039852981766065, \"val\": 0.25698933848352346, \"epoch\": 54}, {\"train\": 0.05950493365526199, \"val\": 1.0710127163540435, \"epoch\": 55}, {\"train\": 0.05337711051106453, \"val\": 1.2213102805738647, \"epoch\": 56}, {\"train\": 0.04438033327460289, \"val\": 1.3472845179270263, \"epoch\": 57}, {\"train\": 0.04481959839661916, \"val\": 1.204864241182804, \"epoch\": 58}, {\"train\": 0.03742846225698789, \"val\": 0.723049644201442, \"epoch\": 59}, {\"train\": 0.04076389595866203, \"val\": 0.24515888097144112, \"epoch\": 60}, {\"train\": 0.04378896579146385, \"val\": 0.3172109522943006, \"epoch\": 61}, {\"train\": 0.04340554028749466, \"val\": 0.1509801476193523, \"epoch\": 62}, {\"train\": 0.04920795063177744, \"val\": 0.4612152667095264, \"epoch\": 63}, {\"train\": 0.043631830563147865, \"val\": 0.17091779291146883, \"epoch\": 64}, {\"train\": 0.03339030221104622, \"val\": 0.18129998475002745, \"epoch\": 65}, {\"train\": 0.040212683379650116, \"val\": 0.26606414024890573, \"epoch\": 66}, {\"train\": 0.035770222544670105, \"val\": 0.2998743731804468, \"epoch\": 67}, {\"train\": 0.04369571432471275, \"val\": 0.20188819758800997, \"epoch\": 68}, {\"train\": 0.04808282355467478, \"val\": 0.37429882448718504, \"epoch\": 69}, {\"train\": 0.05032058929403623, \"val\": 0.7466950208776527, \"epoch\": 70}, {\"train\": 0.06056642904877663, \"val\": 0.8116087606797616, \"epoch\": 71}, {\"train\": 0.06161424641807874, \"val\": 0.3028136209791733, \"epoch\": 72}, {\"train\": 0.05627210686604182, \"val\": 0.2669113946043783, \"epoch\": 73}, {\"train\": 0.06014714762568474, \"val\": 0.4693115533053599, \"epoch\": 74}, {\"train\": 0.04870586345593134, \"val\": 0.4619249796425855, \"epoch\": 75}, {\"train\": 0.04884143049518267, \"val\": 0.21012925328169432, \"epoch\": 76}, {\"train\": 0.03628583997488022, \"val\": 0.12944464649729154, \"epoch\": 77}, {\"train\": 0.05022997036576271, \"val\": 0.21562602762822752, \"epoch\": 78}, {\"train\": 0.04060028990109762, \"val\": 0.1963441764090762, \"epoch\": 79}, {\"train\": 0.03616193619867166, \"val\": 0.2890408950305923, \"epoch\": 80}, {\"train\": 0.0515888457496961, \"val\": 0.22908220943063498, \"epoch\": 81}, {\"train\": 0.05105960493286451, \"val\": 0.23877156138144157, \"epoch\": 82}, {\"train\": 0.04902675996224085, \"val\": 0.24301686527000532, \"epoch\": 83}, {\"train\": 0.03949835399786631, \"val\": 0.2966343323872597, \"epoch\": 84}, {\"train\": 0.04523582632342974, \"val\": 0.19130192516302622, \"epoch\": 85}, {\"train\": 0.047222296396891274, \"val\": 0.34680600878955037, \"epoch\": 86}, {\"train\": 0.04047994315624237, \"val\": 0.5814022482683261, \"epoch\": 87}, {\"train\": 0.0358781764904658, \"val\": 0.5949258355975703, \"epoch\": 88}, {\"train\": 0.03729387124379476, \"val\": 0.3421830989765348, \"epoch\": 89}, {\"train\": 0.03572688872615496, \"val\": 0.5190441400985475, \"epoch\": 90}, {\"train\": 0.03871811367571354, \"val\": 0.4147794853757929, \"epoch\": 91}, {\"train\": 0.04650108516216278, \"val\": 0.4054417138091392, \"epoch\": 92}, {\"train\": 0.053767954309781395, \"val\": 1.064467228204012, \"epoch\": 93}, {\"train\": 0.054920448611179985, \"val\": 0.20989139234405702, \"epoch\": 94}, {\"train\": 0.059785415728886925, \"val\": 0.22859793613768287, \"epoch\": 95}, {\"train\": 0.046306329468886055, \"val\": 0.2500686920316959, \"epoch\": 96}, {\"train\": 0.03797968725363413, \"val\": 0.36419844128605394, \"epoch\": 97}, {\"train\": 0.045838957031567894, \"val\": 0.20483138948954918, \"epoch\": 98}, {\"train\": 0.04840200518568357, \"val\": 0.2678856679376353, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = 'results/transferLrn_des7_tower5_03/'\n",
    "epochs = 100\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                            epochs=epochs,\n",
    "                            saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.097898</td>\n",
       "      <td>0.875786</td>\n",
       "      <td>0.956675</td>\n",
       "      <td>8.136294e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.572653</td>\n",
       "      <td>0.575528</td>\n",
       "      <td>0.942815</td>\n",
       "      <td>-422969.078081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.199252e+06</td>\n",
       "      <td>-1.022990e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre         peakR2  maxAggR2     meanAggR2  \\\n",
       "train  0.000008  0.001856  0.097898       0.875786  0.956675  8.136294e-01   \n",
       "test   0.572653  0.575528  0.942815 -422969.078081  0.000000 -2.199252e+06   \n",
       "\n",
       "           minAggR2  \n",
       "train  0.000000e+00  \n",
       "test  -1.022990e+07  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from data/tower1.0/conmech/design_5_N_200/\n",
      "loaded train set of size 180\n",
      "epoch: 0   trainLoss: 1.0240e+00   valLoss:9.1496e-01  time: 1.56e+00\n",
      "epoch: 1   trainLoss: 7.8112e-01   valLoss:8.9729e-01  time: 1.53e+00\n",
      "epoch: 2   trainLoss: 5.8695e-01   valLoss:8.7684e-01  time: 1.51e+00\n",
      "epoch: 3   trainLoss: 4.2135e-01   valLoss:8.7036e-01  time: 1.52e+00\n",
      "epoch: 4   trainLoss: 2.9587e-01   valLoss:8.5430e-01  time: 1.49e+00\n",
      "epoch: 5   trainLoss: 2.1318e-01   valLoss:8.3181e-01  time: 1.51e+00\n",
      "epoch: 6   trainLoss: 1.4222e-01   valLoss:7.9850e-01  time: 1.51e+00\n",
      "epoch: 7   trainLoss: 1.0168e-01   valLoss:7.6207e-01  time: 1.51e+00\n",
      "epoch: 8   trainLoss: 8.3090e-02   valLoss:7.2234e-01  time: 1.55e+00\n",
      "epoch: 9   trainLoss: 8.1100e-02   valLoss:6.7390e-01  time: 1.49e+00\n",
      "epoch: 10   trainLoss: 8.3343e-02   valLoss:6.2379e-01  time: 1.49e+00\n",
      "epoch: 11   trainLoss: 8.2423e-02   valLoss:5.5289e-01  time: 1.50e+00\n",
      "epoch: 12   trainLoss: 7.6954e-02   valLoss:4.7632e-01  time: 1.50e+00\n",
      "epoch: 13   trainLoss: 6.8568e-02   valLoss:4.1353e-01  time: 1.51e+00\n",
      "epoch: 14   trainLoss: 5.9230e-02   valLoss:3.7041e-01  time: 1.50e+00\n",
      "epoch: 15   trainLoss: 5.1604e-02   valLoss:3.3773e-01  time: 1.50e+00\n",
      "epoch: 16   trainLoss: 4.6674e-02   valLoss:3.0637e-01  time: 1.50e+00\n",
      "epoch: 17   trainLoss: 4.3630e-02   valLoss:2.7222e-01  time: 1.50e+00\n",
      "epoch: 18   trainLoss: 4.1109e-02   valLoss:2.3653e-01  time: 1.50e+00\n",
      "epoch: 19   trainLoss: 3.8276e-02   valLoss:2.0150e-01  time: 1.50e+00\n",
      "epoch: 20   trainLoss: 3.5202e-02   valLoss:1.6107e-01  time: 1.51e+00\n",
      "epoch: 21   trainLoss: 3.2532e-02   valLoss:1.2110e-01  time: 1.53e+00\n",
      "epoch: 22   trainLoss: 3.0744e-02   valLoss:8.9340e-02  time: 1.54e+00\n",
      "epoch: 23   trainLoss: 2.9689e-02   valLoss:6.5541e-02  time: 1.49e+00\n",
      "epoch: 24   trainLoss: 2.9030e-02   valLoss:5.6426e-02  time: 1.51e+00\n",
      "epoch: 25   trainLoss: 2.8605e-02   valLoss:5.6773e-02  time: 1.57e+00\n",
      "epoch: 26   trainLoss: 2.8178e-02   valLoss:6.0331e-02  time: 1.53e+00\n",
      "epoch: 27   trainLoss: 2.7354e-02   valLoss:6.4552e-02  time: 1.53e+00\n",
      "epoch: 28   trainLoss: 2.6387e-02   valLoss:6.7158e-02  time: 1.55e+00\n",
      "epoch: 29   trainLoss: 2.5511e-02   valLoss:6.8367e-02  time: 1.54e+00\n",
      "epoch: 30   trainLoss: 2.4705e-02   valLoss:6.8054e-02  time: 1.52e+00\n",
      "epoch: 31   trainLoss: 2.4021e-02   valLoss:6.6848e-02  time: 1.49e+00\n",
      "epoch: 32   trainLoss: 2.3525e-02   valLoss:6.5445e-02  time: 1.50e+00\n",
      "epoch: 33   trainLoss: 2.3059e-02   valLoss:6.4109e-02  time: 1.49e+00\n",
      "epoch: 34   trainLoss: 2.2413e-02   valLoss:6.2157e-02  time: 1.52e+00\n",
      "epoch: 35   trainLoss: 2.1651e-02   valLoss:5.9474e-02  time: 1.54e+00\n",
      "epoch: 36   trainLoss: 2.0857e-02   valLoss:5.6955e-02  time: 1.54e+00\n",
      "epoch: 37   trainLoss: 2.0029e-02   valLoss:5.4122e-02  time: 1.55e+00\n",
      "epoch: 38   trainLoss: 1.9283e-02   valLoss:5.0046e-02  time: 1.54e+00\n",
      "epoch: 39   trainLoss: 1.8593e-02   valLoss:4.7529e-02  time: 1.50e+00\n",
      "epoch: 40   trainLoss: 1.8098e-02   valLoss:4.4990e-02  time: 1.49e+00\n",
      "epoch: 41   trainLoss: 1.7950e-02   valLoss:4.6071e-02  time: 1.49e+00\n",
      "epoch: 42   trainLoss: 1.8151e-02   valLoss:4.2377e-02  time: 1.49e+00\n",
      "epoch: 43   trainLoss: 1.6717e-02   valLoss:4.3680e-02  time: 1.50e+00\n",
      "epoch: 44   trainLoss: 1.6118e-02   valLoss:4.7025e-02  time: 1.50e+00\n",
      "epoch: 45   trainLoss: 1.6162e-02   valLoss:4.2609e-02  time: 1.49e+00\n",
      "epoch: 46   trainLoss: 1.5355e-02   valLoss:4.3541e-02  time: 1.49e+00\n",
      "epoch: 47   trainLoss: 1.5209e-02   valLoss:4.9229e-02  time: 1.49e+00\n",
      "epoch: 48   trainLoss: 1.4827e-02   valLoss:4.7196e-02  time: 1.50e+00\n",
      "epoch: 49   trainLoss: 1.4557e-02   valLoss:4.1950e-02  time: 1.50e+00\n",
      "epoch: 50   trainLoss: 1.4316e-02   valLoss:4.2514e-02  time: 1.49e+00\n",
      "epoch: 51   trainLoss: 1.3683e-02   valLoss:4.3969e-02  time: 1.49e+00\n",
      "epoch: 52   trainLoss: 1.3455e-02   valLoss:4.0927e-02  time: 1.50e+00\n",
      "epoch: 53   trainLoss: 1.2511e-02   valLoss:3.9606e-02  time: 1.50e+00\n",
      "epoch: 54   trainLoss: 1.1774e-02   valLoss:4.0951e-02  time: 1.49e+00\n",
      "epoch: 55   trainLoss: 1.0516e-02   valLoss:4.1773e-02  time: 1.53e+00\n",
      "epoch: 56   trainLoss: 9.0064e-03   valLoss:4.0032e-02  time: 1.49e+00\n",
      "epoch: 57   trainLoss: 7.3315e-03   valLoss:3.8057e-02  time: 1.53e+00\n",
      "epoch: 58   trainLoss: 5.4958e-03   valLoss:3.6235e-02  time: 1.54e+00\n",
      "epoch: 59   trainLoss: 3.9399e-03   valLoss:3.1508e-02  time: 1.51e+00\n",
      "epoch: 60   trainLoss: 2.8853e-03   valLoss:2.7371e-02  time: 1.49e+00\n",
      "epoch: 61   trainLoss: 2.7597e-03   valLoss:2.4406e-02  time: 1.50e+00\n",
      "epoch: 62   trainLoss: 2.6787e-03   valLoss:2.1332e-02  time: 1.51e+00\n",
      "epoch: 63   trainLoss: 2.3202e-03   valLoss:1.9757e-02  time: 1.54e+00\n",
      "epoch: 64   trainLoss: 1.9660e-03   valLoss:2.1252e-02  time: 1.50e+00\n",
      "epoch: 65   trainLoss: 1.7128e-03   valLoss:2.1510e-02  time: 1.48e+00\n",
      "epoch: 66   trainLoss: 1.6093e-03   valLoss:1.9899e-02  time: 1.50e+00\n",
      "epoch: 67   trainLoss: 1.5988e-03   valLoss:2.1887e-02  time: 1.48e+00\n",
      "epoch: 68   trainLoss: 1.4780e-03   valLoss:2.2355e-02  time: 1.49e+00\n",
      "epoch: 69   trainLoss: 1.3931e-03   valLoss:2.1140e-02  time: 1.49e+00\n",
      "epoch: 70   trainLoss: 1.2655e-03   valLoss:2.2964e-02  time: 1.51e+00\n",
      "epoch: 71   trainLoss: 1.1479e-03   valLoss:2.2129e-02  time: 1.50e+00\n",
      "epoch: 72   trainLoss: 1.0552e-03   valLoss:2.1638e-02  time: 1.50e+00\n",
      "epoch: 73   trainLoss: 9.7197e-04   valLoss:2.2956e-02  time: 1.48e+00\n",
      "epoch: 74   trainLoss: 8.4754e-04   valLoss:2.4952e-02  time: 1.54e+00\n",
      "epoch: 75   trainLoss: 7.9693e-04   valLoss:2.5323e-02  time: 1.54e+00\n",
      "epoch: 76   trainLoss: 7.7544e-04   valLoss:2.5711e-02  time: 1.52e+00\n",
      "epoch: 77   trainLoss: 7.7687e-04   valLoss:2.8168e-02  time: 1.52e+00\n",
      "epoch: 78   trainLoss: 7.6371e-04   valLoss:2.5182e-02  time: 1.55e+00\n",
      "epoch: 79   trainLoss: 7.9200e-04   valLoss:3.0858e-02  time: 1.56e+00\n",
      "epoch: 80   trainLoss: 9.0679e-04   valLoss:2.3773e-02  time: 1.57e+00\n",
      "epoch: 81   trainLoss: 1.1090e-03   valLoss:3.5269e-02  time: 1.57e+00\n",
      "epoch: 82   trainLoss: 1.5099e-03   valLoss:2.6166e-02  time: 1.60e+00\n",
      "epoch: 83   trainLoss: 1.6566e-03   valLoss:3.4847e-02  time: 1.61e+00\n",
      "epoch: 84   trainLoss: 1.5872e-03   valLoss:3.1922e-02  time: 1.56e+00\n",
      "epoch: 85   trainLoss: 1.1488e-03   valLoss:2.9652e-02  time: 1.54e+00\n",
      "epoch: 86   trainLoss: 9.0694e-04   valLoss:3.8076e-02  time: 1.55e+00\n",
      "epoch: 87   trainLoss: 9.5133e-04   valLoss:2.8755e-02  time: 1.49e+00\n",
      "epoch: 88   trainLoss: 8.1089e-04   valLoss:3.1117e-02  time: 1.50e+00\n",
      "epoch: 89   trainLoss: 5.9637e-04   valLoss:3.5486e-02  time: 1.57e+00\n",
      "epoch: 90   trainLoss: 7.8954e-04   valLoss:2.6440e-02  time: 1.60e+00\n",
      "epoch: 91   trainLoss: 7.1241e-04   valLoss:3.0907e-02  time: 1.56e+00\n",
      "epoch: 92   trainLoss: 4.1282e-04   valLoss:3.1879e-02  time: 1.56e+00\n",
      "epoch: 93   trainLoss: 4.9782e-04   valLoss:2.5963e-02  time: 1.59e+00\n",
      "epoch: 94   trainLoss: 6.9370e-04   valLoss:3.1484e-02  time: 1.56e+00\n",
      "epoch: 95   trainLoss: 4.7791e-04   valLoss:3.0256e-02  time: 1.56e+00\n",
      "epoch: 96   trainLoss: 4.1899e-04   valLoss:2.9173e-02  time: 1.59e+00\n",
      "epoch: 97   trainLoss: 5.7237e-04   valLoss:3.2345e-02  time: 1.57e+00\n",
      "epoch: 98   trainLoss: 6.1875e-04   valLoss:3.0714e-02  time: 1.58e+00\n",
      "epoch: 99   trainLoss: 5.5494e-04   valLoss:2.8059e-02  time: 1.58e+00\n",
      "loading checkpoint 63\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2247e+00   valLoss:1.0693e+02  time: 1.62e+00\n",
      "epoch: 1   trainLoss: 3.5071e-01   valLoss:1.0185e+01  time: 1.60e+00\n",
      "epoch: 2   trainLoss: 1.9529e-01   valLoss:4.4538e+00  time: 1.57e+00\n",
      "epoch: 3   trainLoss: 1.2203e-01   valLoss:3.0110e+00  time: 1.57e+00\n",
      "epoch: 4   trainLoss: 9.5303e-02   valLoss:2.0233e+00  time: 1.53e+00\n",
      "epoch: 5   trainLoss: 9.2004e-02   valLoss:1.5448e+00  time: 1.56e+00\n",
      "epoch: 6   trainLoss: 7.8672e-02   valLoss:8.8589e-01  time: 1.55e+00\n",
      "epoch: 7   trainLoss: 7.8146e-02   valLoss:6.1171e-01  time: 1.56e+00\n",
      "epoch: 8   trainLoss: 7.3755e-02   valLoss:4.6337e-01  time: 1.53e+00\n",
      "epoch: 9   trainLoss: 6.8979e-02   valLoss:3.0593e-01  time: 1.55e+00\n",
      "epoch: 10   trainLoss: 6.3103e-02   valLoss:1.9772e-01  time: 1.56e+00\n",
      "epoch: 11   trainLoss: 5.8593e-02   valLoss:1.3437e-01  time: 1.57e+00\n",
      "epoch: 12   trainLoss: 5.6441e-02   valLoss:1.0254e-01  time: 1.55e+00\n",
      "epoch: 13   trainLoss: 5.4270e-02   valLoss:8.8132e-02  time: 1.58e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14   trainLoss: 5.2391e-02   valLoss:7.9953e-02  time: 1.54e+00\n",
      "epoch: 15   trainLoss: 5.0502e-02   valLoss:7.4834e-02  time: 1.54e+00\n",
      "epoch: 16   trainLoss: 4.8075e-02   valLoss:7.1996e-02  time: 1.57e+00\n",
      "epoch: 17   trainLoss: 4.5794e-02   valLoss:6.9901e-02  time: 1.56e+00\n",
      "epoch: 18   trainLoss: 4.3628e-02   valLoss:6.8286e-02  time: 1.55e+00\n",
      "epoch: 19   trainLoss: 4.1384e-02   valLoss:6.6071e-02  time: 1.59e+00\n",
      "epoch: 20   trainLoss: 3.9340e-02   valLoss:6.3342e-02  time: 1.55e+00\n",
      "epoch: 21   trainLoss: 3.7435e-02   valLoss:6.0276e-02  time: 1.56e+00\n",
      "epoch: 22   trainLoss: 3.6229e-02   valLoss:5.6640e-02  time: 1.58e+00\n",
      "epoch: 23   trainLoss: 3.5315e-02   valLoss:5.3432e-02  time: 1.55e+00\n",
      "epoch: 24   trainLoss: 3.3913e-02   valLoss:5.1662e-02  time: 1.55e+00\n",
      "epoch: 25   trainLoss: 3.2514e-02   valLoss:5.0354e-02  time: 1.55e+00\n",
      "epoch: 26   trainLoss: 3.1099e-02   valLoss:4.8795e-02  time: 1.54e+00\n",
      "epoch: 27   trainLoss: 2.9638e-02   valLoss:4.8328e-02  time: 1.55e+00\n",
      "epoch: 28   trainLoss: 2.8406e-02   valLoss:4.7911e-02  time: 1.56e+00\n",
      "epoch: 29   trainLoss: 2.7500e-02   valLoss:4.7546e-02  time: 1.57e+00\n",
      "epoch: 30   trainLoss: 2.6453e-02   valLoss:4.8434e-02  time: 1.53e+00\n",
      "epoch: 31   trainLoss: 2.5711e-02   valLoss:4.9458e-02  time: 1.53e+00\n",
      "epoch: 32   trainLoss: 2.4965e-02   valLoss:4.9988e-02  time: 1.58e+00\n",
      "epoch: 33   trainLoss: 2.4227e-02   valLoss:4.9724e-02  time: 1.52e+00\n",
      "epoch: 34   trainLoss: 2.3462e-02   valLoss:5.0106e-02  time: 1.52e+00\n",
      "epoch: 35   trainLoss: 2.2384e-02   valLoss:4.9452e-02  time: 1.52e+00\n",
      "epoch: 36   trainLoss: 2.1134e-02   valLoss:4.9109e-02  time: 1.51e+00\n",
      "epoch: 37   trainLoss: 1.9811e-02   valLoss:4.9582e-02  time: 1.52e+00\n",
      "epoch: 38   trainLoss: 1.8569e-02   valLoss:4.8187e-02  time: 1.51e+00\n",
      "epoch: 39   trainLoss: 1.7236e-02   valLoss:4.3923e-02  time: 1.54e+00\n",
      "epoch: 40   trainLoss: 1.5809e-02   valLoss:4.0006e-02  time: 1.51e+00\n",
      "epoch: 41   trainLoss: 1.4671e-02   valLoss:3.7258e-02  time: 1.53e+00\n",
      "epoch: 42   trainLoss: 1.7227e-02   valLoss:3.8852e-02  time: 1.53e+00\n",
      "epoch: 43   trainLoss: 2.2275e-02   valLoss:2.8067e-02  time: 1.54e+00\n",
      "epoch: 44   trainLoss: 1.5010e-02   valLoss:2.3662e-02  time: 1.53e+00\n",
      "epoch: 45   trainLoss: 1.2086e-02   valLoss:2.3204e-02  time: 1.52e+00\n",
      "epoch: 46   trainLoss: 1.1386e-02   valLoss:3.1658e-02  time: 1.50e+00\n",
      "epoch: 47   trainLoss: 9.8628e-03   valLoss:4.5624e-02  time: 1.50e+00\n",
      "epoch: 48   trainLoss: 9.4834e-03   valLoss:6.6555e-02  time: 1.51e+00\n",
      "epoch: 49   trainLoss: 6.7490e-03   valLoss:9.5787e-02  time: 1.56e+00\n",
      "epoch: 50   trainLoss: 7.0164e-03   valLoss:1.0561e-01  time: 1.50e+00\n",
      "epoch: 51   trainLoss: 5.5413e-03   valLoss:9.4202e-02  time: 1.52e+00\n",
      "epoch: 52   trainLoss: 4.5018e-03   valLoss:9.7841e-02  time: 1.55e+00\n",
      "epoch: 53   trainLoss: 3.9216e-03   valLoss:1.1541e-01  time: 1.46e+00\n",
      "epoch: 54   trainLoss: 3.3380e-03   valLoss:1.1072e-01  time: 1.47e+00\n",
      "epoch: 55   trainLoss: 3.5724e-03   valLoss:9.5270e-02  time: 1.47e+00\n",
      "epoch: 56   trainLoss: 2.6101e-03   valLoss:8.5608e-02  time: 1.45e+00\n",
      "epoch: 57   trainLoss: 3.1377e-03   valLoss:7.6602e-02  time: 1.45e+00\n",
      "epoch: 58   trainLoss: 2.5342e-03   valLoss:7.7144e-02  time: 1.47e+00\n",
      "epoch: 59   trainLoss: 2.3548e-03   valLoss:5.8797e-02  time: 1.46e+00\n",
      "epoch: 60   trainLoss: 2.3326e-03   valLoss:3.5467e-02  time: 1.44e+00\n",
      "epoch: 61   trainLoss: 2.1287e-03   valLoss:2.9123e-02  time: 1.47e+00\n",
      "epoch: 62   trainLoss: 1.8759e-03   valLoss:1.6992e-02  time: 1.46e+00\n",
      "epoch: 63   trainLoss: 1.7965e-03   valLoss:1.5097e-02  time: 1.47e+00\n",
      "epoch: 64   trainLoss: 1.6326e-03   valLoss:2.0087e-02  time: 1.47e+00\n",
      "epoch: 65   trainLoss: 1.4813e-03   valLoss:2.0966e-02  time: 1.46e+00\n",
      "epoch: 66   trainLoss: 1.5852e-03   valLoss:2.7505e-02  time: 1.46e+00\n",
      "epoch: 67   trainLoss: 1.6259e-03   valLoss:1.8560e-02  time: 1.45e+00\n",
      "epoch: 68   trainLoss: 1.6091e-03   valLoss:2.2707e-02  time: 1.47e+00\n",
      "epoch: 69   trainLoss: 1.6162e-03   valLoss:1.2771e-02  time: 1.47e+00\n",
      "epoch: 70   trainLoss: 1.6578e-03   valLoss:2.0177e-02  time: 1.49e+00\n",
      "epoch: 71   trainLoss: 1.6322e-03   valLoss:1.4138e-02  time: 1.47e+00\n",
      "epoch: 72   trainLoss: 1.4938e-03   valLoss:2.5058e-02  time: 1.46e+00\n",
      "epoch: 73   trainLoss: 1.3253e-03   valLoss:1.9878e-02  time: 1.48e+00\n",
      "epoch: 74   trainLoss: 1.0976e-03   valLoss:2.3280e-02  time: 1.46e+00\n",
      "epoch: 75   trainLoss: 8.5252e-04   valLoss:1.9515e-02  time: 1.47e+00\n",
      "epoch: 76   trainLoss: 7.9232e-04   valLoss:1.4580e-02  time: 1.48e+00\n",
      "epoch: 77   trainLoss: 8.4288e-04   valLoss:1.8653e-02  time: 1.46e+00\n",
      "epoch: 78   trainLoss: 8.8093e-04   valLoss:1.2126e-02  time: 1.46e+00\n",
      "epoch: 79   trainLoss: 9.2837e-04   valLoss:1.4839e-02  time: 1.46e+00\n",
      "epoch: 80   trainLoss: 8.9871e-04   valLoss:8.0381e-03  time: 1.48e+00\n",
      "epoch: 81   trainLoss: 7.9751e-04   valLoss:8.2870e-03  time: 1.47e+00\n",
      "epoch: 82   trainLoss: 6.9489e-04   valLoss:4.4560e-03  time: 1.46e+00\n",
      "epoch: 83   trainLoss: 6.2260e-04   valLoss:4.6039e-03  time: 1.47e+00\n",
      "epoch: 84   trainLoss: 5.6546e-04   valLoss:4.3707e-03  time: 1.48e+00\n",
      "epoch: 85   trainLoss: 5.3251e-04   valLoss:3.6771e-03  time: 1.47e+00\n",
      "epoch: 86   trainLoss: 5.5786e-04   valLoss:5.6557e-03  time: 1.47e+00\n",
      "epoch: 87   trainLoss: 6.0130e-04   valLoss:3.6345e-03  time: 1.49e+00\n",
      "epoch: 88   trainLoss: 6.6057e-04   valLoss:7.0223e-03  time: 1.48e+00\n",
      "epoch: 89   trainLoss: 7.7600e-04   valLoss:3.4863e-03  time: 1.47e+00\n",
      "epoch: 90   trainLoss: 9.6362e-04   valLoss:9.2183e-03  time: 1.48e+00\n",
      "epoch: 91   trainLoss: 1.2198e-03   valLoss:3.6530e-03  time: 1.46e+00\n",
      "epoch: 92   trainLoss: 1.5560e-03   valLoss:1.1531e-02  time: 1.48e+00\n",
      "epoch: 93   trainLoss: 1.6590e-03   valLoss:3.5998e-03  time: 1.47e+00\n",
      "epoch: 94   trainLoss: 1.3403e-03   valLoss:8.3081e-03  time: 1.47e+00\n",
      "epoch: 95   trainLoss: 6.7773e-04   valLoss:5.3220e-03  time: 1.46e+00\n",
      "epoch: 96   trainLoss: 5.0976e-04   valLoss:3.0833e-03  time: 1.46e+00\n",
      "epoch: 97   trainLoss: 8.0628e-04   valLoss:7.0605e-03  time: 1.47e+00\n",
      "epoch: 98   trainLoss: 8.3860e-04   valLoss:2.3681e-03  time: 1.47e+00\n",
      "epoch: 99   trainLoss: 6.0153e-04   valLoss:3.5415e-03  time: 1.47e+00\n",
      "loading checkpoint 98\n",
      "trained 24 random forest models in 4.60 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_500/\n",
      "loaded train set of size 445\n",
      "epoch: 0   trainLoss: 9.6592e-01   valLoss:1.0733e+00  time: 3.75e+00\n",
      "epoch: 1   trainLoss: 5.3217e-01   valLoss:1.0393e+00  time: 3.67e+00\n",
      "epoch: 2   trainLoss: 2.6302e-01   valLoss:9.6739e-01  time: 3.65e+00\n",
      "epoch: 3   trainLoss: 1.3258e-01   valLoss:8.3883e-01  time: 3.67e+00\n",
      "epoch: 4   trainLoss: 8.9744e-02   valLoss:6.4240e-01  time: 3.65e+00\n",
      "epoch: 5   trainLoss: 7.4666e-02   valLoss:4.5355e-01  time: 3.64e+00\n",
      "epoch: 6   trainLoss: 7.2519e-02   valLoss:3.9643e-01  time: 3.65e+00\n",
      "epoch: 7   trainLoss: 6.2300e-02   valLoss:3.9751e-01  time: 3.69e+00\n",
      "epoch: 8   trainLoss: 4.7510e-02   valLoss:3.7483e-01  time: 3.68e+00\n",
      "epoch: 9   trainLoss: 3.7735e-02   valLoss:3.4540e-01  time: 3.77e+00\n",
      "epoch: 10   trainLoss: 3.1082e-02   valLoss:2.3789e-01  time: 3.67e+00\n",
      "epoch: 11   trainLoss: 2.8394e-02   valLoss:1.5006e-01  time: 3.66e+00\n",
      "epoch: 12   trainLoss: 2.5752e-02   valLoss:9.8084e-02  time: 3.64e+00\n",
      "epoch: 13   trainLoss: 2.6483e-02   valLoss:6.6413e-02  time: 3.66e+00\n",
      "epoch: 14   trainLoss: 2.3519e-02   valLoss:5.1462e-02  time: 3.68e+00\n",
      "epoch: 15   trainLoss: 2.2382e-02   valLoss:4.6416e-02  time: 3.67e+00\n",
      "epoch: 16   trainLoss: 2.0354e-02   valLoss:4.8343e-02  time: 3.65e+00\n",
      "epoch: 17   trainLoss: 1.9153e-02   valLoss:5.2279e-02  time: 3.67e+00\n",
      "epoch: 18   trainLoss: 1.8148e-02   valLoss:5.3947e-02  time: 3.63e+00\n",
      "epoch: 19   trainLoss: 1.6953e-02   valLoss:6.4957e-02  time: 3.64e+00\n",
      "epoch: 20   trainLoss: 1.7128e-02   valLoss:7.3387e-02  time: 3.67e+00\n",
      "epoch: 21   trainLoss: 1.5248e-02   valLoss:6.2618e-02  time: 3.68e+00\n",
      "epoch: 22   trainLoss: 1.4634e-02   valLoss:7.5391e-02  time: 3.72e+00\n",
      "epoch: 23   trainLoss: 1.4194e-02   valLoss:7.5936e-02  time: 3.67e+00\n",
      "epoch: 24   trainLoss: 1.5142e-02   valLoss:8.5377e-02  time: 3.67e+00\n",
      "epoch: 25   trainLoss: 1.3583e-02   valLoss:8.0601e-02  time: 3.67e+00\n",
      "epoch: 26   trainLoss: 1.3196e-02   valLoss:7.7205e-02  time: 3.65e+00\n",
      "epoch: 27   trainLoss: 1.2151e-02   valLoss:9.0707e-02  time: 3.66e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28   trainLoss: 1.2478e-02   valLoss:8.2146e-02  time: 3.64e+00\n",
      "epoch: 29   trainLoss: 1.1252e-02   valLoss:7.8661e-02  time: 3.67e+00\n",
      "epoch: 30   trainLoss: 1.0426e-02   valLoss:7.7735e-02  time: 3.64e+00\n",
      "epoch: 31   trainLoss: 9.9164e-03   valLoss:6.9563e-02  time: 3.62e+00\n",
      "epoch: 32   trainLoss: 8.9558e-03   valLoss:7.1810e-02  time: 3.63e+00\n",
      "epoch: 33   trainLoss: 9.4057e-03   valLoss:6.3879e-02  time: 3.61e+00\n",
      "epoch: 34   trainLoss: 7.9179e-03   valLoss:5.5095e-02  time: 3.64e+00\n",
      "epoch: 35   trainLoss: 6.9451e-03   valLoss:5.7783e-02  time: 3.76e+00\n",
      "epoch: 36   trainLoss: 5.8971e-03   valLoss:5.2323e-02  time: 3.65e+00\n",
      "epoch: 37   trainLoss: 5.0184e-03   valLoss:5.4163e-02  time: 3.62e+00\n",
      "epoch: 38   trainLoss: 3.4968e-03   valLoss:5.0693e-02  time: 3.61e+00\n",
      "epoch: 39   trainLoss: 3.2444e-03   valLoss:4.2988e-02  time: 3.61e+00\n",
      "epoch: 40   trainLoss: 3.0880e-03   valLoss:4.5141e-02  time: 3.65e+00\n",
      "epoch: 41   trainLoss: 2.4810e-03   valLoss:3.4646e-02  time: 3.69e+00\n",
      "epoch: 42   trainLoss: 1.6275e-03   valLoss:3.2121e-02  time: 3.68e+00\n",
      "epoch: 43   trainLoss: 1.6793e-03   valLoss:2.7790e-02  time: 3.65e+00\n",
      "epoch: 44   trainLoss: 1.8966e-03   valLoss:2.8109e-02  time: 3.65e+00\n",
      "epoch: 45   trainLoss: 1.3762e-03   valLoss:2.4754e-02  time: 3.68e+00\n",
      "epoch: 46   trainLoss: 1.0681e-03   valLoss:2.3922e-02  time: 3.68e+00\n",
      "epoch: 47   trainLoss: 9.2542e-04   valLoss:2.7840e-02  time: 3.66e+00\n",
      "epoch: 48   trainLoss: 7.0514e-04   valLoss:2.9673e-02  time: 3.71e+00\n",
      "epoch: 49   trainLoss: 6.7495e-04   valLoss:3.2736e-02  time: 3.55e+00\n",
      "epoch: 50   trainLoss: 1.2880e-03   valLoss:3.6348e-02  time: 3.54e+00\n",
      "epoch: 51   trainLoss: 2.2211e-03   valLoss:3.8114e-02  time: 3.54e+00\n",
      "epoch: 52   trainLoss: 1.8624e-03   valLoss:3.3286e-02  time: 3.65e+00\n",
      "epoch: 53   trainLoss: 2.1892e-03   valLoss:3.1388e-02  time: 3.66e+00\n",
      "epoch: 54   trainLoss: 2.3705e-03   valLoss:3.0749e-02  time: 3.65e+00\n",
      "epoch: 55   trainLoss: 3.8178e-03   valLoss:2.7608e-02  time: 3.64e+00\n",
      "epoch: 56   trainLoss: 1.7003e-03   valLoss:2.4943e-02  time: 3.73e+00\n",
      "epoch: 57   trainLoss: 1.6240e-03   valLoss:2.7485e-02  time: 3.69e+00\n",
      "epoch: 58   trainLoss: 1.7358e-03   valLoss:2.4979e-02  time: 3.63e+00\n",
      "epoch: 59   trainLoss: 8.3185e-04   valLoss:2.3475e-02  time: 3.63e+00\n",
      "epoch: 60   trainLoss: 8.9901e-04   valLoss:2.6671e-02  time: 3.63e+00\n",
      "epoch: 61   trainLoss: 1.1519e-03   valLoss:2.1650e-02  time: 3.61e+00\n",
      "epoch: 62   trainLoss: 7.2601e-04   valLoss:2.3745e-02  time: 3.70e+00\n",
      "epoch: 63   trainLoss: 2.0842e-03   valLoss:2.4394e-02  time: 3.68e+00\n",
      "epoch: 64   trainLoss: 1.4371e-03   valLoss:2.2237e-02  time: 3.67e+00\n",
      "epoch: 65   trainLoss: 1.5799e-03   valLoss:2.3568e-02  time: 3.67e+00\n",
      "epoch: 66   trainLoss: 2.3633e-03   valLoss:2.4586e-02  time: 3.69e+00\n",
      "epoch: 67   trainLoss: 2.9355e-03   valLoss:2.4189e-02  time: 3.66e+00\n",
      "epoch: 68   trainLoss: 2.0917e-03   valLoss:1.7890e-02  time: 3.66e+00\n",
      "epoch: 69   trainLoss: 2.0444e-03   valLoss:2.2467e-02  time: 3.65e+00\n",
      "epoch: 70   trainLoss: 1.4431e-03   valLoss:2.4008e-02  time: 3.67e+00\n",
      "epoch: 71   trainLoss: 2.1088e-03   valLoss:1.7843e-02  time: 3.64e+00\n",
      "epoch: 72   trainLoss: 1.4062e-03   valLoss:2.0024e-02  time: 3.65e+00\n",
      "epoch: 73   trainLoss: 1.4863e-03   valLoss:1.9442e-02  time: 3.65e+00\n",
      "epoch: 74   trainLoss: 1.8645e-03   valLoss:2.1595e-02  time: 3.70e+00\n",
      "epoch: 75   trainLoss: 1.3396e-03   valLoss:2.2057e-02  time: 3.63e+00\n",
      "epoch: 76   trainLoss: 9.9678e-04   valLoss:1.6211e-02  time: 3.67e+00\n",
      "epoch: 77   trainLoss: 9.6330e-04   valLoss:2.0900e-02  time: 3.64e+00\n",
      "epoch: 78   trainLoss: 1.6113e-03   valLoss:1.8440e-02  time: 3.62e+00\n",
      "epoch: 79   trainLoss: 5.8993e-04   valLoss:2.1567e-02  time: 3.65e+00\n",
      "epoch: 80   trainLoss: 9.3362e-04   valLoss:1.8463e-02  time: 3.66e+00\n",
      "epoch: 81   trainLoss: 8.1230e-04   valLoss:1.9929e-02  time: 3.68e+00\n",
      "epoch: 82   trainLoss: 9.8921e-04   valLoss:1.4972e-02  time: 3.66e+00\n",
      "epoch: 83   trainLoss: 1.3204e-03   valLoss:1.6320e-02  time: 3.64e+00\n",
      "epoch: 84   trainLoss: 1.1962e-03   valLoss:1.1859e-02  time: 3.64e+00\n",
      "epoch: 85   trainLoss: 8.6063e-04   valLoss:1.3823e-02  time: 3.64e+00\n",
      "epoch: 86   trainLoss: 7.6834e-04   valLoss:1.3296e-02  time: 3.66e+00\n",
      "epoch: 87   trainLoss: 1.2983e-03   valLoss:1.4460e-02  time: 3.77e+00\n",
      "epoch: 88   trainLoss: 9.9579e-04   valLoss:1.6321e-02  time: 3.66e+00\n",
      "epoch: 89   trainLoss: 1.0364e-03   valLoss:1.5466e-02  time: 3.64e+00\n",
      "epoch: 90   trainLoss: 1.0823e-03   valLoss:1.8230e-02  time: 3.68e+00\n",
      "epoch: 91   trainLoss: 9.8973e-04   valLoss:1.7874e-02  time: 3.67e+00\n",
      "epoch: 92   trainLoss: 1.0212e-03   valLoss:1.4803e-02  time: 3.62e+00\n",
      "epoch: 93   trainLoss: 9.8238e-04   valLoss:1.5591e-02  time: 3.56e+00\n",
      "epoch: 94   trainLoss: 1.2739e-03   valLoss:1.1340e-02  time: 3.55e+00\n",
      "epoch: 95   trainLoss: 1.6288e-03   valLoss:1.7332e-02  time: 3.55e+00\n",
      "epoch: 96   trainLoss: 1.5747e-03   valLoss:1.3221e-02  time: 3.58e+00\n",
      "epoch: 97   trainLoss: 2.0784e-03   valLoss:1.1095e-02  time: 3.56e+00\n",
      "epoch: 98   trainLoss: 3.2092e-03   valLoss:2.4125e-02  time: 3.54e+00\n",
      "epoch: 99   trainLoss: 3.2254e-03   valLoss:1.0248e-02  time: 3.65e+00\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 8.0206e-01   valLoss:1.7126e+01  time: 3.73e+00\n",
      "epoch: 1   trainLoss: 1.5934e-01   valLoss:3.0236e+00  time: 3.67e+00\n",
      "epoch: 2   trainLoss: 9.2492e-02   valLoss:1.9233e+00  time: 3.68e+00\n",
      "epoch: 3   trainLoss: 8.1499e-02   valLoss:7.0472e-01  time: 3.62e+00\n",
      "epoch: 4   trainLoss: 7.1715e-02   valLoss:4.6311e-01  time: 3.60e+00\n",
      "epoch: 5   trainLoss: 6.5138e-02   valLoss:1.7604e-01  time: 3.64e+00\n",
      "epoch: 6   trainLoss: 5.7701e-02   valLoss:1.0016e-01  time: 3.63e+00\n",
      "epoch: 7   trainLoss: 5.2194e-02   valLoss:7.7639e-02  time: 3.61e+00\n",
      "epoch: 8   trainLoss: 4.8644e-02   valLoss:6.7167e-02  time: 3.65e+00\n",
      "epoch: 9   trainLoss: 4.4788e-02   valLoss:5.7846e-02  time: 3.66e+00\n",
      "epoch: 10   trainLoss: 4.1956e-02   valLoss:5.0259e-02  time: 3.76e+00\n",
      "epoch: 11   trainLoss: 3.9903e-02   valLoss:4.4881e-02  time: 3.66e+00\n",
      "epoch: 12   trainLoss: 3.8144e-02   valLoss:4.2344e-02  time: 3.69e+00\n",
      "epoch: 13   trainLoss: 3.6378e-02   valLoss:3.9798e-02  time: 3.66e+00\n",
      "epoch: 14   trainLoss: 3.4378e-02   valLoss:3.8882e-02  time: 3.65e+00\n",
      "epoch: 15   trainLoss: 3.2100e-02   valLoss:3.7143e-02  time: 3.66e+00\n",
      "epoch: 16   trainLoss: 3.0824e-02   valLoss:3.8315e-02  time: 3.71e+00\n",
      "epoch: 17   trainLoss: 2.9638e-02   valLoss:3.8560e-02  time: 3.64e+00\n",
      "epoch: 18   trainLoss: 2.8677e-02   valLoss:4.1595e-02  time: 3.62e+00\n",
      "epoch: 19   trainLoss: 2.8114e-02   valLoss:4.0194e-02  time: 3.59e+00\n",
      "epoch: 20   trainLoss: 2.9036e-02   valLoss:3.9886e-02  time: 3.65e+00\n",
      "epoch: 21   trainLoss: 2.6950e-02   valLoss:3.7414e-02  time: 3.66e+00\n",
      "epoch: 22   trainLoss: 2.5757e-02   valLoss:3.5386e-02  time: 3.65e+00\n",
      "epoch: 23   trainLoss: 2.5795e-02   valLoss:4.0716e-02  time: 3.76e+00\n",
      "epoch: 24   trainLoss: 2.3526e-02   valLoss:4.2626e-02  time: 3.72e+00\n",
      "epoch: 25   trainLoss: 2.2211e-02   valLoss:3.6054e-02  time: 3.69e+00\n",
      "epoch: 26   trainLoss: 2.1085e-02   valLoss:3.5900e-02  time: 3.64e+00\n",
      "epoch: 27   trainLoss: 1.9232e-02   valLoss:3.7069e-02  time: 3.61e+00\n",
      "epoch: 28   trainLoss: 1.6611e-02   valLoss:4.2597e-02  time: 3.65e+00\n",
      "epoch: 29   trainLoss: 1.4463e-02   valLoss:3.9224e-02  time: 3.64e+00\n",
      "epoch: 30   trainLoss: 1.1754e-02   valLoss:6.0732e-02  time: 3.65e+00\n",
      "epoch: 31   trainLoss: 1.1225e-02   valLoss:7.9573e-02  time: 3.62e+00\n",
      "epoch: 32   trainLoss: 8.4601e-03   valLoss:7.3329e-02  time: 3.64e+00\n",
      "epoch: 33   trainLoss: 8.4476e-03   valLoss:2.8062e-02  time: 3.62e+00\n",
      "epoch: 34   trainLoss: 1.3489e-02   valLoss:1.7425e-02  time: 3.62e+00\n",
      "epoch: 35   trainLoss: 1.0221e-02   valLoss:1.6938e-02  time: 3.61e+00\n",
      "epoch: 36   trainLoss: 5.4447e-03   valLoss:1.8410e-02  time: 3.72e+00\n",
      "epoch: 37   trainLoss: 4.6880e-03   valLoss:4.5897e-02  time: 3.65e+00\n",
      "epoch: 38   trainLoss: 3.8356e-03   valLoss:5.1642e-02  time: 3.63e+00\n",
      "epoch: 39   trainLoss: 3.9671e-03   valLoss:3.2187e-02  time: 3.64e+00\n",
      "epoch: 40   trainLoss: 2.6910e-03   valLoss:1.3675e-02  time: 3.62e+00\n",
      "epoch: 41   trainLoss: 2.1038e-03   valLoss:7.3595e-03  time: 3.65e+00\n",
      "epoch: 42   trainLoss: 2.6047e-03   valLoss:5.5529e-03  time: 3.64e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43   trainLoss: 2.3589e-03   valLoss:4.9308e-03  time: 3.63e+00\n",
      "epoch: 44   trainLoss: 1.9721e-03   valLoss:8.6349e-03  time: 3.65e+00\n",
      "epoch: 45   trainLoss: 1.6499e-03   valLoss:1.6424e-02  time: 3.64e+00\n",
      "epoch: 46   trainLoss: 1.6369e-03   valLoss:1.3295e-02  time: 3.66e+00\n",
      "epoch: 47   trainLoss: 1.4592e-03   valLoss:8.2018e-03  time: 3.65e+00\n",
      "epoch: 48   trainLoss: 9.6504e-04   valLoss:5.8632e-03  time: 3.60e+00\n",
      "epoch: 49   trainLoss: 1.2173e-03   valLoss:2.2219e-03  time: 3.71e+00\n",
      "epoch: 50   trainLoss: 1.2942e-03   valLoss:7.3752e-03  time: 3.63e+00\n",
      "epoch: 51   trainLoss: 2.0161e-03   valLoss:2.4313e-03  time: 3.62e+00\n",
      "epoch: 52   trainLoss: 1.9502e-03   valLoss:3.6176e-03  time: 3.64e+00\n",
      "epoch: 53   trainLoss: 2.2178e-03   valLoss:3.3448e-03  time: 3.63e+00\n",
      "epoch: 54   trainLoss: 9.8526e-04   valLoss:3.2538e-03  time: 3.61e+00\n",
      "epoch: 55   trainLoss: 8.5776e-04   valLoss:7.4027e-03  time: 3.61e+00\n",
      "epoch: 56   trainLoss: 1.9209e-03   valLoss:3.5708e-03  time: 3.61e+00\n",
      "epoch: 57   trainLoss: 2.1644e-03   valLoss:4.2246e-03  time: 3.63e+00\n",
      "epoch: 58   trainLoss: 1.4195e-03   valLoss:3.4591e-03  time: 3.63e+00\n",
      "epoch: 59   trainLoss: 1.7206e-03   valLoss:4.0632e-03  time: 3.59e+00\n",
      "epoch: 60   trainLoss: 9.8647e-04   valLoss:3.4693e-03  time: 3.60e+00\n",
      "epoch: 61   trainLoss: 9.8497e-04   valLoss:1.5574e-03  time: 3.68e+00\n",
      "epoch: 62   trainLoss: 9.3454e-04   valLoss:2.8500e-03  time: 3.71e+00\n",
      "epoch: 63   trainLoss: 7.6256e-04   valLoss:3.3970e-03  time: 3.61e+00\n",
      "epoch: 64   trainLoss: 6.0970e-04   valLoss:2.3630e-03  time: 3.60e+00\n",
      "epoch: 65   trainLoss: 7.1306e-04   valLoss:2.6250e-03  time: 3.61e+00\n",
      "epoch: 66   trainLoss: 9.8647e-04   valLoss:2.5683e-03  time: 3.62e+00\n",
      "epoch: 67   trainLoss: 1.7676e-03   valLoss:3.9716e-03  time: 3.64e+00\n",
      "epoch: 68   trainLoss: 8.8416e-04   valLoss:4.4537e-03  time: 3.62e+00\n",
      "epoch: 69   trainLoss: 1.0469e-03   valLoss:3.1520e-03  time: 3.62e+00\n",
      "epoch: 70   trainLoss: 1.2902e-03   valLoss:3.3475e-03  time: 3.69e+00\n",
      "epoch: 71   trainLoss: 1.3160e-03   valLoss:2.6248e-03  time: 3.61e+00\n",
      "epoch: 72   trainLoss: 4.8022e-04   valLoss:2.9211e-03  time: 3.56e+00\n",
      "epoch: 73   trainLoss: 9.1723e-04   valLoss:3.6405e-03  time: 3.62e+00\n",
      "epoch: 74   trainLoss: 5.1471e-04   valLoss:3.3774e-03  time: 3.67e+00\n",
      "epoch: 75   trainLoss: 6.4817e-04   valLoss:3.8837e-03  time: 3.70e+00\n",
      "epoch: 76   trainLoss: 1.3250e-03   valLoss:3.5545e-03  time: 3.62e+00\n",
      "epoch: 77   trainLoss: 1.3997e-03   valLoss:4.6077e-03  time: 3.63e+00\n",
      "epoch: 78   trainLoss: 7.2625e-04   valLoss:3.5372e-03  time: 3.62e+00\n",
      "epoch: 79   trainLoss: 1.2592e-03   valLoss:3.1219e-03  time: 3.62e+00\n",
      "epoch: 80   trainLoss: 6.6260e-04   valLoss:5.8632e-03  time: 3.62e+00\n",
      "epoch: 81   trainLoss: 1.3201e-03   valLoss:2.5544e-03  time: 3.61e+00\n",
      "epoch: 82   trainLoss: 3.6158e-04   valLoss:5.5209e-03  time: 3.65e+00\n",
      "epoch: 83   trainLoss: 9.1808e-04   valLoss:3.2638e-03  time: 3.57e+00\n",
      "epoch: 84   trainLoss: 1.7964e-03   valLoss:5.2677e-03  time: 3.59e+00\n",
      "epoch: 85   trainLoss: 1.0135e-03   valLoss:3.5103e-03  time: 3.59e+00\n",
      "epoch: 86   trainLoss: 6.1648e-04   valLoss:2.8990e-03  time: 3.57e+00\n",
      "epoch: 87   trainLoss: 1.6701e-03   valLoss:5.8191e-03  time: 3.58e+00\n",
      "epoch: 88   trainLoss: 2.5652e-03   valLoss:1.9308e-03  time: 3.69e+00\n",
      "epoch: 89   trainLoss: 2.5268e-03   valLoss:5.4182e-03  time: 3.57e+00\n",
      "epoch: 90   trainLoss: 1.2167e-03   valLoss:3.9947e-03  time: 3.58e+00\n",
      "epoch: 91   trainLoss: 2.6899e-03   valLoss:3.9726e-03  time: 3.62e+00\n",
      "epoch: 92   trainLoss: 1.8215e-03   valLoss:4.0416e-03  time: 3.58e+00\n",
      "epoch: 93   trainLoss: 1.3701e-03   valLoss:3.8395e-03  time: 3.60e+00\n",
      "epoch: 94   trainLoss: 1.1062e-03   valLoss:5.5137e-03  time: 3.60e+00\n",
      "epoch: 95   trainLoss: 9.2344e-04   valLoss:6.5385e-03  time: 3.59e+00\n",
      "epoch: 96   trainLoss: 9.4399e-04   valLoss:4.7040e-03  time: 3.59e+00\n",
      "epoch: 97   trainLoss: 9.7849e-04   valLoss:2.9493e-03  time: 3.60e+00\n",
      "epoch: 98   trainLoss: 7.9982e-04   valLoss:3.0070e-03  time: 3.62e+00\n",
      "epoch: 99   trainLoss: 9.6460e-04   valLoss:4.4040e-03  time: 3.57e+00\n",
      "loading checkpoint 61\n",
      "trained 24 random forest models in 8.44 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_10/\n",
      "loaded train set of size 9\n",
      "epoch: 0   trainLoss: 1.0651e+00   valLoss:1.0619e+00  time: 9.42e-02\n",
      "epoch: 1   trainLoss: 8.4127e-01   valLoss:1.0243e+00  time: 9.44e-02\n",
      "epoch: 2   trainLoss: 6.6333e-01   valLoss:9.8936e-01  time: 9.30e-02\n",
      "epoch: 3   trainLoss: 4.8898e-01   valLoss:9.6308e-01  time: 9.46e-02\n",
      "epoch: 4   trainLoss: 3.5163e-01   valLoss:9.1622e-01  time: 9.70e-02\n",
      "epoch: 5   trainLoss: 2.5622e-01   valLoss:8.5462e-01  time: 9.49e-02\n",
      "epoch: 6   trainLoss: 1.9384e-01   valLoss:7.8694e-01  time: 9.52e-02\n",
      "epoch: 7   trainLoss: 1.3460e-01   valLoss:7.2436e-01  time: 9.54e-02\n",
      "epoch: 8   trainLoss: 1.1729e-01   valLoss:6.6861e-01  time: 9.77e-02\n",
      "epoch: 9   trainLoss: 9.9252e-02   valLoss:6.1012e-01  time: 9.88e-02\n",
      "epoch: 10   trainLoss: 9.1415e-02   valLoss:5.4327e-01  time: 9.32e-02\n",
      "epoch: 11   trainLoss: 8.2819e-02   valLoss:4.7977e-01  time: 9.25e-02\n",
      "epoch: 12   trainLoss: 7.5073e-02   valLoss:4.2366e-01  time: 9.34e-02\n",
      "epoch: 13   trainLoss: 6.8378e-02   valLoss:3.7972e-01  time: 9.12e-02\n",
      "epoch: 14   trainLoss: 6.2148e-02   valLoss:3.3943e-01  time: 9.63e-02\n",
      "epoch: 15   trainLoss: 6.0074e-02   valLoss:2.9504e-01  time: 9.37e-02\n",
      "epoch: 16   trainLoss: 5.1553e-02   valLoss:2.4795e-01  time: 9.33e-02\n",
      "epoch: 17   trainLoss: 4.6896e-02   valLoss:2.0245e-01  time: 9.27e-02\n",
      "epoch: 18   trainLoss: 4.1721e-02   valLoss:1.7758e-01  time: 9.08e-02\n",
      "epoch: 19   trainLoss: 3.9551e-02   valLoss:1.6736e-01  time: 9.18e-02\n",
      "epoch: 20   trainLoss: 3.6727e-02   valLoss:1.6370e-01  time: 9.33e-02\n",
      "epoch: 21   trainLoss: 3.5592e-02   valLoss:1.5767e-01  time: 9.27e-02\n",
      "epoch: 22   trainLoss: 3.4570e-02   valLoss:1.6625e-01  time: 9.15e-02\n",
      "epoch: 23   trainLoss: 3.4169e-02   valLoss:1.7865e-01  time: 9.24e-02\n",
      "epoch: 24   trainLoss: 3.3773e-02   valLoss:1.8205e-01  time: 9.11e-02\n",
      "epoch: 25   trainLoss: 3.2811e-02   valLoss:1.7910e-01  time: 9.29e-02\n",
      "epoch: 26   trainLoss: 3.2306e-02   valLoss:1.7730e-01  time: 9.18e-02\n",
      "epoch: 27   trainLoss: 3.1392e-02   valLoss:1.7507e-01  time: 9.22e-02\n",
      "epoch: 28   trainLoss: 3.0356e-02   valLoss:1.5790e-01  time: 9.20e-02\n",
      "epoch: 29   trainLoss: 2.9949e-02   valLoss:1.3413e-01  time: 9.05e-02\n",
      "epoch: 30   trainLoss: 2.9317e-02   valLoss:1.2078e-01  time: 9.27e-02\n",
      "epoch: 31   trainLoss: 2.9153e-02   valLoss:1.1539e-01  time: 9.17e-02\n",
      "epoch: 32   trainLoss: 2.8540e-02   valLoss:1.1520e-01  time: 9.19e-02\n",
      "epoch: 33   trainLoss: 2.8164e-02   valLoss:1.1676e-01  time: 9.27e-02\n",
      "epoch: 34   trainLoss: 2.7584e-02   valLoss:1.1691e-01  time: 9.05e-02\n",
      "epoch: 35   trainLoss: 2.7314e-02   valLoss:1.1477e-01  time: 9.48e-02\n",
      "epoch: 36   trainLoss: 2.7014e-02   valLoss:1.1069e-01  time: 9.26e-02\n",
      "epoch: 37   trainLoss: 2.6541e-02   valLoss:1.0675e-01  time: 9.10e-02\n",
      "epoch: 38   trainLoss: 2.6035e-02   valLoss:1.0323e-01  time: 9.19e-02\n",
      "epoch: 39   trainLoss: 2.5574e-02   valLoss:1.0029e-01  time: 9.30e-02\n",
      "epoch: 40   trainLoss: 2.4857e-02   valLoss:9.7208e-02  time: 9.37e-02\n",
      "epoch: 41   trainLoss: 2.4276e-02   valLoss:9.3949e-02  time: 9.23e-02\n",
      "epoch: 42   trainLoss: 2.3673e-02   valLoss:9.1499e-02  time: 9.29e-02\n",
      "epoch: 43   trainLoss: 2.3102e-02   valLoss:8.9156e-02  time: 9.27e-02\n",
      "epoch: 44   trainLoss: 2.2735e-02   valLoss:8.6994e-02  time: 9.20e-02\n",
      "epoch: 45   trainLoss: 2.2348e-02   valLoss:8.6041e-02  time: 9.22e-02\n",
      "epoch: 46   trainLoss: 2.2091e-02   valLoss:8.6294e-02  time: 9.48e-02\n",
      "epoch: 47   trainLoss: 2.1855e-02   valLoss:8.6538e-02  time: 9.28e-02\n",
      "epoch: 48   trainLoss: 2.1538e-02   valLoss:8.6650e-02  time: 9.05e-02\n",
      "epoch: 49   trainLoss: 2.1323e-02   valLoss:8.7727e-02  time: 9.24e-02\n",
      "epoch: 50   trainLoss: 2.1551e-02   valLoss:8.6168e-02  time: 9.18e-02\n",
      "epoch: 51   trainLoss: 2.0780e-02   valLoss:8.4630e-02  time: 9.12e-02\n",
      "epoch: 52   trainLoss: 2.0820e-02   valLoss:8.3119e-02  time: 9.31e-02\n",
      "epoch: 53   trainLoss: 2.0185e-02   valLoss:7.9607e-02  time: 9.53e-02\n",
      "epoch: 54   trainLoss: 1.9962e-02   valLoss:7.5754e-02  time: 9.19e-02\n",
      "epoch: 55   trainLoss: 1.9216e-02   valLoss:7.3421e-02  time: 9.20e-02\n",
      "epoch: 56   trainLoss: 1.8851e-02   valLoss:7.2730e-02  time: 9.44e-02\n",
      "epoch: 57   trainLoss: 1.8589e-02   valLoss:7.2728e-02  time: 9.27e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58   trainLoss: 1.8328e-02   valLoss:7.1263e-02  time: 9.80e-02\n",
      "epoch: 59   trainLoss: 1.7737e-02   valLoss:6.9755e-02  time: 9.15e-02\n",
      "epoch: 60   trainLoss: 1.7322e-02   valLoss:6.9139e-02  time: 9.65e-02\n",
      "epoch: 61   trainLoss: 1.6800e-02   valLoss:6.7755e-02  time: 9.30e-02\n",
      "epoch: 62   trainLoss: 1.6463e-02   valLoss:6.5624e-02  time: 9.34e-02\n",
      "epoch: 63   trainLoss: 1.5951e-02   valLoss:6.5114e-02  time: 9.10e-02\n",
      "epoch: 64   trainLoss: 1.5407e-02   valLoss:6.5108e-02  time: 9.32e-02\n",
      "epoch: 65   trainLoss: 1.4945e-02   valLoss:6.4311e-02  time: 9.14e-02\n",
      "epoch: 66   trainLoss: 1.4491e-02   valLoss:6.3776e-02  time: 9.13e-02\n",
      "epoch: 67   trainLoss: 1.4110e-02   valLoss:6.3913e-02  time: 9.13e-02\n",
      "epoch: 68   trainLoss: 1.3774e-02   valLoss:6.3831e-02  time: 9.23e-02\n",
      "epoch: 69   trainLoss: 1.3427e-02   valLoss:6.3802e-02  time: 9.16e-02\n",
      "epoch: 70   trainLoss: 1.3029e-02   valLoss:6.3791e-02  time: 9.14e-02\n",
      "epoch: 71   trainLoss: 1.2533e-02   valLoss:6.3592e-02  time: 9.10e-02\n",
      "epoch: 72   trainLoss: 1.1977e-02   valLoss:6.3228e-02  time: 9.16e-02\n",
      "epoch: 73   trainLoss: 1.1479e-02   valLoss:6.3435e-02  time: 9.36e-02\n",
      "epoch: 74   trainLoss: 1.1290e-02   valLoss:6.2542e-02  time: 9.11e-02\n",
      "epoch: 75   trainLoss: 1.1567e-02   valLoss:6.4833e-02  time: 9.18e-02\n",
      "epoch: 76   trainLoss: 1.3971e-02   valLoss:6.3247e-02  time: 9.27e-02\n",
      "epoch: 77   trainLoss: 1.3663e-02   valLoss:5.5577e-02  time: 9.09e-02\n",
      "epoch: 78   trainLoss: 1.1887e-02   valLoss:5.3918e-02  time: 9.27e-02\n",
      "epoch: 79   trainLoss: 1.1832e-02   valLoss:6.1646e-02  time: 9.13e-02\n",
      "epoch: 80   trainLoss: 1.1282e-02   valLoss:6.4359e-02  time: 9.30e-02\n",
      "epoch: 81   trainLoss: 1.1110e-02   valLoss:6.4565e-02  time: 9.45e-02\n",
      "epoch: 82   trainLoss: 1.0762e-02   valLoss:6.5651e-02  time: 9.18e-02\n",
      "epoch: 83   trainLoss: 1.0965e-02   valLoss:6.7039e-02  time: 9.25e-02\n",
      "epoch: 84   trainLoss: 1.0347e-02   valLoss:6.5509e-02  time: 9.98e-02\n",
      "epoch: 85   trainLoss: 1.0624e-02   valLoss:6.2438e-02  time: 9.18e-02\n",
      "epoch: 86   trainLoss: 1.0120e-02   valLoss:5.8780e-02  time: 9.34e-02\n",
      "epoch: 87   trainLoss: 9.9871e-03   valLoss:5.7219e-02  time: 8.99e-02\n",
      "epoch: 88   trainLoss: 9.8755e-03   valLoss:5.7776e-02  time: 9.25e-02\n",
      "epoch: 89   trainLoss: 9.5692e-03   valLoss:5.7937e-02  time: 9.06e-02\n",
      "epoch: 90   trainLoss: 9.3921e-03   valLoss:5.6222e-02  time: 9.32e-02\n",
      "epoch: 91   trainLoss: 9.1588e-03   valLoss:5.4061e-02  time: 8.93e-02\n",
      "epoch: 92   trainLoss: 8.7683e-03   valLoss:5.1995e-02  time: 8.92e-02\n",
      "epoch: 93   trainLoss: 8.3899e-03   valLoss:5.0506e-02  time: 9.06e-02\n",
      "epoch: 94   trainLoss: 7.6621e-03   valLoss:5.0098e-02  time: 9.04e-02\n",
      "epoch: 95   trainLoss: 6.8446e-03   valLoss:4.9861e-02  time: 8.90e-02\n",
      "epoch: 96   trainLoss: 5.6744e-03   valLoss:4.8888e-02  time: 8.98e-02\n",
      "epoch: 97   trainLoss: 4.0716e-03   valLoss:4.8694e-02  time: 8.98e-02\n",
      "epoch: 98   trainLoss: 3.0853e-03   valLoss:5.1517e-02  time: 8.98e-02\n",
      "epoch: 99   trainLoss: 1.0990e-02   valLoss:5.4854e-02  time: 9.00e-02\n",
      "loading checkpoint 97\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2891e+00   valLoss:1.6703e+02  time: 9.09e-02\n",
      "epoch: 1   trainLoss: 3.7775e-01   valLoss:1.1636e+01  time: 9.22e-02\n",
      "epoch: 2   trainLoss: 1.9962e-01   valLoss:3.8830e+00  time: 9.27e-02\n",
      "epoch: 3   trainLoss: 1.6539e-01   valLoss:3.3606e+00  time: 9.19e-02\n",
      "epoch: 4   trainLoss: 9.9708e-02   valLoss:2.1562e+00  time: 9.18e-02\n",
      "epoch: 5   trainLoss: 9.0918e-02   valLoss:1.5171e+00  time: 9.22e-02\n",
      "epoch: 6   trainLoss: 9.1497e-02   valLoss:1.1248e+00  time: 9.28e-02\n",
      "epoch: 7   trainLoss: 7.7456e-02   valLoss:4.0627e-01  time: 9.18e-02\n",
      "epoch: 8   trainLoss: 7.2165e-02   valLoss:2.2884e-01  time: 9.27e-02\n",
      "epoch: 9   trainLoss: 6.7796e-02   valLoss:1.7273e-01  time: 9.16e-02\n",
      "epoch: 10   trainLoss: 6.2776e-02   valLoss:1.4306e-01  time: 9.34e-02\n",
      "epoch: 11   trainLoss: 5.9655e-02   valLoss:1.2134e-01  time: 9.26e-02\n",
      "epoch: 12   trainLoss: 5.5880e-02   valLoss:1.0358e-01  time: 9.19e-02\n",
      "epoch: 13   trainLoss: 5.3035e-02   valLoss:7.9904e-02  time: 9.30e-02\n",
      "epoch: 14   trainLoss: 4.9985e-02   valLoss:7.3364e-02  time: 9.33e-02\n",
      "epoch: 15   trainLoss: 4.8557e-02   valLoss:7.3670e-02  time: 9.20e-02\n",
      "epoch: 16   trainLoss: 4.6891e-02   valLoss:7.3574e-02  time: 9.08e-02\n",
      "epoch: 17   trainLoss: 4.4931e-02   valLoss:7.1780e-02  time: 9.07e-02\n",
      "epoch: 18   trainLoss: 4.4098e-02   valLoss:6.0056e-02  time: 9.11e-02\n",
      "epoch: 19   trainLoss: 4.2551e-02   valLoss:4.8897e-02  time: 9.30e-02\n",
      "epoch: 20   trainLoss: 4.1859e-02   valLoss:4.6026e-02  time: 9.16e-02\n",
      "epoch: 21   trainLoss: 4.0755e-02   valLoss:4.4464e-02  time: 9.13e-02\n",
      "epoch: 22   trainLoss: 4.0128e-02   valLoss:4.4026e-02  time: 9.65e-02\n",
      "epoch: 23   trainLoss: 3.9153e-02   valLoss:4.3719e-02  time: 9.28e-02\n",
      "epoch: 24   trainLoss: 3.8325e-02   valLoss:4.2749e-02  time: 9.14e-02\n",
      "epoch: 25   trainLoss: 3.7482e-02   valLoss:4.1954e-02  time: 9.21e-02\n",
      "epoch: 26   trainLoss: 3.6623e-02   valLoss:4.1177e-02  time: 9.27e-02\n",
      "epoch: 27   trainLoss: 3.5515e-02   valLoss:4.0268e-02  time: 9.45e-02\n",
      "epoch: 28   trainLoss: 3.4363e-02   valLoss:3.9261e-02  time: 9.16e-02\n",
      "epoch: 29   trainLoss: 3.3287e-02   valLoss:3.8979e-02  time: 1.16e-01\n",
      "epoch: 30   trainLoss: 3.2324e-02   valLoss:3.9320e-02  time: 9.16e-02\n",
      "epoch: 31   trainLoss: 3.1556e-02   valLoss:3.8550e-02  time: 9.40e-02\n",
      "epoch: 32   trainLoss: 3.0798e-02   valLoss:3.7365e-02  time: 9.79e-02\n",
      "epoch: 33   trainLoss: 2.9923e-02   valLoss:3.6564e-02  time: 9.46e-02\n",
      "epoch: 34   trainLoss: 2.9058e-02   valLoss:3.5452e-02  time: 9.46e-02\n",
      "epoch: 35   trainLoss: 2.8321e-02   valLoss:3.4256e-02  time: 9.35e-02\n",
      "epoch: 36   trainLoss: 2.7741e-02   valLoss:3.4226e-02  time: 9.23e-02\n",
      "epoch: 37   trainLoss: 2.7240e-02   valLoss:3.4339e-02  time: 9.06e-02\n",
      "epoch: 38   trainLoss: 2.6980e-02   valLoss:3.3667e-02  time: 8.91e-02\n",
      "epoch: 39   trainLoss: 2.6825e-02   valLoss:3.3497e-02  time: 9.00e-02\n",
      "epoch: 40   trainLoss: 2.6758e-02   valLoss:3.4122e-02  time: 8.94e-02\n",
      "epoch: 41   trainLoss: 2.6824e-02   valLoss:3.3187e-02  time: 8.91e-02\n",
      "epoch: 42   trainLoss: 2.6728e-02   valLoss:3.3171e-02  time: 9.07e-02\n",
      "epoch: 43   trainLoss: 2.6670e-02   valLoss:3.3247e-02  time: 8.98e-02\n",
      "epoch: 44   trainLoss: 2.6624e-02   valLoss:3.3197e-02  time: 9.09e-02\n",
      "epoch: 45   trainLoss: 2.6548e-02   valLoss:3.4036e-02  time: 9.08e-02\n",
      "epoch: 46   trainLoss: 2.6460e-02   valLoss:3.3555e-02  time: 8.93e-02\n",
      "epoch: 47   trainLoss: 2.6361e-02   valLoss:3.3749e-02  time: 9.11e-02\n",
      "epoch: 48   trainLoss: 2.6255e-02   valLoss:3.3431e-02  time: 9.05e-02\n",
      "epoch: 49   trainLoss: 2.6187e-02   valLoss:3.3740e-02  time: 8.90e-02\n",
      "epoch: 50   trainLoss: 2.6269e-02   valLoss:3.2844e-02  time: 9.06e-02\n",
      "epoch: 51   trainLoss: 2.6684e-02   valLoss:3.1716e-02  time: 9.00e-02\n",
      "epoch: 52   trainLoss: 2.6452e-02   valLoss:3.3462e-02  time: 9.20e-02\n",
      "epoch: 53   trainLoss: 2.6669e-02   valLoss:3.3013e-02  time: 9.16e-02\n",
      "epoch: 54   trainLoss: 2.6220e-02   valLoss:3.2965e-02  time: 9.40e-02\n",
      "epoch: 55   trainLoss: 2.6283e-02   valLoss:3.3536e-02  time: 9.39e-02\n",
      "epoch: 56   trainLoss: 2.5940e-02   valLoss:3.6135e-02  time: 9.48e-02\n",
      "epoch: 57   trainLoss: 2.6185e-02   valLoss:3.6753e-02  time: 9.01e-02\n",
      "epoch: 58   trainLoss: 2.5871e-02   valLoss:3.6935e-02  time: 9.19e-02\n",
      "epoch: 59   trainLoss: 2.5865e-02   valLoss:3.7933e-02  time: 9.32e-02\n",
      "epoch: 60   trainLoss: 2.5784e-02   valLoss:3.8205e-02  time: 9.14e-02\n",
      "epoch: 61   trainLoss: 2.5573e-02   valLoss:3.7658e-02  time: 8.96e-02\n",
      "epoch: 62   trainLoss: 2.5613e-02   valLoss:3.5570e-02  time: 9.00e-02\n",
      "epoch: 63   trainLoss: 2.5360e-02   valLoss:3.4620e-02  time: 8.97e-02\n",
      "epoch: 64   trainLoss: 2.5309e-02   valLoss:3.5650e-02  time: 8.96e-02\n",
      "epoch: 65   trainLoss: 2.5039e-02   valLoss:3.4919e-02  time: 8.99e-02\n",
      "epoch: 66   trainLoss: 2.4701e-02   valLoss:3.4369e-02  time: 9.02e-02\n",
      "epoch: 67   trainLoss: 2.4487e-02   valLoss:3.5184e-02  time: 8.89e-02\n",
      "epoch: 68   trainLoss: 2.4779e-02   valLoss:3.2423e-02  time: 9.13e-02\n",
      "epoch: 69   trainLoss: 2.6990e-02   valLoss:3.4009e-02  time: 9.63e-02\n",
      "epoch: 70   trainLoss: 2.6028e-02   valLoss:3.8706e-02  time: 9.12e-02\n",
      "epoch: 71   trainLoss: 2.4626e-02   valLoss:3.7640e-02  time: 9.18e-02\n",
      "epoch: 72   trainLoss: 2.4019e-02   valLoss:3.5602e-02  time: 9.18e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73   trainLoss: 2.3863e-02   valLoss:3.6081e-02  time: 9.28e-02\n",
      "epoch: 74   trainLoss: 2.3659e-02   valLoss:3.7884e-02  time: 9.18e-02\n",
      "epoch: 75   trainLoss: 2.3216e-02   valLoss:3.8687e-02  time: 9.18e-02\n",
      "epoch: 76   trainLoss: 2.3108e-02   valLoss:3.6334e-02  time: 9.19e-02\n",
      "epoch: 77   trainLoss: 2.2599e-02   valLoss:3.3332e-02  time: 9.23e-02\n",
      "epoch: 78   trainLoss: 2.2189e-02   valLoss:3.1992e-02  time: 9.19e-02\n",
      "epoch: 79   trainLoss: 2.1920e-02   valLoss:3.1688e-02  time: 9.13e-02\n",
      "epoch: 80   trainLoss: 2.1602e-02   valLoss:3.1445e-02  time: 1.04e-01\n",
      "epoch: 81   trainLoss: 2.1408e-02   valLoss:3.0784e-02  time: 9.26e-02\n",
      "epoch: 82   trainLoss: 2.1297e-02   valLoss:3.0518e-02  time: 9.14e-02\n",
      "epoch: 83   trainLoss: 2.1133e-02   valLoss:2.9979e-02  time: 9.23e-02\n",
      "epoch: 84   trainLoss: 2.1228e-02   valLoss:3.1335e-02  time: 9.19e-02\n",
      "epoch: 85   trainLoss: 2.1780e-02   valLoss:3.0197e-02  time: 9.11e-02\n",
      "epoch: 86   trainLoss: 2.4664e-02   valLoss:3.6035e-02  time: 9.26e-02\n",
      "epoch: 87   trainLoss: 3.1654e-02   valLoss:3.1185e-02  time: 9.11e-02\n",
      "epoch: 88   trainLoss: 2.6221e-02   valLoss:2.7890e-02  time: 9.19e-02\n",
      "epoch: 89   trainLoss: 2.3794e-02   valLoss:2.8118e-02  time: 9.16e-02\n",
      "epoch: 90   trainLoss: 2.5337e-02   valLoss:2.7930e-02  time: 9.17e-02\n",
      "epoch: 91   trainLoss: 2.3157e-02   valLoss:2.8685e-02  time: 9.56e-02\n",
      "epoch: 92   trainLoss: 2.3361e-02   valLoss:2.6990e-02  time: 9.09e-02\n",
      "epoch: 93   trainLoss: 2.2636e-02   valLoss:2.7010e-02  time: 9.23e-02\n",
      "epoch: 94   trainLoss: 2.2650e-02   valLoss:2.7509e-02  time: 9.18e-02\n",
      "epoch: 95   trainLoss: 2.2140e-02   valLoss:2.8906e-02  time: 9.08e-02\n",
      "epoch: 96   trainLoss: 2.1786e-02   valLoss:2.9272e-02  time: 9.27e-02\n",
      "epoch: 97   trainLoss: 2.1844e-02   valLoss:2.8729e-02  time: 9.12e-02\n",
      "epoch: 98   trainLoss: 2.0857e-02   valLoss:2.9702e-02  time: 9.10e-02\n",
      "epoch: 99   trainLoss: 2.1854e-02   valLoss:2.8683e-02  time: 9.22e-02\n",
      "loading checkpoint 92\n",
      "trained 24 random forest models in 2.70 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_100/\n",
      "loaded train set of size 92\n",
      "epoch: 0   trainLoss: 1.0043e+00   valLoss:9.4872e-01  time: 7.75e-01\n",
      "epoch: 1   trainLoss: 7.3499e-01   valLoss:9.3886e-01  time: 7.71e-01\n",
      "epoch: 2   trainLoss: 5.5150e-01   valLoss:9.2294e-01  time: 7.58e-01\n",
      "epoch: 3   trainLoss: 4.1482e-01   valLoss:8.9390e-01  time: 7.79e-01\n",
      "epoch: 4   trainLoss: 2.7764e-01   valLoss:8.6626e-01  time: 7.59e-01\n",
      "epoch: 5   trainLoss: 1.8842e-01   valLoss:8.2001e-01  time: 7.57e-01\n",
      "epoch: 6   trainLoss: 1.3543e-01   valLoss:7.6219e-01  time: 7.50e-01\n",
      "epoch: 7   trainLoss: 9.9669e-02   valLoss:6.9666e-01  time: 7.57e-01\n",
      "epoch: 8   trainLoss: 8.4562e-02   valLoss:6.2945e-01  time: 7.48e-01\n",
      "epoch: 9   trainLoss: 8.2916e-02   valLoss:5.9082e-01  time: 7.62e-01\n",
      "epoch: 10   trainLoss: 8.8078e-02   valLoss:5.8210e-01  time: 7.70e-01\n",
      "epoch: 11   trainLoss: 9.1796e-02   valLoss:6.0258e-01  time: 7.60e-01\n",
      "epoch: 12   trainLoss: 9.1191e-02   valLoss:6.1872e-01  time: 7.65e-01\n",
      "epoch: 13   trainLoss: 8.6122e-02   valLoss:6.4080e-01  time: 7.46e-01\n",
      "epoch: 14   trainLoss: 7.7451e-02   valLoss:6.5578e-01  time: 7.47e-01\n",
      "epoch: 15   trainLoss: 6.8457e-02   valLoss:6.6320e-01  time: 7.44e-01\n",
      "epoch: 16   trainLoss: 6.1284e-02   valLoss:6.5566e-01  time: 7.68e-01\n",
      "epoch: 17   trainLoss: 5.6106e-02   valLoss:6.2566e-01  time: 7.47e-01\n",
      "epoch: 18   trainLoss: 5.2122e-02   valLoss:5.7833e-01  time: 7.50e-01\n",
      "epoch: 19   trainLoss: 4.8918e-02   valLoss:5.3044e-01  time: 7.61e-01\n",
      "epoch: 20   trainLoss: 4.6231e-02   valLoss:4.7593e-01  time: 7.53e-01\n",
      "epoch: 21   trainLoss: 4.2653e-02   valLoss:4.1039e-01  time: 7.52e-01\n",
      "epoch: 22   trainLoss: 3.9654e-02   valLoss:3.3861e-01  time: 7.44e-01\n",
      "epoch: 23   trainLoss: 3.6965e-02   valLoss:2.5431e-01  time: 7.69e-01\n",
      "epoch: 24   trainLoss: 3.5437e-02   valLoss:1.7639e-01  time: 7.41e-01\n",
      "epoch: 25   trainLoss: 3.4539e-02   valLoss:1.1875e-01  time: 7.56e-01\n",
      "epoch: 26   trainLoss: 3.3850e-02   valLoss:8.8247e-02  time: 7.57e-01\n",
      "epoch: 27   trainLoss: 3.3266e-02   valLoss:7.2150e-02  time: 7.46e-01\n",
      "epoch: 28   trainLoss: 3.2107e-02   valLoss:6.6846e-02  time: 7.43e-01\n",
      "epoch: 29   trainLoss: 3.0847e-02   valLoss:6.5498e-02  time: 7.70e-01\n",
      "epoch: 30   trainLoss: 2.9850e-02   valLoss:6.5120e-02  time: 7.45e-01\n",
      "epoch: 31   trainLoss: 2.9140e-02   valLoss:6.5923e-02  time: 7.42e-01\n",
      "epoch: 32   trainLoss: 2.8643e-02   valLoss:6.8079e-02  time: 7.42e-01\n",
      "epoch: 33   trainLoss: 2.8160e-02   valLoss:6.9914e-02  time: 7.42e-01\n",
      "epoch: 34   trainLoss: 2.7670e-02   valLoss:7.0089e-02  time: 7.46e-01\n",
      "epoch: 35   trainLoss: 2.7108e-02   valLoss:7.0507e-02  time: 7.53e-01\n",
      "epoch: 36   trainLoss: 2.6617e-02   valLoss:7.1743e-02  time: 7.66e-01\n",
      "epoch: 37   trainLoss: 2.6175e-02   valLoss:7.2401e-02  time: 7.54e-01\n",
      "epoch: 38   trainLoss: 2.5790e-02   valLoss:7.2388e-02  time: 7.61e-01\n",
      "epoch: 39   trainLoss: 2.5233e-02   valLoss:7.3033e-02  time: 7.75e-01\n",
      "epoch: 40   trainLoss: 2.4596e-02   valLoss:7.4148e-02  time: 7.46e-01\n",
      "epoch: 41   trainLoss: 2.3939e-02   valLoss:7.4633e-02  time: 7.51e-01\n",
      "epoch: 42   trainLoss: 2.3253e-02   valLoss:7.4067e-02  time: 7.61e-01\n",
      "epoch: 43   trainLoss: 2.2480e-02   valLoss:7.3584e-02  time: 7.55e-01\n",
      "epoch: 44   trainLoss: 2.1636e-02   valLoss:7.3278e-02  time: 7.38e-01\n",
      "epoch: 45   trainLoss: 2.0654e-02   valLoss:7.2535e-02  time: 7.41e-01\n",
      "epoch: 46   trainLoss: 1.9583e-02   valLoss:7.1530e-02  time: 7.38e-01\n",
      "epoch: 47   trainLoss: 1.8534e-02   valLoss:7.1569e-02  time: 7.38e-01\n",
      "epoch: 48   trainLoss: 1.7588e-02   valLoss:7.3603e-02  time: 7.42e-01\n",
      "epoch: 49   trainLoss: 1.6522e-02   valLoss:7.5937e-02  time: 7.38e-01\n",
      "epoch: 50   trainLoss: 1.5558e-02   valLoss:7.6935e-02  time: 7.71e-01\n",
      "epoch: 51   trainLoss: 1.4657e-02   valLoss:7.8009e-02  time: 7.39e-01\n",
      "epoch: 52   trainLoss: 1.3779e-02   valLoss:7.8971e-02  time: 7.56e-01\n",
      "epoch: 53   trainLoss: 1.3026e-02   valLoss:7.8608e-02  time: 7.42e-01\n",
      "epoch: 54   trainLoss: 1.2298e-02   valLoss:7.9946e-02  time: 7.38e-01\n",
      "epoch: 55   trainLoss: 1.1635e-02   valLoss:7.9313e-02  time: 7.37e-01\n",
      "epoch: 56   trainLoss: 1.1884e-02   valLoss:7.8830e-02  time: 7.35e-01\n",
      "epoch: 57   trainLoss: 1.0145e-02   valLoss:7.7267e-02  time: 7.47e-01\n",
      "epoch: 58   trainLoss: 9.4897e-03   valLoss:7.5384e-02  time: 7.51e-01\n",
      "epoch: 59   trainLoss: 8.8610e-03   valLoss:7.9510e-02  time: 7.53e-01\n",
      "epoch: 60   trainLoss: 9.0326e-03   valLoss:6.9329e-02  time: 7.39e-01\n",
      "epoch: 61   trainLoss: 1.2906e-02   valLoss:9.2457e-02  time: 7.41e-01\n",
      "epoch: 62   trainLoss: 2.4589e-02   valLoss:9.1671e-02  time: 7.42e-01\n",
      "epoch: 63   trainLoss: 2.0665e-02   valLoss:5.3519e-02  time: 7.41e-01\n",
      "epoch: 64   trainLoss: 1.3912e-02   valLoss:4.8744e-02  time: 7.50e-01\n",
      "epoch: 65   trainLoss: 1.8621e-02   valLoss:4.8287e-02  time: 7.57e-01\n",
      "epoch: 66   trainLoss: 1.3698e-02   valLoss:4.4975e-02  time: 7.43e-01\n",
      "epoch: 67   trainLoss: 1.2291e-02   valLoss:4.2680e-02  time: 7.42e-01\n",
      "epoch: 68   trainLoss: 1.2930e-02   valLoss:4.4045e-02  time: 7.41e-01\n",
      "epoch: 69   trainLoss: 1.0037e-02   valLoss:4.3019e-02  time: 7.53e-01\n",
      "epoch: 70   trainLoss: 9.3731e-03   valLoss:4.1424e-02  time: 7.47e-01\n",
      "epoch: 71   trainLoss: 9.2885e-03   valLoss:4.1873e-02  time: 7.43e-01\n",
      "epoch: 72   trainLoss: 8.2241e-03   valLoss:3.9871e-02  time: 7.51e-01\n",
      "epoch: 73   trainLoss: 8.1315e-03   valLoss:3.6323e-02  time: 7.40e-01\n",
      "epoch: 74   trainLoss: 7.4118e-03   valLoss:3.3190e-02  time: 7.43e-01\n",
      "epoch: 75   trainLoss: 6.5039e-03   valLoss:3.3177e-02  time: 7.52e-01\n",
      "epoch: 76   trainLoss: 6.4322e-03   valLoss:3.5062e-02  time: 7.46e-01\n",
      "epoch: 77   trainLoss: 5.9094e-03   valLoss:3.4543e-02  time: 7.40e-01\n",
      "epoch: 78   trainLoss: 5.3068e-03   valLoss:3.4708e-02  time: 7.44e-01\n",
      "epoch: 79   trainLoss: 4.8619e-03   valLoss:3.6632e-02  time: 7.43e-01\n",
      "epoch: 80   trainLoss: 4.4557e-03   valLoss:3.6144e-02  time: 7.41e-01\n",
      "epoch: 81   trainLoss: 3.8969e-03   valLoss:3.6640e-02  time: 7.39e-01\n",
      "epoch: 82   trainLoss: 3.5916e-03   valLoss:4.0141e-02  time: 7.42e-01\n",
      "epoch: 83   trainLoss: 3.1680e-03   valLoss:3.8307e-02  time: 7.54e-01\n",
      "epoch: 84   trainLoss: 2.4808e-03   valLoss:3.7832e-02  time: 7.58e-01\n",
      "epoch: 85   trainLoss: 2.1626e-03   valLoss:4.0400e-02  time: 7.39e-01\n",
      "epoch: 86   trainLoss: 1.9407e-03   valLoss:3.8669e-02  time: 7.43e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87   trainLoss: 1.6058e-03   valLoss:3.9505e-02  time: 7.44e-01\n",
      "epoch: 88   trainLoss: 1.3601e-03   valLoss:4.0241e-02  time: 7.45e-01\n",
      "epoch: 89   trainLoss: 1.1386e-03   valLoss:3.8888e-02  time: 7.41e-01\n",
      "epoch: 90   trainLoss: 9.8219e-04   valLoss:4.0092e-02  time: 7.38e-01\n",
      "epoch: 91   trainLoss: 9.8068e-04   valLoss:4.0021e-02  time: 7.64e-01\n",
      "epoch: 92   trainLoss: 9.5563e-04   valLoss:3.9883e-02  time: 7.41e-01\n",
      "epoch: 93   trainLoss: 7.9575e-04   valLoss:3.9019e-02  time: 7.37e-01\n",
      "epoch: 94   trainLoss: 6.4610e-04   valLoss:3.9540e-02  time: 7.40e-01\n",
      "epoch: 95   trainLoss: 5.7116e-04   valLoss:3.8911e-02  time: 7.52e-01\n",
      "epoch: 96   trainLoss: 5.4695e-04   valLoss:4.0095e-02  time: 7.40e-01\n",
      "epoch: 97   trainLoss: 5.6563e-04   valLoss:4.0615e-02  time: 7.57e-01\n",
      "epoch: 98   trainLoss: 5.6253e-04   valLoss:4.1399e-02  time: 7.55e-01\n",
      "epoch: 99   trainLoss: 6.2892e-04   valLoss:4.2606e-02  time: 7.48e-01\n",
      "loading checkpoint 75\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2266e+00   valLoss:1.1589e+02  time: 7.89e-01\n",
      "epoch: 1   trainLoss: 3.8441e-01   valLoss:1.0256e+01  time: 7.98e-01\n",
      "epoch: 2   trainLoss: 2.2453e-01   valLoss:3.7765e+00  time: 7.71e-01\n",
      "epoch: 3   trainLoss: 1.4398e-01   valLoss:3.4300e+00  time: 7.53e-01\n",
      "epoch: 4   trainLoss: 1.0422e-01   valLoss:2.3872e+00  time: 7.43e-01\n",
      "epoch: 5   trainLoss: 9.0360e-02   valLoss:1.1149e+00  time: 7.48e-01\n",
      "epoch: 6   trainLoss: 8.6851e-02   valLoss:8.0495e-01  time: 7.45e-01\n",
      "epoch: 7   trainLoss: 8.1396e-02   valLoss:7.1759e-01  time: 7.66e-01\n",
      "epoch: 8   trainLoss: 7.6220e-02   valLoss:5.2910e-01  time: 7.55e-01\n",
      "epoch: 9   trainLoss: 7.1246e-02   valLoss:3.5225e-01  time: 7.52e-01\n",
      "epoch: 10   trainLoss: 6.7615e-02   valLoss:2.5480e-01  time: 7.44e-01\n",
      "epoch: 11   trainLoss: 6.4073e-02   valLoss:2.0060e-01  time: 7.48e-01\n",
      "epoch: 12   trainLoss: 6.0804e-02   valLoss:1.6751e-01  time: 7.60e-01\n",
      "epoch: 13   trainLoss: 5.8320e-02   valLoss:1.3720e-01  time: 7.45e-01\n",
      "epoch: 14   trainLoss: 5.6426e-02   valLoss:1.0863e-01  time: 7.52e-01\n",
      "epoch: 15   trainLoss: 5.4804e-02   valLoss:8.7403e-02  time: 7.48e-01\n",
      "epoch: 16   trainLoss: 5.3148e-02   valLoss:7.3608e-02  time: 7.46e-01\n",
      "epoch: 17   trainLoss: 5.1055e-02   valLoss:6.4552e-02  time: 7.46e-01\n",
      "epoch: 18   trainLoss: 4.8720e-02   valLoss:5.8962e-02  time: 7.47e-01\n",
      "epoch: 19   trainLoss: 4.6351e-02   valLoss:5.5846e-02  time: 7.48e-01\n",
      "epoch: 20   trainLoss: 4.4004e-02   valLoss:5.4092e-02  time: 7.51e-01\n",
      "epoch: 21   trainLoss: 4.1613e-02   valLoss:5.2311e-02  time: 7.54e-01\n",
      "epoch: 22   trainLoss: 3.9239e-02   valLoss:5.0168e-02  time: 7.52e-01\n",
      "epoch: 23   trainLoss: 3.7132e-02   valLoss:4.8093e-02  time: 7.47e-01\n",
      "epoch: 24   trainLoss: 3.5349e-02   valLoss:4.5369e-02  time: 7.43e-01\n",
      "epoch: 25   trainLoss: 3.4015e-02   valLoss:4.3165e-02  time: 7.54e-01\n",
      "epoch: 26   trainLoss: 3.2592e-02   valLoss:4.1964e-02  time: 7.49e-01\n",
      "epoch: 27   trainLoss: 3.1092e-02   valLoss:4.1962e-02  time: 7.69e-01\n",
      "epoch: 28   trainLoss: 2.9631e-02   valLoss:4.2193e-02  time: 7.42e-01\n",
      "epoch: 29   trainLoss: 2.8347e-02   valLoss:3.9997e-02  time: 7.45e-01\n",
      "epoch: 30   trainLoss: 2.7125e-02   valLoss:4.1148e-02  time: 7.45e-01\n",
      "epoch: 31   trainLoss: 2.6026e-02   valLoss:3.9123e-02  time: 7.40e-01\n",
      "epoch: 32   trainLoss: 2.5155e-02   valLoss:4.0426e-02  time: 7.41e-01\n",
      "epoch: 33   trainLoss: 2.4308e-02   valLoss:3.9075e-02  time: 7.53e-01\n",
      "epoch: 34   trainLoss: 2.3378e-02   valLoss:4.2726e-02  time: 7.52e-01\n",
      "epoch: 35   trainLoss: 2.2532e-02   valLoss:3.6492e-02  time: 7.42e-01\n",
      "epoch: 36   trainLoss: 2.2439e-02   valLoss:5.3510e-02  time: 7.44e-01\n",
      "epoch: 37   trainLoss: 2.4764e-02   valLoss:3.1813e-02  time: 7.48e-01\n",
      "epoch: 38   trainLoss: 3.5437e-02   valLoss:3.3732e-02  time: 7.42e-01\n",
      "epoch: 39   trainLoss: 2.2266e-02   valLoss:5.7673e-02  time: 7.41e-01\n",
      "epoch: 40   trainLoss: 2.9294e-02   valLoss:2.4225e-02  time: 7.42e-01\n",
      "epoch: 41   trainLoss: 2.1294e-02   valLoss:2.4531e-02  time: 7.48e-01\n",
      "epoch: 42   trainLoss: 2.4378e-02   valLoss:2.4244e-02  time: 7.49e-01\n",
      "epoch: 43   trainLoss: 1.8487e-02   valLoss:3.3568e-02  time: 7.50e-01\n",
      "epoch: 44   trainLoss: 2.0339e-02   valLoss:3.2994e-02  time: 7.42e-01\n",
      "epoch: 45   trainLoss: 1.8989e-02   valLoss:2.6275e-02  time: 7.43e-01\n",
      "epoch: 46   trainLoss: 1.5958e-02   valLoss:2.5980e-02  time: 7.41e-01\n",
      "epoch: 47   trainLoss: 1.7201e-02   valLoss:2.5431e-02  time: 7.46e-01\n",
      "epoch: 48   trainLoss: 1.4680e-02   valLoss:2.7614e-02  time: 7.72e-01\n",
      "epoch: 49   trainLoss: 1.2693e-02   valLoss:2.6819e-02  time: 7.48e-01\n",
      "epoch: 50   trainLoss: 1.1781e-02   valLoss:2.2219e-02  time: 7.52e-01\n",
      "epoch: 51   trainLoss: 9.7146e-03   valLoss:2.0147e-02  time: 7.43e-01\n",
      "epoch: 52   trainLoss: 8.4238e-03   valLoss:1.9301e-02  time: 7.49e-01\n",
      "epoch: 53   trainLoss: 6.7603e-03   valLoss:2.1053e-02  time: 7.53e-01\n",
      "epoch: 54   trainLoss: 6.4368e-03   valLoss:2.2668e-02  time: 7.60e-01\n",
      "epoch: 55   trainLoss: 4.4207e-03   valLoss:2.3381e-02  time: 7.47e-01\n",
      "epoch: 56   trainLoss: 4.2448e-03   valLoss:2.2389e-02  time: 7.47e-01\n",
      "epoch: 57   trainLoss: 4.0568e-03   valLoss:2.0108e-02  time: 7.42e-01\n",
      "epoch: 58   trainLoss: 3.9304e-03   valLoss:1.9823e-02  time: 7.47e-01\n",
      "epoch: 59   trainLoss: 3.8514e-03   valLoss:2.2021e-02  time: 7.44e-01\n",
      "epoch: 60   trainLoss: 3.6534e-03   valLoss:2.4111e-02  time: 7.46e-01\n",
      "epoch: 61   trainLoss: 3.0555e-03   valLoss:2.2134e-02  time: 7.65e-01\n",
      "epoch: 62   trainLoss: 2.8722e-03   valLoss:1.6422e-02  time: 7.49e-01\n",
      "epoch: 63   trainLoss: 2.7426e-03   valLoss:1.4231e-02  time: 7.47e-01\n",
      "epoch: 64   trainLoss: 2.3883e-03   valLoss:1.1605e-02  time: 7.48e-01\n",
      "epoch: 65   trainLoss: 2.1034e-03   valLoss:1.1154e-02  time: 7.46e-01\n",
      "epoch: 66   trainLoss: 2.0089e-03   valLoss:1.2503e-02  time: 7.45e-01\n",
      "epoch: 67   trainLoss: 1.7875e-03   valLoss:1.2895e-02  time: 7.48e-01\n",
      "epoch: 68   trainLoss: 1.6926e-03   valLoss:1.1262e-02  time: 7.52e-01\n",
      "epoch: 69   trainLoss: 1.5176e-03   valLoss:9.6921e-03  time: 7.44e-01\n",
      "epoch: 70   trainLoss: 1.4137e-03   valLoss:9.5383e-03  time: 7.43e-01\n",
      "epoch: 71   trainLoss: 1.3057e-03   valLoss:9.3705e-03  time: 7.38e-01\n",
      "epoch: 72   trainLoss: 1.1307e-03   valLoss:8.3094e-03  time: 7.44e-01\n",
      "epoch: 73   trainLoss: 1.0443e-03   valLoss:7.7513e-03  time: 7.42e-01\n",
      "epoch: 74   trainLoss: 1.0667e-03   valLoss:7.9112e-03  time: 7.54e-01\n",
      "epoch: 75   trainLoss: 9.1640e-04   valLoss:7.9057e-03  time: 7.48e-01\n",
      "epoch: 76   trainLoss: 9.2247e-04   valLoss:6.4524e-03  time: 7.41e-01\n",
      "epoch: 77   trainLoss: 8.4614e-04   valLoss:5.3044e-03  time: 7.54e-01\n",
      "epoch: 78   trainLoss: 7.5431e-04   valLoss:5.1234e-03  time: 7.44e-01\n",
      "epoch: 79   trainLoss: 7.1444e-04   valLoss:4.6099e-03  time: 7.57e-01\n",
      "epoch: 80   trainLoss: 6.7816e-04   valLoss:4.1418e-03  time: 7.53e-01\n",
      "epoch: 81   trainLoss: 6.4194e-04   valLoss:4.3239e-03  time: 7.57e-01\n",
      "epoch: 82   trainLoss: 5.8284e-04   valLoss:4.5640e-03  time: 7.41e-01\n",
      "epoch: 83   trainLoss: 5.5752e-04   valLoss:4.3331e-03  time: 7.40e-01\n",
      "epoch: 84   trainLoss: 5.5144e-04   valLoss:4.3667e-03  time: 7.42e-01\n",
      "epoch: 85   trainLoss: 5.2526e-04   valLoss:4.4791e-03  time: 7.46e-01\n",
      "epoch: 86   trainLoss: 5.1365e-04   valLoss:3.8286e-03  time: 7.44e-01\n",
      "epoch: 87   trainLoss: 4.7127e-04   valLoss:3.6452e-03  time: 7.51e-01\n",
      "epoch: 88   trainLoss: 4.5055e-04   valLoss:3.6767e-03  time: 7.56e-01\n",
      "epoch: 89   trainLoss: 4.3518e-04   valLoss:3.3385e-03  time: 7.44e-01\n",
      "epoch: 90   trainLoss: 4.1308e-04   valLoss:3.3399e-03  time: 7.46e-01\n",
      "epoch: 91   trainLoss: 3.8355e-04   valLoss:3.3096e-03  time: 7.55e-01\n",
      "epoch: 92   trainLoss: 3.6754e-04   valLoss:3.3156e-03  time: 7.42e-01\n",
      "epoch: 93   trainLoss: 3.5612e-04   valLoss:3.4069e-03  time: 7.46e-01\n",
      "epoch: 94   trainLoss: 3.3512e-04   valLoss:3.2281e-03  time: 7.40e-01\n",
      "epoch: 95   trainLoss: 3.1174e-04   valLoss:3.2070e-03  time: 7.48e-01\n",
      "epoch: 96   trainLoss: 3.1220e-04   valLoss:3.6453e-03  time: 7.44e-01\n",
      "epoch: 97   trainLoss: 2.9579e-04   valLoss:3.6234e-03  time: 7.45e-01\n",
      "epoch: 98   trainLoss: 2.8538e-04   valLoss:3.4102e-03  time: 7.43e-01\n",
      "epoch: 99   trainLoss: 2.6516e-04   valLoss:3.2269e-03  time: 7.53e-01\n",
      "loading checkpoint 95\n",
      "trained 24 random forest models in 3.48 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_50/\n",
      "loaded train set of size 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 1.1013e+00   valLoss:1.0043e+00  time: 4.04e-01\n",
      "epoch: 1   trainLoss: 8.5431e-01   valLoss:9.9331e-01  time: 3.98e-01\n",
      "epoch: 2   trainLoss: 6.5855e-01   valLoss:9.7188e-01  time: 3.99e-01\n",
      "epoch: 3   trainLoss: 4.9731e-01   valLoss:9.4647e-01  time: 4.01e-01\n",
      "epoch: 4   trainLoss: 3.6800e-01   valLoss:9.1667e-01  time: 4.04e-01\n",
      "epoch: 5   trainLoss: 2.5700e-01   valLoss:8.7150e-01  time: 4.02e-01\n",
      "epoch: 6   trainLoss: 1.7598e-01   valLoss:7.9896e-01  time: 4.04e-01\n",
      "epoch: 7   trainLoss: 1.2395e-01   valLoss:7.1733e-01  time: 4.00e-01\n",
      "epoch: 8   trainLoss: 9.7299e-02   valLoss:6.4512e-01  time: 4.05e-01\n",
      "epoch: 9   trainLoss: 8.7671e-02   valLoss:5.8414e-01  time: 3.99e-01\n",
      "epoch: 10   trainLoss: 8.8270e-02   valLoss:5.3568e-01  time: 3.99e-01\n",
      "epoch: 11   trainLoss: 9.1077e-02   valLoss:5.0695e-01  time: 4.00e-01\n",
      "epoch: 12   trainLoss: 9.2323e-02   valLoss:4.9601e-01  time: 4.04e-01\n",
      "epoch: 13   trainLoss: 8.9661e-02   valLoss:4.8133e-01  time: 4.02e-01\n",
      "epoch: 14   trainLoss: 8.6661e-02   valLoss:4.5848e-01  time: 4.26e-01\n",
      "epoch: 15   trainLoss: 8.1980e-02   valLoss:4.2593e-01  time: 4.01e-01\n",
      "epoch: 16   trainLoss: 7.5108e-02   valLoss:3.7677e-01  time: 4.01e-01\n",
      "epoch: 17   trainLoss: 6.8369e-02   valLoss:3.3227e-01  time: 4.05e-01\n",
      "epoch: 18   trainLoss: 6.3484e-02   valLoss:2.9001e-01  time: 4.09e-01\n",
      "epoch: 19   trainLoss: 5.9914e-02   valLoss:2.4545e-01  time: 4.01e-01\n",
      "epoch: 20   trainLoss: 5.7185e-02   valLoss:2.1168e-01  time: 4.04e-01\n",
      "epoch: 21   trainLoss: 5.4843e-02   valLoss:1.7829e-01  time: 3.99e-01\n",
      "epoch: 22   trainLoss: 5.3121e-02   valLoss:1.4961e-01  time: 4.06e-01\n",
      "epoch: 23   trainLoss: 5.1528e-02   valLoss:1.2623e-01  time: 4.00e-01\n",
      "epoch: 24   trainLoss: 4.9689e-02   valLoss:1.0848e-01  time: 4.02e-01\n",
      "epoch: 25   trainLoss: 4.7998e-02   valLoss:1.0211e-01  time: 3.96e-01\n",
      "epoch: 26   trainLoss: 4.6446e-02   valLoss:1.0310e-01  time: 3.97e-01\n",
      "epoch: 27   trainLoss: 4.4961e-02   valLoss:1.0503e-01  time: 3.99e-01\n",
      "epoch: 28   trainLoss: 4.3615e-02   valLoss:1.0488e-01  time: 3.99e-01\n",
      "epoch: 29   trainLoss: 4.2394e-02   valLoss:1.0374e-01  time: 4.06e-01\n",
      "epoch: 30   trainLoss: 4.1261e-02   valLoss:9.9127e-02  time: 4.02e-01\n",
      "epoch: 31   trainLoss: 4.0312e-02   valLoss:9.3248e-02  time: 4.05e-01\n",
      "epoch: 32   trainLoss: 3.9453e-02   valLoss:8.7741e-02  time: 4.07e-01\n",
      "epoch: 33   trainLoss: 3.8838e-02   valLoss:8.2812e-02  time: 3.99e-01\n",
      "epoch: 34   trainLoss: 3.8240e-02   valLoss:7.9410e-02  time: 3.99e-01\n",
      "epoch: 35   trainLoss: 3.7644e-02   valLoss:7.7640e-02  time: 4.01e-01\n",
      "epoch: 36   trainLoss: 3.7335e-02   valLoss:7.7671e-02  time: 4.01e-01\n",
      "epoch: 37   trainLoss: 3.7061e-02   valLoss:7.6475e-02  time: 3.97e-01\n",
      "epoch: 38   trainLoss: 3.7271e-02   valLoss:7.4378e-02  time: 4.05e-01\n",
      "epoch: 39   trainLoss: 3.6512e-02   valLoss:7.2199e-02  time: 4.12e-01\n",
      "epoch: 40   trainLoss: 3.5423e-02   valLoss:7.0070e-02  time: 4.19e-01\n",
      "epoch: 41   trainLoss: 3.5386e-02   valLoss:6.8372e-02  time: 4.04e-01\n",
      "epoch: 42   trainLoss: 3.4322e-02   valLoss:6.7723e-02  time: 4.04e-01\n",
      "epoch: 43   trainLoss: 3.3622e-02   valLoss:6.7313e-02  time: 4.09e-01\n",
      "epoch: 44   trainLoss: 3.3057e-02   valLoss:6.5821e-02  time: 4.03e-01\n",
      "epoch: 45   trainLoss: 3.2515e-02   valLoss:6.3265e-02  time: 4.13e-01\n",
      "epoch: 46   trainLoss: 3.1740e-02   valLoss:6.1195e-02  time: 4.05e-01\n",
      "epoch: 47   trainLoss: 3.1056e-02   valLoss:6.0414e-02  time: 4.24e-01\n",
      "epoch: 48   trainLoss: 3.0386e-02   valLoss:5.9906e-02  time: 4.00e-01\n",
      "epoch: 49   trainLoss: 2.9546e-02   valLoss:5.9330e-02  time: 4.00e-01\n",
      "epoch: 50   trainLoss: 2.8869e-02   valLoss:5.8290e-02  time: 4.01e-01\n",
      "epoch: 51   trainLoss: 2.8471e-02   valLoss:5.7166e-02  time: 4.05e-01\n",
      "epoch: 52   trainLoss: 2.8019e-02   valLoss:5.6884e-02  time: 4.01e-01\n",
      "epoch: 53   trainLoss: 2.7743e-02   valLoss:5.5707e-02  time: 4.02e-01\n",
      "epoch: 54   trainLoss: 2.8689e-02   valLoss:5.7075e-02  time: 4.02e-01\n",
      "epoch: 55   trainLoss: 2.7164e-02   valLoss:5.6725e-02  time: 3.99e-01\n",
      "epoch: 56   trainLoss: 2.9737e-02   valLoss:5.1696e-02  time: 4.02e-01\n",
      "epoch: 57   trainLoss: 2.6499e-02   valLoss:5.1793e-02  time: 4.03e-01\n",
      "epoch: 58   trainLoss: 3.0522e-02   valLoss:5.9153e-02  time: 4.05e-01\n",
      "epoch: 59   trainLoss: 3.0866e-02   valLoss:5.4538e-02  time: 4.12e-01\n",
      "epoch: 60   trainLoss: 2.8065e-02   valLoss:5.2062e-02  time: 4.02e-01\n",
      "epoch: 61   trainLoss: 2.8824e-02   valLoss:5.2152e-02  time: 4.02e-01\n",
      "epoch: 62   trainLoss: 2.8574e-02   valLoss:5.4725e-02  time: 4.01e-01\n",
      "epoch: 63   trainLoss: 2.7288e-02   valLoss:5.5382e-02  time: 3.99e-01\n",
      "epoch: 64   trainLoss: 2.7062e-02   valLoss:5.2740e-02  time: 4.04e-01\n",
      "epoch: 65   trainLoss: 2.6293e-02   valLoss:5.2167e-02  time: 4.06e-01\n",
      "epoch: 66   trainLoss: 2.5509e-02   valLoss:5.4344e-02  time: 4.03e-01\n",
      "epoch: 67   trainLoss: 2.4522e-02   valLoss:5.6005e-02  time: 4.04e-01\n",
      "epoch: 68   trainLoss: 2.3332e-02   valLoss:5.2906e-02  time: 4.15e-01\n",
      "epoch: 69   trainLoss: 2.1747e-02   valLoss:5.1346e-02  time: 4.22e-01\n",
      "epoch: 70   trainLoss: 2.0844e-02   valLoss:5.3525e-02  time: 4.02e-01\n",
      "epoch: 71   trainLoss: 2.0389e-02   valLoss:5.6714e-02  time: 4.29e-01\n",
      "epoch: 72   trainLoss: 2.1170e-02   valLoss:5.5377e-02  time: 3.99e-01\n",
      "epoch: 73   trainLoss: 2.0413e-02   valLoss:5.5886e-02  time: 4.07e-01\n",
      "epoch: 74   trainLoss: 1.9222e-02   valLoss:5.1752e-02  time: 4.04e-01\n",
      "epoch: 75   trainLoss: 2.0736e-02   valLoss:4.8105e-02  time: 4.04e-01\n",
      "epoch: 76   trainLoss: 1.8376e-02   valLoss:4.4736e-02  time: 4.07e-01\n",
      "epoch: 77   trainLoss: 1.7648e-02   valLoss:3.8817e-02  time: 4.04e-01\n",
      "epoch: 78   trainLoss: 1.7515e-02   valLoss:4.0316e-02  time: 4.05e-01\n",
      "epoch: 79   trainLoss: 1.6816e-02   valLoss:4.5079e-02  time: 4.06e-01\n",
      "epoch: 80   trainLoss: 1.5800e-02   valLoss:4.7170e-02  time: 4.08e-01\n",
      "epoch: 81   trainLoss: 1.5805e-02   valLoss:5.0750e-02  time: 4.02e-01\n",
      "epoch: 82   trainLoss: 1.4917e-02   valLoss:4.8151e-02  time: 4.02e-01\n",
      "epoch: 83   trainLoss: 1.4709e-02   valLoss:4.8193e-02  time: 4.15e-01\n",
      "epoch: 84   trainLoss: 1.3939e-02   valLoss:5.0446e-02  time: 4.08e-01\n",
      "epoch: 85   trainLoss: 1.3901e-02   valLoss:5.1362e-02  time: 3.99e-01\n",
      "epoch: 86   trainLoss: 1.3597e-02   valLoss:5.0806e-02  time: 4.04e-01\n",
      "epoch: 87   trainLoss: 1.3136e-02   valLoss:5.0595e-02  time: 4.00e-01\n",
      "epoch: 88   trainLoss: 1.3044e-02   valLoss:5.0368e-02  time: 4.17e-01\n",
      "epoch: 89   trainLoss: 1.2875e-02   valLoss:4.7049e-02  time: 4.13e-01\n",
      "epoch: 90   trainLoss: 1.2576e-02   valLoss:4.5589e-02  time: 4.06e-01\n",
      "epoch: 91   trainLoss: 1.2247e-02   valLoss:4.7042e-02  time: 4.09e-01\n",
      "epoch: 92   trainLoss: 1.1910e-02   valLoss:4.7536e-02  time: 4.20e-01\n",
      "epoch: 93   trainLoss: 1.1787e-02   valLoss:4.6352e-02  time: 4.12e-01\n",
      "epoch: 94   trainLoss: 1.1539e-02   valLoss:4.6214e-02  time: 4.05e-01\n",
      "epoch: 95   trainLoss: 1.1273e-02   valLoss:4.7409e-02  time: 4.04e-01\n",
      "epoch: 96   trainLoss: 1.1108e-02   valLoss:4.5479e-02  time: 4.34e-01\n",
      "epoch: 97   trainLoss: 1.0763e-02   valLoss:4.6199e-02  time: 4.00e-01\n",
      "epoch: 98   trainLoss: 1.0410e-02   valLoss:4.7260e-02  time: 4.06e-01\n",
      "epoch: 99   trainLoss: 1.0042e-02   valLoss:4.7118e-02  time: 4.05e-01\n",
      "loading checkpoint 77\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2519e+00   valLoss:1.0597e+02  time: 4.12e-01\n",
      "epoch: 1   trainLoss: 3.8970e-01   valLoss:1.9779e+01  time: 4.13e-01\n",
      "epoch: 2   trainLoss: 2.0636e-01   valLoss:6.1328e+00  time: 4.10e-01\n",
      "epoch: 3   trainLoss: 1.2637e-01   valLoss:3.1999e+00  time: 4.15e-01\n",
      "epoch: 4   trainLoss: 9.5448e-02   valLoss:2.7203e+00  time: 4.07e-01\n",
      "epoch: 5   trainLoss: 8.5287e-02   valLoss:1.7972e+00  time: 4.04e-01\n",
      "epoch: 6   trainLoss: 8.3343e-02   valLoss:1.0598e+00  time: 4.05e-01\n",
      "epoch: 7   trainLoss: 8.1512e-02   valLoss:5.8967e-01  time: 4.05e-01\n",
      "epoch: 8   trainLoss: 7.5556e-02   valLoss:3.8408e-01  time: 4.06e-01\n",
      "epoch: 9   trainLoss: 7.0152e-02   valLoss:2.8301e-01  time: 4.06e-01\n",
      "epoch: 10   trainLoss: 6.6000e-02   valLoss:2.1705e-01  time: 4.05e-01\n",
      "epoch: 11   trainLoss: 6.2389e-02   valLoss:1.7378e-01  time: 4.03e-01\n",
      "epoch: 12   trainLoss: 5.9664e-02   valLoss:1.5171e-01  time: 4.15e-01\n",
      "epoch: 13   trainLoss: 5.7782e-02   valLoss:1.4248e-01  time: 4.01e-01\n",
      "epoch: 14   trainLoss: 5.6028e-02   valLoss:1.3804e-01  time: 4.08e-01\n",
      "epoch: 15   trainLoss: 5.4066e-02   valLoss:1.3670e-01  time: 4.08e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16   trainLoss: 5.2878e-02   valLoss:1.3080e-01  time: 4.04e-01\n",
      "epoch: 17   trainLoss: 5.1075e-02   valLoss:1.2266e-01  time: 4.04e-01\n",
      "epoch: 18   trainLoss: 4.9352e-02   valLoss:1.1566e-01  time: 4.04e-01\n",
      "epoch: 19   trainLoss: 4.7520e-02   valLoss:1.0857e-01  time: 4.01e-01\n",
      "epoch: 20   trainLoss: 4.5596e-02   valLoss:1.0217e-01  time: 4.01e-01\n",
      "epoch: 21   trainLoss: 4.3879e-02   valLoss:9.4748e-02  time: 4.03e-01\n",
      "epoch: 22   trainLoss: 4.1804e-02   valLoss:8.8826e-02  time: 4.01e-01\n",
      "epoch: 23   trainLoss: 3.9910e-02   valLoss:8.3496e-02  time: 4.01e-01\n",
      "epoch: 24   trainLoss: 3.8093e-02   valLoss:8.0406e-02  time: 4.03e-01\n",
      "epoch: 25   trainLoss: 3.6814e-02   valLoss:7.8342e-02  time: 4.02e-01\n",
      "epoch: 26   trainLoss: 3.5222e-02   valLoss:7.5746e-02  time: 4.13e-01\n",
      "epoch: 27   trainLoss: 3.3816e-02   valLoss:7.4636e-02  time: 4.05e-01\n",
      "epoch: 28   trainLoss: 3.2431e-02   valLoss:7.4991e-02  time: 4.02e-01\n",
      "epoch: 29   trainLoss: 3.0737e-02   valLoss:7.3687e-02  time: 4.03e-01\n",
      "epoch: 30   trainLoss: 2.9249e-02   valLoss:7.1766e-02  time: 4.05e-01\n",
      "epoch: 31   trainLoss: 2.7814e-02   valLoss:7.1497e-02  time: 4.06e-01\n",
      "epoch: 32   trainLoss: 2.6493e-02   valLoss:6.6990e-02  time: 4.05e-01\n",
      "epoch: 33   trainLoss: 2.5609e-02   valLoss:6.3858e-02  time: 4.01e-01\n",
      "epoch: 34   trainLoss: 2.4863e-02   valLoss:6.3140e-02  time: 4.00e-01\n",
      "epoch: 35   trainLoss: 2.3963e-02   valLoss:6.6618e-02  time: 4.06e-01\n",
      "epoch: 36   trainLoss: 2.3039e-02   valLoss:6.3187e-02  time: 4.04e-01\n",
      "epoch: 37   trainLoss: 2.2206e-02   valLoss:5.8411e-02  time: 4.08e-01\n",
      "epoch: 38   trainLoss: 2.1337e-02   valLoss:5.7270e-02  time: 4.10e-01\n",
      "epoch: 39   trainLoss: 2.0319e-02   valLoss:5.2427e-02  time: 4.03e-01\n",
      "epoch: 40   trainLoss: 1.9413e-02   valLoss:4.8732e-02  time: 4.00e-01\n",
      "epoch: 41   trainLoss: 1.8565e-02   valLoss:4.9256e-02  time: 4.03e-01\n",
      "epoch: 42   trainLoss: 1.7770e-02   valLoss:4.8278e-02  time: 4.04e-01\n",
      "epoch: 43   trainLoss: 1.7095e-02   valLoss:4.4664e-02  time: 4.06e-01\n",
      "epoch: 44   trainLoss: 1.6274e-02   valLoss:4.3109e-02  time: 4.05e-01\n",
      "epoch: 45   trainLoss: 1.5312e-02   valLoss:4.5229e-02  time: 4.00e-01\n",
      "epoch: 46   trainLoss: 1.4339e-02   valLoss:4.5733e-02  time: 4.03e-01\n",
      "epoch: 47   trainLoss: 1.3190e-02   valLoss:4.6559e-02  time: 4.02e-01\n",
      "epoch: 48   trainLoss: 1.2270e-02   valLoss:5.2506e-02  time: 4.02e-01\n",
      "epoch: 49   trainLoss: 1.1641e-02   valLoss:4.7857e-02  time: 4.02e-01\n",
      "epoch: 50   trainLoss: 1.2948e-02   valLoss:7.6445e-02  time: 4.10e-01\n",
      "epoch: 51   trainLoss: 2.3504e-02   valLoss:6.4797e-02  time: 4.10e-01\n",
      "epoch: 52   trainLoss: 2.2174e-02   valLoss:7.6339e-02  time: 4.01e-01\n",
      "epoch: 53   trainLoss: 1.8011e-02   valLoss:1.0077e-01  time: 4.01e-01\n",
      "epoch: 54   trainLoss: 1.5933e-02   valLoss:9.0045e-02  time: 4.03e-01\n",
      "epoch: 55   trainLoss: 1.4198e-02   valLoss:7.3996e-02  time: 4.04e-01\n",
      "epoch: 56   trainLoss: 1.2677e-02   valLoss:7.1458e-02  time: 4.02e-01\n",
      "epoch: 57   trainLoss: 1.1198e-02   valLoss:9.1221e-02  time: 4.11e-01\n",
      "epoch: 58   trainLoss: 8.3953e-03   valLoss:1.0958e-01  time: 4.06e-01\n",
      "epoch: 59   trainLoss: 8.6274e-03   valLoss:9.6764e-02  time: 4.03e-01\n",
      "epoch: 60   trainLoss: 6.1380e-03   valLoss:6.6307e-02  time: 4.02e-01\n",
      "epoch: 61   trainLoss: 4.5792e-03   valLoss:4.2849e-02  time: 4.00e-01\n",
      "epoch: 62   trainLoss: 3.9662e-03   valLoss:3.0448e-02  time: 4.00e-01\n",
      "epoch: 63   trainLoss: 3.4750e-03   valLoss:2.1812e-02  time: 4.04e-01\n",
      "epoch: 64   trainLoss: 3.4655e-03   valLoss:1.4469e-02  time: 4.07e-01\n",
      "epoch: 65   trainLoss: 3.6188e-03   valLoss:1.0307e-02  time: 4.04e-01\n",
      "epoch: 66   trainLoss: 3.1262e-03   valLoss:9.0844e-03  time: 4.04e-01\n",
      "epoch: 67   trainLoss: 2.4636e-03   valLoss:9.2338e-03  time: 4.01e-01\n",
      "epoch: 68   trainLoss: 2.4585e-03   valLoss:1.0936e-02  time: 4.08e-01\n",
      "epoch: 69   trainLoss: 2.1665e-03   valLoss:1.5984e-02  time: 4.03e-01\n",
      "epoch: 70   trainLoss: 2.0414e-03   valLoss:1.9042e-02  time: 4.08e-01\n",
      "epoch: 71   trainLoss: 1.8476e-03   valLoss:1.6318e-02  time: 4.05e-01\n",
      "epoch: 72   trainLoss: 1.6102e-03   valLoss:1.0928e-02  time: 4.13e-01\n",
      "epoch: 73   trainLoss: 1.6053e-03   valLoss:7.6258e-03  time: 4.02e-01\n",
      "epoch: 74   trainLoss: 1.4169e-03   valLoss:6.7293e-03  time: 4.02e-01\n",
      "epoch: 75   trainLoss: 1.2894e-03   valLoss:6.4519e-03  time: 4.04e-01\n",
      "epoch: 76   trainLoss: 1.0886e-03   valLoss:6.4618e-03  time: 4.03e-01\n",
      "epoch: 77   trainLoss: 9.5701e-04   valLoss:6.4622e-03  time: 4.05e-01\n",
      "epoch: 78   trainLoss: 1.0223e-03   valLoss:5.5023e-03  time: 4.02e-01\n",
      "epoch: 79   trainLoss: 9.2974e-04   valLoss:4.2912e-03  time: 4.03e-01\n",
      "epoch: 80   trainLoss: 8.6161e-04   valLoss:3.5338e-03  time: 4.03e-01\n",
      "epoch: 81   trainLoss: 7.6826e-04   valLoss:3.4927e-03  time: 4.03e-01\n",
      "epoch: 82   trainLoss: 7.0629e-04   valLoss:3.7069e-03  time: 4.03e-01\n",
      "epoch: 83   trainLoss: 7.1556e-04   valLoss:3.6708e-03  time: 4.04e-01\n",
      "epoch: 84   trainLoss: 6.6266e-04   valLoss:3.9264e-03  time: 4.01e-01\n",
      "epoch: 85   trainLoss: 6.3644e-04   valLoss:4.6444e-03  time: 4.00e-01\n",
      "epoch: 86   trainLoss: 5.9877e-04   valLoss:5.6367e-03  time: 4.04e-01\n",
      "epoch: 87   trainLoss: 5.6862e-04   valLoss:5.9718e-03  time: 4.01e-01\n",
      "epoch: 88   trainLoss: 5.4470e-04   valLoss:5.3908e-03  time: 4.19e-01\n",
      "epoch: 89   trainLoss: 4.9506e-04   valLoss:4.7418e-03  time: 4.13e-01\n",
      "epoch: 90   trainLoss: 4.8667e-04   valLoss:4.4197e-03  time: 4.02e-01\n",
      "epoch: 91   trainLoss: 4.2794e-04   valLoss:4.1722e-03  time: 4.02e-01\n",
      "epoch: 92   trainLoss: 4.0685e-04   valLoss:3.5690e-03  time: 4.01e-01\n",
      "epoch: 93   trainLoss: 3.9653e-04   valLoss:3.0287e-03  time: 3.94e-01\n",
      "epoch: 94   trainLoss: 4.0301e-04   valLoss:3.1051e-03  time: 3.96e-01\n",
      "epoch: 95   trainLoss: 3.6535e-04   valLoss:3.4864e-03  time: 3.94e-01\n",
      "epoch: 96   trainLoss: 3.4290e-04   valLoss:3.4108e-03  time: 3.94e-01\n",
      "epoch: 97   trainLoss: 3.4214e-04   valLoss:2.9663e-03  time: 4.11e-01\n",
      "epoch: 98   trainLoss: 3.2441e-04   valLoss:2.8336e-03  time: 4.01e-01\n",
      "epoch: 99   trainLoss: 2.9808e-04   valLoss:3.0502e-03  time: 4.02e-01\n",
      "loading checkpoint 98\n",
      "trained 24 random forest models in 3.12 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_5/\n",
      "loaded train set of size 4\n",
      "epoch: 0   trainLoss: 9.7133e-01   valLoss:9.8627e-01  time: 5.16e-02\n",
      "epoch: 1   trainLoss: 7.1883e-01   valLoss:9.4785e-01  time: 5.01e-02\n",
      "epoch: 2   trainLoss: 5.3311e-01   valLoss:9.1483e-01  time: 5.09e-02\n",
      "epoch: 3   trainLoss: 3.7954e-01   valLoss:8.7340e-01  time: 5.22e-02\n",
      "epoch: 4   trainLoss: 2.8335e-01   valLoss:8.2700e-01  time: 5.06e-02\n",
      "epoch: 5   trainLoss: 1.7575e-01   valLoss:7.6772e-01  time: 5.07e-02\n",
      "epoch: 6   trainLoss: 1.1750e-01   valLoss:7.1705e-01  time: 5.07e-02\n",
      "epoch: 7   trainLoss: 8.8696e-02   valLoss:6.6757e-01  time: 5.25e-02\n",
      "epoch: 8   trainLoss: 8.3515e-02   valLoss:6.2846e-01  time: 5.20e-02\n",
      "epoch: 9   trainLoss: 8.6949e-02   valLoss:5.9520e-01  time: 5.09e-02\n",
      "epoch: 10   trainLoss: 9.3184e-02   valLoss:5.6158e-01  time: 5.14e-02\n",
      "epoch: 11   trainLoss: 9.5298e-02   valLoss:5.3659e-01  time: 5.04e-02\n",
      "epoch: 12   trainLoss: 9.3065e-02   valLoss:5.2027e-01  time: 5.06e-02\n",
      "epoch: 13   trainLoss: 9.1078e-02   valLoss:5.1228e-01  time: 5.01e-02\n",
      "epoch: 14   trainLoss: 8.1392e-02   valLoss:5.0246e-01  time: 5.40e-02\n",
      "epoch: 15   trainLoss: 7.3539e-02   valLoss:4.8506e-01  time: 5.17e-02\n",
      "epoch: 16   trainLoss: 6.6872e-02   valLoss:4.5903e-01  time: 5.18e-02\n",
      "epoch: 17   trainLoss: 6.4264e-02   valLoss:4.2393e-01  time: 5.02e-02\n",
      "epoch: 18   trainLoss: 6.3411e-02   valLoss:3.9351e-01  time: 5.82e-02\n",
      "epoch: 19   trainLoss: 6.3246e-02   valLoss:3.6343e-01  time: 5.10e-02\n",
      "epoch: 20   trainLoss: 6.3813e-02   valLoss:3.2515e-01  time: 5.02e-02\n",
      "epoch: 21   trainLoss: 6.3981e-02   valLoss:2.8109e-01  time: 5.11e-02\n",
      "epoch: 22   trainLoss: 6.3188e-02   valLoss:2.4162e-01  time: 5.22e-02\n",
      "epoch: 23   trainLoss: 6.1694e-02   valLoss:2.0864e-01  time: 5.09e-02\n",
      "epoch: 24   trainLoss: 6.0279e-02   valLoss:1.8301e-01  time: 5.05e-02\n",
      "epoch: 25   trainLoss: 5.9518e-02   valLoss:1.6433e-01  time: 5.09e-02\n",
      "epoch: 26   trainLoss: 5.8762e-02   valLoss:1.4867e-01  time: 5.10e-02\n",
      "epoch: 27   trainLoss: 5.7602e-02   valLoss:1.3625e-01  time: 5.03e-02\n",
      "epoch: 28   trainLoss: 5.6572e-02   valLoss:1.2591e-01  time: 5.24e-02\n",
      "epoch: 29   trainLoss: 5.5330e-02   valLoss:1.1637e-01  time: 5.07e-02\n",
      "epoch: 30   trainLoss: 5.4367e-02   valLoss:1.0841e-01  time: 5.08e-02\n",
      "epoch: 31   trainLoss: 5.3661e-02   valLoss:1.0230e-01  time: 5.08e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32   trainLoss: 5.2357e-02   valLoss:9.8048e-02  time: 5.07e-02\n",
      "epoch: 33   trainLoss: 5.1082e-02   valLoss:9.4907e-02  time: 5.03e-02\n",
      "epoch: 34   trainLoss: 4.9747e-02   valLoss:9.2386e-02  time: 5.20e-02\n",
      "epoch: 35   trainLoss: 4.8743e-02   valLoss:9.0654e-02  time: 5.10e-02\n",
      "epoch: 36   trainLoss: 4.7397e-02   valLoss:8.9389e-02  time: 5.00e-02\n",
      "epoch: 37   trainLoss: 4.6139e-02   valLoss:8.8371e-02  time: 5.05e-02\n",
      "epoch: 38   trainLoss: 4.4532e-02   valLoss:8.7451e-02  time: 5.10e-02\n",
      "epoch: 39   trainLoss: 4.2919e-02   valLoss:8.7071e-02  time: 5.01e-02\n",
      "epoch: 40   trainLoss: 4.0899e-02   valLoss:8.7737e-02  time: 5.10e-02\n",
      "epoch: 41   trainLoss: 3.8165e-02   valLoss:8.8476e-02  time: 5.25e-02\n",
      "epoch: 42   trainLoss: 3.5084e-02   valLoss:8.8115e-02  time: 5.06e-02\n",
      "epoch: 43   trainLoss: 3.1829e-02   valLoss:8.7329e-02  time: 5.02e-02\n",
      "epoch: 44   trainLoss: 2.8639e-02   valLoss:8.5287e-02  time: 5.05e-02\n",
      "epoch: 45   trainLoss: 2.6858e-02   valLoss:8.2961e-02  time: 5.08e-02\n",
      "epoch: 46   trainLoss: 2.4894e-02   valLoss:8.0593e-02  time: 5.06e-02\n",
      "epoch: 47   trainLoss: 2.7092e-02   valLoss:7.8486e-02  time: 5.03e-02\n",
      "epoch: 48   trainLoss: 2.5197e-02   valLoss:7.5690e-02  time: 5.24e-02\n",
      "epoch: 49   trainLoss: 2.4809e-02   valLoss:8.1169e-02  time: 5.06e-02\n",
      "epoch: 50   trainLoss: 2.1406e-02   valLoss:8.1512e-02  time: 5.01e-02\n",
      "epoch: 51   trainLoss: 2.0700e-02   valLoss:7.6540e-02  time: 5.01e-02\n",
      "epoch: 52   trainLoss: 1.9103e-02   valLoss:7.1452e-02  time: 5.00e-02\n",
      "epoch: 53   trainLoss: 1.8733e-02   valLoss:7.0302e-02  time: 5.10e-02\n",
      "epoch: 54   trainLoss: 1.7829e-02   valLoss:7.2371e-02  time: 5.09e-02\n",
      "epoch: 55   trainLoss: 1.6536e-02   valLoss:7.7424e-02  time: 5.13e-02\n",
      "epoch: 56   trainLoss: 1.4766e-02   valLoss:8.4083e-02  time: 5.11e-02\n",
      "epoch: 57   trainLoss: 1.4121e-02   valLoss:8.5957e-02  time: 4.99e-02\n",
      "epoch: 58   trainLoss: 1.3474e-02   valLoss:8.4026e-02  time: 5.03e-02\n",
      "epoch: 59   trainLoss: 1.3123e-02   valLoss:8.2944e-02  time: 5.02e-02\n",
      "epoch: 60   trainLoss: 1.2495e-02   valLoss:8.2132e-02  time: 5.03e-02\n",
      "epoch: 61   trainLoss: 1.2549e-02   valLoss:8.1181e-02  time: 5.00e-02\n",
      "epoch: 62   trainLoss: 1.2265e-02   valLoss:8.1103e-02  time: 5.01e-02\n",
      "epoch: 63   trainLoss: 1.2301e-02   valLoss:8.2093e-02  time: 5.09e-02\n",
      "epoch: 64   trainLoss: 1.2048e-02   valLoss:8.2983e-02  time: 5.62e-02\n",
      "epoch: 65   trainLoss: 1.2113e-02   valLoss:8.3157e-02  time: 5.18e-02\n",
      "epoch: 66   trainLoss: 1.2038e-02   valLoss:8.2621e-02  time: 5.08e-02\n",
      "epoch: 67   trainLoss: 1.2074e-02   valLoss:8.1609e-02  time: 5.54e-02\n",
      "epoch: 68   trainLoss: 1.1949e-02   valLoss:8.0851e-02  time: 5.00e-02\n",
      "epoch: 69   trainLoss: 1.1877e-02   valLoss:8.0758e-02  time: 5.03e-02\n",
      "epoch: 70   trainLoss: 1.1765e-02   valLoss:8.1220e-02  time: 5.03e-02\n",
      "epoch: 71   trainLoss: 1.1721e-02   valLoss:8.1786e-02  time: 5.05e-02\n",
      "epoch: 72   trainLoss: 1.1601e-02   valLoss:8.2419e-02  time: 5.01e-02\n",
      "epoch: 73   trainLoss: 1.1519e-02   valLoss:8.3446e-02  time: 5.03e-02\n",
      "epoch: 74   trainLoss: 1.1376e-02   valLoss:8.4623e-02  time: 5.06e-02\n",
      "epoch: 75   trainLoss: 1.1297e-02   valLoss:8.5620e-02  time: 5.13e-02\n",
      "epoch: 76   trainLoss: 1.1220e-02   valLoss:8.8000e-02  time: 5.04e-02\n",
      "epoch: 77   trainLoss: 1.2970e-02   valLoss:8.6518e-02  time: 5.09e-02\n",
      "epoch: 78   trainLoss: 1.8831e-02   valLoss:8.4151e-02  time: 5.04e-02\n",
      "epoch: 79   trainLoss: 1.5737e-02   valLoss:8.1700e-02  time: 5.06e-02\n",
      "epoch: 80   trainLoss: 1.3381e-02   valLoss:8.1200e-02  time: 5.00e-02\n",
      "epoch: 81   trainLoss: 1.3422e-02   valLoss:8.2062e-02  time: 5.03e-02\n",
      "epoch: 82   trainLoss: 1.3607e-02   valLoss:8.4861e-02  time: 5.05e-02\n",
      "epoch: 83   trainLoss: 1.3585e-02   valLoss:8.8645e-02  time: 5.11e-02\n",
      "epoch: 84   trainLoss: 1.3408e-02   valLoss:9.2220e-02  time: 5.02e-02\n",
      "epoch: 85   trainLoss: 1.3004e-02   valLoss:9.3416e-02  time: 5.11e-02\n",
      "epoch: 86   trainLoss: 1.2436e-02   valLoss:9.1530e-02  time: 5.18e-02\n",
      "epoch: 87   trainLoss: 1.2328e-02   valLoss:8.8897e-02  time: 5.73e-02\n",
      "epoch: 88   trainLoss: 1.2457e-02   valLoss:8.8443e-02  time: 5.19e-02\n",
      "epoch: 89   trainLoss: 1.2460e-02   valLoss:9.0680e-02  time: 5.19e-02\n",
      "epoch: 90   trainLoss: 1.2292e-02   valLoss:9.4323e-02  time: 5.04e-02\n",
      "epoch: 91   trainLoss: 1.2144e-02   valLoss:9.7159e-02  time: 5.08e-02\n",
      "epoch: 92   trainLoss: 1.2053e-02   valLoss:9.8877e-02  time: 5.01e-02\n",
      "epoch: 93   trainLoss: 1.1975e-02   valLoss:1.0041e-01  time: 5.03e-02\n",
      "epoch: 94   trainLoss: 1.2015e-02   valLoss:1.0205e-01  time: 5.01e-02\n",
      "epoch: 95   trainLoss: 1.2023e-02   valLoss:1.0343e-01  time: 5.15e-02\n",
      "epoch: 96   trainLoss: 1.1946e-02   valLoss:1.0454e-01  time: 5.03e-02\n",
      "epoch: 97   trainLoss: 1.1900e-02   valLoss:1.0587e-01  time: 5.01e-02\n",
      "epoch: 98   trainLoss: 1.1853e-02   valLoss:1.0772e-01  time: 4.99e-02\n",
      "epoch: 99   trainLoss: 1.1833e-02   valLoss:1.0941e-01  time: 5.05e-02\n",
      "loading checkpoint 53\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.0303e+00   valLoss:1.7963e+02  time: 5.08e-02\n",
      "epoch: 1   trainLoss: 4.5099e-01   valLoss:4.5556e+01  time: 5.08e-02\n",
      "epoch: 2   trainLoss: 1.8959e-01   valLoss:1.9522e+01  time: 5.16e-02\n",
      "epoch: 3   trainLoss: 1.5405e-01   valLoss:9.3816e+00  time: 5.05e-02\n",
      "epoch: 4   trainLoss: 8.8900e-02   valLoss:5.0546e+00  time: 5.06e-02\n",
      "epoch: 5   trainLoss: 8.1916e-02   valLoss:2.7492e+00  time: 5.11e-02\n",
      "epoch: 6   trainLoss: 7.9866e-02   valLoss:2.2058e+00  time: 5.05e-02\n",
      "epoch: 7   trainLoss: 7.9364e-02   valLoss:1.6952e+00  time: 5.10e-02\n",
      "epoch: 8   trainLoss: 7.1056e-02   valLoss:1.1936e+00  time: 5.23e-02\n",
      "epoch: 9   trainLoss: 6.3383e-02   valLoss:8.2354e-01  time: 5.07e-02\n",
      "epoch: 10   trainLoss: 5.6884e-02   valLoss:5.1496e-01  time: 5.08e-02\n",
      "epoch: 11   trainLoss: 5.2187e-02   valLoss:3.5949e-01  time: 5.16e-02\n",
      "epoch: 12   trainLoss: 4.8458e-02   valLoss:2.3962e-01  time: 5.03e-02\n",
      "epoch: 13   trainLoss: 4.5744e-02   valLoss:1.6816e-01  time: 5.05e-02\n",
      "epoch: 14   trainLoss: 4.5257e-02   valLoss:1.2490e-01  time: 5.24e-02\n",
      "epoch: 15   trainLoss: 4.3590e-02   valLoss:9.5034e-02  time: 5.04e-02\n",
      "epoch: 16   trainLoss: 4.3014e-02   valLoss:7.3846e-02  time: 5.03e-02\n",
      "epoch: 17   trainLoss: 4.0368e-02   valLoss:6.0149e-02  time: 5.08e-02\n",
      "epoch: 18   trainLoss: 3.9289e-02   valLoss:4.9296e-02  time: 5.07e-02\n",
      "epoch: 19   trainLoss: 3.8026e-02   valLoss:4.5044e-02  time: 6.48e-02\n",
      "epoch: 20   trainLoss: 3.5768e-02   valLoss:4.4231e-02  time: 5.22e-02\n",
      "epoch: 21   trainLoss: 3.5366e-02   valLoss:4.4747e-02  time: 5.18e-02\n",
      "epoch: 22   trainLoss: 3.4718e-02   valLoss:4.6855e-02  time: 5.08e-02\n",
      "epoch: 23   trainLoss: 3.4819e-02   valLoss:4.8489e-02  time: 5.07e-02\n",
      "epoch: 24   trainLoss: 3.4278e-02   valLoss:5.1727e-02  time: 5.02e-02\n",
      "epoch: 25   trainLoss: 3.4471e-02   valLoss:5.6594e-02  time: 5.01e-02\n",
      "epoch: 26   trainLoss: 3.3940e-02   valLoss:6.1538e-02  time: 5.09e-02\n",
      "epoch: 27   trainLoss: 3.3872e-02   valLoss:6.3408e-02  time: 5.02e-02\n",
      "epoch: 28   trainLoss: 3.3410e-02   valLoss:6.3915e-02  time: 5.06e-02\n",
      "epoch: 29   trainLoss: 3.3185e-02   valLoss:6.5214e-02  time: 4.99e-02\n",
      "epoch: 30   trainLoss: 3.2917e-02   valLoss:6.7171e-02  time: 4.97e-02\n",
      "epoch: 31   trainLoss: 3.2333e-02   valLoss:6.8010e-02  time: 4.93e-02\n",
      "epoch: 32   trainLoss: 3.2045e-02   valLoss:6.6167e-02  time: 4.95e-02\n",
      "epoch: 33   trainLoss: 3.1375e-02   valLoss:6.3406e-02  time: 4.94e-02\n",
      "epoch: 34   trainLoss: 3.0982e-02   valLoss:6.0532e-02  time: 4.91e-02\n",
      "epoch: 35   trainLoss: 3.0553e-02   valLoss:5.7714e-02  time: 4.90e-02\n",
      "epoch: 36   trainLoss: 3.0198e-02   valLoss:5.4411e-02  time: 4.95e-02\n",
      "epoch: 37   trainLoss: 3.0091e-02   valLoss:5.0531e-02  time: 4.91e-02\n",
      "epoch: 38   trainLoss: 2.9930e-02   valLoss:4.7673e-02  time: 5.01e-02\n",
      "epoch: 39   trainLoss: 2.9833e-02   valLoss:4.6295e-02  time: 4.98e-02\n",
      "epoch: 40   trainLoss: 2.9765e-02   valLoss:4.6026e-02  time: 5.32e-02\n",
      "epoch: 41   trainLoss: 2.9674e-02   valLoss:4.6234e-02  time: 4.93e-02\n",
      "epoch: 42   trainLoss: 2.9670e-02   valLoss:4.6507e-02  time: 4.98e-02\n",
      "epoch: 43   trainLoss: 2.9568e-02   valLoss:4.7442e-02  time: 4.95e-02\n",
      "epoch: 44   trainLoss: 2.9521e-02   valLoss:4.9386e-02  time: 4.90e-02\n",
      "epoch: 45   trainLoss: 2.9467e-02   valLoss:5.1956e-02  time: 4.95e-02\n",
      "epoch: 46   trainLoss: 2.9398e-02   valLoss:5.4406e-02  time: 5.17e-02\n",
      "epoch: 47   trainLoss: 2.9361e-02   valLoss:5.6148e-02  time: 4.98e-02\n",
      "epoch: 48   trainLoss: 2.9239e-02   valLoss:5.7509e-02  time: 4.88e-02\n",
      "epoch: 49   trainLoss: 2.9132e-02   valLoss:5.8928e-02  time: 4.96e-02\n",
      "epoch: 50   trainLoss: 2.9014e-02   valLoss:6.0939e-02  time: 5.05e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51   trainLoss: 2.8886e-02   valLoss:6.3328e-02  time: 4.94e-02\n",
      "epoch: 52   trainLoss: 2.8798e-02   valLoss:6.5859e-02  time: 4.93e-02\n",
      "epoch: 53   trainLoss: 2.8687e-02   valLoss:6.8317e-02  time: 4.95e-02\n",
      "epoch: 54   trainLoss: 2.8588e-02   valLoss:7.0921e-02  time: 4.88e-02\n",
      "epoch: 55   trainLoss: 2.8470e-02   valLoss:7.3852e-02  time: 4.92e-02\n",
      "epoch: 56   trainLoss: 2.8311e-02   valLoss:7.6663e-02  time: 5.06e-02\n",
      "epoch: 57   trainLoss: 2.8103e-02   valLoss:7.9071e-02  time: 4.89e-02\n",
      "epoch: 58   trainLoss: 2.7828e-02   valLoss:8.1022e-02  time: 4.90e-02\n",
      "epoch: 59   trainLoss: 2.7406e-02   valLoss:8.3302e-02  time: 5.16e-02\n",
      "epoch: 60   trainLoss: 2.6818e-02   valLoss:8.5961e-02  time: 4.95e-02\n",
      "epoch: 61   trainLoss: 2.6114e-02   valLoss:8.4080e-02  time: 4.98e-02\n",
      "epoch: 62   trainLoss: 2.5178e-02   valLoss:7.4569e-02  time: 4.93e-02\n",
      "epoch: 63   trainLoss: 2.4397e-02   valLoss:7.6472e-02  time: 4.94e-02\n",
      "epoch: 64   trainLoss: 2.3756e-02   valLoss:7.5360e-02  time: 4.88e-02\n",
      "epoch: 65   trainLoss: 2.0660e-02   valLoss:7.1791e-02  time: 4.97e-02\n",
      "epoch: 66   trainLoss: 2.5842e-02   valLoss:6.7795e-02  time: 4.96e-02\n",
      "epoch: 67   trainLoss: 1.8806e-02   valLoss:6.1515e-02  time: 5.07e-02\n",
      "epoch: 68   trainLoss: 1.8726e-02   valLoss:7.2191e-02  time: 4.96e-02\n",
      "epoch: 69   trainLoss: 2.0944e-02   valLoss:4.8683e-02  time: 4.89e-02\n",
      "epoch: 70   trainLoss: 2.2680e-02   valLoss:4.8387e-02  time: 4.97e-02\n",
      "epoch: 71   trainLoss: 2.4995e-02   valLoss:5.5920e-02  time: 4.97e-02\n",
      "epoch: 72   trainLoss: 2.2864e-02   valLoss:5.8236e-02  time: 4.93e-02\n",
      "epoch: 73   trainLoss: 2.0179e-02   valLoss:5.8579e-02  time: 5.07e-02\n",
      "epoch: 74   trainLoss: 2.0503e-02   valLoss:5.6400e-02  time: 4.90e-02\n",
      "epoch: 75   trainLoss: 1.9058e-02   valLoss:5.4480e-02  time: 4.93e-02\n",
      "epoch: 76   trainLoss: 1.8471e-02   valLoss:5.1011e-02  time: 4.95e-02\n",
      "epoch: 77   trainLoss: 1.9043e-02   valLoss:4.7168e-02  time: 4.94e-02\n",
      "epoch: 78   trainLoss: 1.8799e-02   valLoss:4.9073e-02  time: 4.98e-02\n",
      "epoch: 79   trainLoss: 1.7843e-02   valLoss:5.4932e-02  time: 4.89e-02\n",
      "epoch: 80   trainLoss: 1.7339e-02   valLoss:5.7870e-02  time: 5.02e-02\n",
      "epoch: 81   trainLoss: 1.7070e-02   valLoss:5.4898e-02  time: 4.97e-02\n",
      "epoch: 82   trainLoss: 1.6732e-02   valLoss:4.5057e-02  time: 4.98e-02\n",
      "epoch: 83   trainLoss: 1.5635e-02   valLoss:4.2789e-02  time: 4.96e-02\n",
      "epoch: 84   trainLoss: 1.5036e-02   valLoss:4.2912e-02  time: 4.96e-02\n",
      "epoch: 85   trainLoss: 1.3702e-02   valLoss:4.5308e-02  time: 4.90e-02\n",
      "epoch: 86   trainLoss: 1.1911e-02   valLoss:5.1090e-02  time: 4.97e-02\n",
      "epoch: 87   trainLoss: 9.8723e-03   valLoss:5.3738e-02  time: 4.89e-02\n",
      "epoch: 88   trainLoss: 7.4698e-03   valLoss:6.1575e-02  time: 4.99e-02\n",
      "epoch: 89   trainLoss: 6.2635e-03   valLoss:5.5566e-02  time: 4.90e-02\n",
      "epoch: 90   trainLoss: 4.1572e-03   valLoss:5.8185e-02  time: 4.96e-02\n",
      "epoch: 91   trainLoss: 1.9355e-03   valLoss:5.9841e-02  time: 4.91e-02\n",
      "epoch: 92   trainLoss: 2.2125e-03   valLoss:5.5638e-02  time: 4.96e-02\n",
      "epoch: 93   trainLoss: 1.9059e-03   valLoss:4.7981e-02  time: 4.92e-02\n",
      "epoch: 94   trainLoss: 1.6366e-03   valLoss:4.1837e-02  time: 4.88e-02\n",
      "epoch: 95   trainLoss: 1.5821e-03   valLoss:3.8365e-02  time: 4.95e-02\n",
      "epoch: 96   trainLoss: 1.1006e-03   valLoss:3.6879e-02  time: 4.97e-02\n",
      "epoch: 97   trainLoss: 1.1853e-03   valLoss:3.6851e-02  time: 4.91e-02\n",
      "epoch: 98   trainLoss: 1.1754e-03   valLoss:3.9144e-02  time: 4.92e-02\n",
      "epoch: 99   trainLoss: 7.9507e-04   valLoss:4.1825e-02  time: 4.97e-02\n",
      "loading checkpoint 97\n",
      "trained 24 random forest models in 2.59 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_1000_v2/\n",
      "loaded train set of size 909\n",
      "epoch: 0   trainLoss: 5.5129e-01   valLoss:8.6796e-01  time: 7.35e+00\n",
      "epoch: 1   trainLoss: 1.3632e-01   valLoss:6.7440e-01  time: 7.49e+00\n",
      "epoch: 2   trainLoss: 7.3826e-02   valLoss:5.8108e-01  time: 7.35e+00\n",
      "epoch: 3   trainLoss: 5.2237e-02   valLoss:5.3648e-01  time: 7.34e+00\n",
      "epoch: 4   trainLoss: 3.6356e-02   valLoss:3.2463e-01  time: 7.30e+00\n",
      "epoch: 5   trainLoss: 3.1143e-02   valLoss:1.3797e-01  time: 7.27e+00\n",
      "epoch: 6   trainLoss: 2.6291e-02   valLoss:7.7493e-02  time: 7.30e+00\n",
      "epoch: 7   trainLoss: 2.3040e-02   valLoss:5.3657e-02  time: 7.40e+00\n",
      "epoch: 8   trainLoss: 2.1610e-02   valLoss:4.3740e-02  time: 7.31e+00\n",
      "epoch: 9   trainLoss: 1.8613e-02   valLoss:3.8680e-02  time: 7.21e+00\n",
      "epoch: 10   trainLoss: 1.5841e-02   valLoss:3.5124e-02  time: 7.27e+00\n",
      "epoch: 11   trainLoss: 1.5229e-02   valLoss:3.0053e-02  time: 7.21e+00\n",
      "epoch: 12   trainLoss: 1.4638e-02   valLoss:2.8078e-02  time: 7.34e+00\n",
      "epoch: 13   trainLoss: 1.4074e-02   valLoss:2.8027e-02  time: 7.33e+00\n",
      "epoch: 14   trainLoss: 1.2274e-02   valLoss:2.8084e-02  time: 7.44e+00\n",
      "epoch: 15   trainLoss: 1.1135e-02   valLoss:2.7487e-02  time: 7.33e+00\n",
      "epoch: 16   trainLoss: 8.0985e-03   valLoss:3.1153e-02  time: 7.39e+00\n",
      "epoch: 17   trainLoss: 7.5302e-03   valLoss:4.0950e-02  time: 7.33e+00\n",
      "epoch: 18   trainLoss: 3.9507e-03   valLoss:3.4567e-02  time: 7.33e+00\n",
      "epoch: 19   trainLoss: 3.8919e-03   valLoss:1.8201e-02  time: 7.30e+00\n",
      "epoch: 20   trainLoss: 4.3500e-03   valLoss:1.7196e-02  time: 7.46e+00\n",
      "epoch: 21   trainLoss: 3.2406e-03   valLoss:1.9325e-02  time: 7.33e+00\n",
      "epoch: 22   trainLoss: 2.7215e-03   valLoss:2.6649e-02  time: 7.33e+00\n",
      "epoch: 23   trainLoss: 3.0235e-03   valLoss:4.2505e-02  time: 7.37e+00\n",
      "epoch: 24   trainLoss: 4.9130e-03   valLoss:2.1780e-02  time: 7.31e+00\n",
      "epoch: 25   trainLoss: 2.9901e-03   valLoss:1.2542e-02  time: 7.30e+00\n",
      "epoch: 26   trainLoss: 3.7241e-03   valLoss:1.5044e-02  time: 7.26e+00\n",
      "epoch: 27   trainLoss: 2.0860e-03   valLoss:1.9423e-02  time: 7.31e+00\n",
      "epoch: 28   trainLoss: 1.7011e-03   valLoss:1.6358e-02  time: 7.31e+00\n",
      "epoch: 29   trainLoss: 2.2873e-03   valLoss:2.4537e-02  time: 7.30e+00\n",
      "epoch: 30   trainLoss: 2.6629e-03   valLoss:1.4255e-02  time: 7.30e+00\n",
      "epoch: 31   trainLoss: 1.8842e-03   valLoss:1.0974e-02  time: 7.27e+00\n",
      "epoch: 32   trainLoss: 3.0092e-03   valLoss:1.5497e-02  time: 7.25e+00\n",
      "epoch: 33   trainLoss: 1.7236e-03   valLoss:1.6820e-02  time: 7.39e+00\n",
      "epoch: 34   trainLoss: 1.7884e-03   valLoss:1.9007e-02  time: 7.26e+00\n",
      "epoch: 35   trainLoss: 1.9332e-03   valLoss:1.5683e-02  time: 7.30e+00\n",
      "epoch: 36   trainLoss: 1.0232e-03   valLoss:2.2992e-02  time: 7.28e+00\n",
      "epoch: 37   trainLoss: 1.0741e-03   valLoss:2.0310e-02  time: 7.35e+00\n",
      "epoch: 38   trainLoss: 1.3286e-03   valLoss:2.2069e-02  time: 7.33e+00\n",
      "epoch: 39   trainLoss: 1.5004e-03   valLoss:2.1160e-02  time: 7.31e+00\n",
      "epoch: 40   trainLoss: 1.6122e-03   valLoss:2.3260e-02  time: 7.37e+00\n",
      "epoch: 41   trainLoss: 1.5071e-03   valLoss:2.1088e-02  time: 7.38e+00\n",
      "epoch: 42   trainLoss: 1.3727e-03   valLoss:2.1895e-02  time: 7.31e+00\n",
      "epoch: 43   trainLoss: 1.1979e-03   valLoss:2.9210e-02  time: 7.34e+00\n",
      "epoch: 44   trainLoss: 9.4041e-04   valLoss:2.3504e-02  time: 7.33e+00\n",
      "epoch: 45   trainLoss: 2.0903e-03   valLoss:2.4911e-02  time: 7.36e+00\n",
      "epoch: 46   trainLoss: 1.8617e-03   valLoss:1.8570e-02  time: 7.27e+00\n",
      "epoch: 47   trainLoss: 3.0963e-03   valLoss:1.4324e-02  time: 7.28e+00\n",
      "epoch: 48   trainLoss: 3.5823e-03   valLoss:2.6370e-02  time: 7.17e+00\n",
      "epoch: 49   trainLoss: 2.5332e-03   valLoss:1.8074e-02  time: 7.16e+00\n",
      "epoch: 50   trainLoss: 1.9779e-03   valLoss:1.9099e-02  time: 7.05e+00\n",
      "epoch: 51   trainLoss: 1.8091e-03   valLoss:1.9407e-02  time: 7.06e+00\n",
      "epoch: 52   trainLoss: 9.9291e-04   valLoss:2.3824e-02  time: 7.05e+00\n",
      "epoch: 53   trainLoss: 1.6007e-03   valLoss:1.4119e-02  time: 7.17e+00\n",
      "epoch: 54   trainLoss: 1.5363e-03   valLoss:2.3096e-02  time: 7.14e+00\n",
      "epoch: 55   trainLoss: 8.2567e-04   valLoss:2.1575e-02  time: 7.14e+00\n",
      "epoch: 56   trainLoss: 6.2390e-04   valLoss:3.4052e-02  time: 7.16e+00\n",
      "epoch: 57   trainLoss: 1.2258e-03   valLoss:3.4043e-02  time: 7.06e+00\n",
      "epoch: 58   trainLoss: 1.2781e-03   valLoss:2.1669e-02  time: 7.14e+00\n",
      "epoch: 59   trainLoss: 1.3328e-03   valLoss:2.7117e-02  time: 7.10e+00\n",
      "epoch: 60   trainLoss: 1.7465e-03   valLoss:3.4089e-02  time: 7.35e+00\n",
      "epoch: 61   trainLoss: 2.1088e-03   valLoss:2.4832e-02  time: 7.11e+00\n",
      "epoch: 62   trainLoss: 1.2703e-03   valLoss:2.2566e-02  time: 7.05e+00\n",
      "epoch: 63   trainLoss: 1.5680e-03   valLoss:1.9528e-02  time: 7.23e+00\n",
      "epoch: 64   trainLoss: 1.1743e-03   valLoss:2.3587e-02  time: 7.29e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65   trainLoss: 1.1035e-03   valLoss:2.3847e-02  time: 7.32e+00\n",
      "epoch: 66   trainLoss: 1.6882e-03   valLoss:1.4350e-02  time: 7.29e+00\n",
      "epoch: 67   trainLoss: 1.7676e-03   valLoss:2.1857e-02  time: 7.30e+00\n",
      "epoch: 68   trainLoss: 1.8205e-03   valLoss:2.0415e-02  time: 7.22e+00\n",
      "epoch: 69   trainLoss: 5.0439e-03   valLoss:9.8256e-03  time: 7.31e+00\n",
      "epoch: 70   trainLoss: 3.1040e-03   valLoss:1.3885e-02  time: 7.36e+00\n",
      "epoch: 71   trainLoss: 2.0010e-03   valLoss:1.1771e-02  time: 7.30e+00\n",
      "epoch: 72   trainLoss: 1.8398e-03   valLoss:5.3538e-03  time: 7.05e+00\n",
      "epoch: 73   trainLoss: 1.8904e-03   valLoss:6.7707e-03  time: 7.13e+00\n",
      "epoch: 74   trainLoss: 3.3174e-03   valLoss:7.7292e-03  time: 7.11e+00\n",
      "epoch: 75   trainLoss: 2.2901e-03   valLoss:1.3072e-02  time: 7.06e+00\n",
      "epoch: 76   trainLoss: 1.8028e-03   valLoss:1.4231e-02  time: 7.18e+00\n",
      "epoch: 77   trainLoss: 1.6744e-03   valLoss:1.5302e-02  time: 7.23e+00\n",
      "epoch: 78   trainLoss: 1.5429e-03   valLoss:1.9527e-02  time: 7.17e+00\n",
      "epoch: 79   trainLoss: 9.3874e-04   valLoss:1.5950e-02  time: 7.30e+00\n",
      "epoch: 80   trainLoss: 8.0779e-04   valLoss:1.4489e-02  time: 7.34e+00\n",
      "epoch: 81   trainLoss: 1.0124e-03   valLoss:2.2691e-02  time: 7.12e+00\n",
      "epoch: 82   trainLoss: 1.0699e-03   valLoss:1.7197e-02  time: 7.20e+00\n",
      "epoch: 83   trainLoss: 8.7942e-04   valLoss:3.1615e-02  time: 7.25e+00\n",
      "epoch: 84   trainLoss: 1.6552e-03   valLoss:2.0156e-02  time: 7.34e+00\n",
      "epoch: 85   trainLoss: 1.6479e-03   valLoss:1.4121e-02  time: 7.34e+00\n",
      "epoch: 86   trainLoss: 1.5324e-03   valLoss:1.6985e-02  time: 7.42e+00\n",
      "epoch: 87   trainLoss: 3.0172e-03   valLoss:1.9383e-02  time: 7.35e+00\n",
      "epoch: 88   trainLoss: 1.7492e-03   valLoss:2.1974e-02  time: 7.37e+00\n",
      "epoch: 89   trainLoss: 3.2174e-03   valLoss:1.7896e-02  time: 7.27e+00\n",
      "epoch: 90   trainLoss: 1.6349e-03   valLoss:1.3248e-02  time: 7.31e+00\n",
      "epoch: 91   trainLoss: 1.7191e-03   valLoss:7.6597e-03  time: 7.31e+00\n",
      "epoch: 92   trainLoss: 1.0444e-03   valLoss:6.8554e-03  time: 7.38e+00\n",
      "epoch: 93   trainLoss: 8.3055e-04   valLoss:8.2306e-03  time: 7.42e+00\n",
      "epoch: 94   trainLoss: 1.2365e-03   valLoss:9.9339e-03  time: 7.37e+00\n",
      "epoch: 95   trainLoss: 1.2262e-03   valLoss:1.5460e-02  time: 7.35e+00\n",
      "epoch: 96   trainLoss: 7.1022e-04   valLoss:1.3498e-02  time: 7.30e+00\n",
      "epoch: 97   trainLoss: 1.2619e-03   valLoss:1.4240e-02  time: 7.36e+00\n",
      "epoch: 98   trainLoss: 1.5930e-03   valLoss:1.3377e-02  time: 7.31e+00\n",
      "epoch: 99   trainLoss: 1.2011e-03   valLoss:1.4842e-02  time: 7.42e+00\n",
      "loading checkpoint 72\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 4.8173e-01   valLoss:2.9414e+00  time: 7.34e+00\n",
      "epoch: 1   trainLoss: 8.8616e-02   valLoss:1.1062e+00  time: 7.36e+00\n",
      "epoch: 2   trainLoss: 6.9966e-02   valLoss:1.8594e-01  time: 7.31e+00\n",
      "epoch: 3   trainLoss: 6.1158e-02   valLoss:8.5444e-02  time: 7.27e+00\n",
      "epoch: 4   trainLoss: 5.0331e-02   valLoss:6.4640e-02  time: 7.32e+00\n",
      "epoch: 5   trainLoss: 4.3499e-02   valLoss:5.0371e-02  time: 7.33e+00\n",
      "epoch: 6   trainLoss: 3.8860e-02   valLoss:4.1921e-02  time: 7.32e+00\n",
      "epoch: 7   trainLoss: 3.3838e-02   valLoss:3.5592e-02  time: 7.33e+00\n",
      "epoch: 8   trainLoss: 2.9258e-02   valLoss:3.4076e-02  time: 7.21e+00\n",
      "epoch: 9   trainLoss: 2.4687e-02   valLoss:3.8202e-02  time: 7.32e+00\n",
      "epoch: 10   trainLoss: 2.0245e-02   valLoss:4.2511e-02  time: 7.28e+00\n",
      "epoch: 11   trainLoss: 1.6772e-02   valLoss:3.6835e-02  time: 7.38e+00\n",
      "epoch: 12   trainLoss: 1.1087e-02   valLoss:4.4128e-02  time: 7.32e+00\n",
      "epoch: 13   trainLoss: 1.1817e-02   valLoss:3.9678e-02  time: 7.10e+00\n",
      "epoch: 14   trainLoss: 9.2037e-03   valLoss:5.2032e-02  time: 7.38e+00\n",
      "epoch: 15   trainLoss: 6.4970e-03   valLoss:4.1416e-02  time: 7.34e+00\n",
      "epoch: 16   trainLoss: 4.5582e-03   valLoss:1.6602e-02  time: 7.37e+00\n",
      "epoch: 17   trainLoss: 2.7236e-03   valLoss:1.6786e-02  time: 7.42e+00\n",
      "epoch: 18   trainLoss: 3.3211e-03   valLoss:6.7702e-03  time: 7.35e+00\n",
      "epoch: 19   trainLoss: 4.2192e-03   valLoss:4.2804e-03  time: 7.38e+00\n",
      "epoch: 20   trainLoss: 3.2723e-03   valLoss:1.5373e-02  time: 7.33e+00\n",
      "epoch: 21   trainLoss: 2.8294e-03   valLoss:4.4984e-03  time: 7.37e+00\n",
      "epoch: 22   trainLoss: 2.2824e-03   valLoss:5.4592e-03  time: 7.34e+00\n",
      "epoch: 23   trainLoss: 2.0091e-03   valLoss:5.3902e-03  time: 7.48e+00\n",
      "epoch: 24   trainLoss: 1.8200e-03   valLoss:4.2700e-03  time: 7.41e+00\n",
      "epoch: 25   trainLoss: 1.8954e-03   valLoss:3.3231e-03  time: 7.40e+00\n",
      "epoch: 26   trainLoss: 1.1650e-03   valLoss:2.0383e-03  time: 7.34e+00\n",
      "epoch: 27   trainLoss: 8.7662e-04   valLoss:4.5134e-03  time: 7.39e+00\n",
      "epoch: 28   trainLoss: 1.2750e-03   valLoss:1.6405e-03  time: 7.23e+00\n",
      "epoch: 29   trainLoss: 9.8060e-04   valLoss:2.4232e-03  time: 7.34e+00\n",
      "epoch: 30   trainLoss: 9.1470e-04   valLoss:4.0300e-03  time: 7.37e+00\n",
      "epoch: 31   trainLoss: 6.4037e-04   valLoss:3.1013e-03  time: 7.35e+00\n",
      "epoch: 32   trainLoss: 7.3239e-04   valLoss:2.6694e-03  time: 7.35e+00\n",
      "epoch: 33   trainLoss: 7.3379e-04   valLoss:2.6397e-03  time: 7.44e+00\n",
      "epoch: 34   trainLoss: 6.5259e-04   valLoss:4.0858e-03  time: 7.35e+00\n",
      "epoch: 35   trainLoss: 6.8324e-04   valLoss:3.7730e-03  time: 7.32e+00\n",
      "epoch: 36   trainLoss: 8.3771e-04   valLoss:2.8369e-03  time: 7.41e+00\n",
      "epoch: 37   trainLoss: 6.8121e-04   valLoss:2.3068e-03  time: 7.33e+00\n",
      "epoch: 38   trainLoss: 5.1137e-04   valLoss:6.0554e-03  time: 7.41e+00\n",
      "epoch: 39   trainLoss: 1.0189e-03   valLoss:5.4540e-03  time: 7.29e+00\n",
      "epoch: 40   trainLoss: 1.6561e-03   valLoss:4.2124e-03  time: 7.34e+00\n",
      "epoch: 41   trainLoss: 1.8496e-03   valLoss:3.5947e-03  time: 7.38e+00\n",
      "epoch: 42   trainLoss: 1.2311e-03   valLoss:2.4840e-03  time: 7.37e+00\n",
      "epoch: 43   trainLoss: 5.2332e-04   valLoss:2.8953e-03  time: 7.49e+00\n",
      "epoch: 44   trainLoss: 1.0468e-03   valLoss:4.0190e-03  time: 7.38e+00\n",
      "epoch: 45   trainLoss: 8.4578e-04   valLoss:2.5282e-03  time: 7.19e+00\n",
      "epoch: 46   trainLoss: 5.9399e-04   valLoss:6.3652e-03  time: 7.19e+00\n",
      "epoch: 47   trainLoss: 1.5993e-03   valLoss:5.3702e-03  time: 7.10e+00\n",
      "epoch: 48   trainLoss: 2.3875e-03   valLoss:1.3687e-03  time: 7.18e+00\n",
      "epoch: 49   trainLoss: 1.2261e-03   valLoss:2.1509e-03  time: 7.32e+00\n",
      "epoch: 50   trainLoss: 1.1332e-03   valLoss:4.2958e-03  time: 7.29e+00\n",
      "epoch: 51   trainLoss: 2.3613e-03   valLoss:4.1601e-03  time: 7.25e+00\n",
      "epoch: 52   trainLoss: 2.1110e-03   valLoss:4.8301e-03  time: 7.28e+00\n",
      "epoch: 53   trainLoss: 1.6071e-03   valLoss:1.2587e-03  time: 7.29e+00\n",
      "epoch: 54   trainLoss: 1.6169e-03   valLoss:1.9174e-03  time: 7.34e+00\n",
      "epoch: 55   trainLoss: 1.9699e-03   valLoss:3.0163e-03  time: 7.36e+00\n",
      "epoch: 56   trainLoss: 2.2646e-03   valLoss:4.2996e-03  time: 7.37e+00\n",
      "epoch: 57   trainLoss: 1.9169e-03   valLoss:2.9811e-03  time: 7.37e+00\n",
      "epoch: 58   trainLoss: 1.7513e-03   valLoss:2.1358e-03  time: 7.33e+00\n",
      "epoch: 59   trainLoss: 1.6681e-03   valLoss:5.8017e-03  time: 7.09e+00\n",
      "epoch: 60   trainLoss: 1.6866e-03   valLoss:1.7500e-03  time: 7.30e+00\n",
      "epoch: 61   trainLoss: 1.4558e-03   valLoss:2.3592e-03  time: 7.31e+00\n",
      "epoch: 62   trainLoss: 1.9533e-03   valLoss:8.3554e-03  time: 7.38e+00\n",
      "epoch: 63   trainLoss: 2.1024e-03   valLoss:5.6833e-03  time: 7.27e+00\n",
      "epoch: 64   trainLoss: 2.0132e-03   valLoss:5.8256e-03  time: 7.32e+00\n",
      "epoch: 65   trainLoss: 1.6213e-03   valLoss:3.4772e-03  time: 7.38e+00\n",
      "epoch: 66   trainLoss: 2.4547e-03   valLoss:2.4986e-03  time: 7.35e+00\n",
      "epoch: 67   trainLoss: 1.6876e-03   valLoss:7.6910e-03  time: 7.32e+00\n",
      "epoch: 68   trainLoss: 1.1312e-03   valLoss:4.2029e-03  time: 7.43e+00\n",
      "epoch: 69   trainLoss: 8.0975e-04   valLoss:2.9549e-03  time: 7.41e+00\n",
      "epoch: 70   trainLoss: 1.0297e-03   valLoss:3.5755e-03  time: 7.37e+00\n",
      "epoch: 71   trainLoss: 9.4658e-04   valLoss:4.0907e-03  time: 7.34e+00\n",
      "epoch: 72   trainLoss: 8.1183e-04   valLoss:3.4302e-03  time: 7.31e+00\n",
      "epoch: 73   trainLoss: 7.4047e-04   valLoss:2.8253e-03  time: 7.35e+00\n",
      "epoch: 74   trainLoss: 6.5245e-04   valLoss:2.1618e-03  time: 7.34e+00\n",
      "epoch: 75   trainLoss: 5.5853e-04   valLoss:2.2636e-03  time: 7.40e+00\n",
      "epoch: 76   trainLoss: 8.6073e-04   valLoss:3.3471e-03  time: 7.31e+00\n",
      "epoch: 77   trainLoss: 1.1015e-03   valLoss:5.8298e-03  time: 7.28e+00\n",
      "epoch: 78   trainLoss: 1.1220e-03   valLoss:3.7656e-03  time: 7.30e+00\n",
      "epoch: 79   trainLoss: 7.2045e-04   valLoss:1.8094e-03  time: 7.36e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80   trainLoss: 1.2033e-03   valLoss:3.5856e-03  time: 7.30e+00\n",
      "epoch: 81   trainLoss: 2.6827e-03   valLoss:6.4507e-03  time: 7.37e+00\n",
      "epoch: 82   trainLoss: 2.2796e-03   valLoss:4.1399e-03  time: 7.42e+00\n",
      "epoch: 83   trainLoss: 1.6256e-03   valLoss:3.8682e-03  time: 7.31e+00\n",
      "epoch: 84   trainLoss: 1.7521e-03   valLoss:3.8034e-03  time: 7.24e+00\n",
      "epoch: 85   trainLoss: 1.5351e-03   valLoss:2.7038e-03  time: 7.28e+00\n",
      "epoch: 86   trainLoss: 1.8329e-03   valLoss:2.5617e-03  time: 7.24e+00\n",
      "epoch: 87   trainLoss: 2.4852e-03   valLoss:2.9439e-03  time: 7.33e+00\n",
      "epoch: 88   trainLoss: 1.2292e-03   valLoss:2.1285e-03  time: 7.38e+00\n",
      "epoch: 89   trainLoss: 1.5395e-03   valLoss:2.5823e-03  time: 7.41e+00\n",
      "epoch: 90   trainLoss: 1.5879e-03   valLoss:1.9070e-03  time: 7.36e+00\n",
      "epoch: 91   trainLoss: 7.0114e-04   valLoss:1.9474e-03  time: 7.34e+00\n",
      "epoch: 92   trainLoss: 8.5399e-04   valLoss:3.3999e-03  time: 7.24e+00\n",
      "epoch: 93   trainLoss: 4.7510e-04   valLoss:4.2632e-03  time: 7.27e+00\n",
      "epoch: 94   trainLoss: 8.7488e-04   valLoss:3.8989e-03  time: 7.29e+00\n",
      "epoch: 95   trainLoss: 6.9475e-04   valLoss:2.8379e-03  time: 7.34e+00\n",
      "epoch: 96   trainLoss: 5.0364e-04   valLoss:2.2856e-03  time: 7.38e+00\n",
      "epoch: 97   trainLoss: 6.0763e-04   valLoss:1.7107e-03  time: 7.26e+00\n",
      "epoch: 98   trainLoss: 8.0687e-04   valLoss:3.9182e-03  time: 7.34e+00\n",
      "epoch: 99   trainLoss: 1.4947e-03   valLoss:7.1869e-03  time: 7.30e+00\n",
      "loading checkpoint 53\n",
      "trained 24 random forest models in 16.10 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_20/\n",
      "loaded train set of size 18\n",
      "epoch: 0   trainLoss: 8.0044e-01   valLoss:8.7891e-01  time: 1.60e-01\n",
      "epoch: 1   trainLoss: 5.4952e-01   valLoss:8.6871e-01  time: 1.65e-01\n",
      "epoch: 2   trainLoss: 3.8248e-01   valLoss:8.5474e-01  time: 1.70e-01\n",
      "epoch: 3   trainLoss: 2.6127e-01   valLoss:8.2959e-01  time: 1.65e-01\n",
      "epoch: 4   trainLoss: 1.6431e-01   valLoss:8.0064e-01  time: 1.71e-01\n",
      "epoch: 5   trainLoss: 1.1611e-01   valLoss:7.7670e-01  time: 1.65e-01\n",
      "epoch: 6   trainLoss: 1.2312e-01   valLoss:7.5753e-01  time: 1.65e-01\n",
      "epoch: 7   trainLoss: 1.0665e-01   valLoss:7.4827e-01  time: 1.66e-01\n",
      "epoch: 8   trainLoss: 1.0301e-01   valLoss:7.4343e-01  time: 1.66e-01\n",
      "epoch: 9   trainLoss: 9.0829e-02   valLoss:7.4419e-01  time: 1.67e-01\n",
      "epoch: 10   trainLoss: 7.8330e-02   valLoss:7.4659e-01  time: 1.67e-01\n",
      "epoch: 11   trainLoss: 6.5579e-02   valLoss:7.4351e-01  time: 1.68e-01\n",
      "epoch: 12   trainLoss: 5.6365e-02   valLoss:7.4098e-01  time: 1.70e-01\n",
      "epoch: 13   trainLoss: 5.1567e-02   valLoss:7.1950e-01  time: 1.66e-01\n",
      "epoch: 14   trainLoss: 4.9101e-02   valLoss:6.6168e-01  time: 1.66e-01\n",
      "epoch: 15   trainLoss: 4.6581e-02   valLoss:5.8216e-01  time: 1.67e-01\n",
      "epoch: 16   trainLoss: 4.3124e-02   valLoss:5.2512e-01  time: 1.65e-01\n",
      "epoch: 17   trainLoss: 4.0743e-02   valLoss:4.7522e-01  time: 1.69e-01\n",
      "epoch: 18   trainLoss: 3.8845e-02   valLoss:4.2603e-01  time: 1.67e-01\n",
      "epoch: 19   trainLoss: 3.6367e-02   valLoss:3.7688e-01  time: 1.65e-01\n",
      "epoch: 20   trainLoss: 3.4743e-02   valLoss:3.3445e-01  time: 1.69e-01\n",
      "epoch: 21   trainLoss: 3.3356e-02   valLoss:2.9035e-01  time: 1.68e-01\n",
      "epoch: 22   trainLoss: 3.2198e-02   valLoss:2.3726e-01  time: 1.72e-01\n",
      "epoch: 23   trainLoss: 3.1485e-02   valLoss:1.9843e-01  time: 1.68e-01\n",
      "epoch: 24   trainLoss: 3.0640e-02   valLoss:1.7857e-01  time: 1.65e-01\n",
      "epoch: 25   trainLoss: 2.9805e-02   valLoss:1.2339e-01  time: 1.68e-01\n",
      "epoch: 26   trainLoss: 2.9497e-02   valLoss:8.0833e-02  time: 1.68e-01\n",
      "epoch: 27   trainLoss: 2.8855e-02   valLoss:7.5748e-02  time: 1.68e-01\n",
      "epoch: 28   trainLoss: 2.8375e-02   valLoss:1.0527e-01  time: 1.68e-01\n",
      "epoch: 29   trainLoss: 2.8050e-02   valLoss:1.4656e-01  time: 1.64e-01\n",
      "epoch: 30   trainLoss: 2.7630e-02   valLoss:1.8480e-01  time: 1.68e-01\n",
      "epoch: 31   trainLoss: 2.7308e-02   valLoss:2.2784e-01  time: 1.74e-01\n",
      "epoch: 32   trainLoss: 2.6941e-02   valLoss:2.5572e-01  time: 1.74e-01\n",
      "epoch: 33   trainLoss: 2.6672e-02   valLoss:2.6892e-01  time: 1.70e-01\n",
      "epoch: 34   trainLoss: 2.6436e-02   valLoss:2.9227e-01  time: 1.67e-01\n",
      "epoch: 35   trainLoss: 2.6110e-02   valLoss:3.0731e-01  time: 1.77e-01\n",
      "epoch: 36   trainLoss: 2.5811e-02   valLoss:3.0311e-01  time: 1.72e-01\n",
      "epoch: 37   trainLoss: 2.5536e-02   valLoss:2.9965e-01  time: 1.78e-01\n",
      "epoch: 38   trainLoss: 2.5215e-02   valLoss:2.9840e-01  time: 1.81e-01\n",
      "epoch: 39   trainLoss: 2.4835e-02   valLoss:2.8517e-01  time: 1.78e-01\n",
      "epoch: 40   trainLoss: 2.4384e-02   valLoss:2.7464e-01  time: 1.65e-01\n",
      "epoch: 41   trainLoss: 2.3888e-02   valLoss:2.7300e-01  time: 1.67e-01\n",
      "epoch: 42   trainLoss: 2.3250e-02   valLoss:2.6035e-01  time: 1.89e-01\n",
      "epoch: 43   trainLoss: 2.2469e-02   valLoss:2.5592e-01  time: 1.67e-01\n",
      "epoch: 44   trainLoss: 2.1574e-02   valLoss:2.5117e-01  time: 1.66e-01\n",
      "epoch: 45   trainLoss: 2.0739e-02   valLoss:2.4097e-01  time: 1.67e-01\n",
      "epoch: 46   trainLoss: 2.0142e-02   valLoss:2.4614e-01  time: 1.81e-01\n",
      "epoch: 47   trainLoss: 1.9792e-02   valLoss:2.2961e-01  time: 1.87e-01\n",
      "epoch: 48   trainLoss: 1.9551e-02   valLoss:2.4806e-01  time: 1.89e-01\n",
      "epoch: 49   trainLoss: 1.9518e-02   valLoss:2.1891e-01  time: 1.70e-01\n",
      "epoch: 50   trainLoss: 1.9962e-02   valLoss:2.4852e-01  time: 1.69e-01\n",
      "epoch: 51   trainLoss: 1.9804e-02   valLoss:2.0740e-01  time: 1.65e-01\n",
      "epoch: 52   trainLoss: 1.9012e-02   valLoss:2.2518e-01  time: 1.67e-01\n",
      "epoch: 53   trainLoss: 1.8173e-02   valLoss:2.2060e-01  time: 1.68e-01\n",
      "epoch: 54   trainLoss: 1.7802e-02   valLoss:1.9105e-01  time: 1.65e-01\n",
      "epoch: 55   trainLoss: 1.7714e-02   valLoss:2.0100e-01  time: 1.65e-01\n",
      "epoch: 56   trainLoss: 1.6890e-02   valLoss:1.9134e-01  time: 1.68e-01\n",
      "epoch: 57   trainLoss: 1.6504e-02   valLoss:1.7007e-01  time: 1.66e-01\n",
      "epoch: 58   trainLoss: 1.6283e-02   valLoss:1.7334e-01  time: 1.67e-01\n",
      "epoch: 59   trainLoss: 1.5789e-02   valLoss:1.6905e-01  time: 1.67e-01\n",
      "epoch: 60   trainLoss: 1.5562e-02   valLoss:1.6221e-01  time: 1.64e-01\n",
      "epoch: 61   trainLoss: 1.5404e-02   valLoss:1.6995e-01  time: 1.64e-01\n",
      "epoch: 62   trainLoss: 1.5089e-02   valLoss:1.6632e-01  time: 1.70e-01\n",
      "epoch: 63   trainLoss: 1.4764e-02   valLoss:1.5937e-01  time: 1.65e-01\n",
      "epoch: 64   trainLoss: 1.4586e-02   valLoss:1.6183e-01  time: 1.68e-01\n",
      "epoch: 65   trainLoss: 1.4320e-02   valLoss:1.4881e-01  time: 1.72e-01\n",
      "epoch: 66   trainLoss: 1.3885e-02   valLoss:1.3954e-01  time: 1.65e-01\n",
      "epoch: 67   trainLoss: 1.3645e-02   valLoss:1.4309e-01  time: 1.65e-01\n",
      "epoch: 68   trainLoss: 1.3463e-02   valLoss:1.3509e-01  time: 1.70e-01\n",
      "epoch: 69   trainLoss: 1.3213e-02   valLoss:1.3687e-01  time: 1.65e-01\n",
      "epoch: 70   trainLoss: 1.2836e-02   valLoss:1.3161e-01  time: 1.67e-01\n",
      "epoch: 71   trainLoss: 1.2528e-02   valLoss:1.2509e-01  time: 1.67e-01\n",
      "epoch: 72   trainLoss: 1.2212e-02   valLoss:1.2206e-01  time: 1.69e-01\n",
      "epoch: 73   trainLoss: 1.1926e-02   valLoss:1.1713e-01  time: 1.65e-01\n",
      "epoch: 74   trainLoss: 1.1701e-02   valLoss:1.1456e-01  time: 1.67e-01\n",
      "epoch: 75   trainLoss: 1.1475e-02   valLoss:1.1069e-01  time: 1.67e-01\n",
      "epoch: 76   trainLoss: 1.1468e-02   valLoss:1.1271e-01  time: 1.68e-01\n",
      "epoch: 77   trainLoss: 1.2624e-02   valLoss:1.1149e-01  time: 1.69e-01\n",
      "epoch: 78   trainLoss: 1.5397e-02   valLoss:1.2849e-01  time: 1.66e-01\n",
      "epoch: 79   trainLoss: 1.5344e-02   valLoss:9.1305e-02  time: 1.65e-01\n",
      "epoch: 80   trainLoss: 1.7478e-02   valLoss:1.2572e-01  time: 1.71e-01\n",
      "epoch: 81   trainLoss: 1.4519e-02   valLoss:1.1500e-01  time: 1.65e-01\n",
      "epoch: 82   trainLoss: 1.1369e-02   valLoss:9.2474e-02  time: 1.64e-01\n",
      "epoch: 83   trainLoss: 1.2703e-02   valLoss:1.0019e-01  time: 1.64e-01\n",
      "epoch: 84   trainLoss: 1.0082e-02   valLoss:1.0709e-01  time: 1.65e-01\n",
      "epoch: 85   trainLoss: 1.0712e-02   valLoss:9.0194e-02  time: 1.65e-01\n",
      "epoch: 86   trainLoss: 8.5131e-03   valLoss:8.0182e-02  time: 1.65e-01\n",
      "epoch: 87   trainLoss: 8.1504e-03   valLoss:8.3429e-02  time: 1.67e-01\n",
      "epoch: 88   trainLoss: 6.4420e-03   valLoss:8.3504e-02  time: 1.71e-01\n",
      "epoch: 89   trainLoss: 1.1641e-02   valLoss:8.3606e-02  time: 1.73e-01\n",
      "epoch: 90   trainLoss: 1.5998e-02   valLoss:7.8537e-02  time: 1.68e-01\n",
      "epoch: 91   trainLoss: 1.2772e-02   valLoss:8.1804e-02  time: 1.70e-01\n",
      "epoch: 92   trainLoss: 6.3240e-03   valLoss:8.7870e-02  time: 1.66e-01\n",
      "epoch: 93   trainLoss: 6.2500e-03   valLoss:7.9851e-02  time: 1.70e-01\n",
      "epoch: 94   trainLoss: 5.5849e-03   valLoss:7.5130e-02  time: 1.66e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 95   trainLoss: 4.4051e-03   valLoss:8.5405e-02  time: 1.66e-01\n",
      "epoch: 96   trainLoss: 2.8846e-03   valLoss:9.0409e-02  time: 1.67e-01\n",
      "epoch: 97   trainLoss: 3.0451e-03   valLoss:7.9798e-02  time: 1.65e-01\n",
      "epoch: 98   trainLoss: 2.3297e-03   valLoss:7.3670e-02  time: 1.65e-01\n",
      "epoch: 99   trainLoss: 2.1411e-03   valLoss:7.5274e-02  time: 1.67e-01\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.2497e+00   valLoss:1.0558e+02  time: 1.74e-01\n",
      "epoch: 1   trainLoss: 3.7625e-01   valLoss:6.8793e+00  time: 1.65e-01\n",
      "epoch: 2   trainLoss: 1.8915e-01   valLoss:2.7728e+00  time: 1.66e-01\n",
      "epoch: 3   trainLoss: 1.3331e-01   valLoss:2.2430e+00  time: 1.66e-01\n",
      "epoch: 4   trainLoss: 1.1167e-01   valLoss:1.4621e+00  time: 1.67e-01\n",
      "epoch: 5   trainLoss: 9.6433e-02   valLoss:7.7700e-01  time: 1.67e-01\n",
      "epoch: 6   trainLoss: 9.3786e-02   valLoss:5.9719e-01  time: 1.73e-01\n",
      "epoch: 7   trainLoss: 8.8909e-02   valLoss:5.9221e-01  time: 1.61e-01\n",
      "epoch: 8   trainLoss: 8.7287e-02   valLoss:3.9202e-01  time: 1.74e-01\n",
      "epoch: 9   trainLoss: 8.3729e-02   valLoss:2.5823e-01  time: 1.68e-01\n",
      "epoch: 10   trainLoss: 7.3493e-02   valLoss:2.2406e-01  time: 1.71e-01\n",
      "epoch: 11   trainLoss: 6.7029e-02   valLoss:1.9260e-01  time: 1.75e-01\n",
      "epoch: 12   trainLoss: 6.2530e-02   valLoss:1.4975e-01  time: 1.68e-01\n",
      "epoch: 13   trainLoss: 5.8864e-02   valLoss:1.1909e-01  time: 1.66e-01\n",
      "epoch: 14   trainLoss: 5.5712e-02   valLoss:9.6117e-02  time: 1.66e-01\n",
      "epoch: 15   trainLoss: 5.3266e-02   valLoss:8.4708e-02  time: 1.70e-01\n",
      "epoch: 16   trainLoss: 5.1612e-02   valLoss:7.8194e-02  time: 1.66e-01\n",
      "epoch: 17   trainLoss: 5.0603e-02   valLoss:7.4333e-02  time: 1.69e-01\n",
      "epoch: 18   trainLoss: 4.8944e-02   valLoss:7.1755e-02  time: 1.66e-01\n",
      "epoch: 19   trainLoss: 4.7266e-02   valLoss:6.9208e-02  time: 1.63e-01\n",
      "epoch: 20   trainLoss: 4.6077e-02   valLoss:6.3810e-02  time: 1.62e-01\n",
      "epoch: 21   trainLoss: 4.4137e-02   valLoss:5.7510e-02  time: 1.62e-01\n",
      "epoch: 22   trainLoss: 4.2119e-02   valLoss:5.3272e-02  time: 1.61e-01\n",
      "epoch: 23   trainLoss: 4.0199e-02   valLoss:5.0636e-02  time: 1.61e-01\n",
      "epoch: 24   trainLoss: 3.7726e-02   valLoss:5.0155e-02  time: 1.62e-01\n",
      "epoch: 25   trainLoss: 3.5632e-02   valLoss:4.6353e-02  time: 1.67e-01\n",
      "epoch: 26   trainLoss: 3.4604e-02   valLoss:4.4554e-02  time: 1.62e-01\n",
      "epoch: 27   trainLoss: 3.2499e-02   valLoss:4.3175e-02  time: 1.63e-01\n",
      "epoch: 28   trainLoss: 3.0984e-02   valLoss:3.9063e-02  time: 1.62e-01\n",
      "epoch: 29   trainLoss: 2.9512e-02   valLoss:3.9191e-02  time: 1.63e-01\n",
      "epoch: 30   trainLoss: 2.8734e-02   valLoss:3.4539e-02  time: 1.65e-01\n",
      "epoch: 31   trainLoss: 2.8148e-02   valLoss:3.3039e-02  time: 1.66e-01\n",
      "epoch: 32   trainLoss: 2.8729e-02   valLoss:3.5826e-02  time: 1.62e-01\n",
      "epoch: 33   trainLoss: 2.8259e-02   valLoss:3.4964e-02  time: 1.62e-01\n",
      "epoch: 34   trainLoss: 2.7965e-02   valLoss:3.0700e-02  time: 1.62e-01\n",
      "epoch: 35   trainLoss: 2.6731e-02   valLoss:3.1087e-02  time: 1.63e-01\n",
      "epoch: 36   trainLoss: 2.5862e-02   valLoss:3.4828e-02  time: 1.64e-01\n",
      "epoch: 37   trainLoss: 2.5587e-02   valLoss:3.5763e-02  time: 1.65e-01\n",
      "epoch: 38   trainLoss: 2.5040e-02   valLoss:3.4025e-02  time: 1.62e-01\n",
      "epoch: 39   trainLoss: 2.4382e-02   valLoss:3.4004e-02  time: 1.64e-01\n",
      "epoch: 40   trainLoss: 2.3414e-02   valLoss:3.1977e-02  time: 1.62e-01\n",
      "epoch: 41   trainLoss: 2.2988e-02   valLoss:3.0040e-02  time: 1.62e-01\n",
      "epoch: 42   trainLoss: 2.2471e-02   valLoss:3.0659e-02  time: 1.65e-01\n",
      "epoch: 43   trainLoss: 2.1724e-02   valLoss:2.9919e-02  time: 1.63e-01\n",
      "epoch: 44   trainLoss: 2.1272e-02   valLoss:2.8629e-02  time: 1.61e-01\n",
      "epoch: 45   trainLoss: 2.0939e-02   valLoss:2.6186e-02  time: 1.63e-01\n",
      "epoch: 46   trainLoss: 2.0856e-02   valLoss:2.8561e-02  time: 1.64e-01\n",
      "epoch: 47   trainLoss: 2.2430e-02   valLoss:2.7735e-02  time: 1.67e-01\n",
      "epoch: 48   trainLoss: 2.3782e-02   valLoss:2.3627e-02  time: 1.63e-01\n",
      "epoch: 49   trainLoss: 1.9350e-02   valLoss:2.2912e-02  time: 1.62e-01\n",
      "epoch: 50   trainLoss: 1.8766e-02   valLoss:2.6827e-02  time: 1.62e-01\n",
      "epoch: 51   trainLoss: 1.9742e-02   valLoss:2.2368e-02  time: 1.64e-01\n",
      "epoch: 52   trainLoss: 1.6919e-02   valLoss:2.1950e-02  time: 1.62e-01\n",
      "epoch: 53   trainLoss: 1.8697e-02   valLoss:2.5459e-02  time: 1.63e-01\n",
      "epoch: 54   trainLoss: 1.6972e-02   valLoss:2.2669e-02  time: 1.62e-01\n",
      "epoch: 55   trainLoss: 1.6508e-02   valLoss:1.9340e-02  time: 1.62e-01\n",
      "epoch: 56   trainLoss: 1.6725e-02   valLoss:1.8603e-02  time: 1.75e-01\n",
      "epoch: 57   trainLoss: 1.5579e-02   valLoss:2.1167e-02  time: 1.66e-01\n",
      "epoch: 58   trainLoss: 1.5967e-02   valLoss:1.9467e-02  time: 1.68e-01\n",
      "epoch: 59   trainLoss: 1.4830e-02   valLoss:1.7870e-02  time: 1.64e-01\n",
      "epoch: 60   trainLoss: 1.5380e-02   valLoss:1.8880e-02  time: 1.64e-01\n",
      "epoch: 61   trainLoss: 1.4210e-02   valLoss:2.3246e-02  time: 1.63e-01\n",
      "epoch: 62   trainLoss: 1.4514e-02   valLoss:2.0281e-02  time: 1.63e-01\n",
      "epoch: 63   trainLoss: 1.3390e-02   valLoss:1.8136e-02  time: 1.65e-01\n",
      "epoch: 64   trainLoss: 1.3453e-02   valLoss:1.8590e-02  time: 1.69e-01\n",
      "epoch: 65   trainLoss: 1.2318e-02   valLoss:1.9675e-02  time: 1.71e-01\n",
      "epoch: 66   trainLoss: 1.2094e-02   valLoss:1.8179e-02  time: 1.62e-01\n",
      "epoch: 67   trainLoss: 1.0694e-02   valLoss:1.7188e-02  time: 1.62e-01\n",
      "epoch: 68   trainLoss: 9.9570e-03   valLoss:1.6753e-02  time: 1.73e-01\n",
      "epoch: 69   trainLoss: 7.9530e-03   valLoss:1.6874e-02  time: 1.69e-01\n",
      "epoch: 70   trainLoss: 6.2887e-03   valLoss:1.5492e-02  time: 1.65e-01\n",
      "epoch: 71   trainLoss: 4.8442e-03   valLoss:1.3037e-02  time: 1.66e-01\n",
      "epoch: 72   trainLoss: 4.1158e-03   valLoss:1.4788e-02  time: 1.64e-01\n",
      "epoch: 73   trainLoss: 6.4998e-03   valLoss:1.4092e-02  time: 1.67e-01\n",
      "epoch: 74   trainLoss: 1.0981e-02   valLoss:1.1530e-02  time: 1.65e-01\n",
      "epoch: 75   trainLoss: 1.0710e-02   valLoss:8.1376e-03  time: 1.66e-01\n",
      "epoch: 76   trainLoss: 5.2560e-03   valLoss:6.3678e-03  time: 1.68e-01\n",
      "epoch: 77   trainLoss: 4.4930e-03   valLoss:8.2950e-03  time: 1.67e-01\n",
      "epoch: 78   trainLoss: 5.0314e-03   valLoss:4.1698e-03  time: 1.66e-01\n",
      "epoch: 79   trainLoss: 2.0299e-03   valLoss:6.5637e-03  time: 1.68e-01\n",
      "epoch: 80   trainLoss: 4.4700e-03   valLoss:4.8264e-03  time: 1.67e-01\n",
      "epoch: 81   trainLoss: 2.2237e-03   valLoss:4.3179e-03  time: 1.85e-01\n",
      "epoch: 82   trainLoss: 3.1661e-03   valLoss:4.6539e-03  time: 1.68e-01\n",
      "epoch: 83   trainLoss: 1.7933e-03   valLoss:7.9584e-03  time: 1.75e-01\n",
      "epoch: 84   trainLoss: 2.6826e-03   valLoss:4.5689e-03  time: 1.68e-01\n",
      "epoch: 85   trainLoss: 1.4368e-03   valLoss:4.1270e-03  time: 1.65e-01\n",
      "epoch: 86   trainLoss: 1.9907e-03   valLoss:4.2538e-03  time: 1.65e-01\n",
      "epoch: 87   trainLoss: 1.2937e-03   valLoss:4.4214e-03  time: 1.67e-01\n",
      "epoch: 88   trainLoss: 1.4787e-03   valLoss:4.3959e-03  time: 1.63e-01\n",
      "epoch: 89   trainLoss: 1.1271e-03   valLoss:5.9564e-03  time: 1.65e-01\n",
      "epoch: 90   trainLoss: 1.2379e-03   valLoss:5.5060e-03  time: 1.66e-01\n",
      "epoch: 91   trainLoss: 9.0090e-04   valLoss:4.5935e-03  time: 1.65e-01\n",
      "epoch: 92   trainLoss: 8.9090e-04   valLoss:3.4391e-03  time: 1.65e-01\n",
      "epoch: 93   trainLoss: 8.3074e-04   valLoss:2.5793e-03  time: 1.72e-01\n",
      "epoch: 94   trainLoss: 8.0055e-04   valLoss:2.6420e-03  time: 1.65e-01\n",
      "epoch: 95   trainLoss: 7.6217e-04   valLoss:3.1695e-03  time: 1.64e-01\n",
      "epoch: 96   trainLoss: 5.9894e-04   valLoss:3.0317e-03  time: 1.68e-01\n",
      "epoch: 97   trainLoss: 5.9748e-04   valLoss:2.8139e-03  time: 1.67e-01\n",
      "epoch: 98   trainLoss: 4.9933e-04   valLoss:4.2547e-03  time: 1.66e-01\n",
      "epoch: 99   trainLoss: 5.8516e-04   valLoss:5.0779e-03  time: 1.66e-01\n",
      "loading checkpoint 93\n",
      "trained 24 random forest models in 2.86 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.612660e-06</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.486692</td>\n",
       "      <td>0.536522</td>\n",
       "      <td>-3.974415</td>\n",
       "      <td>-29.030809</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.648987e-06</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.062413</td>\n",
       "      <td>0.468563</td>\n",
       "      <td>0.506576</td>\n",
       "      <td>-4.294050</td>\n",
       "      <td>-30.468199</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.932928e-07</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>0.956268</td>\n",
       "      <td>0.954468</td>\n",
       "      <td>0.270790</td>\n",
       "      <td>-1.486604</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.061693e-07</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.024227</td>\n",
       "      <td>0.945178</td>\n",
       "      <td>0.945599</td>\n",
       "      <td>0.231130</td>\n",
       "      <td>-1.780260</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.131397e-09</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.998570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997160</td>\n",
       "      <td>0.992229</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.270368e-08</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.989022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982041</td>\n",
       "      <td>0.960091</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.367840e-07</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.713401</td>\n",
       "      <td>0.767651</td>\n",
       "      <td>-1.515137</td>\n",
       "      <td>-25.614310</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.243793e-07</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>0.741568</td>\n",
       "      <td>-1.697320</td>\n",
       "      <td>-27.940751</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.271638e-07</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>0.959450</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>0.473793</td>\n",
       "      <td>-0.536891</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.338067e-07</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.018385</td>\n",
       "      <td>0.951296</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.437894</td>\n",
       "      <td>-0.740617</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.158964e-09</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.999409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.997374</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.266645e-09</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.995024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993059</td>\n",
       "      <td>0.981957</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.886366e-06</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.083397</td>\n",
       "      <td>-0.424460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.737696</td>\n",
       "      <td>-211.298138</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.481012e-06</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.096120</td>\n",
       "      <td>-1.697787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.612247</td>\n",
       "      <td>-100.271279</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.154125e-06</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.071465</td>\n",
       "      <td>0.818804</td>\n",
       "      <td>0.882942</td>\n",
       "      <td>-11.716410</td>\n",
       "      <td>-91.798700</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.247056e-06</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.087185</td>\n",
       "      <td>-0.364015</td>\n",
       "      <td>0.706282</td>\n",
       "      <td>-14.074079</td>\n",
       "      <td>-121.411290</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.776951e-08</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.896669</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934986</td>\n",
       "      <td>0.879464</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.175642e-07</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.706580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697966</td>\n",
       "      <td>0.275579</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.728538e-06</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.070121</td>\n",
       "      <td>0.232811</td>\n",
       "      <td>0.557284</td>\n",
       "      <td>-10.051742</td>\n",
       "      <td>-110.696062</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.693151e-06</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.069583</td>\n",
       "      <td>0.147030</td>\n",
       "      <td>0.497931</td>\n",
       "      <td>-8.559309</td>\n",
       "      <td>-91.717945</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.637443e-07</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>0.938188</td>\n",
       "      <td>0.959178</td>\n",
       "      <td>-0.238522</td>\n",
       "      <td>-3.772992</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.888311e-07</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.916057</td>\n",
       "      <td>0.931158</td>\n",
       "      <td>-0.136907</td>\n",
       "      <td>-2.912010</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.720290e-09</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994171</td>\n",
       "      <td>0.986461</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.481531e-08</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.983613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969825</td>\n",
       "      <td>0.938624</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.260420e-06</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.093121</td>\n",
       "      <td>0.701397</td>\n",
       "      <td>0.745905</td>\n",
       "      <td>-11.009008</td>\n",
       "      <td>-86.632650</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.107212e-06</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.091012</td>\n",
       "      <td>0.696861</td>\n",
       "      <td>0.703439</td>\n",
       "      <td>-14.806078</td>\n",
       "      <td>-122.583282</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.380132e-07</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.969351</td>\n",
       "      <td>0.968733</td>\n",
       "      <td>0.189519</td>\n",
       "      <td>-2.331655</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.633494e-07</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>0.951169</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>-0.067929</td>\n",
       "      <td>-3.299464</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.488652e-08</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.991520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990064</td>\n",
       "      <td>0.975970</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.793416e-08</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.966132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946783</td>\n",
       "      <td>0.860296</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.182210e-06</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.117959</td>\n",
       "      <td>-2.363450</td>\n",
       "      <td>0.499059</td>\n",
       "      <td>-39.916896</td>\n",
       "      <td>-220.534163</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.570441e-05</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.162454</td>\n",
       "      <td>-0.223449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-109.490867</td>\n",
       "      <td>-1060.577924</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.716427e-06</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-3.662871</td>\n",
       "      <td>0.944837</td>\n",
       "      <td>-11.330361</td>\n",
       "      <td>-45.592655</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.048410e-06</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.093964</td>\n",
       "      <td>-5.695734</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>-7.994005</td>\n",
       "      <td>-30.036171</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.581980e-08</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.853370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866559</td>\n",
       "      <td>0.672870</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.649102e-07</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.022996</td>\n",
       "      <td>0.268229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.425495</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.340974e-07</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>0.751028</td>\n",
       "      <td>0.915744</td>\n",
       "      <td>-0.065485</td>\n",
       "      <td>-10.513129</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.433512e-07</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.768646</td>\n",
       "      <td>0.914059</td>\n",
       "      <td>-0.108448</td>\n",
       "      <td>-11.343919</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.020595e-07</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>0.968647</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>0.450683</td>\n",
       "      <td>-2.114185</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.075825e-07</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.966606</td>\n",
       "      <td>0.975587</td>\n",
       "      <td>0.425588</td>\n",
       "      <td>-2.268542</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6.428365e-10</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999430</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.434288e-09</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995909</td>\n",
       "      <td>0.988855</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6.159089e-06</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.106892</td>\n",
       "      <td>-1.426024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.607958</td>\n",
       "      <td>-74.470042</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.662944e-06</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>-2.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-13.669665</td>\n",
       "      <td>-80.265803</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.156408e-07</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.024893</td>\n",
       "      <td>0.978001</td>\n",
       "      <td>0.974115</td>\n",
       "      <td>-0.160297</td>\n",
       "      <td>-6.350562</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.890555e-07</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.030883</td>\n",
       "      <td>0.822910</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>-0.709324</td>\n",
       "      <td>-7.715040</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.137943e-08</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.988266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958866</td>\n",
       "      <td>0.891131</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.289478e-07</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859837</td>\n",
       "      <td>0.680350</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse       mae       mre    peakR2  maxAggR2   meanAggR2  \\\n",
       "0   1.612660e-06  0.001015  0.061899  0.486692  0.536522   -3.974415   \n",
       "1   1.648987e-06  0.001022  0.062413  0.468563  0.506576   -4.294050   \n",
       "2   1.932928e-07  0.000362  0.023980  0.956268  0.954468    0.270790   \n",
       "3   2.061693e-07  0.000368  0.024227  0.945178  0.945599    0.231130   \n",
       "4   3.131397e-09  0.000028  0.001508  0.998570  1.000000    0.997160   \n",
       "5   2.270368e-08  0.000073  0.003875  0.989022  1.000000    0.982041   \n",
       "6   8.367840e-07  0.000714  0.043766  0.713401  0.767651   -1.515137   \n",
       "7   8.243793e-07  0.000710  0.043701  0.691233  0.741568   -1.697320   \n",
       "8   1.271638e-07  0.000283  0.018216  0.959450  0.979335    0.473793   \n",
       "9   1.338067e-07  0.000288  0.018385  0.951296  0.975409    0.437894   \n",
       "10  1.158964e-09  0.000017  0.000915  0.999409  1.000000    0.998997   \n",
       "11  8.266645e-09  0.000046  0.002399  0.995024  1.000000    0.993059   \n",
       "12  3.886366e-06  0.001365  0.083397 -0.424460  0.000000  -17.737696   \n",
       "13  6.481012e-06  0.001713  0.096120 -1.697787  0.000000  -15.612247   \n",
       "14  2.154125e-06  0.001034  0.071465  0.818804  0.882942  -11.716410   \n",
       "15  4.247056e-06  0.001420  0.087185 -0.364015  0.706282  -14.074079   \n",
       "16  8.776951e-08  0.000165  0.008356  0.896669  1.000000    0.934986   \n",
       "17  4.175642e-07  0.000325  0.015694  0.706580  1.000000    0.697966   \n",
       "18  2.728538e-06  0.001154  0.070121  0.232811  0.557284  -10.051742   \n",
       "19  2.693151e-06  0.001146  0.069583  0.147030  0.497931   -8.559309   \n",
       "20  2.637443e-07  0.000408  0.025623  0.938188  0.959178   -0.238522   \n",
       "21  2.888311e-07  0.000421  0.026262  0.916057  0.931158   -0.136907   \n",
       "22  6.720290e-09  0.000041  0.002165  0.996866  1.000000    0.994171   \n",
       "23  3.481531e-08  0.000095  0.005039  0.983613  1.000000    0.969825   \n",
       "24  3.260420e-06  0.001418  0.093121  0.701397  0.745905  -11.009008   \n",
       "25  3.107212e-06  0.001381  0.091012  0.696861  0.703439  -14.806078   \n",
       "26  2.380132e-07  0.000391  0.025950  0.969351  0.968733    0.189519   \n",
       "27  2.633494e-07  0.000413  0.026980  0.951169  0.950673   -0.067929   \n",
       "28  1.488652e-08  0.000060  0.003150  0.991520  1.000000    0.990064   \n",
       "29  5.793416e-08  0.000135  0.007174  0.966132  1.000000    0.946783   \n",
       "30  5.182210e-06  0.001809  0.117959 -2.363450  0.499059  -39.916896   \n",
       "31  1.570441e-05  0.002829  0.162454 -0.223449  0.000000 -109.490867   \n",
       "32  2.716427e-06  0.001217  0.070361 -3.662871  0.944837  -11.330361   \n",
       "33  9.048410e-06  0.001862  0.093964 -5.695734  0.172969   -7.994005   \n",
       "34  6.581980e-08  0.000165  0.008624  0.853370  1.000000    0.866559   \n",
       "35  9.649102e-07  0.000470  0.022996  0.268229  1.000000    0.425495   \n",
       "36  4.340974e-07  0.000504  0.030557  0.751028  0.915744   -0.065485   \n",
       "37  4.433512e-07  0.000508  0.030724  0.768646  0.914059   -0.108448   \n",
       "38  1.020595e-07  0.000246  0.015255  0.968647  0.977010    0.450683   \n",
       "39  1.075825e-07  0.000249  0.015339  0.966606  0.975587    0.425588   \n",
       "40  6.428365e-10  0.000012  0.000643  0.999621  1.000000    0.999430   \n",
       "41  4.434288e-09  0.000033  0.001698  0.997109  1.000000    0.995909   \n",
       "42  6.159089e-06  0.001919  0.106892 -1.426024  0.000000  -12.607958   \n",
       "43  6.662944e-06  0.001961  0.111363 -2.132991  0.000000  -13.669665   \n",
       "44  2.156408e-07  0.000378  0.024893  0.978001  0.974115   -0.160297   \n",
       "45  4.890555e-07  0.000509  0.030883  0.822910  0.830078   -0.709324   \n",
       "46  4.137943e-08  0.000114  0.005999  0.988266  1.000000    0.958866   \n",
       "47  1.289478e-07  0.000216  0.011486  0.934343  1.000000    0.859837   \n",
       "\n",
       "       minAggR2              Model    Set  Train Size  \n",
       "0    -29.030809              Fresh  Train         180  \n",
       "1    -30.468199              Fresh   Test         180  \n",
       "2     -1.486604  Transfer learning  Train         180  \n",
       "3     -1.780260  Transfer learning   Test         180  \n",
       "4      0.992229      Random Forest  Train         180  \n",
       "5      0.960091      Random Forest   Test         180  \n",
       "6    -25.614310              Fresh  Train         445  \n",
       "7    -27.940751              Fresh   Test         445  \n",
       "8     -0.536891  Transfer learning  Train         445  \n",
       "9     -0.740617  Transfer learning   Test         445  \n",
       "10     0.997374      Random Forest  Train         445  \n",
       "11     0.981957      Random Forest   Test         445  \n",
       "12  -211.298138              Fresh  Train           9  \n",
       "13  -100.271279              Fresh   Test           9  \n",
       "14   -91.798700  Transfer learning  Train           9  \n",
       "15  -121.411290  Transfer learning   Test           9  \n",
       "16     0.879464      Random Forest  Train           9  \n",
       "17     0.275579      Random Forest   Test           9  \n",
       "18  -110.696062              Fresh  Train          92  \n",
       "19   -91.717945              Fresh   Test          92  \n",
       "20    -3.772992  Transfer learning  Train          92  \n",
       "21    -2.912010  Transfer learning   Test          92  \n",
       "22     0.986461      Random Forest  Train          92  \n",
       "23     0.938624      Random Forest   Test          92  \n",
       "24   -86.632650              Fresh  Train          47  \n",
       "25  -122.583282              Fresh   Test          47  \n",
       "26    -2.331655  Transfer learning  Train          47  \n",
       "27    -3.299464  Transfer learning   Test          47  \n",
       "28     0.975970      Random Forest  Train          47  \n",
       "29     0.860296      Random Forest   Test          47  \n",
       "30  -220.534163              Fresh  Train           4  \n",
       "31 -1060.577924              Fresh   Test           4  \n",
       "32   -45.592655  Transfer learning  Train           4  \n",
       "33   -30.036171  Transfer learning   Test           4  \n",
       "34     0.672870      Random Forest  Train           4  \n",
       "35    -0.033782      Random Forest   Test           4  \n",
       "36   -10.513129              Fresh  Train         909  \n",
       "37   -11.343919              Fresh   Test         909  \n",
       "38    -2.114185  Transfer learning  Train         909  \n",
       "39    -2.268542  Transfer learning   Test         909  \n",
       "40     0.998656      Random Forest  Train         909  \n",
       "41     0.988855      Random Forest   Test         909  \n",
       "42   -74.470042              Fresh  Train          18  \n",
       "43   -80.265803              Fresh   Test          18  \n",
       "44    -6.350562  Transfer learning  Train          18  \n",
       "45    -7.715040  Transfer learning   Test          18  \n",
       "46     0.891131      Random Forest  Train          18  \n",
       "47     0.680350      Random Forest   Test          18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainDataDirs = glob.glob(os.path.join(dataDir, 'design_9*/'))\n",
    "trainDataDirs = glob.glob(os.path.join(dataDir, '*/'))\n",
    "trainDataDirs.remove(testDir)\n",
    "\n",
    "allResults = []\n",
    "for trainDataDir in trainDataDirs:\n",
    "#     trainDataUnfiltered = loadConmechGraphs(trainDataDir)\n",
    "    trainDataUnfiltered = loadConmechGraphs(trainDataDir, loadDims=[0])\n",
    "\n",
    "    trainData = filterbyDispValue(trainDataUnfiltered, maxDispCutoff)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'reading from {trainDataDir}')\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=epochs, \n",
    "                         saveDir=saveDir+f'{trainSize:05}/gcn/')\n",
    "    plotHistory(history)\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Fresh'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Fresh'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile,\n",
    "                             epochs=epochs, \n",
    "                             saveDir=saveDir+f'{trainSize:05}/ptrGcn/')\n",
    "    plotHistory(history)\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Transfer learning'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Transfer learning'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                  useXFeatures=False,\n",
    "                  saveDir=saveDir+f'{trainSize:05}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['Model'] = 'Random Forest'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['Model'] = 'Random Forest'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.612660e-06</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.486692</td>\n",
       "      <td>0.536522</td>\n",
       "      <td>-3.974415</td>\n",
       "      <td>-29.030809</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.648987e-06</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.062413</td>\n",
       "      <td>0.468563</td>\n",
       "      <td>0.506576</td>\n",
       "      <td>-4.294050</td>\n",
       "      <td>-30.468199</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.932928e-07</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>0.956268</td>\n",
       "      <td>0.954468</td>\n",
       "      <td>0.270790</td>\n",
       "      <td>-1.486604</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.061693e-07</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.024227</td>\n",
       "      <td>0.945178</td>\n",
       "      <td>0.945599</td>\n",
       "      <td>0.231130</td>\n",
       "      <td>-1.780260</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.131397e-09</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.998570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997160</td>\n",
       "      <td>0.992229</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.270368e-08</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.989022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982041</td>\n",
       "      <td>0.960091</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.367840e-07</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.713401</td>\n",
       "      <td>0.767651</td>\n",
       "      <td>-1.515137</td>\n",
       "      <td>-25.614310</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.243793e-07</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>0.741568</td>\n",
       "      <td>-1.697320</td>\n",
       "      <td>-27.940751</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.271638e-07</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>0.959450</td>\n",
       "      <td>0.979335</td>\n",
       "      <td>0.473793</td>\n",
       "      <td>-0.536891</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.338067e-07</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.018385</td>\n",
       "      <td>0.951296</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.437894</td>\n",
       "      <td>-0.740617</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.158964e-09</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.999409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.997374</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.266645e-09</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.995024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993059</td>\n",
       "      <td>0.981957</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.886366e-06</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.083397</td>\n",
       "      <td>-0.424460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.737696</td>\n",
       "      <td>-211.298138</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.481012e-06</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.096120</td>\n",
       "      <td>-1.697787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.612247</td>\n",
       "      <td>-100.271279</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.154125e-06</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.071465</td>\n",
       "      <td>0.818804</td>\n",
       "      <td>0.882942</td>\n",
       "      <td>-11.716410</td>\n",
       "      <td>-91.798700</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.247056e-06</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.087185</td>\n",
       "      <td>-0.364015</td>\n",
       "      <td>0.706282</td>\n",
       "      <td>-14.074079</td>\n",
       "      <td>-121.411290</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.776951e-08</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.896669</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934986</td>\n",
       "      <td>0.879464</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.175642e-07</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.706580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697966</td>\n",
       "      <td>0.275579</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.728538e-06</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.070121</td>\n",
       "      <td>0.232811</td>\n",
       "      <td>0.557284</td>\n",
       "      <td>-10.051742</td>\n",
       "      <td>-110.696062</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.693151e-06</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.069583</td>\n",
       "      <td>0.147030</td>\n",
       "      <td>0.497931</td>\n",
       "      <td>-8.559309</td>\n",
       "      <td>-91.717945</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.637443e-07</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>0.938188</td>\n",
       "      <td>0.959178</td>\n",
       "      <td>-0.238522</td>\n",
       "      <td>-3.772992</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.888311e-07</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.916057</td>\n",
       "      <td>0.931158</td>\n",
       "      <td>-0.136907</td>\n",
       "      <td>-2.912010</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.720290e-09</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994171</td>\n",
       "      <td>0.986461</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.481531e-08</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.983613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969825</td>\n",
       "      <td>0.938624</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.260420e-06</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.093121</td>\n",
       "      <td>0.701397</td>\n",
       "      <td>0.745905</td>\n",
       "      <td>-11.009008</td>\n",
       "      <td>-86.632650</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.107212e-06</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.091012</td>\n",
       "      <td>0.696861</td>\n",
       "      <td>0.703439</td>\n",
       "      <td>-14.806078</td>\n",
       "      <td>-122.583282</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.380132e-07</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.969351</td>\n",
       "      <td>0.968733</td>\n",
       "      <td>0.189519</td>\n",
       "      <td>-2.331655</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.633494e-07</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>0.951169</td>\n",
       "      <td>0.950673</td>\n",
       "      <td>-0.067929</td>\n",
       "      <td>-3.299464</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.488652e-08</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.991520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990064</td>\n",
       "      <td>0.975970</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.793416e-08</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.966132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946783</td>\n",
       "      <td>0.860296</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.182210e-06</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.117959</td>\n",
       "      <td>-2.363450</td>\n",
       "      <td>0.499059</td>\n",
       "      <td>-39.916896</td>\n",
       "      <td>-220.534163</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.570441e-05</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.162454</td>\n",
       "      <td>-0.223449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-109.490867</td>\n",
       "      <td>-1060.577924</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.716427e-06</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-3.662871</td>\n",
       "      <td>0.944837</td>\n",
       "      <td>-11.330361</td>\n",
       "      <td>-45.592655</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.048410e-06</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.093964</td>\n",
       "      <td>-5.695734</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>-7.994005</td>\n",
       "      <td>-30.036171</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.581980e-08</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.853370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866559</td>\n",
       "      <td>0.672870</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.649102e-07</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.022996</td>\n",
       "      <td>0.268229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.425495</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.340974e-07</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>0.751028</td>\n",
       "      <td>0.915744</td>\n",
       "      <td>-0.065485</td>\n",
       "      <td>-10.513129</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.433512e-07</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.768646</td>\n",
       "      <td>0.914059</td>\n",
       "      <td>-0.108448</td>\n",
       "      <td>-11.343919</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.020595e-07</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>0.968647</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>0.450683</td>\n",
       "      <td>-2.114185</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.075825e-07</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.966606</td>\n",
       "      <td>0.975587</td>\n",
       "      <td>0.425588</td>\n",
       "      <td>-2.268542</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6.428365e-10</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999430</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.434288e-09</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995909</td>\n",
       "      <td>0.988855</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6.159089e-06</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.106892</td>\n",
       "      <td>-1.426024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.607958</td>\n",
       "      <td>-74.470042</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.662944e-06</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>-2.132991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-13.669665</td>\n",
       "      <td>-80.265803</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.156408e-07</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.024893</td>\n",
       "      <td>0.978001</td>\n",
       "      <td>0.974115</td>\n",
       "      <td>-0.160297</td>\n",
       "      <td>-6.350562</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.890555e-07</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.030883</td>\n",
       "      <td>0.822910</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>-0.709324</td>\n",
       "      <td>-7.715040</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.137943e-08</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.988266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958866</td>\n",
       "      <td>0.891131</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.289478e-07</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859837</td>\n",
       "      <td>0.680350</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse       mae       mre    peakR2  maxAggR2   meanAggR2  \\\n",
       "0   1.612660e-06  0.001015  0.061899  0.486692  0.536522   -3.974415   \n",
       "1   1.648987e-06  0.001022  0.062413  0.468563  0.506576   -4.294050   \n",
       "2   1.932928e-07  0.000362  0.023980  0.956268  0.954468    0.270790   \n",
       "3   2.061693e-07  0.000368  0.024227  0.945178  0.945599    0.231130   \n",
       "4   3.131397e-09  0.000028  0.001508  0.998570  1.000000    0.997160   \n",
       "5   2.270368e-08  0.000073  0.003875  0.989022  1.000000    0.982041   \n",
       "6   8.367840e-07  0.000714  0.043766  0.713401  0.767651   -1.515137   \n",
       "7   8.243793e-07  0.000710  0.043701  0.691233  0.741568   -1.697320   \n",
       "8   1.271638e-07  0.000283  0.018216  0.959450  0.979335    0.473793   \n",
       "9   1.338067e-07  0.000288  0.018385  0.951296  0.975409    0.437894   \n",
       "10  1.158964e-09  0.000017  0.000915  0.999409  1.000000    0.998997   \n",
       "11  8.266645e-09  0.000046  0.002399  0.995024  1.000000    0.993059   \n",
       "12  3.886366e-06  0.001365  0.083397 -0.424460  0.000000  -17.737696   \n",
       "13  6.481012e-06  0.001713  0.096120 -1.697787  0.000000  -15.612247   \n",
       "14  2.154125e-06  0.001034  0.071465  0.818804  0.882942  -11.716410   \n",
       "15  4.247056e-06  0.001420  0.087185 -0.364015  0.706282  -14.074079   \n",
       "16  8.776951e-08  0.000165  0.008356  0.896669  1.000000    0.934986   \n",
       "17  4.175642e-07  0.000325  0.015694  0.706580  1.000000    0.697966   \n",
       "18  2.728538e-06  0.001154  0.070121  0.232811  0.557284  -10.051742   \n",
       "19  2.693151e-06  0.001146  0.069583  0.147030  0.497931   -8.559309   \n",
       "20  2.637443e-07  0.000408  0.025623  0.938188  0.959178   -0.238522   \n",
       "21  2.888311e-07  0.000421  0.026262  0.916057  0.931158   -0.136907   \n",
       "22  6.720290e-09  0.000041  0.002165  0.996866  1.000000    0.994171   \n",
       "23  3.481531e-08  0.000095  0.005039  0.983613  1.000000    0.969825   \n",
       "24  3.260420e-06  0.001418  0.093121  0.701397  0.745905  -11.009008   \n",
       "25  3.107212e-06  0.001381  0.091012  0.696861  0.703439  -14.806078   \n",
       "26  2.380132e-07  0.000391  0.025950  0.969351  0.968733    0.189519   \n",
       "27  2.633494e-07  0.000413  0.026980  0.951169  0.950673   -0.067929   \n",
       "28  1.488652e-08  0.000060  0.003150  0.991520  1.000000    0.990064   \n",
       "29  5.793416e-08  0.000135  0.007174  0.966132  1.000000    0.946783   \n",
       "30  5.182210e-06  0.001809  0.117959 -2.363450  0.499059  -39.916896   \n",
       "31  1.570441e-05  0.002829  0.162454 -0.223449  0.000000 -109.490867   \n",
       "32  2.716427e-06  0.001217  0.070361 -3.662871  0.944837  -11.330361   \n",
       "33  9.048410e-06  0.001862  0.093964 -5.695734  0.172969   -7.994005   \n",
       "34  6.581980e-08  0.000165  0.008624  0.853370  1.000000    0.866559   \n",
       "35  9.649102e-07  0.000470  0.022996  0.268229  1.000000    0.425495   \n",
       "36  4.340974e-07  0.000504  0.030557  0.751028  0.915744   -0.065485   \n",
       "37  4.433512e-07  0.000508  0.030724  0.768646  0.914059   -0.108448   \n",
       "38  1.020595e-07  0.000246  0.015255  0.968647  0.977010    0.450683   \n",
       "39  1.075825e-07  0.000249  0.015339  0.966606  0.975587    0.425588   \n",
       "40  6.428365e-10  0.000012  0.000643  0.999621  1.000000    0.999430   \n",
       "41  4.434288e-09  0.000033  0.001698  0.997109  1.000000    0.995909   \n",
       "42  6.159089e-06  0.001919  0.106892 -1.426024  0.000000  -12.607958   \n",
       "43  6.662944e-06  0.001961  0.111363 -2.132991  0.000000  -13.669665   \n",
       "44  2.156408e-07  0.000378  0.024893  0.978001  0.974115   -0.160297   \n",
       "45  4.890555e-07  0.000509  0.030883  0.822910  0.830078   -0.709324   \n",
       "46  4.137943e-08  0.000114  0.005999  0.988266  1.000000    0.958866   \n",
       "47  1.289478e-07  0.000216  0.011486  0.934343  1.000000    0.859837   \n",
       "\n",
       "       minAggR2              Model    Set  Train Size  \n",
       "0    -29.030809              Fresh  Train         180  \n",
       "1    -30.468199              Fresh   Test         180  \n",
       "2     -1.486604  Transfer learning  Train         180  \n",
       "3     -1.780260  Transfer learning   Test         180  \n",
       "4      0.992229      Random Forest  Train         180  \n",
       "5      0.960091      Random Forest   Test         180  \n",
       "6    -25.614310              Fresh  Train         445  \n",
       "7    -27.940751              Fresh   Test         445  \n",
       "8     -0.536891  Transfer learning  Train         445  \n",
       "9     -0.740617  Transfer learning   Test         445  \n",
       "10     0.997374      Random Forest  Train         445  \n",
       "11     0.981957      Random Forest   Test         445  \n",
       "12  -211.298138              Fresh  Train           9  \n",
       "13  -100.271279              Fresh   Test           9  \n",
       "14   -91.798700  Transfer learning  Train           9  \n",
       "15  -121.411290  Transfer learning   Test           9  \n",
       "16     0.879464      Random Forest  Train           9  \n",
       "17     0.275579      Random Forest   Test           9  \n",
       "18  -110.696062              Fresh  Train          92  \n",
       "19   -91.717945              Fresh   Test          92  \n",
       "20    -3.772992  Transfer learning  Train          92  \n",
       "21    -2.912010  Transfer learning   Test          92  \n",
       "22     0.986461      Random Forest  Train          92  \n",
       "23     0.938624      Random Forest   Test          92  \n",
       "24   -86.632650              Fresh  Train          47  \n",
       "25  -122.583282              Fresh   Test          47  \n",
       "26    -2.331655  Transfer learning  Train          47  \n",
       "27    -3.299464  Transfer learning   Test          47  \n",
       "28     0.975970      Random Forest  Train          47  \n",
       "29     0.860296      Random Forest   Test          47  \n",
       "30  -220.534163              Fresh  Train           4  \n",
       "31 -1060.577924              Fresh   Test           4  \n",
       "32   -45.592655  Transfer learning  Train           4  \n",
       "33   -30.036171  Transfer learning   Test           4  \n",
       "34     0.672870      Random Forest  Train           4  \n",
       "35    -0.033782      Random Forest   Test           4  \n",
       "36   -10.513129              Fresh  Train         909  \n",
       "37   -11.343919              Fresh   Test         909  \n",
       "38    -2.114185  Transfer learning  Train         909  \n",
       "39    -2.268542  Transfer learning   Test         909  \n",
       "40     0.998656      Random Forest  Train         909  \n",
       "41     0.988855      Random Forest   Test         909  \n",
       "42   -74.470042              Fresh  Train          18  \n",
       "43   -80.265803              Fresh   Test          18  \n",
       "44    -6.350562  Transfer learning  Train          18  \n",
       "45    -7.715040  Transfer learning   Test          18  \n",
       "46     0.891131      Random Forest  Train          18  \n",
       "47     0.680350      Random Forest   Test          18  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(allResults)\n",
    "df = pd.read_csv('results/transferLrn_des7_tower5_03/testResults.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('Fresh', 'GCN')\n",
    "df = df.replace('Transfer learning', 'GCN with transfer learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-09a34d7346514eb68d59bc8b5cdd2dab\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-09a34d7346514eb68d59bc8b5cdd2dab\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-09a34d7346514eb68d59bc8b5cdd2dab\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-67bcb45ddd6545efd3bf16d556b50366\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Model\"}, {\"type\": \"quantitative\", \"field\": \"mse\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Train Size\", \"scale\": {\"type\": \"log\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".1e\"}, \"field\": \"mse\", \"scale\": {}, \"title\": \"MSE\"}}, \"height\": 200, \"title\": \"Transfer learning - tower\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-67bcb45ddd6545efd3bf16d556b50366\": [{\"mse\": 1.6489869949509741e-06, \"mae\": 0.0010219491086900234, \"mre\": 0.06241277605295181, \"peakR2\": 0.468562671217021, \"maxAggR2\": 0.5065755798010573, \"meanAggR2\": -4.2940499146652895, \"minAggR2\": -30.468198587032266, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 2.06169318062166e-07, \"mae\": 0.0003684453258756548, \"mre\": 0.02422749251127243, \"peakR2\": 0.9451784527343684, \"maxAggR2\": 0.9455989815972404, \"meanAggR2\": 0.2311299290426655, \"minAggR2\": -1.7802604470678594, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 2.270367984799143e-08, \"mae\": 7.289101560977701e-05, \"mre\": 0.0038753485686063268, \"peakR2\": 0.9890216246094508, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9820405842532182, \"minAggR2\": 0.9600907713301188, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 8.243792990469957e-07, \"mae\": 0.0007101365481503308, \"mre\": 0.04370100051164627, \"peakR2\": 0.6912330410299372, \"maxAggR2\": 0.7415684656875812, \"meanAggR2\": -1.6973199450110747, \"minAggR2\": -27.940750607885647, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 1.3380666530338203e-07, \"mae\": 0.00028754895902238786, \"mre\": 0.01838509179651737, \"peakR2\": 0.9512955919127491, \"maxAggR2\": 0.9754092853010448, \"meanAggR2\": 0.43789412941085065, \"minAggR2\": -0.7406172039730625, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 8.266644528720645e-09, \"mae\": 4.55747430360581e-05, \"mre\": 0.0023994864884670022, \"peakR2\": 0.9950241273824098, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9930588024617784, \"minAggR2\": 0.9819567938952988, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 6.481012405856745e-06, \"mae\": 0.0017128720646724105, \"mre\": 0.09612014144659042, \"peakR2\": -1.6977867885747786, \"maxAggR2\": 0.0, \"meanAggR2\": -15.612247494430413, \"minAggR2\": -100.27127863757569, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 4.247056494932622e-06, \"mae\": 0.001419674139469862, \"mre\": 0.08718487620353699, \"peakR2\": -0.3640148100937981, \"maxAggR2\": 0.7062819792997681, \"meanAggR2\": -14.074079144121162, \"minAggR2\": -121.41129045601409, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 4.175642200339261e-07, \"mae\": 0.00032462566316766167, \"mre\": 0.015694297850164775, \"peakR2\": 0.7065797970660795, \"maxAggR2\": 1.0, \"meanAggR2\": 0.6979656153818867, \"minAggR2\": 0.2755788823827131, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 2.693151373023284e-06, \"mae\": 0.0011457019718363881, \"mre\": 0.0695832222700119, \"peakR2\": 0.14703023156700334, \"maxAggR2\": 0.4979313741251361, \"meanAggR2\": -8.5593090192651, \"minAggR2\": -91.717944669452, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 2.888310746129718e-07, \"mae\": 0.0004212940984871239, \"mre\": 0.02626221813261509, \"peakR2\": 0.9160571035839692, \"maxAggR2\": 0.9311578126024892, \"meanAggR2\": -0.13690667730769085, \"minAggR2\": -2.9120098973183883, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 3.4815307127164546e-08, \"mae\": 9.45390471367948e-05, \"mre\": 0.0050387775673914935, \"peakR2\": 0.9836131669883212, \"maxAggR2\": 1.0, \"meanAggR2\": 0.96982472325747, \"minAggR2\": 0.9386242636165608, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 3.1072115689312336e-06, \"mae\": 0.0013808079529553652, \"mre\": 0.09101150929927826, \"peakR2\": 0.6968612465683868, \"maxAggR2\": 0.7034393703163916, \"meanAggR2\": -14.8060780589552, \"minAggR2\": -122.58328202507884, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 2.6334939207117714e-07, \"mae\": 0.000413340050727129, \"mre\": 0.026980215683579445, \"peakR2\": 0.9511693498308572, \"maxAggR2\": 0.950672848869046, \"meanAggR2\": -0.0679292868965901, \"minAggR2\": -3.2994642714338065, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 5.793415611979296e-08, \"mae\": 0.00013473150205925444, \"mre\": 0.007173647080262205, \"peakR2\": 0.9661316567584228, \"maxAggR2\": 1.0, \"meanAggR2\": 0.946783168711852, \"minAggR2\": 0.8602955623016051, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 1.5704410543548875e-05, \"mae\": 0.0028294650837779045, \"mre\": 0.16245360672473907, \"peakR2\": -0.22344917631205896, \"maxAggR2\": 0.0, \"meanAggR2\": -109.49086743229373, \"minAggR2\": -1060.5779235528792, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 9.048409992828963e-06, \"mae\": 0.0018616027664393187, \"mre\": 0.09396380186080933, \"peakR2\": -5.695734042738723, \"maxAggR2\": 0.17296933501166525, \"meanAggR2\": -7.994004644610164, \"minAggR2\": -30.0361713153469, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 9.64910236938212e-07, \"mae\": 0.0004695843252824453, \"mre\": 0.02299636026565237, \"peakR2\": 0.26822868657869114, \"maxAggR2\": 1.0, \"meanAggR2\": 0.425495037103728, \"minAggR2\": -0.03378223906838218, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 4.4335118332128326e-07, \"mae\": 0.000507935241330415, \"mre\": 0.030724188312888145, \"peakR2\": 0.7686463178535713, \"maxAggR2\": 0.9140586693512788, \"meanAggR2\": -0.10844759908494506, \"minAggR2\": -11.343918826331015, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 1.0758245849729063e-07, \"mae\": 0.00024891059729270637, \"mre\": 0.015338900499045847, \"peakR2\": 0.9666060744315429, \"maxAggR2\": 0.9755866052236092, \"meanAggR2\": 0.4255879965415104, \"minAggR2\": -2.2685418986872428, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 4.4342884381791865e-09, \"mae\": 3.251142771237028e-05, \"mre\": 0.0016980734025805475, \"peakR2\": 0.9971092063052815, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9959087157619548, \"minAggR2\": 0.9888553174034656, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 6.662943633273244e-06, \"mae\": 0.001960789784789085, \"mre\": 0.11136305332183838, \"peakR2\": -2.132990904120473, \"maxAggR2\": 0.0, \"meanAggR2\": -13.669664611887345, \"minAggR2\": -80.2658025512335, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 4.89055480556999e-07, \"mae\": 0.0005085978773422539, \"mre\": 0.030883362516760826, \"peakR2\": 0.8229100792368672, \"maxAggR2\": 0.8300775923124314, \"meanAggR2\": -0.7093240327751481, \"minAggR2\": -7.7150399642133225, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 1.2894778664323496e-07, \"mae\": 0.00021603129309523102, \"mre\": 0.011485813718158742, \"peakR2\": 0.9343427224081748, \"maxAggR2\": 1.0, \"meanAggR2\": 0.8598370997927707, \"minAggR2\": 0.6803503766510703, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 18}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df.Set=='Test']).mark_circle().encode(\n",
    "    x=alt.X('Train Size:Q', scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('mse:Q', title='MSE', scale=alt.Scale(), axis=alt.Axis(format='.1e')),\n",
    "    color='Model',\n",
    "    tooltip=['Model', 'mse']\n",
    ").properties(width=400, height=200, title='Transfer learning - tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-cfc07183bd594c48b945431b41fdd860\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cfc07183bd594c48b945431b41fdd860\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cfc07183bd594c48b945431b41fdd860\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3d22c7f1d34e539f7ff3b265b415a17c\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"trial\"}, \"opacity\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false}, \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false, \"title\": \"loss\"}, \"field\": \"value\", \"scale\": {\"clamp\": true, \"domain\": [0, 1]}}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-3d22c7f1d34e539f7ff3b265b415a17c\": [{\"epoch\": 0, \"train\": 0.8004417419433594, \"val\": 0.8789132568571303, \"time\": 0.1605372428894043, \"trial\": \"gcn\"}, {\"epoch\": 1, \"train\": 0.5495244264602661, \"val\": 0.868705047501458, \"time\": 0.1655752658843994, \"trial\": \"gcn\"}, {\"epoch\": 2, \"train\": 0.3824802339076996, \"val\": 0.8547419110933939, \"time\": 0.1705327033996582, \"trial\": \"gcn\"}, {\"epoch\": 3, \"train\": 0.2612672448158264, \"val\": 0.8295939432250129, \"time\": 0.16559839248657227, \"trial\": \"gcn\"}, {\"epoch\": 4, \"train\": 0.16430994868278506, \"val\": 0.8006395896275839, \"time\": 0.17150354385375974, \"trial\": \"gcn\"}, {\"epoch\": 5, \"train\": 0.11610889434814453, \"val\": 0.7767037848631541, \"time\": 0.1654491424560547, \"trial\": \"gcn\"}, {\"epoch\": 6, \"train\": 0.12312401831150055, \"val\": 0.7575336396694183, \"time\": 0.1658172607421875, \"trial\": \"gcn\"}, {\"epoch\": 7, \"train\": 0.10665017366409303, \"val\": 0.7482667863368988, \"time\": 0.1663968563079834, \"trial\": \"gcn\"}, {\"epoch\": 8, \"train\": 0.1030062660574913, \"val\": 0.7434307833512624, \"time\": 0.16658687591552734, \"trial\": \"gcn\"}, {\"epoch\": 9, \"train\": 0.09082944691181184, \"val\": 0.7441885537571378, \"time\": 0.16777276992797852, \"trial\": \"gcn\"}, {\"epoch\": 10, \"train\": 0.07832977920770645, \"val\": 0.7465893593099382, \"time\": 0.16700983047485352, \"trial\": \"gcn\"}, {\"epoch\": 11, \"train\": 0.06557910144329071, \"val\": 0.7435068918599023, \"time\": 0.16808223724365234, \"trial\": \"gcn\"}, {\"epoch\": 12, \"train\": 0.056365199387073524, \"val\": 0.7409786317083571, \"time\": 0.17022109031677246, \"trial\": \"gcn\"}, {\"epoch\": 13, \"train\": 0.05156687647104263, \"val\": 0.7194984522130754, \"time\": 0.16625118255615234, \"trial\": \"gcn\"}, {\"epoch\": 14, \"train\": 0.04910103976726532, \"val\": 0.6616801934109794, \"time\": 0.1665666103363037, \"trial\": \"gcn\"}, {\"epoch\": 15, \"train\": 0.046580720692873, \"val\": 0.5821615805228552, \"time\": 0.1681065559387207, \"trial\": \"gcn\"}, {\"epoch\": 16, \"train\": 0.043124187737703316, \"val\": 0.5251175297631158, \"time\": 0.16576814651489258, \"trial\": \"gcn\"}, {\"epoch\": 17, \"train\": 0.04074280709028244, \"val\": 0.4752245611614651, \"time\": 0.1696615219116211, \"trial\": \"gcn\"}, {\"epoch\": 18, \"train\": 0.03884470090270042, \"val\": 0.4260303543673621, \"time\": 0.16692900657653809, \"trial\": \"gcn\"}, {\"epoch\": 19, \"train\": 0.03636736050248146, \"val\": 0.3768801035152541, \"time\": 0.16516995429992676, \"trial\": \"gcn\"}, {\"epoch\": 20, \"train\": 0.034743402153253555, \"val\": 0.3344507896237903, \"time\": 0.16911935806274414, \"trial\": \"gcn\"}, {\"epoch\": 21, \"train\": 0.03335585817694664, \"val\": 0.29034623503685, \"time\": 0.16843080520629886, \"trial\": \"gcn\"}, {\"epoch\": 22, \"train\": 0.03219809755682945, \"val\": 0.2372567032774289, \"time\": 0.17208385467529294, \"trial\": \"gcn\"}, {\"epoch\": 23, \"train\": 0.031485002487897866, \"val\": 0.19842853314346734, \"time\": 0.1685187816619873, \"trial\": \"gcn\"}, {\"epoch\": 24, \"train\": 0.03064012341201305, \"val\": 0.1785741994778315, \"time\": 0.1655585765838623, \"trial\": \"gcn\"}, {\"epoch\": 25, \"train\": 0.029804565012454987, \"val\": 0.12338984509309132, \"time\": 0.1681368350982666, \"trial\": \"gcn\"}, {\"epoch\": 26, \"train\": 0.02949709445238113, \"val\": 0.08083313206831615, \"time\": 0.16832852363586426, \"trial\": \"gcn\"}, {\"epoch\": 27, \"train\": 0.02885489538311958, \"val\": 0.07574792951345444, \"time\": 0.16807866096496582, \"trial\": \"gcn\"}, {\"epoch\": 28, \"train\": 0.0283745639026165, \"val\": 0.10526540564994018, \"time\": 0.16815853118896484, \"trial\": \"gcn\"}, {\"epoch\": 29, \"train\": 0.028049727901816368, \"val\": 0.14655965152713987, \"time\": 0.16466760635375974, \"trial\": \"gcn\"}, {\"epoch\": 30, \"train\": 0.027629591524600983, \"val\": 0.18480416449407736, \"time\": 0.16826748847961426, \"trial\": \"gcn\"}, {\"epoch\": 31, \"train\": 0.02730837091803551, \"val\": 0.2278419420537021, \"time\": 0.174513578414917, \"trial\": \"gcn\"}, {\"epoch\": 32, \"train\": 0.026940783485770226, \"val\": 0.25572465484340984, \"time\": 0.17462801933288574, \"trial\": \"gcn\"}, {\"epoch\": 33, \"train\": 0.026671549305319786, \"val\": 0.2689196038991213, \"time\": 0.17000436782836914, \"trial\": \"gcn\"}, {\"epoch\": 34, \"train\": 0.02643587440252304, \"val\": 0.2922684670322471, \"time\": 0.1674206256866455, \"trial\": \"gcn\"}, {\"epoch\": 35, \"train\": 0.026109537109732628, \"val\": 0.3073088311486774, \"time\": 0.17750883102416992, \"trial\": \"gcn\"}, {\"epoch\": 36, \"train\": 0.025810541585087776, \"val\": 0.3031055693411165, \"time\": 0.17292284965515134, \"trial\": \"gcn\"}, {\"epoch\": 37, \"train\": 0.025535769760608677, \"val\": 0.29964690158764523, \"time\": 0.17852473258972168, \"trial\": \"gcn\"}, {\"epoch\": 38, \"train\": 0.025214500725269318, \"val\": 0.2984040131171544, \"time\": 0.1814148426055908, \"trial\": \"gcn\"}, {\"epoch\": 39, \"train\": 0.02483456768095493, \"val\": 0.2851703564325969, \"time\": 0.17809152603149414, \"trial\": \"gcn\"}, {\"epoch\": 40, \"train\": 0.02438371255993843, \"val\": 0.2746428901122676, \"time\": 0.16515493392944336, \"trial\": \"gcn\"}, {\"epoch\": 41, \"train\": 0.023888189345598217, \"val\": 0.2729983718858825, \"time\": 0.16711163520812988, \"trial\": \"gcn\"}, {\"epoch\": 42, \"train\": 0.02325014024972916, \"val\": 0.26034676614734864, \"time\": 0.18917489051818848, \"trial\": \"gcn\"}, {\"epoch\": 43, \"train\": 0.02246922627091408, \"val\": 0.25591798106001484, \"time\": 0.16687560081481936, \"trial\": \"gcn\"}, {\"epoch\": 44, \"train\": 0.021574487909674644, \"val\": 0.2511673325465785, \"time\": 0.16649556159973145, \"trial\": \"gcn\"}, {\"epoch\": 45, \"train\": 0.020739128813147545, \"val\": 0.2409726041886541, \"time\": 0.1672370433807373, \"trial\": \"gcn\"}, {\"epoch\": 46, \"train\": 0.02014176733791828, \"val\": 0.2461368383632766, \"time\": 0.18227386474609372, \"trial\": \"gcn\"}, {\"epoch\": 47, \"train\": 0.019792292267084118, \"val\": 0.22961484289003745, \"time\": 0.18781542778015134, \"trial\": \"gcn\"}, {\"epoch\": 48, \"train\": 0.019551297649741173, \"val\": 0.2480574490295516, \"time\": 0.18942570686340326, \"trial\": \"gcn\"}, {\"epoch\": 49, \"train\": 0.01951760426163673, \"val\": 0.218913325212068, \"time\": 0.1704413890838623, \"trial\": \"gcn\"}, {\"epoch\": 50, \"train\": 0.019962288439273838, \"val\": 0.2485176945726077, \"time\": 0.16934919357299805, \"trial\": \"gcn\"}, {\"epoch\": 51, \"train\": 0.0198040921241045, \"val\": 0.20740140229463574, \"time\": 0.16574597358703613, \"trial\": \"gcn\"}, {\"epoch\": 52, \"train\": 0.01901196874678135, \"val\": 0.22517829057243136, \"time\": 0.1668262481689453, \"trial\": \"gcn\"}, {\"epoch\": 53, \"train\": 0.0181727297604084, \"val\": 0.2206047661602497, \"time\": 0.16825532913208008, \"trial\": \"gcn\"}, {\"epoch\": 54, \"train\": 0.017802206799387932, \"val\": 0.1910494762576289, \"time\": 0.16517043113708496, \"trial\": \"gcn\"}, {\"epoch\": 55, \"train\": 0.017713820561766624, \"val\": 0.20099846811758149, \"time\": 0.16537761688232422, \"trial\": \"gcn\"}, {\"epoch\": 56, \"train\": 0.01688988134264946, \"val\": 0.1913374084979296, \"time\": 0.16803383827209473, \"trial\": \"gcn\"}, {\"epoch\": 57, \"train\": 0.016504336148500443, \"val\": 0.17007105404304135, \"time\": 0.16595149040222168, \"trial\": \"gcn\"}, {\"epoch\": 58, \"train\": 0.016283022239804268, \"val\": 0.17334417150252396, \"time\": 0.16708874702453613, \"trial\": \"gcn\"}, {\"epoch\": 59, \"train\": 0.015789026394486427, \"val\": 0.169048474687669, \"time\": 0.1676790714263916, \"trial\": \"gcn\"}, {\"epoch\": 60, \"train\": 0.015562092885375025, \"val\": 0.1622085351910856, \"time\": 0.16382837295532227, \"trial\": \"gcn\"}, {\"epoch\": 61, \"train\": 0.015403921715915203, \"val\": 0.16994519076413578, \"time\": 0.16449356079101562, \"trial\": \"gcn\"}, {\"epoch\": 62, \"train\": 0.01508946530520916, \"val\": 0.16632225840455955, \"time\": 0.17095446586608887, \"trial\": \"gcn\"}, {\"epoch\": 63, \"train\": 0.014764020219445229, \"val\": 0.1593734822753403, \"time\": 0.16530919075012207, \"trial\": \"gcn\"}, {\"epoch\": 64, \"train\": 0.014586055651307106, \"val\": 0.16182775071097746, \"time\": 0.1685800552368164, \"trial\": \"gcn\"}, {\"epoch\": 65, \"train\": 0.014320063404738903, \"val\": 0.1488054162926144, \"time\": 0.1727604866027832, \"trial\": \"gcn\"}, {\"epoch\": 66, \"train\": 0.013884858228266241, \"val\": 0.1395365684810612, \"time\": 0.1652357578277588, \"trial\": \"gcn\"}, {\"epoch\": 67, \"train\": 0.013645326718688013, \"val\": 0.14309275026122728, \"time\": 0.16490721702575686, \"trial\": \"gcn\"}, {\"epoch\": 68, \"train\": 0.01346330065280199, \"val\": 0.13508781314724022, \"time\": 0.17078614234924314, \"trial\": \"gcn\"}, {\"epoch\": 69, \"train\": 0.01321288850158453, \"val\": 0.13687021533648172, \"time\": 0.16560935974121094, \"trial\": \"gcn\"}, {\"epoch\": 70, \"train\": 0.012835679575800896, \"val\": 0.13161180251174506, \"time\": 0.1670997142791748, \"trial\": \"gcn\"}, {\"epoch\": 71, \"train\": 0.012528172694146631, \"val\": 0.12508930121031073, \"time\": 0.16774749755859375, \"trial\": \"gcn\"}, {\"epoch\": 72, \"train\": 0.01221243292093277, \"val\": 0.12205682922568585, \"time\": 0.1694960594177246, \"trial\": \"gcn\"}, {\"epoch\": 73, \"train\": 0.011925730854272842, \"val\": 0.11713343382709555, \"time\": 0.16577577590942386, \"trial\": \"gcn\"}, {\"epoch\": 74, \"train\": 0.011701100505888462, \"val\": 0.11455859492222467, \"time\": 0.16738533973693848, \"trial\": \"gcn\"}, {\"epoch\": 75, \"train\": 0.011474676430225372, \"val\": 0.11068630549642773, \"time\": 0.16729187965393064, \"trial\": \"gcn\"}, {\"epoch\": 76, \"train\": 0.011467921547591686, \"val\": 0.11271277132133645, \"time\": 0.16814661026000974, \"trial\": \"gcn\"}, {\"epoch\": 77, \"train\": 0.012623590417206287, \"val\": 0.11149358418252732, \"time\": 0.16936016082763672, \"trial\": \"gcn\"}, {\"epoch\": 78, \"train\": 0.015396581962704659, \"val\": 0.12849151177538765, \"time\": 0.16604137420654294, \"trial\": \"gcn\"}, {\"epoch\": 79, \"train\": 0.015344408340752125, \"val\": 0.09130547092192703, \"time\": 0.16556835174560547, \"trial\": \"gcn\"}, {\"epoch\": 80, \"train\": 0.017477618530392647, \"val\": 0.12571539978186289, \"time\": 0.17172503471374512, \"trial\": \"gcn\"}, {\"epoch\": 81, \"train\": 0.014519271440804003, \"val\": 0.11499995655483668, \"time\": 0.16549181938171387, \"trial\": \"gcn\"}, {\"epoch\": 82, \"train\": 0.011368946172297, \"val\": 0.09247424577673276, \"time\": 0.1640787124633789, \"trial\": \"gcn\"}, {\"epoch\": 83, \"train\": 0.012702882289886476, \"val\": 0.10018816673093373, \"time\": 0.1639420986175537, \"trial\": \"gcn\"}, {\"epoch\": 84, \"train\": 0.010082046501338482, \"val\": 0.10709351197712952, \"time\": 0.16578960418701172, \"trial\": \"gcn\"}, {\"epoch\": 85, \"train\": 0.010712409391999243, \"val\": 0.09019424517949423, \"time\": 0.16556692123413086, \"trial\": \"gcn\"}, {\"epoch\": 86, \"train\": 0.008513064123690128, \"val\": 0.08018205666707623, \"time\": 0.16490602493286133, \"trial\": \"gcn\"}, {\"epoch\": 87, \"train\": 0.008150415495038033, \"val\": 0.08342876492275132, \"time\": 0.16771960258483887, \"trial\": \"gcn\"}, {\"epoch\": 88, \"train\": 0.006441979669034481, \"val\": 0.08350362752874692, \"time\": 0.1710059642791748, \"trial\": \"gcn\"}, {\"epoch\": 89, \"train\": 0.011641277000308037, \"val\": 0.08360644098785189, \"time\": 0.17359352111816406, \"trial\": \"gcn\"}, {\"epoch\": 90, \"train\": 0.015998082235455513, \"val\": 0.07853677144481076, \"time\": 0.16874194145202634, \"trial\": \"gcn\"}, {\"epoch\": 91, \"train\": 0.012771755456924437, \"val\": 0.0818041165669759, \"time\": 0.17000889778137207, \"trial\": \"gcn\"}, {\"epoch\": 92, \"train\": 0.0063239545561373225, \"val\": 0.08787039149966505, \"time\": 0.16669559478759766, \"trial\": \"gcn\"}, {\"epoch\": 93, \"train\": 0.006250049453228712, \"val\": 0.07985058354420795, \"time\": 0.17034053802490234, \"trial\": \"gcn\"}, {\"epoch\": 94, \"train\": 0.0055848597548902035, \"val\": 0.07513020146224234, \"time\": 0.1660444736480713, \"trial\": \"gcn\"}, {\"epoch\": 95, \"train\": 0.004405052866786718, \"val\": 0.08540544027669562, \"time\": 0.1662280559539795, \"trial\": \"gcn\"}, {\"epoch\": 96, \"train\": 0.0028846170753240585, \"val\": 0.09040850131875937, \"time\": 0.16759920120239258, \"trial\": \"gcn\"}, {\"epoch\": 97, \"train\": 0.003045121906325221, \"val\": 0.07979842089116572, \"time\": 0.1658000946044922, \"trial\": \"gcn\"}, {\"epoch\": 98, \"train\": 0.002329687587916851, \"val\": 0.07367007175667419, \"time\": 0.16524600982666016, \"trial\": \"gcn\"}, {\"epoch\": 99, \"train\": 0.0021410565823316574, \"val\": 0.07527350711946686, \"time\": 0.16821670532226562, \"trial\": \"gcn\"}, {\"epoch\": 0, \"train\": 1.2497053146362305, \"val\": 105.58094957139757, \"time\": 0.1743621826171875, \"trial\": \"ptrGcn\"}, {\"epoch\": 1, \"train\": 0.3762534856796265, \"val\": 6.879347456826104, \"time\": 0.1654214859008789, \"trial\": \"ptrGcn\"}, {\"epoch\": 2, \"train\": 0.18914838135242465, \"val\": 2.7727909684181213, \"time\": 0.1668992042541504, \"trial\": \"ptrGcn\"}, {\"epoch\": 3, \"train\": 0.1333148628473282, \"val\": 2.2430215345488653, \"time\": 0.16692376136779785, \"trial\": \"ptrGcn\"}, {\"epoch\": 4, \"train\": 0.11167073249816896, \"val\": 1.4621253278520372, \"time\": 0.1670377254486084, \"trial\": \"ptrGcn\"}, {\"epoch\": 5, \"train\": 0.09643328934907912, \"val\": 0.7769992748896279, \"time\": 0.16721606254577634, \"trial\": \"ptrGcn\"}, {\"epoch\": 6, \"train\": 0.09378580003976822, \"val\": 0.5971881416108873, \"time\": 0.1733860969543457, \"trial\": \"ptrGcn\"}, {\"epoch\": 7, \"train\": 0.08890943974256517, \"val\": 0.5922087381283442, \"time\": 0.16179633140563965, \"trial\": \"ptrGcn\"}, {\"epoch\": 8, \"train\": 0.08728703111410141, \"val\": 0.3920153855449624, \"time\": 0.1741800308227539, \"trial\": \"ptrGcn\"}, {\"epoch\": 9, \"train\": 0.08372864872217177, \"val\": 0.25822708672947353, \"time\": 0.16824603080749512, \"trial\": \"ptrGcn\"}, {\"epoch\": 10, \"train\": 0.07349336892366409, \"val\": 0.22405890996257466, \"time\": 0.1710970401763916, \"trial\": \"ptrGcn\"}, {\"epoch\": 11, \"train\": 0.06702898442745209, \"val\": 0.19260171718067592, \"time\": 0.17534732818603516, \"trial\": \"ptrGcn\"}, {\"epoch\": 12, \"train\": 0.06252995878458023, \"val\": 0.1497462505681647, \"time\": 0.16848134994506836, \"trial\": \"ptrGcn\"}, {\"epoch\": 13, \"train\": 0.058864429593086236, \"val\": 0.1190936757872502, \"time\": 0.1669313907623291, \"trial\": \"ptrGcn\"}, {\"epoch\": 14, \"train\": 0.05571182444691658, \"val\": 0.09611732367840077, \"time\": 0.16652536392211914, \"trial\": \"ptrGcn\"}, {\"epoch\": 15, \"train\": 0.053266111761331565, \"val\": 0.08470849754909675, \"time\": 0.17064189910888672, \"trial\": \"ptrGcn\"}, {\"epoch\": 16, \"train\": 0.05161194503307343, \"val\": 0.07819391311042839, \"time\": 0.1669323444366455, \"trial\": \"ptrGcn\"}, {\"epoch\": 17, \"train\": 0.05060257762670517, \"val\": 0.0743328347388241, \"time\": 0.169752836227417, \"trial\": \"ptrGcn\"}, {\"epoch\": 18, \"train\": 0.04894358292222023, \"val\": 0.07175453503926595, \"time\": 0.16698670387268064, \"trial\": \"ptrGcn\"}, {\"epoch\": 19, \"train\": 0.04726635664701462, \"val\": 0.0692078669865926, \"time\": 0.16361618041992188, \"trial\": \"ptrGcn\"}, {\"epoch\": 20, \"train\": 0.0460769347846508, \"val\": 0.06380963780813748, \"time\": 0.16259527206420898, \"trial\": \"ptrGcn\"}, {\"epoch\": 21, \"train\": 0.04413653910160065, \"val\": 0.057509782827562764, \"time\": 0.1618785858154297, \"trial\": \"ptrGcn\"}, {\"epoch\": 22, \"train\": 0.042118676006793976, \"val\": 0.05327221916781531, \"time\": 0.16187381744384766, \"trial\": \"ptrGcn\"}, {\"epoch\": 23, \"train\": 0.04019879549741745, \"val\": 0.050636192887193635, \"time\": 0.16157817840576172, \"trial\": \"ptrGcn\"}, {\"epoch\": 24, \"train\": 0.037725884467363364, \"val\": 0.050155233281354114, \"time\": 0.16278839111328125, \"trial\": \"ptrGcn\"}, {\"epoch\": 25, \"train\": 0.035631638020277016, \"val\": 0.046352996594376035, \"time\": 0.1672680377960205, \"trial\": \"ptrGcn\"}, {\"epoch\": 26, \"train\": 0.034604139626026154, \"val\": 0.04455366420249144, \"time\": 0.16236186027526855, \"trial\": \"ptrGcn\"}, {\"epoch\": 27, \"train\": 0.032499264925718314, \"val\": 0.043175027084847294, \"time\": 0.16355395317077634, \"trial\": \"ptrGcn\"}, {\"epoch\": 28, \"train\": 0.030983915552496914, \"val\": 0.03906268481579092, \"time\": 0.16274476051330564, \"trial\": \"ptrGcn\"}, {\"epoch\": 29, \"train\": 0.029512170702219013, \"val\": 0.03919144326614009, \"time\": 0.16324925422668454, \"trial\": \"ptrGcn\"}, {\"epoch\": 30, \"train\": 0.028733581304550167, \"val\": 0.034539464447233416, \"time\": 0.16583037376403809, \"trial\": \"ptrGcn\"}, {\"epoch\": 31, \"train\": 0.028147961944341663, \"val\": 0.03303898819204834, \"time\": 0.16603899002075195, \"trial\": \"ptrGcn\"}, {\"epoch\": 32, \"train\": 0.028729137033224106, \"val\": 0.03582573785550065, \"time\": 0.16268682479858398, \"trial\": \"ptrGcn\"}, {\"epoch\": 33, \"train\": 0.028259171172976494, \"val\": 0.034963616583910256, \"time\": 0.16268587112426758, \"trial\": \"ptrGcn\"}, {\"epoch\": 34, \"train\": 0.02796456962823868, \"val\": 0.0307002743292186, \"time\": 0.16217732429504395, \"trial\": \"ptrGcn\"}, {\"epoch\": 35, \"train\": 0.026730872690677643, \"val\": 0.031086611871918038, \"time\": 0.16314125061035156, \"trial\": \"ptrGcn\"}, {\"epoch\": 36, \"train\": 0.02586226724088192, \"val\": 0.034827750693592764, \"time\": 0.16480684280395508, \"trial\": \"ptrGcn\"}, {\"epoch\": 37, \"train\": 0.025586947798728943, \"val\": 0.03576297944204675, \"time\": 0.16531705856323242, \"trial\": \"ptrGcn\"}, {\"epoch\": 38, \"train\": 0.025040162727236748, \"val\": 0.03402541536423895, \"time\": 0.16216516494750974, \"trial\": \"ptrGcn\"}, {\"epoch\": 39, \"train\": 0.024381859228014943, \"val\": 0.034004209459655814, \"time\": 0.16414189338684082, \"trial\": \"ptrGcn\"}, {\"epoch\": 40, \"train\": 0.023414356634020802, \"val\": 0.03197705621520678, \"time\": 0.16225481033325195, \"trial\": \"ptrGcn\"}, {\"epoch\": 41, \"train\": 0.022987695410847664, \"val\": 0.030040344637301236, \"time\": 0.1619865894317627, \"trial\": \"ptrGcn\"}, {\"epoch\": 42, \"train\": 0.02247106097638607, \"val\": 0.03065917020042737, \"time\": 0.16561293601989746, \"trial\": \"ptrGcn\"}, {\"epoch\": 43, \"train\": 0.02172433026134968, \"val\": 0.029918965780072738, \"time\": 0.16382551193237305, \"trial\": \"ptrGcn\"}, {\"epoch\": 44, \"train\": 0.02127186395227909, \"val\": 0.02862941359894143, \"time\": 0.16174674034118652, \"trial\": \"ptrGcn\"}, {\"epoch\": 45, \"train\": 0.020938901230692863, \"val\": 0.026186341419816017, \"time\": 0.16333866119384766, \"trial\": \"ptrGcn\"}, {\"epoch\": 46, \"train\": 0.02085585333406925, \"val\": 0.028560791888998613, \"time\": 0.16400384902954102, \"trial\": \"ptrGcn\"}, {\"epoch\": 47, \"train\": 0.02243038453161716, \"val\": 0.027735387492511008, \"time\": 0.16703319549560547, \"trial\": \"ptrGcn\"}, {\"epoch\": 48, \"train\": 0.02378175593912601, \"val\": 0.023626529197725985, \"time\": 0.16301846504211426, \"trial\": \"ptrGcn\"}, {\"epoch\": 49, \"train\": 0.01935000531375408, \"val\": 0.02291158855789237, \"time\": 0.16217780113220215, \"trial\": \"ptrGcn\"}, {\"epoch\": 50, \"train\": 0.01876628771424293, \"val\": 0.026826945857869253, \"time\": 0.16231203079223633, \"trial\": \"ptrGcn\"}, {\"epoch\": 51, \"train\": 0.019741712138056755, \"val\": 0.022368467671589717, \"time\": 0.1641104221343994, \"trial\": \"ptrGcn\"}, {\"epoch\": 52, \"train\": 0.016919013112783432, \"val\": 0.02195016512026389, \"time\": 0.16247224807739258, \"trial\": \"ptrGcn\"}, {\"epoch\": 53, \"train\": 0.01869707740843296, \"val\": 0.025458536472999387, \"time\": 0.1637265682220459, \"trial\": \"ptrGcn\"}, {\"epoch\": 54, \"train\": 0.016972489655017853, \"val\": 0.022669462021440268, \"time\": 0.16184329986572266, \"trial\": \"ptrGcn\"}, {\"epoch\": 55, \"train\": 0.01650750264525414, \"val\": 0.019339813436898917, \"time\": 0.16241049766540527, \"trial\": \"ptrGcn\"}, {\"epoch\": 56, \"train\": 0.01672511175274849, \"val\": 0.01860342097158233, \"time\": 0.17567038536071775, \"trial\": \"ptrGcn\"}, {\"epoch\": 57, \"train\": 0.015578864142298697, \"val\": 0.021166766735000744, \"time\": 0.1660141944885254, \"trial\": \"ptrGcn\"}, {\"epoch\": 58, \"train\": 0.01596724987030029, \"val\": 0.019466569233271808, \"time\": 0.16861772537231445, \"trial\": \"ptrGcn\"}, {\"epoch\": 59, \"train\": 0.014830487780272959, \"val\": 0.017869943209613364, \"time\": 0.16414523124694824, \"trial\": \"ptrGcn\"}, {\"epoch\": 60, \"train\": 0.015379807911813259, \"val\": 0.018880335562345054, \"time\": 0.16406035423278809, \"trial\": \"ptrGcn\"}, {\"epoch\": 61, \"train\": 0.014209595508873463, \"val\": 0.023246475133217044, \"time\": 0.16300559043884275, \"trial\": \"ptrGcn\"}, {\"epoch\": 62, \"train\": 0.014514475129544737, \"val\": 0.02028116369102564, \"time\": 0.163743257522583, \"trial\": \"ptrGcn\"}, {\"epoch\": 63, \"train\": 0.013389616273343565, \"val\": 0.01813584590692901, \"time\": 0.16542959213256836, \"trial\": \"ptrGcn\"}, {\"epoch\": 64, \"train\": 0.013452960178256037, \"val\": 0.018590147115497127, \"time\": 0.16988611221313474, \"trial\": \"ptrGcn\"}, {\"epoch\": 65, \"train\": 0.012317842803895472, \"val\": 0.01967510741410984, \"time\": 0.17190980911254886, \"trial\": \"ptrGcn\"}, {\"epoch\": 66, \"train\": 0.012094493024051191, \"val\": 0.01817856159889036, \"time\": 0.1619584560394287, \"trial\": \"ptrGcn\"}, {\"epoch\": 67, \"train\": 0.01069373358041048, \"val\": 0.017188075024427638, \"time\": 0.1622915267944336, \"trial\": \"ptrGcn\"}, {\"epoch\": 68, \"train\": 0.009956995956599712, \"val\": 0.016753372374094196, \"time\": 0.17290115356445312, \"trial\": \"ptrGcn\"}, {\"epoch\": 69, \"train\": 0.0079529769718647, \"val\": 0.01687355370571216, \"time\": 0.16927766799926758, \"trial\": \"ptrGcn\"}, {\"epoch\": 70, \"train\": 0.006288668606430292, \"val\": 0.01549226055956549, \"time\": 0.16499876976013186, \"trial\": \"ptrGcn\"}, {\"epoch\": 71, \"train\": 0.004844222217798233, \"val\": 0.01303729870253139, \"time\": 0.16650986671447754, \"trial\": \"ptrGcn\"}, {\"epoch\": 72, \"train\": 0.00411581713706255, \"val\": 0.014788144966587424, \"time\": 0.16467499732971191, \"trial\": \"ptrGcn\"}, {\"epoch\": 73, \"train\": 0.006499817594885826, \"val\": 0.014091642314775122, \"time\": 0.16749119758605954, \"trial\": \"ptrGcn\"}, {\"epoch\": 74, \"train\": 0.010981316678225994, \"val\": 0.01153001008141372, \"time\": 0.16574621200561526, \"trial\": \"ptrGcn\"}, {\"epoch\": 75, \"train\": 0.010710137896239758, \"val\": 0.00813760886537946, \"time\": 0.16667485237121582, \"trial\": \"ptrGcn\"}, {\"epoch\": 76, \"train\": 0.00525599392130971, \"val\": 0.006367846715470983, \"time\": 0.1685636043548584, \"trial\": \"ptrGcn\"}, {\"epoch\": 77, \"train\": 0.004492953885346651, \"val\": 0.008295001449166901, \"time\": 0.1672821044921875, \"trial\": \"ptrGcn\"}, {\"epoch\": 78, \"train\": 0.0050313672982156285, \"val\": 0.00416976921648408, \"time\": 0.16655659675598145, \"trial\": \"ptrGcn\"}, {\"epoch\": 79, \"train\": 0.0020298617891967297, \"val\": 0.006563744236094256, \"time\": 0.16796207427978516, \"trial\": \"ptrGcn\"}, {\"epoch\": 80, \"train\": 0.004469961859285832, \"val\": 0.0048264437743152176, \"time\": 0.16718745231628418, \"trial\": \"ptrGcn\"}, {\"epoch\": 81, \"train\": 0.002223689109086991, \"val\": 0.004317919962987717, \"time\": 0.1858847141265869, \"trial\": \"ptrGcn\"}, {\"epoch\": 82, \"train\": 0.003166147973388433, \"val\": 0.004653903869135927, \"time\": 0.16788053512573242, \"trial\": \"ptrGcn\"}, {\"epoch\": 83, \"train\": 0.0017933447379618887, \"val\": 0.0079583645824136, \"time\": 0.17513084411621094, \"trial\": \"ptrGcn\"}, {\"epoch\": 84, \"train\": 0.002682601101696491, \"val\": 0.004568885740203162, \"time\": 0.16818761825561526, \"trial\": \"ptrGcn\"}, {\"epoch\": 85, \"train\": 0.001436847960576415, \"val\": 0.004127049999725487, \"time\": 0.16584396362304688, \"trial\": \"ptrGcn\"}, {\"epoch\": 86, \"train\": 0.001990701537579298, \"val\": 0.004253825897143947, \"time\": 0.1650257110595703, \"trial\": \"ptrGcn\"}, {\"epoch\": 87, \"train\": 0.001293709152378142, \"val\": 0.004421431105583906, \"time\": 0.16716432571411133, \"trial\": \"ptrGcn\"}, {\"epoch\": 88, \"train\": 0.0014786802930757403, \"val\": 0.004395929410950177, \"time\": 0.1636483669281006, \"trial\": \"ptrGcn\"}, {\"epoch\": 89, \"train\": 0.0011270581744611265, \"val\": 0.005956357758906152, \"time\": 0.1652214527130127, \"trial\": \"ptrGcn\"}, {\"epoch\": 90, \"train\": 0.0012378966202959418, \"val\": 0.005506007181894448, \"time\": 0.1665785312652588, \"trial\": \"ptrGcn\"}, {\"epoch\": 91, \"train\": 0.0009009049390442668, \"val\": 0.004593452917308443, \"time\": 0.16552948951721191, \"trial\": \"ptrGcn\"}, {\"epoch\": 92, \"train\": 0.0008908976451493801, \"val\": 0.0034390686002249518, \"time\": 0.1652357578277588, \"trial\": \"ptrGcn\"}, {\"epoch\": 93, \"train\": 0.0008307445677928628, \"val\": 0.002579322957899421, \"time\": 0.1729276180267334, \"trial\": \"ptrGcn\"}, {\"epoch\": 94, \"train\": 0.0008005544077605009, \"val\": 0.0026420123823401, \"time\": 0.16499590873718262, \"trial\": \"ptrGcn\"}, {\"epoch\": 95, \"train\": 0.0007621656986884773, \"val\": 0.003169466327461932, \"time\": 0.16456985473632812, \"trial\": \"ptrGcn\"}, {\"epoch\": 96, \"train\": 0.0005989401834085584, \"val\": 0.0030316582285902565, \"time\": 0.16795706748962402, \"trial\": \"ptrGcn\"}, {\"epoch\": 97, \"train\": 0.0005974828964099287, \"val\": 0.002813938836981025, \"time\": 0.1676170825958252, \"trial\": \"ptrGcn\"}, {\"epoch\": 98, \"train\": 0.0004993282491341233, \"val\": 0.004254681498019232, \"time\": 0.16585350036621094, \"trial\": \"ptrGcn\"}, {\"epoch\": 99, \"train\": 0.000585161498747766, \"val\": 0.005077917503917383, \"time\": 0.1666405200958252, \"trial\": \"ptrGcn\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trialDir = 'results/transferLrn_des7_tower5_03/00018/'\n",
    "gcnHist = pd.read_csv(trialDir+'gcn/trainlog.csv', header=None, names=['epoch', 'train', 'val', 'time'])\n",
    "gcnHist['trial'] = 'gcn'\n",
    "ptrGcnHist = pd.read_csv(trialDir+'/ptrGcn/trainlog.csv', header=None, names=['epoch', 'train', 'val', 'time'])\n",
    "ptrGcnHist['trial'] = 'ptrGcn'\n",
    "df = pd.concat([gcnHist, ptrGcnHist], axis=0)\n",
    "\n",
    "alt.Chart(df).transform_fold(\n",
    "            ['train', 'val'],\n",
    "            as_=['set', 'value']\n",
    "            ).mark_line().encode(\n",
    "                alt.X('epoch:Q', axis=alt.Axis(grid=False),),\n",
    "                alt.Y('value:Q', axis=alt.Axis(title='loss', grid=False), scale=alt.Scale(domain=[0,1], clamp=True)),\n",
    "                color=alt.Color('trial:N'),\n",
    "                opacity=alt.Opacity('set:N'),\n",
    "                tooltip=['epoch:Q', 'value:Q']\n",
    "            ).properties(width=400, height=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
