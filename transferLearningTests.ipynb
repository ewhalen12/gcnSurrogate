{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from gcnSurrogate.models.feastnetSurrogateModel import FeaStNet\n",
    "from gcnSurrogate.models.pointRegressorSurrogateModel import PointRegressor\n",
    "from gcnSurrogate.readers.loadConmechGraphs import loadConmechGraphs\n",
    "from gcnSurrogate.visualization.altTrussViz import plotTruss, interactiveErrorPlot\n",
    "from gcnSurrogate.util.gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.476726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.870080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.045080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.089405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.187214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>102.305313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.476726\n",
       "std       4.870080\n",
       "min       0.013036\n",
       "25%       0.045080\n",
       "50%       0.089405\n",
       "75%       0.187214\n",
       "max     102.305313"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataDir = \"data/2D_Truss_v1.3/gh/\"\n",
    "# testFile = os.path.join(dataDir, 'design_9_N_1000.csv')\n",
    "# allGraphsUnfiltered = loadGhGraphs(testFile, NUM_DV=5)\n",
    "dataDir = \"data/endLoadsv1.0/conmech/\"\n",
    "testDir = os.path.join(dataDir, 'design_7_N_1000/')\n",
    "allGraphsUnfiltered = loadConmechGraphs(testDir)\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.107378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.042086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.079557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.139971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.411479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.107378\n",
       "std      0.088163\n",
       "min      0.013036\n",
       "25%      0.042086\n",
       "50%      0.079557\n",
       "75%      0.139971\n",
       "max      0.411479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max().item()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrainFiles = glob.glob(os.path.join(dataDir, '*1000.csv'))\n",
    "# pretrainFiles.remove(testFile)\n",
    "\n",
    "# allPretrainGraphs = []\n",
    "# for pretrainFile in pretrainFiles:\n",
    "#     designName = pretrainDir.split('/')[-1].split('_N')[0]\n",
    "#     print(f'loading {designName}')\n",
    "#     graphsUnfiltered = loadGhGraphs(pretrainFile, NUM_DV=5)\n",
    "#     graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "#     allPretrainGraphs.extend(graphs)\n",
    "\n",
    "# print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "# pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_7\n",
      "loaded 900 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainDirs = glob.glob('data/2D_Truss_v1.3/conmech/design_7_N_1000*/')\n",
    "# pretrainDirs.remove(testDir)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainDir in pretrainDirs:\n",
    "    designName = pretrainDir.split('/')[-2].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    graphsUnfiltered = loadConmechGraphs(pretrainDir)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 8.8630e-01   valLoss:8.6201e-01  time: 2.17e+00\n",
      "epoch: 1   trainLoss: 6.6728e-01   valLoss:8.4085e-01  time: 1.88e+00\n",
      "epoch: 2   trainLoss: 5.3656e-01   valLoss:9.0043e-01  time: 1.86e+00\n",
      "epoch: 3   trainLoss: 4.4838e-01   valLoss:1.2044e+00  time: 1.85e+00\n",
      "epoch: 4   trainLoss: 3.7129e-01   valLoss:1.5456e+00  time: 1.84e+00\n",
      "epoch: 5   trainLoss: 3.2526e-01   valLoss:2.1799e+00  time: 1.90e+00\n",
      "epoch: 6   trainLoss: 3.0470e-01   valLoss:2.4997e+00  time: 1.91e+00\n",
      "epoch: 7   trainLoss: 2.5104e-01   valLoss:2.1932e+00  time: 1.86e+00\n",
      "epoch: 8   trainLoss: 2.3973e-01   valLoss:1.3317e+00  time: 1.87e+00\n",
      "epoch: 9   trainLoss: 1.9596e-01   valLoss:8.6915e-01  time: 1.86e+00\n",
      "epoch: 10   trainLoss: 1.5860e-01   valLoss:5.6002e-01  time: 1.85e+00\n",
      "epoch: 11   trainLoss: 1.5497e-01   valLoss:1.1216e+00  time: 1.87e+00\n",
      "epoch: 12   trainLoss: 1.4554e-01   valLoss:5.9140e-01  time: 1.86e+00\n",
      "epoch: 13   trainLoss: 1.3024e-01   valLoss:6.9048e-01  time: 1.86e+00\n",
      "epoch: 14   trainLoss: 1.2037e-01   valLoss:1.2683e+00  time: 1.86e+00\n",
      "epoch: 15   trainLoss: 1.2482e-01   valLoss:5.1213e-01  time: 1.85e+00\n",
      "epoch: 16   trainLoss: 1.0259e-01   valLoss:7.9886e-01  time: 1.85e+00\n",
      "epoch: 17   trainLoss: 1.1634e-01   valLoss:3.8508e-01  time: 1.90e+00\n",
      "epoch: 18   trainLoss: 9.4361e-02   valLoss:6.4547e-01  time: 1.85e+00\n",
      "epoch: 19   trainLoss: 9.0598e-02   valLoss:4.4010e-01  time: 1.84e+00\n",
      "epoch: 20   trainLoss: 8.2729e-02   valLoss:3.8441e-01  time: 1.86e+00\n",
      "epoch: 21   trainLoss: 1.0386e-01   valLoss:3.3542e-01  time: 1.84e+00\n",
      "epoch: 22   trainLoss: 7.7394e-02   valLoss:4.0471e-01  time: 1.90e+00\n",
      "epoch: 23   trainLoss: 7.2277e-02   valLoss:3.3804e-01  time: 1.94e+00\n",
      "epoch: 24   trainLoss: 6.7675e-02   valLoss:3.3301e-01  time: 1.87e+00\n",
      "epoch: 25   trainLoss: 6.4807e-02   valLoss:3.0265e-01  time: 1.91e+00\n",
      "epoch: 26   trainLoss: 7.2827e-02   valLoss:3.0616e-01  time: 1.84e+00\n",
      "epoch: 27   trainLoss: 6.7554e-02   valLoss:2.1660e-01  time: 1.96e+00\n",
      "epoch: 28   trainLoss: 7.4928e-02   valLoss:4.0775e-01  time: 1.92e+00\n",
      "epoch: 29   trainLoss: 6.1144e-02   valLoss:2.5507e-01  time: 1.88e+00\n",
      "epoch: 30   trainLoss: 5.7039e-02   valLoss:3.0685e-01  time: 1.88e+00\n",
      "epoch: 31   trainLoss: 5.2766e-02   valLoss:2.5607e-01  time: 1.87e+00\n",
      "epoch: 32   trainLoss: 4.4854e-02   valLoss:3.0052e-01  time: 1.86e+00\n",
      "epoch: 33   trainLoss: 5.0213e-02   valLoss:3.1419e-01  time: 1.87e+00\n",
      "epoch: 34   trainLoss: 5.5348e-02   valLoss:3.2828e-01  time: 1.88e+00\n",
      "epoch: 35   trainLoss: 5.3898e-02   valLoss:2.1537e-01  time: 1.93e+00\n",
      "epoch: 36   trainLoss: 6.0259e-02   valLoss:2.2226e-01  time: 1.94e+00\n",
      "epoch: 37   trainLoss: 6.0893e-02   valLoss:2.5295e-01  time: 1.86e+00\n",
      "epoch: 38   trainLoss: 5.2578e-02   valLoss:3.1924e-01  time: 1.86e+00\n",
      "epoch: 39   trainLoss: 6.2122e-02   valLoss:3.6868e-01  time: 1.85e+00\n",
      "epoch: 40   trainLoss: 6.5441e-02   valLoss:3.3718e-01  time: 1.85e+00\n",
      "epoch: 41   trainLoss: 5.6187e-02   valLoss:4.1199e-01  time: 1.87e+00\n",
      "epoch: 42   trainLoss: 5.4044e-02   valLoss:2.5316e-01  time: 1.87e+00\n",
      "epoch: 43   trainLoss: 6.1538e-02   valLoss:2.7163e-01  time: 1.87e+00\n",
      "epoch: 44   trainLoss: 5.0174e-02   valLoss:2.7718e-01  time: 1.86e+00\n",
      "epoch: 45   trainLoss: 3.8612e-02   valLoss:3.9139e-01  time: 1.85e+00\n",
      "epoch: 46   trainLoss: 3.8227e-02   valLoss:2.7001e-01  time: 1.87e+00\n",
      "epoch: 47   trainLoss: 3.5928e-02   valLoss:2.3745e-01  time: 1.86e+00\n",
      "epoch: 48   trainLoss: 4.3380e-02   valLoss:2.0045e-01  time: 1.87e+00\n",
      "epoch: 49   trainLoss: 7.1507e-02   valLoss:2.3741e-01  time: 1.93e+00\n",
      "epoch: 50   trainLoss: 6.4494e-02   valLoss:3.8464e-01  time: 1.87e+00\n",
      "epoch: 51   trainLoss: 5.1870e-02   valLoss:3.0169e-01  time: 1.89e+00\n",
      "epoch: 52   trainLoss: 5.1909e-02   valLoss:2.6269e-01  time: 1.90e+00\n",
      "epoch: 53   trainLoss: 4.6621e-02   valLoss:2.6118e-01  time: 1.92e+00\n",
      "epoch: 54   trainLoss: 3.6461e-02   valLoss:1.9672e-01  time: 1.87e+00\n",
      "epoch: 55   trainLoss: 4.3495e-02   valLoss:2.8030e-01  time: 1.87e+00\n",
      "epoch: 56   trainLoss: 3.6005e-02   valLoss:2.7090e-01  time: 1.90e+00\n",
      "epoch: 57   trainLoss: 6.0656e-02   valLoss:3.3990e-01  time: 1.92e+00\n",
      "epoch: 58   trainLoss: 4.7371e-02   valLoss:3.0196e-01  time: 1.88e+00\n",
      "epoch: 59   trainLoss: 3.8142e-02   valLoss:2.3662e-01  time: 1.91e+00\n",
      "epoch: 60   trainLoss: 3.0919e-02   valLoss:1.4801e-01  time: 1.92e+00\n",
      "epoch: 61   trainLoss: 3.2839e-02   valLoss:2.0995e-01  time: 1.92e+00\n",
      "epoch: 62   trainLoss: 3.6391e-02   valLoss:2.2165e-01  time: 1.94e+00\n",
      "epoch: 63   trainLoss: 3.5020e-02   valLoss:2.2089e-01  time: 1.86e+00\n",
      "epoch: 64   trainLoss: 3.0178e-02   valLoss:2.3546e-01  time: 1.85e+00\n",
      "epoch: 65   trainLoss: 3.0854e-02   valLoss:2.9575e-01  time: 1.86e+00\n",
      "epoch: 66   trainLoss: 3.8894e-02   valLoss:2.0805e-01  time: 1.84e+00\n",
      "epoch: 67   trainLoss: 4.0095e-02   valLoss:1.9138e-01  time: 1.86e+00\n",
      "epoch: 68   trainLoss: 3.6543e-02   valLoss:1.2513e-01  time: 1.87e+00\n",
      "epoch: 69   trainLoss: 2.9665e-02   valLoss:2.5553e-01  time: 1.88e+00\n",
      "epoch: 70   trainLoss: 2.7884e-02   valLoss:1.1244e-01  time: 1.86e+00\n",
      "epoch: 71   trainLoss: 3.1659e-02   valLoss:1.7584e-01  time: 1.91e+00\n",
      "epoch: 72   trainLoss: 2.7280e-02   valLoss:1.4571e-01  time: 1.93e+00\n",
      "epoch: 73   trainLoss: 2.8932e-02   valLoss:1.1668e-01  time: 1.90e+00\n",
      "epoch: 74   trainLoss: 4.4860e-02   valLoss:1.7647e-01  time: 1.87e+00\n",
      "epoch: 75   trainLoss: 3.8598e-02   valLoss:2.2184e-01  time: 1.86e+00\n",
      "epoch: 76   trainLoss: 3.4178e-02   valLoss:1.3856e-01  time: 1.92e+00\n",
      "epoch: 77   trainLoss: 3.4552e-02   valLoss:2.3021e-01  time: 1.86e+00\n",
      "epoch: 78   trainLoss: 3.9150e-02   valLoss:1.4412e-01  time: 1.91e+00\n",
      "epoch: 79   trainLoss: 3.6870e-02   valLoss:2.0803e-01  time: 1.94e+00\n",
      "epoch: 80   trainLoss: 3.3765e-02   valLoss:1.7973e-01  time: 1.92e+00\n",
      "epoch: 81   trainLoss: 3.0766e-02   valLoss:1.1865e-01  time: 1.90e+00\n",
      "epoch: 82   trainLoss: 2.6761e-02   valLoss:1.6968e-01  time: 1.93e+00\n",
      "epoch: 83   trainLoss: 3.0714e-02   valLoss:1.9850e-01  time: 1.93e+00\n",
      "epoch: 84   trainLoss: 3.3034e-02   valLoss:1.3700e-01  time: 1.92e+00\n",
      "epoch: 85   trainLoss: 2.8075e-02   valLoss:1.3527e-01  time: 1.90e+00\n",
      "epoch: 86   trainLoss: 4.1890e-02   valLoss:1.3571e-01  time: 1.86e+00\n",
      "epoch: 87   trainLoss: 3.2327e-02   valLoss:1.2892e-01  time: 1.85e+00\n",
      "epoch: 88   trainLoss: 3.3495e-02   valLoss:8.6752e-02  time: 1.92e+00\n",
      "epoch: 89   trainLoss: 3.2562e-02   valLoss:1.9206e-01  time: 1.94e+00\n",
      "epoch: 90   trainLoss: 3.9385e-02   valLoss:2.9055e-01  time: 1.98e+00\n",
      "epoch: 91   trainLoss: 2.9427e-02   valLoss:9.6709e-02  time: 1.95e+00\n",
      "epoch: 92   trainLoss: 3.2246e-02   valLoss:9.0636e-02  time: 2.05e+00\n",
      "epoch: 93   trainLoss: 3.6178e-02   valLoss:1.2980e-01  time: 2.10e+00\n",
      "epoch: 94   trainLoss: 3.9909e-02   valLoss:1.2030e-01  time: 1.97e+00\n",
      "epoch: 95   trainLoss: 4.0087e-02   valLoss:1.7075e-01  time: 2.09e+00\n",
      "epoch: 96   trainLoss: 3.8853e-02   valLoss:2.1781e-01  time: 1.91e+00\n",
      "epoch: 97   trainLoss: 4.3900e-02   valLoss:1.5747e-01  time: 2.06e+00\n",
      "epoch: 98   trainLoss: 4.9223e-02   valLoss:1.6021e-01  time: 2.00e+00\n",
      "epoch: 99   trainLoss: 4.2462e-02   valLoss:1.7340e-01  time: 2.02e+00\n",
      "loading checkpoint 88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1231fd013f034e19a442f743758a7116\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1231fd013f034e19a442f743758a7116\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1231fd013f034e19a442f743758a7116\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-04d03e352298caf363d7305114013db2\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-04d03e352298caf363d7305114013db2\": [{\"train\": 0.8863035639127096, \"val\": 0.8620125345057912, \"epoch\": 0}, {\"train\": 0.6672807733217875, \"val\": 0.8408499904528812, \"epoch\": 1}, {\"train\": 0.5365556875864664, \"val\": 0.9004338061919919, \"epoch\": 2}, {\"train\": 0.44838348031044006, \"val\": 1.2043881994706613, \"epoch\": 3}, {\"train\": 0.3712877730528514, \"val\": 1.5456068222169523, \"epoch\": 4}, {\"train\": 0.3252551754315694, \"val\": 2.1798657216407635, \"epoch\": 5}, {\"train\": 0.3046972354253133, \"val\": 2.4996956180643153, \"epoch\": 6}, {\"train\": 0.2510373592376709, \"val\": 2.1931762248277664, \"epoch\": 7}, {\"train\": 0.23972854018211365, \"val\": 1.3316590245675157, \"epoch\": 8}, {\"train\": 0.19595885773499808, \"val\": 0.8691489027606116, \"epoch\": 9}, {\"train\": 0.15859500567118326, \"val\": 0.5600227186801257, \"epoch\": 10}, {\"train\": 0.1549742023150126, \"val\": 1.1216368042760425, \"epoch\": 11}, {\"train\": 0.1455358862876892, \"val\": 0.5914026002089182, \"epoch\": 12}, {\"train\": 0.13023901730775833, \"val\": 0.6904805835750368, \"epoch\": 13}, {\"train\": 0.12037211904923122, \"val\": 1.2683469664167475, \"epoch\": 14}, {\"train\": 0.12482121338446935, \"val\": 0.5121313323439272, \"epoch\": 15}, {\"train\": 0.1025896817445755, \"val\": 0.7988567039922432, \"epoch\": 16}, {\"train\": 0.1163390005628268, \"val\": 0.38508050914991787, \"epoch\": 17}, {\"train\": 0.09436054776112239, \"val\": 0.6454713574438183, \"epoch\": 18}, {\"train\": 0.09059778600931168, \"val\": 0.44009793599446617, \"epoch\": 19}, {\"train\": 0.08272928496201833, \"val\": 0.3844071361615702, \"epoch\": 20}, {\"train\": 0.10385726888974507, \"val\": 0.3354244985928138, \"epoch\": 21}, {\"train\": 0.07739401484529178, \"val\": 0.40471141321791543, \"epoch\": 22}, {\"train\": 0.07227727274099986, \"val\": 0.3380362359461961, \"epoch\": 23}, {\"train\": 0.0676749882598718, \"val\": 0.3330146103821419, \"epoch\": 24}, {\"train\": 0.06480708842476209, \"val\": 0.3026482853693543, \"epoch\": 25}, {\"train\": 0.07282694925864537, \"val\": 0.30616105914253877, \"epoch\": 26}, {\"train\": 0.06755379090706508, \"val\": 0.21659841963124496, \"epoch\": 27}, {\"train\": 0.07492763797442119, \"val\": 0.4077450620079482, \"epoch\": 28}, {\"train\": 0.06114387263854345, \"val\": 0.2550666453661742, \"epoch\": 29}, {\"train\": 0.05703901251157125, \"val\": 0.30684856941095656, \"epoch\": 30}, {\"train\": 0.052765571822722755, \"val\": 0.2560735291559939, \"epoch\": 31}, {\"train\": 0.04485389838616053, \"val\": 0.30052317374006465, \"epoch\": 32}, {\"train\": 0.050213176757097244, \"val\": 0.3141912638727162, \"epoch\": 33}, {\"train\": 0.05534828454256058, \"val\": 0.3282784569249661, \"epoch\": 34}, {\"train\": 0.05389757454395294, \"val\": 0.21536681772106223, \"epoch\": 35}, {\"train\": 0.06025909508268038, \"val\": 0.22225687015387746, \"epoch\": 36}, {\"train\": 0.06089306126038233, \"val\": 0.2529502722883114, \"epoch\": 37}, {\"train\": 0.05257811521490415, \"val\": 0.3192395587624223, \"epoch\": 38}, {\"train\": 0.06212243313590685, \"val\": 0.36868039644150824, \"epoch\": 39}, {\"train\": 0.06544087082147598, \"val\": 0.3371792176669395, \"epoch\": 40}, {\"train\": 0.056186811377604805, \"val\": 0.4119907461727659, \"epoch\": 41}, {\"train\": 0.05404409269491831, \"val\": 0.2531578655389172, \"epoch\": 42}, {\"train\": 0.061538077890872955, \"val\": 0.27163248493991515, \"epoch\": 43}, {\"train\": 0.05017371475696564, \"val\": 0.27718339590700686, \"epoch\": 44}, {\"train\": 0.03861166164278984, \"val\": 0.391391031789007, \"epoch\": 45}, {\"train\": 0.03822706639766693, \"val\": 0.27001431210104515, \"epoch\": 46}, {\"train\": 0.035928173611561455, \"val\": 0.23745011305091557, \"epoch\": 47}, {\"train\": 0.04338041072090467, \"val\": 0.20045324061755782, \"epoch\": 48}, {\"train\": 0.07150690381725629, \"val\": 0.23740561616572517, \"epoch\": 49}, {\"train\": 0.06449429815014203, \"val\": 0.3846353640572892, \"epoch\": 50}, {\"train\": 0.05186969414353371, \"val\": 0.30169334196382097, \"epoch\": 51}, {\"train\": 0.051909410705169044, \"val\": 0.2626912233040289, \"epoch\": 52}, {\"train\": 0.04662134374181429, \"val\": 0.261182932248684, \"epoch\": 53}, {\"train\": 0.03646107266346613, \"val\": 0.19672215357767764, \"epoch\": 54}, {\"train\": 0.04349475602308909, \"val\": 0.280301681705923, \"epoch\": 55}, {\"train\": 0.03600477365156015, \"val\": 0.2708995291721766, \"epoch\": 56}, {\"train\": 0.06065599371989568, \"val\": 0.33990235335573005, \"epoch\": 57}, {\"train\": 0.04737115651369095, \"val\": 0.30196358811593166, \"epoch\": 58}, {\"train\": 0.038142205526431404, \"val\": 0.23661878992010046, \"epoch\": 59}, {\"train\": 0.03091874656577905, \"val\": 0.1480146161366806, \"epoch\": 60}, {\"train\": 0.03283948823809624, \"val\": 0.20995049816728742, \"epoch\": 61}, {\"train\": 0.036390598863363266, \"val\": 0.2216469951249935, \"epoch\": 62}, {\"train\": 0.0350201241672039, \"val\": 0.22088587419302375, \"epoch\": 63}, {\"train\": 0.030177954584360123, \"val\": 0.235462744054557, \"epoch\": 64}, {\"train\": 0.030854302148024242, \"val\": 0.29574565294164196, \"epoch\": 65}, {\"train\": 0.03889355746408304, \"val\": 0.2080487103080722, \"epoch\": 66}, {\"train\": 0.04009541248281797, \"val\": 0.1913819321396726, \"epoch\": 67}, {\"train\": 0.036542908598979316, \"val\": 0.1251277523253251, \"epoch\": 68}, {\"train\": 0.029665162786841393, \"val\": 0.2555257868780582, \"epoch\": 69}, {\"train\": 0.02788367619117101, \"val\": 0.11244278447874995, \"epoch\": 70}, {\"train\": 0.03165858301023642, \"val\": 0.17583559258599524, \"epoch\": 71}, {\"train\": 0.027280255531271298, \"val\": 0.1457082304275698, \"epoch\": 72}, {\"train\": 0.028932420536875725, \"val\": 0.11668223386837376, \"epoch\": 73}, {\"train\": 0.04485959932208061, \"val\": 0.17647393312719134, \"epoch\": 74}, {\"train\": 0.03859750864406427, \"val\": 0.22184012468076414, \"epoch\": 75}, {\"train\": 0.034177942822376885, \"val\": 0.13856129484527088, \"epoch\": 76}, {\"train\": 0.03455246798694134, \"val\": 0.23021480480415954, \"epoch\": 77}, {\"train\": 0.039150276531775795, \"val\": 0.14411550314959, \"epoch\": 78}, {\"train\": 0.036870160450538, \"val\": 0.20802951549283333, \"epoch\": 79}, {\"train\": 0.033765221014618874, \"val\": 0.17973261109070368, \"epoch\": 80}, {\"train\": 0.030765583738684654, \"val\": 0.11864813349416686, \"epoch\": 81}, {\"train\": 0.026760975519816082, \"val\": 0.1696846394247755, \"epoch\": 82}, {\"train\": 0.030714113265275955, \"val\": 0.19849539581272338, \"epoch\": 83}, {\"train\": 0.03303352867563566, \"val\": 0.13700183485117223, \"epoch\": 84}, {\"train\": 0.028074864918986957, \"val\": 0.13527057932482825, \"epoch\": 85}, {\"train\": 0.04188977306087812, \"val\": 0.1357131376310631, \"epoch\": 86}, {\"train\": 0.03232713664571444, \"val\": 0.12892463890559697, \"epoch\": 87}, {\"train\": 0.0334952287375927, \"val\": 0.08675165869019649, \"epoch\": 88}, {\"train\": 0.032562319189310074, \"val\": 0.19205890962923014, \"epoch\": 89}, {\"train\": 0.03938542368511359, \"val\": 0.29055073946645416, \"epoch\": 90}, {\"train\": 0.02942702981332938, \"val\": 0.09670945438383906, \"epoch\": 91}, {\"train\": 0.032246009136239685, \"val\": 0.09063567350515062, \"epoch\": 92}, {\"train\": 0.036178309470415115, \"val\": 0.12980294976797369, \"epoch\": 93}, {\"train\": 0.03990876736740271, \"val\": 0.12030083185268772, \"epoch\": 94}, {\"train\": 0.04008732487758001, \"val\": 0.17075078609158043, \"epoch\": 95}, {\"train\": 0.038853186493118606, \"val\": 0.21781334825963886, \"epoch\": 96}, {\"train\": 0.04389999434351921, \"val\": 0.15746529984667346, \"epoch\": 97}, {\"train\": 0.04922342052062353, \"val\": 0.1602070539275667, \"epoch\": 98}, {\"train\": 0.042461659759283066, \"val\": 0.1733955913171586, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = 'results/transferLrn_endloads_des7_01/'\n",
    "epochs = 100\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                            epochs=epochs,\n",
    "                            saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.086164</td>\n",
       "      <td>0.878713</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>0.823229</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.043491</td>\n",
       "      <td>0.709887</td>\n",
       "      <td>-0.497045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.183517</td>\n",
       "      <td>-3.72334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre    peakR2  maxAggR2  meanAggR2  minAggR2\n",
       "train  0.000006  0.001660  0.086164  0.878713  0.952191   0.823229   0.00000\n",
       "test   0.004393  0.043491  0.709887 -0.497045  0.000000  -1.183517  -3.72334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train set of size 915\n",
      "epoch: 0   trainLoss: 8.2305e-01   valLoss:9.2133e-01  time: 7.88e+00\n",
      "epoch: 1   trainLoss: 5.0989e-01   valLoss:9.5012e-01  time: 7.83e+00\n",
      "epoch: 2   trainLoss: 3.6870e-01   valLoss:1.0310e+00  time: 7.90e+00\n",
      "epoch: 3   trainLoss: 2.7486e-01   valLoss:1.1288e+00  time: 7.82e+00\n",
      "epoch: 4   trainLoss: 2.0767e-01   valLoss:1.3173e+00  time: 7.85e+00\n",
      "epoch: 5   trainLoss: 1.7632e-01   valLoss:7.4724e-01  time: 8.30e+00\n",
      "epoch: 6   trainLoss: 1.6974e-01   valLoss:3.6580e-01  time: 7.88e+00\n",
      "epoch: 7   trainLoss: 1.2860e-01   valLoss:2.8667e-01  time: 7.99e+00\n",
      "epoch: 8   trainLoss: 1.0812e-01   valLoss:2.6082e-01  time: 8.06e+00\n",
      "epoch: 9   trainLoss: 9.4504e-02   valLoss:2.4619e-01  time: 8.20e+00\n",
      "epoch: 10   trainLoss: 8.1976e-02   valLoss:2.0037e-01  time: 7.91e+00\n",
      "epoch: 11   trainLoss: 7.8250e-02   valLoss:2.1785e-01  time: 8.22e+00\n",
      "epoch: 12   trainLoss: 7.7427e-02   valLoss:2.8085e-01  time: 8.13e+00\n",
      "epoch: 13   trainLoss: 6.6617e-02   valLoss:2.3471e-01  time: 7.95e+00\n",
      "epoch: 14   trainLoss: 6.7805e-02   valLoss:2.6177e-01  time: 7.79e+00\n",
      "epoch: 15   trainLoss: 5.8059e-02   valLoss:2.0714e-01  time: 7.95e+00\n",
      "epoch: 16   trainLoss: 5.7325e-02   valLoss:3.1303e-01  time: 8.14e+00\n",
      "epoch: 17   trainLoss: 5.3887e-02   valLoss:2.2948e-01  time: 8.03e+00\n",
      "epoch: 18   trainLoss: 6.2680e-02   valLoss:3.4150e-01  time: 7.92e+00\n",
      "epoch: 19   trainLoss: 7.0079e-02   valLoss:2.7520e-01  time: 7.85e+00\n",
      "epoch: 20   trainLoss: 4.9745e-02   valLoss:2.0175e-01  time: 7.64e+00\n",
      "epoch: 21   trainLoss: 4.7517e-02   valLoss:1.9676e-01  time: 7.51e+00\n",
      "epoch: 22   trainLoss: 4.9887e-02   valLoss:2.4042e-01  time: 8.50e+00\n",
      "epoch: 23   trainLoss: 5.0268e-02   valLoss:2.6988e-01  time: 8.36e+00\n",
      "epoch: 24   trainLoss: 5.4135e-02   valLoss:3.2222e-01  time: 7.93e+00\n",
      "epoch: 25   trainLoss: 4.4111e-02   valLoss:1.9735e-01  time: 8.75e+00\n",
      "epoch: 26   trainLoss: 4.8663e-02   valLoss:2.2986e-01  time: 8.32e+00\n",
      "epoch: 27   trainLoss: 4.4484e-02   valLoss:2.3327e-01  time: 8.30e+00\n",
      "epoch: 28   trainLoss: 3.8077e-02   valLoss:1.5792e-01  time: 7.93e+00\n",
      "epoch: 29   trainLoss: 3.9092e-02   valLoss:1.8207e-01  time: 8.35e+00\n",
      "epoch: 30   trainLoss: 4.5879e-02   valLoss:2.2755e-01  time: 8.30e+00\n",
      "epoch: 31   trainLoss: 3.9097e-02   valLoss:1.8235e-01  time: 8.23e+00\n",
      "epoch: 32   trainLoss: 4.4416e-02   valLoss:2.6066e-01  time: 7.90e+00\n",
      "epoch: 33   trainLoss: 6.2889e-02   valLoss:2.7940e-01  time: 7.82e+00\n",
      "epoch: 34   trainLoss: 6.2384e-02   valLoss:2.8569e-01  time: 7.97e+00\n",
      "epoch: 35   trainLoss: 5.9265e-02   valLoss:1.5377e-01  time: 7.87e+00\n",
      "epoch: 36   trainLoss: 4.4857e-02   valLoss:1.2788e-01  time: 7.55e+00\n",
      "epoch: 37   trainLoss: 4.0671e-02   valLoss:2.3188e-01  time: 7.51e+00\n",
      "epoch: 38   trainLoss: 4.2846e-02   valLoss:1.9226e-01  time: 7.83e+00\n",
      "epoch: 39   trainLoss: 4.2456e-02   valLoss:1.4613e-01  time: 7.65e+00\n",
      "epoch: 40   trainLoss: 4.0266e-02   valLoss:1.2315e-01  time: 7.78e+00\n",
      "epoch: 41   trainLoss: 3.0867e-02   valLoss:1.4327e-01  time: 7.76e+00\n",
      "epoch: 42   trainLoss: 4.2862e-02   valLoss:9.9069e-02  time: 7.63e+00\n",
      "epoch: 43   trainLoss: 3.3431e-02   valLoss:9.9528e-02  time: 7.65e+00\n",
      "epoch: 44   trainLoss: 2.6106e-02   valLoss:1.3128e-01  time: 7.53e+00\n",
      "epoch: 45   trainLoss: 2.9901e-02   valLoss:1.2441e-01  time: 7.81e+00\n",
      "epoch: 46   trainLoss: 2.8795e-02   valLoss:1.1811e-01  time: 7.96e+00\n",
      "epoch: 47   trainLoss: 2.9922e-02   valLoss:1.3165e-01  time: 7.53e+00\n",
      "epoch: 48   trainLoss: 2.7222e-02   valLoss:7.2357e-02  time: 7.79e+00\n",
      "epoch: 49   trainLoss: 2.5577e-02   valLoss:1.2211e-01  time: 7.88e+00\n",
      "epoch: 50   trainLoss: 2.7336e-02   valLoss:1.1059e-01  time: 7.81e+00\n",
      "epoch: 51   trainLoss: 3.0635e-02   valLoss:1.6298e-01  time: 7.81e+00\n",
      "epoch: 52   trainLoss: 3.1231e-02   valLoss:1.0162e-01  time: 7.58e+00\n",
      "epoch: 53   trainLoss: 2.8490e-02   valLoss:1.2533e-01  time: 7.60e+00\n",
      "epoch: 54   trainLoss: 3.6255e-02   valLoss:1.3820e-01  time: 7.60e+00\n",
      "epoch: 55   trainLoss: 3.4379e-02   valLoss:1.6595e-01  time: 7.61e+00\n",
      "epoch: 56   trainLoss: 4.1614e-02   valLoss:1.2207e-01  time: 7.85e+00\n",
      "epoch: 57   trainLoss: 4.3743e-02   valLoss:8.5333e-02  time: 8.03e+00\n",
      "epoch: 58   trainLoss: 3.8572e-02   valLoss:8.3038e-02  time: 7.77e+00\n",
      "epoch: 59   trainLoss: 3.8855e-02   valLoss:8.8549e-02  time: 7.57e+00\n",
      "epoch: 60   trainLoss: 4.1209e-02   valLoss:6.1776e-02  time: 7.95e+00\n",
      "epoch: 61   trainLoss: 3.2882e-02   valLoss:1.0527e-01  time: 7.90e+00\n",
      "epoch: 62   trainLoss: 3.1261e-02   valLoss:8.9130e-02  time: 7.88e+00\n",
      "epoch: 63   trainLoss: 3.1806e-02   valLoss:6.9172e-02  time: 7.79e+00\n",
      "epoch: 64   trainLoss: 2.9882e-02   valLoss:6.1575e-02  time: 7.57e+00\n",
      "epoch: 65   trainLoss: 3.0592e-02   valLoss:8.4714e-02  time: 7.77e+00\n",
      "epoch: 66   trainLoss: 2.9323e-02   valLoss:7.3394e-02  time: 7.82e+00\n",
      "epoch: 67   trainLoss: 2.9821e-02   valLoss:1.5862e-01  time: 7.67e+00\n",
      "epoch: 68   trainLoss: 4.1088e-02   valLoss:1.5265e-01  time: 7.93e+00\n",
      "epoch: 69   trainLoss: 3.3722e-02   valLoss:7.2396e-02  time: 7.70e+00\n",
      "epoch: 70   trainLoss: 3.7800e-02   valLoss:7.4893e-02  time: 7.78e+00\n",
      "epoch: 71   trainLoss: 2.7955e-02   valLoss:6.4761e-02  time: 7.83e+00\n",
      "epoch: 72   trainLoss: 2.7767e-02   valLoss:6.0631e-02  time: 7.95e+00\n",
      "epoch: 73   trainLoss: 2.7235e-02   valLoss:7.4856e-02  time: 7.85e+00\n",
      "epoch: 74   trainLoss: 4.7505e-02   valLoss:1.3132e-01  time: 7.91e+00\n",
      "epoch: 75   trainLoss: 3.9214e-02   valLoss:6.8857e-02  time: 7.89e+00\n",
      "epoch: 76   trainLoss: 3.2863e-02   valLoss:7.0540e-02  time: 7.55e+00\n",
      "epoch: 77   trainLoss: 3.7035e-02   valLoss:4.8544e-02  time: 7.60e+00\n",
      "epoch: 78   trainLoss: 4.7246e-02   valLoss:7.6699e-02  time: 7.90e+00\n",
      "epoch: 79   trainLoss: 3.8787e-02   valLoss:7.8997e-02  time: 7.53e+00\n",
      "epoch: 80   trainLoss: 3.4897e-02   valLoss:7.1227e-02  time: 7.55e+00\n",
      "epoch: 81   trainLoss: 3.0544e-02   valLoss:6.0193e-02  time: 7.54e+00\n",
      "epoch: 82   trainLoss: 2.5710e-02   valLoss:5.2419e-02  time: 7.79e+00\n",
      "epoch: 83   trainLoss: 2.2766e-02   valLoss:3.3494e-02  time: 7.55e+00\n",
      "epoch: 84   trainLoss: 2.1529e-02   valLoss:3.4557e-02  time: 7.50e+00\n",
      "epoch: 85   trainLoss: 3.3854e-02   valLoss:9.2097e-02  time: 7.76e+00\n",
      "epoch: 86   trainLoss: 4.3729e-02   valLoss:6.2922e-02  time: 7.63e+00\n",
      "epoch: 87   trainLoss: 4.0759e-02   valLoss:4.8525e-02  time: 7.87e+00\n",
      "epoch: 88   trainLoss: 4.5522e-02   valLoss:1.4558e-01  time: 7.85e+00\n",
      "epoch: 89   trainLoss: 4.1134e-02   valLoss:6.3445e-02  time: 7.53e+00\n",
      "epoch: 90   trainLoss: 3.5782e-02   valLoss:5.7692e-02  time: 7.61e+00\n",
      "epoch: 91   trainLoss: 2.8745e-02   valLoss:4.7222e-02  time: 7.79e+00\n",
      "epoch: 92   trainLoss: 2.2729e-02   valLoss:7.1138e-02  time: 7.62e+00\n",
      "epoch: 93   trainLoss: 2.9136e-02   valLoss:5.1821e-02  time: 7.86e+00\n",
      "epoch: 94   trainLoss: 2.0937e-02   valLoss:5.2925e-02  time: 7.91e+00\n",
      "epoch: 95   trainLoss: 2.1277e-02   valLoss:2.7219e-02  time: 7.52e+00\n",
      "epoch: 96   trainLoss: 2.3056e-02   valLoss:6.2545e-02  time: 7.78e+00\n",
      "epoch: 97   trainLoss: 2.9207e-02   valLoss:3.8951e-02  time: 7.84e+00\n",
      "epoch: 98   trainLoss: 2.2021e-02   valLoss:2.3245e-02  time: 7.59e+00\n",
      "epoch: 99   trainLoss: 1.8365e-02   valLoss:3.6536e-02  time: 7.85e+00\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 2.6951e+01   valLoss:2.7485e+01  time: 8.16e+00\n",
      "epoch: 1   trainLoss: 1.9928e+01   valLoss:1.8698e+01  time: 7.84e+00\n",
      "epoch: 2   trainLoss: 1.6039e+01   valLoss:1.6862e+01  time: 8.00e+00\n",
      "epoch: 3   trainLoss: 1.1589e+01   valLoss:1.7636e+01  time: 7.96e+00\n",
      "epoch: 4   trainLoss: 8.7400e+00   valLoss:1.6749e+01  time: 7.47e+00\n",
      "epoch: 5   trainLoss: 5.9457e+00   valLoss:1.1004e+01  time: 7.39e+00\n",
      "epoch: 6   trainLoss: 4.4670e+00   valLoss:9.7159e+00  time: 7.44e+00\n",
      "epoch: 7   trainLoss: 3.4638e+00   valLoss:5.9174e+00  time: 7.55e+00\n",
      "epoch: 8   trainLoss: 2.8139e+00   valLoss:4.9529e+00  time: 7.55e+00\n",
      "epoch: 9   trainLoss: 2.6231e+00   valLoss:4.0435e+00  time: 7.40e+00\n",
      "epoch: 10   trainLoss: 2.5258e+00   valLoss:2.4245e+00  time: 7.66e+00\n",
      "epoch: 11   trainLoss: 2.0020e+00   valLoss:2.4469e+00  time: 7.34e+00\n",
      "epoch: 12   trainLoss: 1.8683e+00   valLoss:4.5882e+00  time: 7.60e+00\n",
      "epoch: 13   trainLoss: 2.1006e+00   valLoss:5.6252e+00  time: 7.35e+00\n",
      "epoch: 14   trainLoss: 1.8442e+00   valLoss:2.3230e+01  time: 7.61e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15   trainLoss: 1.8244e+00   valLoss:2.0599e+01  time: 7.56e+00\n",
      "epoch: 16   trainLoss: 1.9154e+00   valLoss:6.4602e+00  time: 7.59e+00\n",
      "epoch: 17   trainLoss: 1.6011e+00   valLoss:6.7002e+00  time: 7.35e+00\n",
      "epoch: 18   trainLoss: 1.8178e+00   valLoss:4.2328e+00  time: 7.39e+00\n",
      "epoch: 19   trainLoss: 1.3971e+00   valLoss:5.1665e+00  time: 7.47e+00\n",
      "epoch: 20   trainLoss: 1.2661e+00   valLoss:6.6926e+00  time: 7.43e+00\n",
      "epoch: 21   trainLoss: 1.2189e+00   valLoss:2.2483e+00  time: 7.38e+00\n",
      "epoch: 22   trainLoss: 1.1638e+00   valLoss:2.1929e+00  time: 7.49e+00\n",
      "epoch: 23   trainLoss: 1.3899e+00   valLoss:4.4565e+00  time: 7.38e+00\n",
      "epoch: 24   trainLoss: 1.2264e+00   valLoss:1.8534e+00  time: 7.35e+00\n",
      "epoch: 25   trainLoss: 9.6995e-01   valLoss:3.0094e+00  time: 7.65e+00\n",
      "epoch: 26   trainLoss: 9.7075e-01   valLoss:2.9440e+00  time: 7.40e+00\n",
      "epoch: 27   trainLoss: 1.2304e+00   valLoss:1.0654e+01  time: 7.45e+00\n",
      "epoch: 28   trainLoss: 9.4523e-01   valLoss:2.1100e+00  time: 7.39e+00\n",
      "epoch: 29   trainLoss: 9.6623e-01   valLoss:4.2720e+00  time: 7.34e+00\n",
      "epoch: 30   trainLoss: 1.0043e+00   valLoss:1.4216e+00  time: 7.51e+00\n",
      "epoch: 31   trainLoss: 9.3512e-01   valLoss:1.3610e+00  time: 7.35e+00\n",
      "epoch: 32   trainLoss: 7.5773e-01   valLoss:2.0973e+00  time: 7.34e+00\n",
      "epoch: 33   trainLoss: 7.1272e-01   valLoss:1.7623e+00  time: 7.35e+00\n",
      "epoch: 34   trainLoss: 8.4215e-01   valLoss:3.2886e+00  time: 7.40e+00\n",
      "epoch: 35   trainLoss: 8.5082e-01   valLoss:1.5735e+00  time: 7.78e+00\n",
      "epoch: 36   trainLoss: 7.5982e-01   valLoss:2.7779e+00  time: 7.50e+00\n",
      "epoch: 37   trainLoss: 7.3052e-01   valLoss:4.9833e+00  time: 7.73e+00\n",
      "epoch: 38   trainLoss: 7.1706e-01   valLoss:1.7406e+00  time: 7.68e+00\n",
      "epoch: 39   trainLoss: 7.6062e-01   valLoss:1.2925e+00  time: 7.71e+00\n",
      "epoch: 40   trainLoss: 7.6172e-01   valLoss:1.5115e+00  time: 7.59e+00\n",
      "epoch: 41   trainLoss: 7.0986e-01   valLoss:3.8850e+00  time: 7.41e+00\n",
      "epoch: 42   trainLoss: 8.4416e-01   valLoss:2.2438e+00  time: 7.37e+00\n",
      "epoch: 43   trainLoss: 6.5650e-01   valLoss:1.0593e+01  time: 7.40e+00\n",
      "epoch: 44   trainLoss: 8.3996e-01   valLoss:1.7071e+00  time: 7.65e+00\n",
      "epoch: 45   trainLoss: 8.6774e-01   valLoss:1.9992e+00  time: 7.55e+00\n",
      "epoch: 46   trainLoss: 9.6176e-01   valLoss:1.8519e+00  time: 7.72e+00\n",
      "epoch: 47   trainLoss: 8.5844e-01   valLoss:2.2772e+00  time: 7.77e+00\n",
      "epoch: 48   trainLoss: 8.8439e-01   valLoss:1.3727e+00  time: 7.87e+00\n",
      "epoch: 49   trainLoss: 7.9732e-01   valLoss:1.7046e+00  time: 7.79e+00\n",
      "epoch: 50   trainLoss: 7.7786e-01   valLoss:3.6773e+00  time: 7.54e+00\n",
      "epoch: 51   trainLoss: 7.4608e-01   valLoss:1.8127e+00  time: 7.40e+00\n",
      "epoch: 52   trainLoss: 1.0036e+00   valLoss:1.0512e+00  time: 7.49e+00\n",
      "epoch: 53   trainLoss: 7.7971e-01   valLoss:2.3319e+00  time: 7.64e+00\n",
      "epoch: 54   trainLoss: 6.3923e-01   valLoss:2.1201e+00  time: 7.64e+00\n",
      "epoch: 55   trainLoss: 6.1274e-01   valLoss:2.9947e+00  time: 7.61e+00\n",
      "epoch: 56   trainLoss: 7.4183e-01   valLoss:2.0094e+00  time: 7.68e+00\n",
      "epoch: 57   trainLoss: 6.0455e-01   valLoss:5.4981e+00  time: 7.70e+00\n",
      "epoch: 58   trainLoss: 6.0118e-01   valLoss:5.9299e+00  time: 7.60e+00\n",
      "epoch: 59   trainLoss: 6.4715e-01   valLoss:6.2859e+00  time: 7.79e+00\n",
      "epoch: 60   trainLoss: 9.9377e-01   valLoss:4.6891e+00  time: 7.68e+00\n",
      "epoch: 61   trainLoss: 8.7712e-01   valLoss:6.8237e+00  time: 7.70e+00\n",
      "epoch: 62   trainLoss: 7.4453e-01   valLoss:4.2418e+00  time: 7.68e+00\n",
      "epoch: 63   trainLoss: 8.5781e-01   valLoss:4.7671e+00  time: 7.49e+00\n",
      "epoch: 64   trainLoss: 7.8695e-01   valLoss:2.2760e+00  time: 7.67e+00\n",
      "epoch: 65   trainLoss: 6.7685e-01   valLoss:1.1228e+00  time: 7.59e+00\n",
      "epoch: 66   trainLoss: 6.1205e-01   valLoss:5.2515e+00  time: 7.59e+00\n",
      "epoch: 67   trainLoss: 5.3982e-01   valLoss:1.4160e+00  time: 7.73e+00\n",
      "epoch: 68   trainLoss: 5.3569e-01   valLoss:3.3245e+00  time: 7.76e+00\n",
      "epoch: 69   trainLoss: 5.1344e-01   valLoss:3.1056e+00  time: 7.32e+00\n",
      "epoch: 70   trainLoss: 4.8404e-01   valLoss:2.0552e+00  time: 7.48e+00\n",
      "epoch: 71   trainLoss: 5.3170e-01   valLoss:1.2265e+00  time: 7.44e+00\n",
      "epoch: 72   trainLoss: 4.5754e-01   valLoss:2.0321e+00  time: 7.57e+00\n",
      "epoch: 73   trainLoss: 4.3051e-01   valLoss:3.2791e+00  time: 7.72e+00\n",
      "epoch: 74   trainLoss: 4.0138e-01   valLoss:1.5454e+00  time: 7.38e+00\n",
      "epoch: 75   trainLoss: 3.3171e-01   valLoss:1.6833e+00  time: 7.33e+00\n",
      "epoch: 76   trainLoss: 4.1223e-01   valLoss:1.9423e+00  time: 7.75e+00\n",
      "epoch: 77   trainLoss: 5.0738e-01   valLoss:1.4005e+00  time: 7.73e+00\n",
      "epoch: 78   trainLoss: 3.5710e-01   valLoss:7.6228e-01  time: 7.70e+00\n",
      "epoch: 79   trainLoss: 3.3613e-01   valLoss:1.1810e+00  time: 7.67e+00\n",
      "epoch: 80   trainLoss: 2.7335e-01   valLoss:1.2445e+00  time: 7.75e+00\n",
      "epoch: 81   trainLoss: 4.4617e-01   valLoss:1.4449e+00  time: 7.47e+00\n",
      "epoch: 82   trainLoss: 4.6308e-01   valLoss:1.7118e+00  time: 7.75e+00\n",
      "epoch: 83   trainLoss: 4.5235e-01   valLoss:1.7575e+00  time: 7.59e+00\n",
      "epoch: 84   trainLoss: 4.6931e-01   valLoss:1.0929e+00  time: 7.34e+00\n",
      "epoch: 85   trainLoss: 5.7829e-01   valLoss:2.2591e+00  time: 7.68e+00\n",
      "epoch: 86   trainLoss: 4.3990e-01   valLoss:2.2366e+00  time: 7.45e+00\n",
      "epoch: 87   trainLoss: 4.5219e-01   valLoss:1.5514e+00  time: 7.52e+00\n",
      "epoch: 88   trainLoss: 4.5663e-01   valLoss:2.1996e+00  time: 7.73e+00\n",
      "epoch: 89   trainLoss: 4.5507e-01   valLoss:2.1246e+00  time: 7.75e+00\n",
      "epoch: 90   trainLoss: 5.9883e-01   valLoss:6.6323e+00  time: 7.66e+00\n",
      "epoch: 91   trainLoss: 3.8259e-01   valLoss:8.6088e+00  time: 7.70e+00\n",
      "epoch: 92   trainLoss: 3.2262e-01   valLoss:5.3665e+00  time: 7.66e+00\n",
      "epoch: 93   trainLoss: 3.4047e-01   valLoss:4.1108e+00  time: 7.75e+00\n",
      "epoch: 94   trainLoss: 4.5224e-01   valLoss:4.0546e+00  time: 7.62e+00\n",
      "epoch: 95   trainLoss: 3.7740e-01   valLoss:1.8475e+00  time: 7.62e+00\n",
      "epoch: 96   trainLoss: 2.6832e-01   valLoss:1.3273e+00  time: 7.59e+00\n",
      "epoch: 97   trainLoss: 3.6490e-01   valLoss:3.2834e+00  time: 7.44e+00\n",
      "epoch: 98   trainLoss: 2.9723e-01   valLoss:5.9413e-01  time: 7.38e+00\n",
      "epoch: 99   trainLoss: 4.1296e-01   valLoss:1.8149e+00  time: 7.51e+00\n",
      "loading checkpoint 98\n",
      "trained 30 random forest models in 21.25 seconds\n",
      "loaded train set of size 185\n",
      "epoch: 0   trainLoss: 1.0569e+00   valLoss:9.6538e-01  time: 1.61e+00\n",
      "epoch: 1   trainLoss: 8.7186e-01   valLoss:9.5471e-01  time: 1.58e+00\n",
      "epoch: 2   trainLoss: 7.7019e-01   valLoss:9.3652e-01  time: 1.49e+00\n",
      "epoch: 3   trainLoss: 6.7492e-01   valLoss:9.1691e-01  time: 1.57e+00\n",
      "epoch: 4   trainLoss: 6.0997e-01   valLoss:9.0396e-01  time: 1.56e+00\n",
      "epoch: 5   trainLoss: 5.5063e-01   valLoss:9.1073e-01  time: 1.55e+00\n",
      "epoch: 6   trainLoss: 5.0567e-01   valLoss:9.5400e-01  time: 1.56e+00\n",
      "epoch: 7   trainLoss: 4.5863e-01   valLoss:1.0370e+00  time: 1.64e+00\n",
      "epoch: 8   trainLoss: 4.1274e-01   valLoss:1.1706e+00  time: 1.56e+00\n",
      "epoch: 9   trainLoss: 3.7302e-01   valLoss:1.4495e+00  time: 1.56e+00\n",
      "epoch: 10   trainLoss: 3.4551e-01   valLoss:2.0211e+00  time: 1.55e+00\n",
      "epoch: 11   trainLoss: 3.0126e-01   valLoss:3.0723e+00  time: 1.55e+00\n",
      "epoch: 12   trainLoss: 2.6914e-01   valLoss:4.7547e+00  time: 1.55e+00\n",
      "epoch: 13   trainLoss: 2.4363e-01   valLoss:7.0259e+00  time: 1.51e+00\n",
      "epoch: 14   trainLoss: 2.3894e-01   valLoss:9.9772e+00  time: 1.56e+00\n",
      "epoch: 15   trainLoss: 2.1285e-01   valLoss:1.3296e+01  time: 1.58e+00\n",
      "epoch: 16   trainLoss: 1.9607e-01   valLoss:1.4477e+01  time: 1.56e+00\n",
      "epoch: 17   trainLoss: 1.7283e-01   valLoss:1.4641e+01  time: 1.55e+00\n",
      "epoch: 18   trainLoss: 1.5697e-01   valLoss:1.5016e+01  time: 1.52e+00\n",
      "epoch: 19   trainLoss: 1.4472e-01   valLoss:1.4362e+01  time: 1.54e+00\n",
      "epoch: 20   trainLoss: 1.2978e-01   valLoss:1.3387e+01  time: 1.54e+00\n",
      "epoch: 21   trainLoss: 1.2379e-01   valLoss:1.1483e+01  time: 1.50e+00\n",
      "epoch: 22   trainLoss: 1.3201e-01   valLoss:1.0067e+01  time: 1.48e+00\n",
      "epoch: 23   trainLoss: 1.1206e-01   valLoss:7.9583e+00  time: 1.49e+00\n",
      "epoch: 24   trainLoss: 1.1153e-01   valLoss:6.0220e+00  time: 1.56e+00\n",
      "epoch: 25   trainLoss: 1.0184e-01   valLoss:3.8629e+00  time: 1.58e+00\n",
      "epoch: 26   trainLoss: 9.4118e-02   valLoss:3.1088e+00  time: 1.57e+00\n",
      "epoch: 27   trainLoss: 8.2635e-02   valLoss:2.5706e+00  time: 1.56e+00\n",
      "epoch: 28   trainLoss: 7.8066e-02   valLoss:1.8518e+00  time: 1.56e+00\n",
      "epoch: 29   trainLoss: 7.0441e-02   valLoss:1.5983e+00  time: 1.56e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30   trainLoss: 6.3618e-02   valLoss:1.6582e+00  time: 1.55e+00\n",
      "epoch: 31   trainLoss: 6.6581e-02   valLoss:1.5642e+00  time: 1.55e+00\n",
      "epoch: 32   trainLoss: 7.1668e-02   valLoss:1.7114e+00  time: 1.52e+00\n",
      "epoch: 33   trainLoss: 6.3438e-02   valLoss:1.3648e+00  time: 1.52e+00\n",
      "epoch: 34   trainLoss: 4.8213e-02   valLoss:1.2689e+00  time: 1.49e+00\n",
      "epoch: 35   trainLoss: 6.0739e-02   valLoss:1.3861e+00  time: 1.49e+00\n",
      "epoch: 36   trainLoss: 6.0462e-02   valLoss:1.3675e+00  time: 1.49e+00\n",
      "epoch: 37   trainLoss: 4.7917e-02   valLoss:1.5128e+00  time: 1.56e+00\n",
      "epoch: 38   trainLoss: 4.9866e-02   valLoss:1.7106e+00  time: 1.57e+00\n",
      "epoch: 39   trainLoss: 3.9271e-02   valLoss:1.5482e+00  time: 1.57e+00\n",
      "epoch: 40   trainLoss: 4.4066e-02   valLoss:1.5005e+00  time: 1.49e+00\n",
      "epoch: 41   trainLoss: 3.3576e-02   valLoss:1.5621e+00  time: 1.54e+00\n",
      "epoch: 42   trainLoss: 3.5388e-02   valLoss:1.2697e+00  time: 1.49e+00\n",
      "epoch: 43   trainLoss: 3.2727e-02   valLoss:1.3754e+00  time: 1.49e+00\n",
      "epoch: 44   trainLoss: 2.9544e-02   valLoss:1.3672e+00  time: 1.49e+00\n",
      "epoch: 45   trainLoss: 2.7503e-02   valLoss:1.3636e+00  time: 1.54e+00\n",
      "epoch: 46   trainLoss: 2.5712e-02   valLoss:1.4830e+00  time: 1.50e+00\n",
      "epoch: 47   trainLoss: 2.5430e-02   valLoss:1.3720e+00  time: 1.57e+00\n",
      "epoch: 48   trainLoss: 2.0477e-02   valLoss:1.3061e+00  time: 1.56e+00\n",
      "epoch: 49   trainLoss: 2.2488e-02   valLoss:1.3429e+00  time: 1.55e+00\n",
      "epoch: 50   trainLoss: 1.9567e-02   valLoss:1.2708e+00  time: 1.56e+00\n",
      "epoch: 51   trainLoss: 1.8147e-02   valLoss:1.3048e+00  time: 1.57e+00\n",
      "epoch: 52   trainLoss: 1.7767e-02   valLoss:1.3463e+00  time: 1.51e+00\n",
      "epoch: 53   trainLoss: 1.6144e-02   valLoss:1.2899e+00  time: 1.57e+00\n",
      "epoch: 54   trainLoss: 1.5587e-02   valLoss:1.2751e+00  time: 1.57e+00\n",
      "epoch: 55   trainLoss: 1.4840e-02   valLoss:1.2724e+00  time: 1.53e+00\n",
      "epoch: 56   trainLoss: 1.4380e-02   valLoss:1.0482e+00  time: 1.49e+00\n",
      "epoch: 57   trainLoss: 1.6153e-02   valLoss:1.2361e+00  time: 1.49e+00\n",
      "epoch: 58   trainLoss: 1.6270e-02   valLoss:1.1398e+00  time: 1.54e+00\n",
      "epoch: 59   trainLoss: 2.0021e-02   valLoss:1.1947e+00  time: 1.59e+00\n",
      "epoch: 60   trainLoss: 2.1077e-02   valLoss:9.6768e-01  time: 1.58e+00\n",
      "epoch: 61   trainLoss: 1.4209e-02   valLoss:1.0377e+00  time: 1.51e+00\n",
      "epoch: 62   trainLoss: 1.0998e-02   valLoss:1.1142e+00  time: 1.60e+00\n",
      "epoch: 63   trainLoss: 1.4893e-02   valLoss:9.7945e-01  time: 1.54e+00\n",
      "epoch: 64   trainLoss: 1.4939e-02   valLoss:1.0860e+00  time: 1.55e+00\n",
      "epoch: 65   trainLoss: 1.0069e-02   valLoss:1.0586e+00  time: 1.59e+00\n",
      "epoch: 66   trainLoss: 1.1433e-02   valLoss:9.5404e-01  time: 1.59e+00\n",
      "epoch: 67   trainLoss: 1.1969e-02   valLoss:9.7595e-01  time: 1.56e+00\n",
      "epoch: 68   trainLoss: 8.8982e-03   valLoss:9.1038e-01  time: 1.48e+00\n",
      "epoch: 69   trainLoss: 9.4468e-03   valLoss:9.0593e-01  time: 1.47e+00\n",
      "epoch: 70   trainLoss: 9.9410e-03   valLoss:8.8568e-01  time: 1.48e+00\n",
      "epoch: 71   trainLoss: 6.7848e-03   valLoss:8.9800e-01  time: 1.50e+00\n",
      "epoch: 72   trainLoss: 8.6680e-03   valLoss:8.3891e-01  time: 1.49e+00\n",
      "epoch: 73   trainLoss: 7.1854e-03   valLoss:8.6909e-01  time: 1.48e+00\n",
      "epoch: 74   trainLoss: 6.2542e-03   valLoss:8.7319e-01  time: 1.49e+00\n",
      "epoch: 75   trainLoss: 6.9626e-03   valLoss:7.7734e-01  time: 1.47e+00\n",
      "epoch: 76   trainLoss: 6.9593e-03   valLoss:7.9510e-01  time: 1.50e+00\n",
      "epoch: 77   trainLoss: 5.4676e-03   valLoss:7.4630e-01  time: 1.49e+00\n",
      "epoch: 78   trainLoss: 6.7117e-03   valLoss:8.1564e-01  time: 1.50e+00\n",
      "epoch: 79   trainLoss: 8.5064e-03   valLoss:7.0387e-01  time: 1.56e+00\n",
      "epoch: 80   trainLoss: 1.2816e-02   valLoss:1.0093e+00  time: 1.55e+00\n",
      "epoch: 81   trainLoss: 2.7690e-02   valLoss:6.0995e-01  time: 1.54e+00\n",
      "epoch: 82   trainLoss: 3.7199e-02   valLoss:8.7386e-01  time: 1.55e+00\n",
      "epoch: 83   trainLoss: 1.9185e-02   valLoss:8.8224e-01  time: 1.56e+00\n",
      "epoch: 84   trainLoss: 1.7473e-02   valLoss:6.7729e-01  time: 1.58e+00\n",
      "epoch: 85   trainLoss: 1.7432e-02   valLoss:6.6858e-01  time: 1.51e+00\n",
      "epoch: 86   trainLoss: 1.5075e-02   valLoss:8.1539e-01  time: 1.57e+00\n",
      "epoch: 87   trainLoss: 1.4016e-02   valLoss:7.2646e-01  time: 1.56e+00\n",
      "epoch: 88   trainLoss: 1.2066e-02   valLoss:5.8336e-01  time: 1.49e+00\n",
      "epoch: 89   trainLoss: 1.2078e-02   valLoss:6.1977e-01  time: 1.50e+00\n",
      "epoch: 90   trainLoss: 1.0045e-02   valLoss:7.2720e-01  time: 1.56e+00\n",
      "epoch: 91   trainLoss: 9.7989e-03   valLoss:7.1365e-01  time: 1.49e+00\n",
      "epoch: 92   trainLoss: 9.4717e-03   valLoss:6.1694e-01  time: 1.55e+00\n",
      "epoch: 93   trainLoss: 7.5857e-03   valLoss:6.0947e-01  time: 1.51e+00\n",
      "epoch: 94   trainLoss: 8.6873e-03   valLoss:6.5399e-01  time: 1.54e+00\n",
      "epoch: 95   trainLoss: 6.1167e-03   valLoss:6.9990e-01  time: 1.49e+00\n",
      "epoch: 96   trainLoss: 7.4174e-03   valLoss:6.4891e-01  time: 1.49e+00\n",
      "epoch: 97   trainLoss: 5.5802e-03   valLoss:5.9783e-01  time: 1.48e+00\n",
      "epoch: 98   trainLoss: 6.1899e-03   valLoss:6.1245e-01  time: 1.52e+00\n",
      "epoch: 99   trainLoss: 5.0852e-03   valLoss:6.3496e-01  time: 1.74e+00\n",
      "loading checkpoint 88\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.0366e+01   valLoss:3.5925e+01  time: 1.52e+00\n",
      "epoch: 1   trainLoss: 2.5367e+01   valLoss:3.2355e+01  time: 1.49e+00\n",
      "epoch: 2   trainLoss: 2.3376e+01   valLoss:3.1534e+01  time: 1.50e+00\n",
      "epoch: 3   trainLoss: 2.2417e+01   valLoss:3.0992e+01  time: 1.50e+00\n",
      "epoch: 4   trainLoss: 2.0732e+01   valLoss:2.9946e+01  time: 1.49e+00\n",
      "epoch: 5   trainLoss: 1.9613e+01   valLoss:2.6747e+01  time: 1.56e+00\n",
      "epoch: 6   trainLoss: 1.8176e+01   valLoss:2.3667e+01  time: 1.52e+00\n",
      "epoch: 7   trainLoss: 1.7124e+01   valLoss:2.1783e+01  time: 1.49e+00\n",
      "epoch: 8   trainLoss: 1.6093e+01   valLoss:1.7712e+01  time: 1.55e+00\n",
      "epoch: 9   trainLoss: 1.4896e+01   valLoss:1.6175e+01  time: 1.48e+00\n",
      "epoch: 10   trainLoss: 1.4191e+01   valLoss:1.2986e+01  time: 1.49e+00\n",
      "epoch: 11   trainLoss: 1.2919e+01   valLoss:1.2172e+01  time: 1.48e+00\n",
      "epoch: 12   trainLoss: 1.1981e+01   valLoss:9.2961e+00  time: 1.48e+00\n",
      "epoch: 13   trainLoss: 1.0951e+01   valLoss:7.8279e+00  time: 1.48e+00\n",
      "epoch: 14   trainLoss: 1.0091e+01   valLoss:7.9302e+00  time: 1.49e+00\n",
      "epoch: 15   trainLoss: 9.0717e+00   valLoss:6.6187e+00  time: 1.49e+00\n",
      "epoch: 16   trainLoss: 8.2956e+00   valLoss:6.5840e+00  time: 1.49e+00\n",
      "epoch: 17   trainLoss: 7.6510e+00   valLoss:6.6819e+00  time: 1.54e+00\n",
      "epoch: 18   trainLoss: 6.6723e+00   valLoss:8.2722e+00  time: 1.56e+00\n",
      "epoch: 19   trainLoss: 5.9776e+00   valLoss:8.7773e+00  time: 1.57e+00\n",
      "epoch: 20   trainLoss: 5.4232e+00   valLoss:9.1293e+00  time: 1.57e+00\n",
      "epoch: 21   trainLoss: 4.5954e+00   valLoss:9.8496e+00  time: 1.57e+00\n",
      "epoch: 22   trainLoss: 4.2298e+00   valLoss:8.4619e+00  time: 1.57e+00\n",
      "epoch: 23   trainLoss: 3.8331e+00   valLoss:9.2026e+00  time: 1.55e+00\n",
      "epoch: 24   trainLoss: 3.1575e+00   valLoss:1.1573e+01  time: 1.55e+00\n",
      "epoch: 25   trainLoss: 2.9356e+00   valLoss:9.8038e+00  time: 1.50e+00\n",
      "epoch: 26   trainLoss: 2.5743e+00   valLoss:1.2007e+01  time: 1.49e+00\n",
      "epoch: 27   trainLoss: 2.1263e+00   valLoss:1.4316e+01  time: 1.51e+00\n",
      "epoch: 28   trainLoss: 2.1071e+00   valLoss:1.0863e+01  time: 1.48e+00\n",
      "epoch: 29   trainLoss: 1.9813e+00   valLoss:9.6794e+00  time: 1.49e+00\n",
      "epoch: 30   trainLoss: 1.8636e+00   valLoss:1.3752e+01  time: 1.50e+00\n",
      "epoch: 31   trainLoss: 1.8347e+00   valLoss:7.8609e+00  time: 1.49e+00\n",
      "epoch: 32   trainLoss: 1.4706e+00   valLoss:6.2037e+00  time: 1.52e+00\n",
      "epoch: 33   trainLoss: 1.2884e+00   valLoss:6.0620e+00  time: 1.48e+00\n",
      "epoch: 34   trainLoss: 1.1852e+00   valLoss:5.7927e+00  time: 1.49e+00\n",
      "epoch: 35   trainLoss: 1.0395e+00   valLoss:5.0494e+00  time: 1.57e+00\n",
      "epoch: 36   trainLoss: 1.1068e+00   valLoss:3.8171e+00  time: 1.48e+00\n",
      "epoch: 37   trainLoss: 9.3048e-01   valLoss:3.1170e+00  time: 1.49e+00\n",
      "epoch: 38   trainLoss: 9.1994e-01   valLoss:2.2016e+00  time: 1.48e+00\n",
      "epoch: 39   trainLoss: 8.8314e-01   valLoss:2.1054e+00  time: 1.48e+00\n",
      "epoch: 40   trainLoss: 8.3292e-01   valLoss:2.5172e+00  time: 1.49e+00\n",
      "epoch: 41   trainLoss: 8.9933e-01   valLoss:1.9146e+00  time: 1.50e+00\n",
      "epoch: 42   trainLoss: 8.6116e-01   valLoss:1.3605e+00  time: 1.50e+00\n",
      "epoch: 43   trainLoss: 8.5598e-01   valLoss:1.2228e+00  time: 1.49e+00\n",
      "epoch: 44   trainLoss: 7.2792e-01   valLoss:1.1580e+00  time: 1.48e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45   trainLoss: 6.4518e-01   valLoss:1.5558e+00  time: 1.50e+00\n",
      "epoch: 46   trainLoss: 6.6394e-01   valLoss:1.6361e+00  time: 1.51e+00\n",
      "epoch: 47   trainLoss: 6.1287e-01   valLoss:1.4671e+00  time: 1.57e+00\n",
      "epoch: 48   trainLoss: 5.8264e-01   valLoss:1.5587e+00  time: 1.55e+00\n",
      "epoch: 49   trainLoss: 5.1087e-01   valLoss:1.9051e+00  time: 1.56e+00\n",
      "epoch: 50   trainLoss: 5.2403e-01   valLoss:1.8243e+00  time: 1.55e+00\n",
      "epoch: 51   trainLoss: 4.9104e-01   valLoss:1.8925e+00  time: 1.56e+00\n",
      "epoch: 52   trainLoss: 4.6794e-01   valLoss:2.2327e+00  time: 1.55e+00\n",
      "epoch: 53   trainLoss: 4.4604e-01   valLoss:2.4958e+00  time: 1.55e+00\n",
      "epoch: 54   trainLoss: 4.0803e-01   valLoss:2.2439e+00  time: 1.57e+00\n",
      "epoch: 55   trainLoss: 4.0454e-01   valLoss:2.2010e+00  time: 1.57e+00\n",
      "epoch: 56   trainLoss: 3.9952e-01   valLoss:3.3729e+00  time: 1.53e+00\n",
      "epoch: 57   trainLoss: 3.9075e-01   valLoss:3.7844e+00  time: 1.54e+00\n",
      "epoch: 58   trainLoss: 3.6631e-01   valLoss:5.0956e+00  time: 1.55e+00\n",
      "epoch: 59   trainLoss: 3.2637e-01   valLoss:4.6712e+00  time: 1.55e+00\n",
      "epoch: 60   trainLoss: 3.1895e-01   valLoss:4.2687e+00  time: 1.55e+00\n",
      "epoch: 61   trainLoss: 3.2755e-01   valLoss:4.7831e+00  time: 1.55e+00\n",
      "epoch: 62   trainLoss: 3.1422e-01   valLoss:4.6381e+00  time: 1.49e+00\n",
      "epoch: 63   trainLoss: 2.9558e-01   valLoss:4.1888e+00  time: 1.49e+00\n",
      "epoch: 64   trainLoss: 2.9521e-01   valLoss:5.4838e+00  time: 1.56e+00\n",
      "epoch: 65   trainLoss: 3.2543e-01   valLoss:3.4105e+00  time: 1.56e+00\n",
      "epoch: 66   trainLoss: 3.7154e-01   valLoss:3.4196e+00  time: 1.54e+00\n",
      "epoch: 67   trainLoss: 4.5010e-01   valLoss:2.3804e+00  time: 1.58e+00\n",
      "epoch: 68   trainLoss: 3.8892e-01   valLoss:1.9980e+00  time: 1.58e+00\n",
      "epoch: 69   trainLoss: 3.0751e-01   valLoss:1.4265e+00  time: 1.48e+00\n",
      "epoch: 70   trainLoss: 2.3990e-01   valLoss:6.7361e-01  time: 1.49e+00\n",
      "epoch: 71   trainLoss: 2.6831e-01   valLoss:1.0867e+00  time: 1.49e+00\n",
      "epoch: 72   trainLoss: 3.0752e-01   valLoss:1.2362e+00  time: 1.52e+00\n",
      "epoch: 73   trainLoss: 2.3713e-01   valLoss:6.6485e-01  time: 1.55e+00\n",
      "epoch: 74   trainLoss: 2.2493e-01   valLoss:1.1817e+00  time: 1.51e+00\n",
      "epoch: 75   trainLoss: 2.5639e-01   valLoss:8.0711e-01  time: 1.49e+00\n",
      "epoch: 76   trainLoss: 2.0271e-01   valLoss:7.0635e-01  time: 1.53e+00\n",
      "epoch: 77   trainLoss: 1.8874e-01   valLoss:1.0389e+00  time: 1.58e+00\n",
      "epoch: 78   trainLoss: 2.2075e-01   valLoss:9.2093e-01  time: 1.49e+00\n",
      "epoch: 79   trainLoss: 1.7246e-01   valLoss:1.2226e+00  time: 1.48e+00\n",
      "epoch: 80   trainLoss: 1.5288e-01   valLoss:1.6523e+00  time: 1.48e+00\n",
      "epoch: 81   trainLoss: 1.8170e-01   valLoss:1.3147e+00  time: 1.50e+00\n",
      "epoch: 82   trainLoss: 1.6147e-01   valLoss:1.3902e+00  time: 1.49e+00\n",
      "epoch: 83   trainLoss: 1.2994e-01   valLoss:1.3123e+00  time: 1.54e+00\n",
      "epoch: 84   trainLoss: 1.4337e-01   valLoss:1.2610e+00  time: 1.53e+00\n",
      "epoch: 85   trainLoss: 1.4376e-01   valLoss:1.1413e+00  time: 1.47e+00\n",
      "epoch: 86   trainLoss: 1.2342e-01   valLoss:9.1708e-01  time: 1.55e+00\n",
      "epoch: 87   trainLoss: 1.2047e-01   valLoss:1.0071e+00  time: 1.50e+00\n",
      "epoch: 88   trainLoss: 1.2486e-01   valLoss:9.6605e-01  time: 1.48e+00\n",
      "epoch: 89   trainLoss: 1.1333e-01   valLoss:1.0013e+00  time: 1.48e+00\n",
      "epoch: 90   trainLoss: 1.0224e-01   valLoss:1.1985e+00  time: 1.50e+00\n",
      "epoch: 91   trainLoss: 1.0986e-01   valLoss:8.4416e-01  time: 1.48e+00\n",
      "epoch: 92   trainLoss: 1.1081e-01   valLoss:8.5699e-01  time: 1.48e+00\n",
      "epoch: 93   trainLoss: 9.7112e-02   valLoss:8.6821e-01  time: 1.48e+00\n",
      "epoch: 94   trainLoss: 8.6850e-02   valLoss:7.9186e-01  time: 1.56e+00\n",
      "epoch: 95   trainLoss: 8.9805e-02   valLoss:8.9417e-01  time: 1.48e+00\n",
      "epoch: 96   trainLoss: 9.2374e-02   valLoss:6.8758e-01  time: 1.48e+00\n",
      "epoch: 97   trainLoss: 8.5415e-02   valLoss:6.2558e-01  time: 1.50e+00\n",
      "epoch: 98   trainLoss: 7.6889e-02   valLoss:5.0097e-01  time: 1.48e+00\n",
      "epoch: 99   trainLoss: 7.3501e-02   valLoss:5.0656e-01  time: 1.55e+00\n",
      "loading checkpoint 98\n",
      "trained 30 random forest models in 6.03 seconds\n",
      "loaded train set of size 46\n",
      "epoch: 0   trainLoss: 9.8115e-01   valLoss:9.8422e-01  time: 4.00e-01\n",
      "epoch: 1   trainLoss: 8.3914e-01   valLoss:9.6379e-01  time: 4.00e-01\n",
      "epoch: 2   trainLoss: 6.9354e-01   valLoss:9.3334e-01  time: 4.02e-01\n",
      "epoch: 3   trainLoss: 5.9620e-01   valLoss:8.9580e-01  time: 4.00e-01\n",
      "epoch: 4   trainLoss: 5.1452e-01   valLoss:8.7185e-01  time: 4.04e-01\n",
      "epoch: 5   trainLoss: 4.5804e-01   valLoss:8.9317e-01  time: 3.99e-01\n",
      "epoch: 6   trainLoss: 4.1228e-01   valLoss:9.6123e-01  time: 4.07e-01\n",
      "epoch: 7   trainLoss: 3.7174e-01   valLoss:1.0582e+00  time: 3.95e-01\n",
      "epoch: 8   trainLoss: 3.3470e-01   valLoss:1.1696e+00  time: 3.98e-01\n",
      "epoch: 9   trainLoss: 2.8479e-01   valLoss:1.3286e+00  time: 3.99e-01\n",
      "epoch: 10   trainLoss: 2.4006e-01   valLoss:1.6295e+00  time: 4.02e-01\n",
      "epoch: 11   trainLoss: 2.0279e-01   valLoss:2.2832e+00  time: 4.05e-01\n",
      "epoch: 12   trainLoss: 1.7936e-01   valLoss:3.4076e+00  time: 3.98e-01\n",
      "epoch: 13   trainLoss: 1.5684e-01   valLoss:4.5509e+00  time: 4.00e-01\n",
      "epoch: 14   trainLoss: 1.3518e-01   valLoss:5.6026e+00  time: 3.96e-01\n",
      "epoch: 15   trainLoss: 1.2066e-01   valLoss:6.6957e+00  time: 4.01e-01\n",
      "epoch: 16   trainLoss: 1.0409e-01   valLoss:7.5526e+00  time: 3.98e-01\n",
      "epoch: 17   trainLoss: 9.3203e-02   valLoss:8.3501e+00  time: 4.04e-01\n",
      "epoch: 18   trainLoss: 8.4291e-02   valLoss:8.9406e+00  time: 4.02e-01\n",
      "epoch: 19   trainLoss: 6.9184e-02   valLoss:9.4731e+00  time: 3.99e-01\n",
      "epoch: 20   trainLoss: 6.2639e-02   valLoss:8.6941e+00  time: 4.02e-01\n",
      "epoch: 21   trainLoss: 5.4319e-02   valLoss:6.8669e+00  time: 3.98e-01\n",
      "epoch: 22   trainLoss: 5.0816e-02   valLoss:5.0529e+00  time: 4.16e-01\n",
      "epoch: 23   trainLoss: 4.5914e-02   valLoss:2.5683e+00  time: 4.18e-01\n",
      "epoch: 24   trainLoss: 4.2458e-02   valLoss:1.6950e+00  time: 4.16e-01\n",
      "epoch: 25   trainLoss: 3.3850e-02   valLoss:8.8483e-01  time: 4.15e-01\n",
      "epoch: 26   trainLoss: 3.0394e-02   valLoss:3.5245e-01  time: 4.25e-01\n",
      "epoch: 27   trainLoss: 2.9077e-02   valLoss:2.6674e-01  time: 4.12e-01\n",
      "epoch: 28   trainLoss: 2.6829e-02   valLoss:2.1126e-01  time: 4.10e-01\n",
      "epoch: 29   trainLoss: 2.0821e-02   valLoss:2.1750e-01  time: 4.00e-01\n",
      "epoch: 30   trainLoss: 2.0765e-02   valLoss:2.0991e-01  time: 4.19e-01\n",
      "epoch: 31   trainLoss: 1.8231e-02   valLoss:2.1364e-01  time: 4.18e-01\n",
      "epoch: 32   trainLoss: 1.5546e-02   valLoss:2.2500e-01  time: 4.16e-01\n",
      "epoch: 33   trainLoss: 1.3731e-02   valLoss:2.1655e-01  time: 4.13e-01\n",
      "epoch: 34   trainLoss: 1.3194e-02   valLoss:2.1982e-01  time: 4.18e-01\n",
      "epoch: 35   trainLoss: 1.1316e-02   valLoss:2.2766e-01  time: 4.13e-01\n",
      "epoch: 36   trainLoss: 9.6961e-03   valLoss:2.1833e-01  time: 4.19e-01\n",
      "epoch: 37   trainLoss: 9.3948e-03   valLoss:2.2863e-01  time: 3.86e-01\n",
      "epoch: 38   trainLoss: 8.4490e-03   valLoss:2.1791e-01  time: 3.89e-01\n",
      "epoch: 39   trainLoss: 7.0367e-03   valLoss:2.2501e-01  time: 3.85e-01\n",
      "epoch: 40   trainLoss: 6.2753e-03   valLoss:2.2096e-01  time: 3.86e-01\n",
      "epoch: 41   trainLoss: 6.2320e-03   valLoss:2.2143e-01  time: 3.85e-01\n",
      "epoch: 42   trainLoss: 5.1686e-03   valLoss:2.1703e-01  time: 4.16e-01\n",
      "epoch: 43   trainLoss: 4.6367e-03   valLoss:2.0459e-01  time: 4.16e-01\n",
      "epoch: 44   trainLoss: 4.3521e-03   valLoss:2.0185e-01  time: 4.18e-01\n",
      "epoch: 45   trainLoss: 3.9044e-03   valLoss:2.1353e-01  time: 4.12e-01\n",
      "epoch: 46   trainLoss: 3.5423e-03   valLoss:1.8303e-01  time: 4.14e-01\n",
      "epoch: 47   trainLoss: 3.4736e-03   valLoss:2.2645e-01  time: 4.16e-01\n",
      "epoch: 48   trainLoss: 3.8776e-03   valLoss:1.6603e-01  time: 4.13e-01\n",
      "epoch: 49   trainLoss: 4.2107e-03   valLoss:2.3219e-01  time: 4.26e-01\n",
      "epoch: 50   trainLoss: 4.5185e-03   valLoss:1.4997e-01  time: 4.11e-01\n",
      "epoch: 51   trainLoss: 4.1680e-03   valLoss:1.9498e-01  time: 4.19e-01\n",
      "epoch: 52   trainLoss: 2.6707e-03   valLoss:1.7105e-01  time: 4.17e-01\n",
      "epoch: 53   trainLoss: 1.9860e-03   valLoss:1.5058e-01  time: 4.09e-01\n",
      "epoch: 54   trainLoss: 2.4992e-03   valLoss:1.8130e-01  time: 4.16e-01\n",
      "epoch: 55   trainLoss: 2.5297e-03   valLoss:1.4901e-01  time: 4.13e-01\n",
      "epoch: 56   trainLoss: 1.9497e-03   valLoss:1.5408e-01  time: 4.15e-01\n",
      "epoch: 57   trainLoss: 1.5382e-03   valLoss:1.6062e-01  time: 4.13e-01\n",
      "epoch: 58   trainLoss: 1.5032e-03   valLoss:1.3483e-01  time: 4.14e-01\n",
      "epoch: 59   trainLoss: 1.5610e-03   valLoss:1.5986e-01  time: 4.13e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60   trainLoss: 1.6445e-03   valLoss:1.3249e-01  time: 4.17e-01\n",
      "epoch: 61   trainLoss: 1.7040e-03   valLoss:1.4450e-01  time: 4.15e-01\n",
      "epoch: 62   trainLoss: 1.7737e-03   valLoss:1.3136e-01  time: 3.90e-01\n",
      "epoch: 63   trainLoss: 3.0360e-03   valLoss:1.5058e-01  time: 3.85e-01\n",
      "epoch: 64   trainLoss: 6.7398e-03   valLoss:1.1914e-01  time: 3.83e-01\n",
      "epoch: 65   trainLoss: 1.4142e-02   valLoss:1.8546e-01  time: 3.85e-01\n",
      "epoch: 66   trainLoss: 2.2467e-02   valLoss:1.0320e-01  time: 4.00e-01\n",
      "epoch: 67   trainLoss: 1.0302e-02   valLoss:1.1035e-01  time: 3.89e-01\n",
      "epoch: 68   trainLoss: 6.2341e-03   valLoss:1.1758e-01  time: 4.20e-01\n",
      "epoch: 69   trainLoss: 1.1493e-02   valLoss:9.0771e-02  time: 4.11e-01\n",
      "epoch: 70   trainLoss: 3.7581e-03   valLoss:7.8927e-02  time: 4.15e-01\n",
      "epoch: 71   trainLoss: 9.2247e-03   valLoss:7.9036e-02  time: 4.13e-01\n",
      "epoch: 72   trainLoss: 2.8128e-03   valLoss:9.8772e-02  time: 4.11e-01\n",
      "epoch: 73   trainLoss: 7.2053e-03   valLoss:8.2085e-02  time: 4.17e-01\n",
      "epoch: 74   trainLoss: 2.3471e-03   valLoss:7.6523e-02  time: 4.12e-01\n",
      "epoch: 75   trainLoss: 5.3601e-03   valLoss:8.0433e-02  time: 4.14e-01\n",
      "epoch: 76   trainLoss: 2.7522e-03   valLoss:8.9289e-02  time: 4.12e-01\n",
      "epoch: 77   trainLoss: 3.5897e-03   valLoss:8.7267e-02  time: 4.14e-01\n",
      "epoch: 78   trainLoss: 2.6339e-03   valLoss:7.6035e-02  time: 4.16e-01\n",
      "epoch: 79   trainLoss: 2.4667e-03   valLoss:7.2020e-02  time: 4.17e-01\n",
      "epoch: 80   trainLoss: 2.4778e-03   valLoss:8.1057e-02  time: 4.18e-01\n",
      "epoch: 81   trainLoss: 1.7499e-03   valLoss:8.7333e-02  time: 3.82e-01\n",
      "epoch: 82   trainLoss: 2.1513e-03   valLoss:7.9281e-02  time: 3.82e-01\n",
      "epoch: 83   trainLoss: 1.3623e-03   valLoss:7.5728e-02  time: 3.83e-01\n",
      "epoch: 84   trainLoss: 1.7912e-03   valLoss:8.3528e-02  time: 3.83e-01\n",
      "epoch: 85   trainLoss: 1.1636e-03   valLoss:8.8062e-02  time: 3.81e-01\n",
      "epoch: 86   trainLoss: 1.3741e-03   valLoss:8.1927e-02  time: 3.82e-01\n",
      "epoch: 87   trainLoss: 1.0472e-03   valLoss:7.7600e-02  time: 3.83e-01\n",
      "epoch: 88   trainLoss: 1.0626e-03   valLoss:7.3486e-02  time: 3.84e-01\n",
      "epoch: 89   trainLoss: 9.2581e-04   valLoss:7.7425e-02  time: 3.82e-01\n",
      "epoch: 90   trainLoss: 7.9872e-04   valLoss:8.1861e-02  time: 3.89e-01\n",
      "epoch: 91   trainLoss: 8.1186e-04   valLoss:7.1959e-02  time: 3.83e-01\n",
      "epoch: 92   trainLoss: 6.7069e-04   valLoss:6.8728e-02  time: 3.84e-01\n",
      "epoch: 93   trainLoss: 6.7788e-04   valLoss:6.8630e-02  time: 3.83e-01\n",
      "epoch: 94   trainLoss: 6.0928e-04   valLoss:7.4051e-02  time: 4.21e-01\n",
      "epoch: 95   trainLoss: 6.4946e-04   valLoss:7.0810e-02  time: 3.85e-01\n",
      "epoch: 96   trainLoss: 6.7255e-04   valLoss:7.2789e-02  time: 3.81e-01\n",
      "epoch: 97   trainLoss: 1.0090e-03   valLoss:5.9401e-02  time: 3.85e-01\n",
      "epoch: 98   trainLoss: 2.2936e-03   valLoss:8.6163e-02  time: 3.85e-01\n",
      "epoch: 99   trainLoss: 6.0407e-03   valLoss:6.1816e-02  time: 4.13e-01\n",
      "loading checkpoint 97\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 2.9007e+01   valLoss:3.3066e+01  time: 4.60e-01\n",
      "epoch: 1   trainLoss: 2.3531e+01   valLoss:2.8501e+01  time: 4.67e-01\n",
      "epoch: 2   trainLoss: 2.1292e+01   valLoss:2.6594e+01  time: 4.65e-01\n",
      "epoch: 3   trainLoss: 1.9533e+01   valLoss:2.6118e+01  time: 4.62e-01\n",
      "epoch: 4   trainLoss: 1.8107e+01   valLoss:2.4843e+01  time: 4.60e-01\n",
      "epoch: 5   trainLoss: 1.6952e+01   valLoss:2.4858e+01  time: 4.75e-01\n",
      "epoch: 6   trainLoss: 1.5848e+01   valLoss:2.5030e+01  time: 4.64e-01\n",
      "epoch: 7   trainLoss: 1.4910e+01   valLoss:2.3249e+01  time: 4.60e-01\n",
      "epoch: 8   trainLoss: 1.3843e+01   valLoss:2.2617e+01  time: 4.18e-01\n",
      "epoch: 9   trainLoss: 1.2708e+01   valLoss:2.1645e+01  time: 4.18e-01\n",
      "epoch: 10   trainLoss: 1.1861e+01   valLoss:1.8525e+01  time: 4.17e-01\n",
      "epoch: 11   trainLoss: 1.0895e+01   valLoss:1.7011e+01  time: 4.13e-01\n",
      "epoch: 12   trainLoss: 1.0093e+01   valLoss:1.2619e+01  time: 4.14e-01\n",
      "epoch: 13   trainLoss: 9.6123e+00   valLoss:1.6365e+01  time: 4.19e-01\n",
      "epoch: 14   trainLoss: 8.7333e+00   valLoss:1.6823e+01  time: 4.15e-01\n",
      "epoch: 15   trainLoss: 7.7614e+00   valLoss:1.3763e+01  time: 4.18e-01\n",
      "epoch: 16   trainLoss: 7.0868e+00   valLoss:1.1791e+01  time: 4.15e-01\n",
      "epoch: 17   trainLoss: 6.2480e+00   valLoss:1.1475e+01  time: 4.20e-01\n",
      "epoch: 18   trainLoss: 5.7241e+00   valLoss:1.0088e+01  time: 4.22e-01\n",
      "epoch: 19   trainLoss: 5.0196e+00   valLoss:7.8021e+00  time: 3.93e-01\n",
      "epoch: 20   trainLoss: 4.4859e+00   valLoss:8.4357e+00  time: 3.91e-01\n",
      "epoch: 21   trainLoss: 3.9311e+00   valLoss:8.9734e+00  time: 3.97e-01\n",
      "epoch: 22   trainLoss: 3.5339e+00   valLoss:5.2638e+00  time: 3.93e-01\n",
      "epoch: 23   trainLoss: 2.9965e+00   valLoss:3.6273e+00  time: 3.91e-01\n",
      "epoch: 24   trainLoss: 2.7390e+00   valLoss:5.6769e+00  time: 3.93e-01\n",
      "epoch: 25   trainLoss: 2.3597e+00   valLoss:4.4808e+00  time: 3.92e-01\n",
      "epoch: 26   trainLoss: 2.0296e+00   valLoss:2.9037e+00  time: 4.01e-01\n",
      "epoch: 27   trainLoss: 1.7937e+00   valLoss:3.1030e+00  time: 4.28e-01\n",
      "epoch: 28   trainLoss: 1.5665e+00   valLoss:1.9520e+00  time: 4.47e-01\n",
      "epoch: 29   trainLoss: 1.5324e+00   valLoss:2.6931e+00  time: 4.13e-01\n",
      "epoch: 30   trainLoss: 1.3709e+00   valLoss:1.7883e+00  time: 4.21e-01\n",
      "epoch: 31   trainLoss: 1.2055e+00   valLoss:2.3799e+00  time: 4.19e-01\n",
      "epoch: 32   trainLoss: 1.2374e+00   valLoss:2.7337e+00  time: 4.11e-01\n",
      "epoch: 33   trainLoss: 1.1358e+00   valLoss:5.9987e+00  time: 4.17e-01\n",
      "epoch: 34   trainLoss: 9.3438e-01   valLoss:2.6104e+00  time: 4.60e-01\n",
      "epoch: 35   trainLoss: 8.3431e-01   valLoss:2.1690e+00  time: 4.59e-01\n",
      "epoch: 36   trainLoss: 8.8576e-01   valLoss:2.4480e+00  time: 4.64e-01\n",
      "epoch: 37   trainLoss: 7.3351e-01   valLoss:3.7524e+00  time: 4.69e-01\n",
      "epoch: 38   trainLoss: 7.9223e-01   valLoss:3.0295e+00  time: 4.72e-01\n",
      "epoch: 39   trainLoss: 7.4967e-01   valLoss:2.3202e+00  time: 4.90e-01\n",
      "epoch: 40   trainLoss: 6.7990e-01   valLoss:2.9936e+00  time: 4.31e-01\n",
      "epoch: 41   trainLoss: 6.8661e-01   valLoss:3.7597e+00  time: 4.73e-01\n",
      "epoch: 42   trainLoss: 5.8330e-01   valLoss:3.4112e+00  time: 4.64e-01\n",
      "epoch: 43   trainLoss: 6.1410e-01   valLoss:2.8326e+00  time: 4.75e-01\n",
      "epoch: 44   trainLoss: 5.1892e-01   valLoss:2.0074e+00  time: 4.31e-01\n",
      "epoch: 45   trainLoss: 5.1862e-01   valLoss:2.6523e+00  time: 4.26e-01\n",
      "epoch: 46   trainLoss: 4.7258e-01   valLoss:2.9470e+00  time: 4.23e-01\n",
      "epoch: 47   trainLoss: 4.5175e-01   valLoss:2.0076e+00  time: 4.47e-01\n",
      "epoch: 48   trainLoss: 4.0229e-01   valLoss:1.3186e+00  time: 4.30e-01\n",
      "epoch: 49   trainLoss: 3.8876e-01   valLoss:1.1263e+00  time: 4.42e-01\n",
      "epoch: 50   trainLoss: 3.6297e-01   valLoss:9.5940e-01  time: 4.44e-01\n",
      "epoch: 51   trainLoss: 3.3269e-01   valLoss:8.2963e-01  time: 4.07e-01\n",
      "epoch: 52   trainLoss: 3.3356e-01   valLoss:7.0562e-01  time: 4.33e-01\n",
      "epoch: 53   trainLoss: 2.9663e-01   valLoss:6.4723e-01  time: 4.18e-01\n",
      "epoch: 54   trainLoss: 3.0117e-01   valLoss:5.5344e-01  time: 4.18e-01\n",
      "epoch: 55   trainLoss: 2.6921e-01   valLoss:5.9817e-01  time: 4.23e-01\n",
      "epoch: 56   trainLoss: 2.6752e-01   valLoss:8.3464e-01  time: 4.67e-01\n",
      "epoch: 57   trainLoss: 2.4347e-01   valLoss:9.6585e-01  time: 4.62e-01\n",
      "epoch: 58   trainLoss: 2.3150e-01   valLoss:9.6262e-01  time: 4.24e-01\n",
      "epoch: 59   trainLoss: 2.2143e-01   valLoss:1.3274e+00  time: 4.49e-01\n",
      "epoch: 60   trainLoss: 2.0039e-01   valLoss:1.0924e+00  time: 4.69e-01\n",
      "epoch: 61   trainLoss: 1.9594e-01   valLoss:6.7700e-01  time: 4.65e-01\n",
      "epoch: 62   trainLoss: 1.7914e-01   valLoss:5.9893e-01  time: 4.34e-01\n",
      "epoch: 63   trainLoss: 1.7755e-01   valLoss:5.8232e-01  time: 5.23e-01\n",
      "epoch: 64   trainLoss: 1.6553e-01   valLoss:5.5516e-01  time: 5.36e-01\n",
      "epoch: 65   trainLoss: 1.5951e-01   valLoss:4.1593e-01  time: 5.52e-01\n",
      "epoch: 66   trainLoss: 1.5322e-01   valLoss:3.6180e-01  time: 5.45e-01\n",
      "epoch: 67   trainLoss: 1.4371e-01   valLoss:3.2233e-01  time: 5.49e-01\n",
      "epoch: 68   trainLoss: 1.4324e-01   valLoss:3.3684e-01  time: 5.23e-01\n",
      "epoch: 69   trainLoss: 1.3473e-01   valLoss:3.0896e-01  time: 4.75e-01\n",
      "epoch: 70   trainLoss: 1.2817e-01   valLoss:3.6383e-01  time: 4.79e-01\n",
      "epoch: 71   trainLoss: 1.2376e-01   valLoss:3.2214e-01  time: 4.80e-01\n",
      "epoch: 72   trainLoss: 1.1592e-01   valLoss:2.8415e-01  time: 4.90e-01\n",
      "epoch: 73   trainLoss: 1.1284e-01   valLoss:3.3488e-01  time: 4.89e-01\n",
      "epoch: 74   trainLoss: 1.0805e-01   valLoss:3.6773e-01  time: 4.19e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75   trainLoss: 1.0201e-01   valLoss:4.9303e-01  time: 4.00e-01\n",
      "epoch: 76   trainLoss: 1.0047e-01   valLoss:4.5217e-01  time: 4.85e-01\n",
      "epoch: 77   trainLoss: 9.5112e-02   valLoss:4.9114e-01  time: 4.93e-01\n",
      "epoch: 78   trainLoss: 9.2067e-02   valLoss:4.8446e-01  time: 4.03e-01\n",
      "epoch: 79   trainLoss: 8.8569e-02   valLoss:5.6671e-01  time: 4.44e-01\n",
      "epoch: 80   trainLoss: 8.5629e-02   valLoss:5.1438e-01  time: 4.03e-01\n",
      "epoch: 81   trainLoss: 8.2532e-02   valLoss:6.5625e-01  time: 3.96e-01\n",
      "epoch: 82   trainLoss: 8.1516e-02   valLoss:6.4239e-01  time: 3.96e-01\n",
      "epoch: 83   trainLoss: 8.1031e-02   valLoss:1.0391e+00  time: 4.66e-01\n",
      "epoch: 84   trainLoss: 8.9416e-02   valLoss:5.4814e-01  time: 4.42e-01\n",
      "epoch: 85   trainLoss: 1.1666e-01   valLoss:1.7990e+00  time: 4.20e-01\n",
      "epoch: 86   trainLoss: 1.9586e-01   valLoss:9.0118e-01  time: 4.16e-01\n",
      "epoch: 87   trainLoss: 3.2334e-01   valLoss:1.8094e+00  time: 4.76e-01\n",
      "epoch: 88   trainLoss: 3.8254e-01   valLoss:1.2655e+00  time: 4.73e-01\n",
      "epoch: 89   trainLoss: 1.1931e-01   valLoss:1.8218e+00  time: 4.71e-01\n",
      "epoch: 90   trainLoss: 1.6140e-01   valLoss:2.0352e+00  time: 4.67e-01\n",
      "epoch: 91   trainLoss: 2.2895e-01   valLoss:2.3604e+00  time: 4.67e-01\n",
      "epoch: 92   trainLoss: 8.7179e-02   valLoss:3.1147e+00  time: 4.58e-01\n",
      "epoch: 93   trainLoss: 1.9928e-01   valLoss:1.5987e+00  time: 4.24e-01\n",
      "epoch: 94   trainLoss: 1.0095e-01   valLoss:1.1253e+00  time: 4.17e-01\n",
      "epoch: 95   trainLoss: 1.3439e-01   valLoss:1.0824e+00  time: 4.19e-01\n",
      "epoch: 96   trainLoss: 1.0112e-01   valLoss:8.1404e-01  time: 4.18e-01\n",
      "epoch: 97   trainLoss: 1.0391e-01   valLoss:7.3226e-01  time: 4.16e-01\n",
      "epoch: 98   trainLoss: 9.3590e-02   valLoss:7.6146e-01  time: 4.13e-01\n",
      "epoch: 99   trainLoss: 8.2932e-02   valLoss:7.3543e-01  time: 4.55e-01\n",
      "loading checkpoint 72\n",
      "trained 30 random forest models in 4.01 seconds\n",
      "loaded train set of size 97\n",
      "epoch: 0   trainLoss: 9.6803e-01   valLoss:9.5660e-01  time: 8.21e-01\n",
      "epoch: 1   trainLoss: 8.1035e-01   valLoss:9.4646e-01  time: 8.01e-01\n",
      "epoch: 2   trainLoss: 7.0162e-01   valLoss:9.2703e-01  time: 7.88e-01\n",
      "epoch: 3   trainLoss: 6.0560e-01   valLoss:9.0604e-01  time: 8.04e-01\n",
      "epoch: 4   trainLoss: 5.3734e-01   valLoss:8.8503e-01  time: 7.91e-01\n",
      "epoch: 5   trainLoss: 4.7546e-01   valLoss:8.7297e-01  time: 7.90e-01\n",
      "epoch: 6   trainLoss: 4.3971e-01   valLoss:8.8618e-01  time: 7.94e-01\n",
      "epoch: 7   trainLoss: 4.0097e-01   valLoss:9.4029e-01  time: 7.81e-01\n",
      "epoch: 8   trainLoss: 3.6484e-01   valLoss:1.0486e+00  time: 8.11e-01\n",
      "epoch: 9   trainLoss: 3.3582e-01   valLoss:1.2316e+00  time: 8.29e-01\n",
      "epoch: 10   trainLoss: 2.8662e-01   valLoss:1.5790e+00  time: 8.31e-01\n",
      "epoch: 11   trainLoss: 2.4211e-01   valLoss:2.3732e+00  time: 8.29e-01\n",
      "epoch: 12   trainLoss: 2.0712e-01   valLoss:3.8670e+00  time: 8.31e-01\n",
      "epoch: 13   trainLoss: 1.7104e-01   valLoss:6.0725e+00  time: 8.21e-01\n",
      "epoch: 14   trainLoss: 1.4678e-01   valLoss:8.6789e+00  time: 8.33e-01\n",
      "epoch: 15   trainLoss: 1.5939e-01   valLoss:1.0051e+01  time: 8.21e-01\n",
      "epoch: 16   trainLoss: 1.3963e-01   valLoss:1.0770e+01  time: 8.14e-01\n",
      "epoch: 17   trainLoss: 1.1057e-01   valLoss:1.0130e+01  time: 7.82e-01\n",
      "epoch: 18   trainLoss: 9.6784e-02   valLoss:8.3412e+00  time: 7.84e-01\n",
      "epoch: 19   trainLoss: 8.7882e-02   valLoss:6.9379e+00  time: 7.87e-01\n",
      "epoch: 20   trainLoss: 7.9389e-02   valLoss:6.1913e+00  time: 8.05e-01\n",
      "epoch: 21   trainLoss: 6.4351e-02   valLoss:4.9814e+00  time: 8.14e-01\n",
      "epoch: 22   trainLoss: 6.5820e-02   valLoss:3.5071e+00  time: 8.25e-01\n",
      "epoch: 23   trainLoss: 5.6706e-02   valLoss:2.3023e+00  time: 8.16e-01\n",
      "epoch: 24   trainLoss: 4.9249e-02   valLoss:1.4316e+00  time: 7.87e-01\n",
      "epoch: 25   trainLoss: 4.7441e-02   valLoss:8.0806e-01  time: 8.01e-01\n",
      "epoch: 26   trainLoss: 4.4907e-02   valLoss:5.7064e-01  time: 8.67e-01\n",
      "epoch: 27   trainLoss: 3.9907e-02   valLoss:3.3934e-01  time: 8.22e-01\n",
      "epoch: 28   trainLoss: 3.4535e-02   valLoss:2.3524e-01  time: 8.33e-01\n",
      "epoch: 29   trainLoss: 3.3230e-02   valLoss:1.9389e-01  time: 7.94e-01\n",
      "epoch: 30   trainLoss: 2.8778e-02   valLoss:1.6018e-01  time: 9.29e-01\n",
      "epoch: 31   trainLoss: 2.6971e-02   valLoss:1.5372e-01  time: 9.38e-01\n",
      "epoch: 32   trainLoss: 2.4836e-02   valLoss:1.4668e-01  time: 7.89e-01\n",
      "epoch: 33   trainLoss: 2.2323e-02   valLoss:1.2641e-01  time: 8.08e-01\n",
      "epoch: 34   trainLoss: 1.9634e-02   valLoss:1.3170e-01  time: 8.14e-01\n",
      "epoch: 35   trainLoss: 1.8731e-02   valLoss:1.5615e-01  time: 7.96e-01\n",
      "epoch: 36   trainLoss: 1.5972e-02   valLoss:1.4886e-01  time: 8.21e-01\n",
      "epoch: 37   trainLoss: 1.5436e-02   valLoss:1.3223e-01  time: 8.27e-01\n",
      "epoch: 38   trainLoss: 1.3165e-02   valLoss:1.3144e-01  time: 8.20e-01\n",
      "epoch: 39   trainLoss: 1.3096e-02   valLoss:1.2105e-01  time: 7.74e-01\n",
      "epoch: 40   trainLoss: 1.1415e-02   valLoss:1.2243e-01  time: 7.81e-01\n",
      "epoch: 41   trainLoss: 1.0679e-02   valLoss:1.2915e-01  time: 8.24e-01\n",
      "epoch: 42   trainLoss: 9.7857e-03   valLoss:1.2072e-01  time: 8.20e-01\n",
      "epoch: 43   trainLoss: 8.8862e-03   valLoss:1.2549e-01  time: 8.16e-01\n",
      "epoch: 44   trainLoss: 8.3957e-03   valLoss:1.2024e-01  time: 8.24e-01\n",
      "epoch: 45   trainLoss: 7.1774e-03   valLoss:1.2739e-01  time: 8.19e-01\n",
      "epoch: 46   trainLoss: 7.4478e-03   valLoss:1.2482e-01  time: 8.12e-01\n",
      "epoch: 47   trainLoss: 6.7123e-03   valLoss:1.3583e-01  time: 8.25e-01\n",
      "epoch: 48   trainLoss: 6.5763e-03   valLoss:1.1851e-01  time: 8.31e-01\n",
      "epoch: 49   trainLoss: 7.7580e-03   valLoss:1.6247e-01  time: 8.16e-01\n",
      "epoch: 50   trainLoss: 9.1450e-03   valLoss:1.4082e-01  time: 8.22e-01\n",
      "epoch: 51   trainLoss: 1.3565e-02   valLoss:1.6713e-01  time: 7.89e-01\n",
      "epoch: 52   trainLoss: 1.4213e-02   valLoss:1.3404e-01  time: 7.77e-01\n",
      "epoch: 53   trainLoss: 9.8386e-03   valLoss:1.4911e-01  time: 7.82e-01\n",
      "epoch: 54   trainLoss: 7.5479e-03   valLoss:1.4223e-01  time: 7.74e-01\n",
      "epoch: 55   trainLoss: 6.6374e-03   valLoss:1.1967e-01  time: 7.81e-01\n",
      "epoch: 56   trainLoss: 7.5738e-03   valLoss:1.2469e-01  time: 8.10e-01\n",
      "epoch: 57   trainLoss: 5.2548e-03   valLoss:1.2091e-01  time: 8.15e-01\n",
      "epoch: 58   trainLoss: 4.5363e-03   valLoss:1.1561e-01  time: 8.25e-01\n",
      "epoch: 59   trainLoss: 5.9569e-03   valLoss:1.2249e-01  time: 7.82e-01\n",
      "epoch: 60   trainLoss: 4.1407e-03   valLoss:1.2468e-01  time: 7.90e-01\n",
      "epoch: 61   trainLoss: 4.2583e-03   valLoss:1.2656e-01  time: 7.86e-01\n",
      "epoch: 62   trainLoss: 4.1015e-03   valLoss:1.3164e-01  time: 7.98e-01\n",
      "epoch: 63   trainLoss: 3.4455e-03   valLoss:1.3289e-01  time: 8.24e-01\n",
      "epoch: 64   trainLoss: 3.1953e-03   valLoss:1.2626e-01  time: 8.16e-01\n",
      "epoch: 65   trainLoss: 3.2576e-03   valLoss:1.4074e-01  time: 8.06e-01\n",
      "epoch: 66   trainLoss: 3.0230e-03   valLoss:1.2984e-01  time: 8.20e-01\n",
      "epoch: 67   trainLoss: 2.5736e-03   valLoss:1.1680e-01  time: 8.20e-01\n",
      "epoch: 68   trainLoss: 3.1309e-03   valLoss:1.3216e-01  time: 8.21e-01\n",
      "epoch: 69   trainLoss: 2.4973e-03   valLoss:1.5012e-01  time: 7.91e-01\n",
      "epoch: 70   trainLoss: 2.6559e-03   valLoss:1.3451e-01  time: 7.80e-01\n",
      "epoch: 71   trainLoss: 2.8853e-03   valLoss:1.6041e-01  time: 7.76e-01\n",
      "epoch: 72   trainLoss: 3.5699e-03   valLoss:1.6160e-01  time: 8.22e-01\n",
      "epoch: 73   trainLoss: 5.0192e-03   valLoss:1.8804e-01  time: 7.83e-01\n",
      "epoch: 74   trainLoss: 7.9283e-03   valLoss:1.6597e-01  time: 8.20e-01\n",
      "epoch: 75   trainLoss: 1.3085e-02   valLoss:2.2824e-01  time: 8.24e-01\n",
      "epoch: 76   trainLoss: 1.4964e-02   valLoss:1.6754e-01  time: 8.22e-01\n",
      "epoch: 77   trainLoss: 6.3897e-03   valLoss:1.5780e-01  time: 7.63e-01\n",
      "epoch: 78   trainLoss: 3.6337e-03   valLoss:1.9310e-01  time: 8.01e-01\n",
      "epoch: 79   trainLoss: 8.8386e-03   valLoss:1.9464e-01  time: 8.51e-01\n",
      "epoch: 80   trainLoss: 3.1984e-03   valLoss:1.6871e-01  time: 7.86e-01\n",
      "epoch: 81   trainLoss: 5.1140e-03   valLoss:1.6514e-01  time: 7.63e-01\n",
      "epoch: 82   trainLoss: 4.9755e-03   valLoss:1.6925e-01  time: 7.61e-01\n",
      "epoch: 83   trainLoss: 2.5663e-03   valLoss:1.6596e-01  time: 7.64e-01\n",
      "epoch: 84   trainLoss: 5.0267e-03   valLoss:1.4170e-01  time: 7.62e-01\n",
      "epoch: 85   trainLoss: 1.9935e-03   valLoss:1.3093e-01  time: 7.60e-01\n",
      "epoch: 86   trainLoss: 3.9441e-03   valLoss:1.3338e-01  time: 7.73e-01\n",
      "epoch: 87   trainLoss: 2.1512e-03   valLoss:1.3592e-01  time: 8.13e-01\n",
      "epoch: 88   trainLoss: 2.9940e-03   valLoss:1.2671e-01  time: 7.84e-01\n",
      "epoch: 89   trainLoss: 2.2097e-03   valLoss:1.1304e-01  time: 8.34e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90   trainLoss: 2.1459e-03   valLoss:1.1622e-01  time: 8.41e-01\n",
      "epoch: 91   trainLoss: 2.1638e-03   valLoss:1.2324e-01  time: 7.66e-01\n",
      "epoch: 92   trainLoss: 1.6442e-03   valLoss:1.2093e-01  time: 7.97e-01\n",
      "epoch: 93   trainLoss: 1.8046e-03   valLoss:1.1822e-01  time: 8.19e-01\n",
      "epoch: 94   trainLoss: 1.3700e-03   valLoss:1.4095e-01  time: 7.63e-01\n",
      "epoch: 95   trainLoss: 1.8548e-03   valLoss:1.1970e-01  time: 8.07e-01\n",
      "epoch: 96   trainLoss: 1.4026e-03   valLoss:1.3793e-01  time: 8.30e-01\n",
      "epoch: 97   trainLoss: 2.6686e-03   valLoss:1.3074e-01  time: 8.29e-01\n",
      "epoch: 98   trainLoss: 4.9448e-03   valLoss:1.8920e-01  time: 8.22e-01\n",
      "epoch: 99   trainLoss: 1.4140e-02   valLoss:1.4058e-01  time: 7.86e-01\n",
      "loading checkpoint 89\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.1924e+01   valLoss:3.6152e+01  time: 9.66e-01\n",
      "epoch: 1   trainLoss: 2.7075e+01   valLoss:2.9764e+01  time: 9.23e-01\n",
      "epoch: 2   trainLoss: 2.4925e+01   valLoss:3.2121e+01  time: 9.26e-01\n",
      "epoch: 3   trainLoss: 2.4171e+01   valLoss:3.0582e+01  time: 9.35e-01\n",
      "epoch: 4   trainLoss: 2.2196e+01   valLoss:2.7865e+01  time: 8.03e-01\n",
      "epoch: 5   trainLoss: 2.1250e+01   valLoss:2.7555e+01  time: 7.83e-01\n",
      "epoch: 6   trainLoss: 1.9662e+01   valLoss:2.6470e+01  time: 7.80e-01\n",
      "epoch: 7   trainLoss: 1.8615e+01   valLoss:2.6844e+01  time: 7.95e-01\n",
      "epoch: 8   trainLoss: 1.7443e+01   valLoss:2.5643e+01  time: 8.13e-01\n",
      "epoch: 9   trainLoss: 1.6325e+01   valLoss:2.4528e+01  time: 7.91e-01\n",
      "epoch: 10   trainLoss: 1.5187e+01   valLoss:2.3322e+01  time: 8.36e-01\n",
      "epoch: 11   trainLoss: 1.4098e+01   valLoss:2.1634e+01  time: 7.89e-01\n",
      "epoch: 12   trainLoss: 1.3062e+01   valLoss:1.9070e+01  time: 8.08e-01\n",
      "epoch: 13   trainLoss: 1.1996e+01   valLoss:1.6098e+01  time: 8.31e-01\n",
      "epoch: 14   trainLoss: 1.1006e+01   valLoss:1.3117e+01  time: 8.17e-01\n",
      "epoch: 15   trainLoss: 1.0074e+01   valLoss:1.2207e+01  time: 7.81e-01\n",
      "epoch: 16   trainLoss: 9.1373e+00   valLoss:1.3786e+01  time: 8.33e-01\n",
      "epoch: 17   trainLoss: 8.3991e+00   valLoss:1.1358e+01  time: 8.19e-01\n",
      "epoch: 18   trainLoss: 7.5655e+00   valLoss:1.0417e+01  time: 7.79e-01\n",
      "epoch: 19   trainLoss: 6.7384e+00   valLoss:9.1333e+00  time: 8.19e-01\n",
      "epoch: 20   trainLoss: 5.9465e+00   valLoss:6.4793e+00  time: 8.31e-01\n",
      "epoch: 21   trainLoss: 5.2952e+00   valLoss:5.4961e+00  time: 7.84e-01\n",
      "epoch: 22   trainLoss: 4.6010e+00   valLoss:4.2026e+00  time: 8.02e-01\n",
      "epoch: 23   trainLoss: 4.1121e+00   valLoss:3.1294e+00  time: 8.36e-01\n",
      "epoch: 24   trainLoss: 3.5841e+00   valLoss:3.2870e+00  time: 8.29e-01\n",
      "epoch: 25   trainLoss: 3.1110e+00   valLoss:2.9938e+00  time: 8.35e-01\n",
      "epoch: 26   trainLoss: 2.7266e+00   valLoss:2.3840e+00  time: 8.20e-01\n",
      "epoch: 27   trainLoss: 2.4009e+00   valLoss:2.6688e+00  time: 7.91e-01\n",
      "epoch: 28   trainLoss: 2.1288e+00   valLoss:2.9500e+00  time: 7.86e-01\n",
      "epoch: 29   trainLoss: 1.9027e+00   valLoss:5.3887e+00  time: 7.79e-01\n",
      "epoch: 30   trainLoss: 1.7724e+00   valLoss:5.2630e+00  time: 7.84e-01\n",
      "epoch: 31   trainLoss: 1.7529e+00   valLoss:7.1789e+00  time: 8.19e-01\n",
      "epoch: 32   trainLoss: 1.7517e+00   valLoss:4.2965e+00  time: 8.25e-01\n",
      "epoch: 33   trainLoss: 1.4744e+00   valLoss:6.0515e+00  time: 7.80e-01\n",
      "epoch: 34   trainLoss: 1.1867e+00   valLoss:5.4384e+00  time: 7.76e-01\n",
      "epoch: 35   trainLoss: 1.2832e+00   valLoss:4.4616e+00  time: 7.73e-01\n",
      "epoch: 36   trainLoss: 9.8771e-01   valLoss:4.9286e+00  time: 7.96e-01\n",
      "epoch: 37   trainLoss: 1.0411e+00   valLoss:4.8031e+00  time: 8.14e-01\n",
      "epoch: 38   trainLoss: 9.2853e-01   valLoss:4.8542e+00  time: 8.21e-01\n",
      "epoch: 39   trainLoss: 9.2187e-01   valLoss:4.1420e+00  time: 8.26e-01\n",
      "epoch: 40   trainLoss: 8.4143e-01   valLoss:2.6357e+00  time: 8.35e-01\n",
      "epoch: 41   trainLoss: 7.9630e-01   valLoss:3.1772e+00  time: 7.83e-01\n",
      "epoch: 42   trainLoss: 7.9215e-01   valLoss:3.9796e+00  time: 7.78e-01\n",
      "epoch: 43   trainLoss: 7.6946e-01   valLoss:4.9053e+00  time: 7.84e-01\n",
      "epoch: 44   trainLoss: 6.9255e-01   valLoss:6.3128e+00  time: 7.82e-01\n",
      "epoch: 45   trainLoss: 6.1888e-01   valLoss:5.8345e+00  time: 7.76e-01\n",
      "epoch: 46   trainLoss: 6.1096e-01   valLoss:5.9821e+00  time: 7.83e-01\n",
      "epoch: 47   trainLoss: 5.9478e-01   valLoss:6.1381e+00  time: 8.05e-01\n",
      "epoch: 48   trainLoss: 5.4623e-01   valLoss:5.1205e+00  time: 8.28e-01\n",
      "epoch: 49   trainLoss: 4.8653e-01   valLoss:4.6391e+00  time: 7.83e-01\n",
      "epoch: 50   trainLoss: 4.5172e-01   valLoss:4.6401e+00  time: 7.75e-01\n",
      "epoch: 51   trainLoss: 4.7216e-01   valLoss:5.3952e+00  time: 7.83e-01\n",
      "epoch: 52   trainLoss: 4.1231e-01   valLoss:5.2798e+00  time: 7.83e-01\n",
      "epoch: 53   trainLoss: 3.9004e-01   valLoss:5.7560e+00  time: 8.17e-01\n",
      "epoch: 54   trainLoss: 3.5685e-01   valLoss:7.0589e+00  time: 7.74e-01\n",
      "epoch: 55   trainLoss: 3.5196e-01   valLoss:7.2383e+00  time: 7.78e-01\n",
      "epoch: 56   trainLoss: 3.4589e-01   valLoss:1.0640e+01  time: 7.75e-01\n",
      "epoch: 57   trainLoss: 3.1444e-01   valLoss:1.2037e+01  time: 7.81e-01\n",
      "epoch: 58   trainLoss: 2.8951e-01   valLoss:1.1151e+01  time: 7.78e-01\n",
      "epoch: 59   trainLoss: 2.8265e-01   valLoss:1.1316e+01  time: 8.38e-01\n",
      "epoch: 60   trainLoss: 2.7575e-01   valLoss:7.4113e+00  time: 7.93e-01\n",
      "epoch: 61   trainLoss: 2.5184e-01   valLoss:4.9645e+00  time: 8.23e-01\n",
      "epoch: 62   trainLoss: 2.3731e-01   valLoss:3.5262e+00  time: 7.84e-01\n",
      "epoch: 63   trainLoss: 2.2913e-01   valLoss:2.1749e+00  time: 7.80e-01\n",
      "epoch: 64   trainLoss: 2.1789e-01   valLoss:2.0560e+00  time: 7.82e-01\n",
      "epoch: 65   trainLoss: 2.1159e-01   valLoss:1.7198e+00  time: 7.75e-01\n",
      "epoch: 66   trainLoss: 1.9921e-01   valLoss:2.0510e+00  time: 7.81e-01\n",
      "epoch: 67   trainLoss: 1.8306e-01   valLoss:2.1828e+00  time: 8.34e-01\n",
      "epoch: 68   trainLoss: 1.7568e-01   valLoss:1.7121e+00  time: 7.78e-01\n",
      "epoch: 69   trainLoss: 1.7454e-01   valLoss:9.6432e-01  time: 7.80e-01\n",
      "epoch: 70   trainLoss: 1.7573e-01   valLoss:9.7065e-01  time: 9.04e-01\n",
      "epoch: 71   trainLoss: 1.7887e-01   valLoss:6.2934e-01  time: 9.39e-01\n",
      "epoch: 72   trainLoss: 2.0238e-01   valLoss:1.3051e+00  time: 9.43e-01\n",
      "epoch: 73   trainLoss: 2.4391e-01   valLoss:2.2792e+00  time: 9.39e-01\n",
      "epoch: 74   trainLoss: 2.9389e-01   valLoss:2.7584e+00  time: 9.40e-01\n",
      "epoch: 75   trainLoss: 2.7015e-01   valLoss:2.3997e+00  time: 9.40e-01\n",
      "epoch: 76   trainLoss: 1.7012e-01   valLoss:1.9426e+00  time: 8.94e-01\n",
      "epoch: 77   trainLoss: 1.3527e-01   valLoss:1.6261e+00  time: 8.29e-01\n",
      "epoch: 78   trainLoss: 1.9405e-01   valLoss:1.7070e+00  time: 9.02e-01\n",
      "epoch: 79   trainLoss: 1.5563e-01   valLoss:1.6229e+00  time: 8.45e-01\n",
      "epoch: 80   trainLoss: 1.0383e-01   valLoss:1.6723e+00  time: 9.44e-01\n",
      "epoch: 81   trainLoss: 1.4357e-01   valLoss:1.2498e+00  time: 9.24e-01\n",
      "epoch: 82   trainLoss: 1.2257e-01   valLoss:9.7129e-01  time: 8.31e-01\n",
      "epoch: 83   trainLoss: 1.0172e-01   valLoss:8.5830e-01  time: 8.08e-01\n",
      "epoch: 84   trainLoss: 1.1660e-01   valLoss:7.9186e-01  time: 7.96e-01\n",
      "epoch: 85   trainLoss: 8.7473e-02   valLoss:5.7777e-01  time: 8.51e-01\n",
      "epoch: 86   trainLoss: 8.6363e-02   valLoss:5.3908e-01  time: 9.21e-01\n",
      "epoch: 87   trainLoss: 9.7544e-02   valLoss:5.1477e-01  time: 8.07e-01\n",
      "epoch: 88   trainLoss: 7.3315e-02   valLoss:3.6568e-01  time: 7.90e-01\n",
      "epoch: 89   trainLoss: 8.0043e-02   valLoss:3.7689e-01  time: 8.98e-01\n",
      "epoch: 90   trainLoss: 7.3042e-02   valLoss:3.0785e-01  time: 8.55e-01\n",
      "epoch: 91   trainLoss: 5.9210e-02   valLoss:2.8409e-01  time: 9.15e-01\n",
      "epoch: 92   trainLoss: 6.8862e-02   valLoss:4.0798e-01  time: 9.22e-01\n",
      "epoch: 93   trainLoss: 5.9851e-02   valLoss:5.4656e-01  time: 8.69e-01\n",
      "epoch: 94   trainLoss: 5.5700e-02   valLoss:7.6401e-01  time: 8.24e-01\n",
      "epoch: 95   trainLoss: 5.9020e-02   valLoss:1.0981e+00  time: 8.14e-01\n",
      "epoch: 96   trainLoss: 4.6932e-02   valLoss:1.4487e+00  time: 8.70e-01\n",
      "epoch: 97   trainLoss: 4.5709e-02   valLoss:1.4392e+00  time: 8.48e-01\n",
      "epoch: 98   trainLoss: 4.7124e-02   valLoss:1.5071e+00  time: 8.83e-01\n",
      "epoch: 99   trainLoss: 3.7815e-02   valLoss:1.5367e+00  time: 8.71e-01\n",
      "loading checkpoint 91\n",
      "trained 30 random forest models in 4.71 seconds\n",
      "loaded train set of size 9\n",
      "epoch: 0   trainLoss: 1.0296e+00   valLoss:9.5793e-01  time: 9.80e-02\n",
      "epoch: 1   trainLoss: 8.7156e-01   valLoss:9.4888e-01  time: 9.95e-02\n",
      "epoch: 2   trainLoss: 7.6800e-01   valLoss:9.3267e-01  time: 9.80e-02\n",
      "epoch: 3   trainLoss: 6.9275e-01   valLoss:9.1416e-01  time: 9.15e-02\n",
      "epoch: 4   trainLoss: 5.7034e-01   valLoss:8.9858e-01  time: 9.04e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5   trainLoss: 5.1409e-01   valLoss:8.9443e-01  time: 9.26e-02\n",
      "epoch: 6   trainLoss: 4.3418e-01   valLoss:9.0324e-01  time: 9.07e-02\n",
      "epoch: 7   trainLoss: 3.6685e-01   valLoss:9.3477e-01  time: 9.07e-02\n",
      "epoch: 8   trainLoss: 3.1100e-01   valLoss:9.9324e-01  time: 9.13e-02\n",
      "epoch: 9   trainLoss: 2.5517e-01   valLoss:1.0698e+00  time: 9.10e-02\n",
      "epoch: 10   trainLoss: 2.0834e-01   valLoss:1.1494e+00  time: 9.42e-02\n",
      "epoch: 11   trainLoss: 1.6816e-01   valLoss:1.2101e+00  time: 9.10e-02\n",
      "epoch: 12   trainLoss: 1.4251e-01   valLoss:1.2566e+00  time: 9.05e-02\n",
      "epoch: 13   trainLoss: 1.1678e-01   valLoss:1.2815e+00  time: 9.06e-02\n",
      "epoch: 14   trainLoss: 1.0292e-01   valLoss:1.2923e+00  time: 1.04e-01\n",
      "epoch: 15   trainLoss: 9.4504e-02   valLoss:1.2885e+00  time: 9.39e-02\n",
      "epoch: 16   trainLoss: 8.5036e-02   valLoss:1.2654e+00  time: 9.17e-02\n",
      "epoch: 17   trainLoss: 7.7361e-02   valLoss:1.2122e+00  time: 9.23e-02\n",
      "epoch: 18   trainLoss: 6.7536e-02   valLoss:1.1434e+00  time: 9.13e-02\n",
      "epoch: 19   trainLoss: 5.7978e-02   valLoss:1.0740e+00  time: 9.07e-02\n",
      "epoch: 20   trainLoss: 4.8201e-02   valLoss:1.0098e+00  time: 9.06e-02\n",
      "epoch: 21   trainLoss: 4.0263e-02   valLoss:9.3616e-01  time: 1.07e-01\n",
      "epoch: 22   trainLoss: 3.4237e-02   valLoss:8.5264e-01  time: 1.06e-01\n",
      "epoch: 23   trainLoss: 3.0396e-02   valLoss:7.7442e-01  time: 1.07e-01\n",
      "epoch: 24   trainLoss: 2.6101e-02   valLoss:6.8423e-01  time: 1.07e-01\n",
      "epoch: 25   trainLoss: 2.3263e-02   valLoss:5.9539e-01  time: 1.08e-01\n",
      "epoch: 26   trainLoss: 1.9931e-02   valLoss:5.0212e-01  time: 1.07e-01\n",
      "epoch: 27   trainLoss: 1.6892e-02   valLoss:4.2053e-01  time: 1.06e-01\n",
      "epoch: 28   trainLoss: 1.4776e-02   valLoss:3.7774e-01  time: 1.08e-01\n",
      "epoch: 29   trainLoss: 1.2275e-02   valLoss:3.5147e-01  time: 1.09e-01\n",
      "epoch: 30   trainLoss: 1.0400e-02   valLoss:3.3495e-01  time: 1.06e-01\n",
      "epoch: 31   trainLoss: 8.5061e-03   valLoss:3.2037e-01  time: 1.08e-01\n",
      "epoch: 32   trainLoss: 7.6129e-03   valLoss:3.0809e-01  time: 1.07e-01\n",
      "epoch: 33   trainLoss: 6.6207e-03   valLoss:2.9640e-01  time: 1.06e-01\n",
      "epoch: 34   trainLoss: 6.4364e-03   valLoss:2.8163e-01  time: 1.07e-01\n",
      "epoch: 35   trainLoss: 5.1132e-03   valLoss:2.7227e-01  time: 1.06e-01\n",
      "epoch: 36   trainLoss: 4.3288e-03   valLoss:2.6789e-01  time: 1.19e-01\n",
      "epoch: 37   trainLoss: 4.3839e-03   valLoss:2.6272e-01  time: 9.64e-02\n",
      "epoch: 38   trainLoss: 3.7355e-03   valLoss:2.6195e-01  time: 9.66e-02\n",
      "epoch: 39   trainLoss: 3.2807e-03   valLoss:2.5298e-01  time: 9.93e-02\n",
      "epoch: 40   trainLoss: 2.6998e-03   valLoss:2.5249e-01  time: 9.79e-02\n",
      "epoch: 41   trainLoss: 2.1640e-03   valLoss:2.4591e-01  time: 9.76e-02\n",
      "epoch: 42   trainLoss: 2.2944e-03   valLoss:2.4180e-01  time: 9.93e-02\n",
      "epoch: 43   trainLoss: 1.9686e-03   valLoss:2.3852e-01  time: 9.74e-02\n",
      "epoch: 44   trainLoss: 1.4961e-03   valLoss:2.3312e-01  time: 9.84e-02\n",
      "epoch: 45   trainLoss: 1.4281e-03   valLoss:2.3334e-01  time: 1.11e-01\n",
      "epoch: 46   trainLoss: 1.3292e-03   valLoss:2.3572e-01  time: 1.08e-01\n",
      "epoch: 47   trainLoss: 1.1147e-03   valLoss:2.3489e-01  time: 1.09e-01\n",
      "epoch: 48   trainLoss: 1.0413e-03   valLoss:2.3767e-01  time: 1.08e-01\n",
      "epoch: 49   trainLoss: 8.9342e-04   valLoss:2.3665e-01  time: 1.06e-01\n",
      "epoch: 50   trainLoss: 7.6784e-04   valLoss:2.3519e-01  time: 1.13e-01\n",
      "epoch: 51   trainLoss: 7.1762e-04   valLoss:2.3694e-01  time: 1.13e-01\n",
      "epoch: 52   trainLoss: 6.6233e-04   valLoss:2.3465e-01  time: 1.11e-01\n",
      "epoch: 53   trainLoss: 4.9803e-04   valLoss:2.3394e-01  time: 1.07e-01\n",
      "epoch: 54   trainLoss: 4.6207e-04   valLoss:2.3539e-01  time: 1.06e-01\n",
      "epoch: 55   trainLoss: 4.5916e-04   valLoss:2.3160e-01  time: 1.16e-01\n",
      "epoch: 56   trainLoss: 3.6941e-04   valLoss:2.2980e-01  time: 1.01e-01\n",
      "epoch: 57   trainLoss: 3.4198e-04   valLoss:2.2841e-01  time: 9.59e-02\n",
      "epoch: 58   trainLoss: 3.2203e-04   valLoss:2.2509e-01  time: 9.70e-02\n",
      "epoch: 59   trainLoss: 2.9740e-04   valLoss:2.2254e-01  time: 9.64e-02\n",
      "epoch: 60   trainLoss: 2.7885e-04   valLoss:2.2042e-01  time: 9.84e-02\n",
      "epoch: 61   trainLoss: 2.9389e-04   valLoss:2.2198e-01  time: 1.06e-01\n",
      "epoch: 62   trainLoss: 3.6922e-04   valLoss:2.1389e-01  time: 1.07e-01\n",
      "epoch: 63   trainLoss: 6.3067e-04   valLoss:2.1815e-01  time: 1.07e-01\n",
      "epoch: 64   trainLoss: 1.0417e-03   valLoss:2.1143e-01  time: 1.18e-01\n",
      "epoch: 65   trainLoss: 1.9561e-03   valLoss:2.1773e-01  time: 1.10e-01\n",
      "epoch: 66   trainLoss: 2.7069e-03   valLoss:1.9722e-01  time: 9.84e-02\n",
      "epoch: 67   trainLoss: 2.2799e-03   valLoss:1.9130e-01  time: 9.71e-02\n",
      "epoch: 68   trainLoss: 5.0797e-04   valLoss:1.8866e-01  time: 9.81e-02\n",
      "epoch: 69   trainLoss: 1.1639e-03   valLoss:1.8351e-01  time: 9.69e-02\n",
      "epoch: 70   trainLoss: 1.1450e-03   valLoss:1.7602e-01  time: 9.76e-02\n",
      "epoch: 71   trainLoss: 4.9137e-04   valLoss:1.7167e-01  time: 9.90e-02\n",
      "epoch: 72   trainLoss: 1.0983e-03   valLoss:1.7037e-01  time: 9.93e-02\n",
      "epoch: 73   trainLoss: 3.9451e-04   valLoss:1.6783e-01  time: 1.07e-01\n",
      "epoch: 74   trainLoss: 7.1570e-04   valLoss:1.6629e-01  time: 1.05e-01\n",
      "epoch: 75   trainLoss: 4.6036e-04   valLoss:1.6610e-01  time: 9.62e-02\n",
      "epoch: 76   trainLoss: 4.1099e-04   valLoss:1.6277e-01  time: 9.82e-02\n",
      "epoch: 77   trainLoss: 5.0652e-04   valLoss:1.6095e-01  time: 9.67e-02\n",
      "epoch: 78   trainLoss: 3.2186e-04   valLoss:1.6354e-01  time: 9.70e-02\n",
      "epoch: 79   trainLoss: 4.4491e-04   valLoss:1.6068e-01  time: 9.54e-02\n",
      "epoch: 80   trainLoss: 2.0692e-04   valLoss:1.5728e-01  time: 9.82e-02\n",
      "epoch: 81   trainLoss: 3.8097e-04   valLoss:1.6050e-01  time: 9.76e-02\n",
      "epoch: 82   trainLoss: 1.8954e-04   valLoss:1.6091e-01  time: 9.68e-02\n",
      "epoch: 83   trainLoss: 2.8848e-04   valLoss:1.5761e-01  time: 9.26e-02\n",
      "epoch: 84   trainLoss: 2.0036e-04   valLoss:1.5842e-01  time: 9.18e-02\n",
      "epoch: 85   trainLoss: 1.8031e-04   valLoss:1.5770e-01  time: 9.28e-02\n",
      "epoch: 86   trainLoss: 1.8257e-04   valLoss:1.5639e-01  time: 9.16e-02\n",
      "epoch: 87   trainLoss: 1.1536e-04   valLoss:1.5731e-01  time: 9.21e-02\n",
      "epoch: 88   trainLoss: 2.1371e-04   valLoss:1.5579e-01  time: 9.39e-02\n",
      "epoch: 89   trainLoss: 1.1081e-04   valLoss:1.5688e-01  time: 9.18e-02\n",
      "epoch: 90   trainLoss: 1.4047e-04   valLoss:1.5726e-01  time: 9.57e-02\n",
      "epoch: 91   trainLoss: 1.7645e-04   valLoss:1.5648e-01  time: 9.09e-02\n",
      "epoch: 92   trainLoss: 2.3233e-04   valLoss:1.5906e-01  time: 9.20e-02\n",
      "epoch: 93   trainLoss: 5.9941e-04   valLoss:1.5383e-01  time: 9.17e-02\n",
      "epoch: 94   trainLoss: 1.6617e-03   valLoss:1.6011e-01  time: 9.41e-02\n",
      "epoch: 95   trainLoss: 5.1834e-03   valLoss:1.5475e-01  time: 1.01e-01\n",
      "epoch: 96   trainLoss: 1.1572e-02   valLoss:1.4396e-01  time: 9.11e-02\n",
      "epoch: 97   trainLoss: 7.6573e-03   valLoss:1.3429e-01  time: 1.08e-01\n",
      "epoch: 98   trainLoss: 4.0915e-03   valLoss:1.2254e-01  time: 1.06e-01\n",
      "epoch: 99   trainLoss: 5.0885e-03   valLoss:1.1962e-01  time: 1.07e-01\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.3987e+01   valLoss:2.2793e+01  time: 1.11e-01\n",
      "epoch: 1   trainLoss: 2.8199e+01   valLoss:2.1468e+01  time: 1.08e-01\n",
      "epoch: 2   trainLoss: 2.5718e+01   valLoss:2.0077e+01  time: 1.07e-01\n",
      "epoch: 3   trainLoss: 2.3947e+01   valLoss:1.9742e+01  time: 1.11e-01\n",
      "epoch: 4   trainLoss: 2.2362e+01   valLoss:2.0235e+01  time: 1.08e-01\n",
      "epoch: 5   trainLoss: 2.1194e+01   valLoss:2.0037e+01  time: 1.08e-01\n",
      "epoch: 6   trainLoss: 1.9991e+01   valLoss:1.8447e+01  time: 1.07e-01\n",
      "epoch: 7   trainLoss: 1.8759e+01   valLoss:1.7635e+01  time: 1.10e-01\n",
      "epoch: 8   trainLoss: 1.7626e+01   valLoss:1.7202e+01  time: 1.12e-01\n",
      "epoch: 9   trainLoss: 1.6490e+01   valLoss:1.7027e+01  time: 9.62e-02\n",
      "epoch: 10   trainLoss: 1.5410e+01   valLoss:1.6335e+01  time: 9.66e-02\n",
      "epoch: 11   trainLoss: 1.4336e+01   valLoss:1.5498e+01  time: 9.98e-02\n",
      "epoch: 12   trainLoss: 1.3309e+01   valLoss:1.5618e+01  time: 9.63e-02\n",
      "epoch: 13   trainLoss: 1.2307e+01   valLoss:1.4366e+01  time: 9.67e-02\n",
      "epoch: 14   trainLoss: 1.1508e+01   valLoss:1.2953e+01  time: 9.78e-02\n",
      "epoch: 15   trainLoss: 1.0964e+01   valLoss:1.4307e+01  time: 1.01e-01\n",
      "epoch: 16   trainLoss: 9.5519e+00   valLoss:1.3587e+01  time: 9.67e-02\n",
      "epoch: 17   trainLoss: 8.8008e+00   valLoss:1.2272e+01  time: 9.65e-02\n",
      "epoch: 18   trainLoss: 7.9281e+00   valLoss:1.2452e+01  time: 1.01e-01\n",
      "epoch: 19   trainLoss: 7.0300e+00   valLoss:1.3820e+01  time: 9.64e-02\n",
      "epoch: 20   trainLoss: 6.2943e+00   valLoss:1.1883e+01  time: 9.86e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21   trainLoss: 5.4836e+00   valLoss:9.3357e+00  time: 9.85e-02\n",
      "epoch: 22   trainLoss: 4.8622e+00   valLoss:8.2721e+00  time: 9.70e-02\n",
      "epoch: 23   trainLoss: 4.2418e+00   valLoss:7.8896e+00  time: 9.74e-02\n",
      "epoch: 24   trainLoss: 3.7068e+00   valLoss:6.8064e+00  time: 9.99e-02\n",
      "epoch: 25   trainLoss: 3.2189e+00   valLoss:4.9660e+00  time: 9.90e-02\n",
      "epoch: 26   trainLoss: 2.8331e+00   valLoss:3.4253e+00  time: 9.89e-02\n",
      "epoch: 27   trainLoss: 2.4865e+00   valLoss:2.7233e+00  time: 9.72e-02\n",
      "epoch: 28   trainLoss: 2.2225e+00   valLoss:2.5711e+00  time: 9.68e-02\n",
      "epoch: 29   trainLoss: 1.9661e+00   valLoss:2.3534e+00  time: 1.08e-01\n",
      "epoch: 30   trainLoss: 1.7831e+00   valLoss:1.8361e+00  time: 1.16e-01\n",
      "epoch: 31   trainLoss: 1.5725e+00   valLoss:1.4894e+00  time: 1.07e-01\n",
      "epoch: 32   trainLoss: 1.4272e+00   valLoss:1.4925e+00  time: 1.07e-01\n",
      "epoch: 33   trainLoss: 1.3120e+00   valLoss:1.4610e+00  time: 1.06e-01\n",
      "epoch: 34   trainLoss: 1.2399e+00   valLoss:1.4621e+00  time: 1.07e-01\n",
      "epoch: 35   trainLoss: 1.1770e+00   valLoss:1.3131e+00  time: 1.09e-01\n",
      "epoch: 36   trainLoss: 1.0918e+00   valLoss:1.2039e+00  time: 1.08e-01\n",
      "epoch: 37   trainLoss: 9.8702e-01   valLoss:1.1784e+00  time: 1.07e-01\n",
      "epoch: 38   trainLoss: 9.5675e-01   valLoss:9.6828e-01  time: 1.15e-01\n",
      "epoch: 39   trainLoss: 8.9218e-01   valLoss:1.2700e+00  time: 1.08e-01\n",
      "epoch: 40   trainLoss: 9.2049e-01   valLoss:1.2021e+00  time: 1.07e-01\n",
      "epoch: 41   trainLoss: 9.9484e-01   valLoss:1.4824e+00  time: 1.07e-01\n",
      "epoch: 42   trainLoss: 1.0571e+00   valLoss:9.8384e-01  time: 1.09e-01\n",
      "epoch: 43   trainLoss: 7.9307e-01   valLoss:8.0842e-01  time: 1.11e-01\n",
      "epoch: 44   trainLoss: 6.9251e-01   valLoss:9.1970e-01  time: 1.09e-01\n",
      "epoch: 45   trainLoss: 7.4465e-01   valLoss:9.9565e-01  time: 1.07e-01\n",
      "epoch: 46   trainLoss: 6.1124e-01   valLoss:1.1268e+00  time: 1.06e-01\n",
      "epoch: 47   trainLoss: 6.6197e-01   valLoss:8.1745e-01  time: 1.16e-01\n",
      "epoch: 48   trainLoss: 5.4895e-01   valLoss:8.3038e-01  time: 1.08e-01\n",
      "epoch: 49   trainLoss: 5.7694e-01   valLoss:7.4260e-01  time: 1.09e-01\n",
      "epoch: 50   trainLoss: 4.9117e-01   valLoss:7.5577e-01  time: 1.09e-01\n",
      "epoch: 51   trainLoss: 5.0817e-01   valLoss:7.1419e-01  time: 1.08e-01\n",
      "epoch: 52   trainLoss: 4.3436e-01   valLoss:7.9636e-01  time: 1.10e-01\n",
      "epoch: 53   trainLoss: 4.4535e-01   valLoss:7.7267e-01  time: 1.07e-01\n",
      "epoch: 54   trainLoss: 3.9595e-01   valLoss:7.7976e-01  time: 1.09e-01\n",
      "epoch: 55   trainLoss: 4.0034e-01   valLoss:8.3607e-01  time: 9.93e-02\n",
      "epoch: 56   trainLoss: 3.7376e-01   valLoss:8.5784e-01  time: 9.73e-02\n",
      "epoch: 57   trainLoss: 3.6456e-01   valLoss:6.9932e-01  time: 9.92e-02\n",
      "epoch: 58   trainLoss: 3.3615e-01   valLoss:5.5041e-01  time: 1.09e-01\n",
      "epoch: 59   trainLoss: 3.2270e-01   valLoss:4.4298e-01  time: 1.09e-01\n",
      "epoch: 60   trainLoss: 3.0154e-01   valLoss:4.0709e-01  time: 1.09e-01\n",
      "epoch: 61   trainLoss: 2.8669e-01   valLoss:3.4640e-01  time: 1.08e-01\n",
      "epoch: 62   trainLoss: 2.7128e-01   valLoss:3.0608e-01  time: 1.09e-01\n",
      "epoch: 63   trainLoss: 2.5464e-01   valLoss:2.7769e-01  time: 1.08e-01\n",
      "epoch: 64   trainLoss: 2.4631e-01   valLoss:2.6455e-01  time: 1.08e-01\n",
      "epoch: 65   trainLoss: 2.2778e-01   valLoss:2.6017e-01  time: 1.08e-01\n",
      "epoch: 66   trainLoss: 2.1773e-01   valLoss:2.3421e-01  time: 1.16e-01\n",
      "epoch: 67   trainLoss: 2.0368e-01   valLoss:2.3023e-01  time: 1.07e-01\n",
      "epoch: 68   trainLoss: 1.9418e-01   valLoss:2.4268e-01  time: 1.13e-01\n",
      "epoch: 69   trainLoss: 1.8120e-01   valLoss:2.6364e-01  time: 1.08e-01\n",
      "epoch: 70   trainLoss: 1.7475e-01   valLoss:2.1131e-01  time: 1.10e-01\n",
      "epoch: 71   trainLoss: 1.6061e-01   valLoss:2.0261e-01  time: 1.07e-01\n",
      "epoch: 72   trainLoss: 1.5609e-01   valLoss:1.8866e-01  time: 1.07e-01\n",
      "epoch: 73   trainLoss: 1.4481e-01   valLoss:1.8305e-01  time: 1.07e-01\n",
      "epoch: 74   trainLoss: 1.4132e-01   valLoss:1.7972e-01  time: 1.07e-01\n",
      "epoch: 75   trainLoss: 1.3329e-01   valLoss:1.8416e-01  time: 1.09e-01\n",
      "epoch: 76   trainLoss: 1.2904e-01   valLoss:1.8717e-01  time: 9.70e-02\n",
      "epoch: 77   trainLoss: 1.2446e-01   valLoss:1.9407e-01  time: 9.64e-02\n",
      "epoch: 78   trainLoss: 1.2036e-01   valLoss:1.9346e-01  time: 1.06e-01\n",
      "epoch: 79   trainLoss: 1.1698e-01   valLoss:1.9751e-01  time: 1.06e-01\n",
      "epoch: 80   trainLoss: 1.1321e-01   valLoss:2.2644e-01  time: 1.14e-01\n",
      "epoch: 81   trainLoss: 1.0978e-01   valLoss:2.8857e-01  time: 1.02e-01\n",
      "epoch: 82   trainLoss: 1.0675e-01   valLoss:2.9544e-01  time: 9.70e-02\n",
      "epoch: 83   trainLoss: 1.0315e-01   valLoss:2.7544e-01  time: 1.06e-01\n",
      "epoch: 84   trainLoss: 1.0061e-01   valLoss:3.1055e-01  time: 1.12e-01\n",
      "epoch: 85   trainLoss: 9.6788e-02   valLoss:3.6143e-01  time: 9.96e-02\n",
      "epoch: 86   trainLoss: 9.4587e-02   valLoss:3.4161e-01  time: 9.23e-02\n",
      "epoch: 87   trainLoss: 9.1060e-02   valLoss:3.2149e-01  time: 9.17e-02\n",
      "epoch: 88   trainLoss: 8.8968e-02   valLoss:3.5597e-01  time: 9.13e-02\n",
      "epoch: 89   trainLoss: 8.5894e-02   valLoss:4.1168e-01  time: 9.11e-02\n",
      "epoch: 90   trainLoss: 8.3263e-02   valLoss:4.0359e-01  time: 9.53e-02\n",
      "epoch: 91   trainLoss: 8.0386e-02   valLoss:3.8422e-01  time: 9.17e-02\n",
      "epoch: 92   trainLoss: 7.7832e-02   valLoss:4.1251e-01  time: 9.23e-02\n",
      "epoch: 93   trainLoss: 7.4765e-02   valLoss:4.3437e-01  time: 9.20e-02\n",
      "epoch: 94   trainLoss: 7.2052e-02   valLoss:3.9688e-01  time: 9.19e-02\n",
      "epoch: 95   trainLoss: 6.9518e-02   valLoss:3.9954e-01  time: 9.38e-02\n",
      "epoch: 96   trainLoss: 6.7357e-02   valLoss:3.6663e-01  time: 9.34e-02\n",
      "epoch: 97   trainLoss: 6.5442e-02   valLoss:3.5285e-01  time: 9.17e-02\n",
      "epoch: 98   trainLoss: 6.3702e-02   valLoss:3.5692e-01  time: 9.21e-02\n",
      "epoch: 99   trainLoss: 6.1527e-02   valLoss:3.6541e-01  time: 9.25e-02\n",
      "loading checkpoint 74\n",
      "trained 30 random forest models in 3.69 seconds\n",
      "loaded train set of size 19\n",
      "epoch: 0   trainLoss: 1.0804e+00   valLoss:1.0445e+00  time: 1.74e-01\n",
      "epoch: 1   trainLoss: 9.0109e-01   valLoss:1.0208e+00  time: 1.75e-01\n",
      "epoch: 2   trainLoss: 8.2207e-01   valLoss:9.9355e-01  time: 1.83e-01\n",
      "epoch: 3   trainLoss: 7.0877e-01   valLoss:9.5946e-01  time: 1.74e-01\n",
      "epoch: 4   trainLoss: 6.0756e-01   valLoss:9.1675e-01  time: 1.80e-01\n",
      "epoch: 5   trainLoss: 5.1650e-01   valLoss:8.8201e-01  time: 1.75e-01\n",
      "epoch: 6   trainLoss: 4.4294e-01   valLoss:8.7844e-01  time: 1.71e-01\n",
      "epoch: 7   trainLoss: 3.6763e-01   valLoss:9.2853e-01  time: 1.71e-01\n",
      "epoch: 8   trainLoss: 2.9836e-01   valLoss:1.0401e+00  time: 1.69e-01\n",
      "epoch: 9   trainLoss: 2.4493e-01   valLoss:1.1875e+00  time: 1.69e-01\n",
      "epoch: 10   trainLoss: 2.0355e-01   valLoss:1.2928e+00  time: 1.73e-01\n",
      "epoch: 11   trainLoss: 1.6658e-01   valLoss:1.3341e+00  time: 1.70e-01\n",
      "epoch: 12   trainLoss: 1.4418e-01   valLoss:1.3626e+00  time: 1.71e-01\n",
      "epoch: 13   trainLoss: 1.1684e-01   valLoss:1.4150e+00  time: 1.71e-01\n",
      "epoch: 14   trainLoss: 9.6157e-02   valLoss:1.5143e+00  time: 1.70e-01\n",
      "epoch: 15   trainLoss: 7.9344e-02   valLoss:1.6840e+00  time: 1.69e-01\n",
      "epoch: 16   trainLoss: 6.5393e-02   valLoss:1.8559e+00  time: 1.71e-01\n",
      "epoch: 17   trainLoss: 5.3535e-02   valLoss:1.9757e+00  time: 1.69e-01\n",
      "epoch: 18   trainLoss: 4.6376e-02   valLoss:2.0340e+00  time: 1.72e-01\n",
      "epoch: 19   trainLoss: 3.8507e-02   valLoss:1.9920e+00  time: 1.70e-01\n",
      "epoch: 20   trainLoss: 3.3222e-02   valLoss:1.9522e+00  time: 1.71e-01\n",
      "epoch: 21   trainLoss: 2.9673e-02   valLoss:1.8956e+00  time: 1.70e-01\n",
      "epoch: 22   trainLoss: 2.3615e-02   valLoss:1.9341e+00  time: 1.71e-01\n",
      "epoch: 23   trainLoss: 2.0688e-02   valLoss:1.8464e+00  time: 1.69e-01\n",
      "epoch: 24   trainLoss: 1.8557e-02   valLoss:1.8103e+00  time: 1.69e-01\n",
      "epoch: 25   trainLoss: 1.7833e-02   valLoss:1.5065e+00  time: 1.70e-01\n",
      "epoch: 26   trainLoss: 2.3497e-02   valLoss:1.6000e+00  time: 1.68e-01\n",
      "epoch: 27   trainLoss: 2.7447e-02   valLoss:1.2385e+00  time: 1.69e-01\n",
      "epoch: 28   trainLoss: 1.3703e-02   valLoss:9.8323e-01  time: 1.82e-01\n",
      "epoch: 29   trainLoss: 1.4935e-02   valLoss:8.2713e-01  time: 1.82e-01\n",
      "epoch: 30   trainLoss: 1.2071e-02   valLoss:7.1629e-01  time: 1.90e-01\n",
      "epoch: 31   trainLoss: 1.2179e-02   valLoss:6.2227e-01  time: 1.81e-01\n",
      "epoch: 32   trainLoss: 1.1975e-02   valLoss:5.7805e-01  time: 1.75e-01\n",
      "epoch: 33   trainLoss: 8.3900e-03   valLoss:5.2263e-01  time: 1.78e-01\n",
      "epoch: 34   trainLoss: 7.2942e-03   valLoss:5.0764e-01  time: 1.77e-01\n",
      "epoch: 35   trainLoss: 7.2789e-03   valLoss:4.8497e-01  time: 1.83e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36   trainLoss: 5.6699e-03   valLoss:4.6723e-01  time: 1.89e-01\n",
      "epoch: 37   trainLoss: 4.8677e-03   valLoss:4.6075e-01  time: 1.91e-01\n",
      "epoch: 38   trainLoss: 4.6337e-03   valLoss:4.2822e-01  time: 1.89e-01\n",
      "epoch: 39   trainLoss: 3.6444e-03   valLoss:3.8057e-01  time: 1.79e-01\n",
      "epoch: 40   trainLoss: 4.1609e-03   valLoss:3.6345e-01  time: 1.76e-01\n",
      "epoch: 41   trainLoss: 2.9820e-03   valLoss:3.6194e-01  time: 1.77e-01\n",
      "epoch: 42   trainLoss: 3.0594e-03   valLoss:3.4370e-01  time: 1.75e-01\n",
      "epoch: 43   trainLoss: 2.3604e-03   valLoss:3.2541e-01  time: 1.77e-01\n",
      "epoch: 44   trainLoss: 2.4305e-03   valLoss:3.1760e-01  time: 1.76e-01\n",
      "epoch: 45   trainLoss: 1.9704e-03   valLoss:2.9711e-01  time: 1.80e-01\n",
      "epoch: 46   trainLoss: 1.7449e-03   valLoss:2.6432e-01  time: 1.74e-01\n",
      "epoch: 47   trainLoss: 1.8302e-03   valLoss:2.4357e-01  time: 1.79e-01\n",
      "epoch: 48   trainLoss: 1.3993e-03   valLoss:2.3793e-01  time: 1.77e-01\n",
      "epoch: 49   trainLoss: 1.3842e-03   valLoss:2.3218e-01  time: 1.82e-01\n",
      "epoch: 50   trainLoss: 1.0802e-03   valLoss:2.2230e-01  time: 1.88e-01\n",
      "epoch: 51   trainLoss: 1.1513e-03   valLoss:2.1108e-01  time: 1.85e-01\n",
      "epoch: 52   trainLoss: 9.7439e-04   valLoss:1.9909e-01  time: 1.84e-01\n",
      "epoch: 53   trainLoss: 8.6494e-04   valLoss:1.9008e-01  time: 1.76e-01\n",
      "epoch: 54   trainLoss: 8.3362e-04   valLoss:1.8300e-01  time: 1.74e-01\n",
      "epoch: 55   trainLoss: 6.9468e-04   valLoss:1.7260e-01  time: 1.74e-01\n",
      "epoch: 56   trainLoss: 7.0374e-04   valLoss:1.6323e-01  time: 1.78e-01\n",
      "epoch: 57   trainLoss: 6.1597e-04   valLoss:1.5953e-01  time: 1.74e-01\n",
      "epoch: 58   trainLoss: 5.3852e-04   valLoss:1.5413e-01  time: 1.78e-01\n",
      "epoch: 59   trainLoss: 5.2364e-04   valLoss:1.4647e-01  time: 1.74e-01\n",
      "epoch: 60   trainLoss: 4.5333e-04   valLoss:1.4471e-01  time: 1.77e-01\n",
      "epoch: 61   trainLoss: 4.1893e-04   valLoss:1.4420e-01  time: 1.77e-01\n",
      "epoch: 62   trainLoss: 3.8817e-04   valLoss:1.3748e-01  time: 1.74e-01\n",
      "epoch: 63   trainLoss: 3.6001e-04   valLoss:1.3304e-01  time: 1.77e-01\n",
      "epoch: 64   trainLoss: 3.0507e-04   valLoss:1.3177e-01  time: 1.73e-01\n",
      "epoch: 65   trainLoss: 3.1827e-04   valLoss:1.2631e-01  time: 1.76e-01\n",
      "epoch: 66   trainLoss: 2.7937e-04   valLoss:1.2334e-01  time: 1.76e-01\n",
      "epoch: 67   trainLoss: 2.4720e-04   valLoss:1.2195e-01  time: 1.76e-01\n",
      "epoch: 68   trainLoss: 2.3314e-04   valLoss:1.1618e-01  time: 1.77e-01\n",
      "epoch: 69   trainLoss: 2.3837e-04   valLoss:1.1561e-01  time: 1.74e-01\n",
      "epoch: 70   trainLoss: 2.0915e-04   valLoss:1.1635e-01  time: 1.78e-01\n",
      "epoch: 71   trainLoss: 1.8063e-04   valLoss:1.1409e-01  time: 1.73e-01\n",
      "epoch: 72   trainLoss: 1.8932e-04   valLoss:1.1537e-01  time: 1.74e-01\n",
      "epoch: 73   trainLoss: 1.6658e-04   valLoss:1.1303e-01  time: 1.80e-01\n",
      "epoch: 74   trainLoss: 1.7674e-04   valLoss:1.1240e-01  time: 1.75e-01\n",
      "epoch: 75   trainLoss: 1.6419e-04   valLoss:1.1084e-01  time: 1.75e-01\n",
      "epoch: 76   trainLoss: 1.5216e-04   valLoss:1.0916e-01  time: 1.74e-01\n",
      "epoch: 77   trainLoss: 1.5265e-04   valLoss:1.0848e-01  time: 1.76e-01\n",
      "epoch: 78   trainLoss: 1.4081e-04   valLoss:1.1098e-01  time: 1.76e-01\n",
      "epoch: 79   trainLoss: 1.7727e-04   valLoss:1.0570e-01  time: 1.88e-01\n",
      "epoch: 80   trainLoss: 3.6763e-04   valLoss:1.1522e-01  time: 1.76e-01\n",
      "epoch: 81   trainLoss: 1.2899e-03   valLoss:9.8528e-02  time: 1.76e-01\n",
      "epoch: 82   trainLoss: 6.1024e-03   valLoss:1.6938e-01  time: 1.73e-01\n",
      "epoch: 83   trainLoss: 2.4598e-02   valLoss:1.0980e-01  time: 1.76e-01\n",
      "epoch: 84   trainLoss: 3.9382e-02   valLoss:1.2790e-01  time: 1.79e-01\n",
      "epoch: 85   trainLoss: 1.1155e-02   valLoss:1.5163e-01  time: 1.80e-01\n",
      "epoch: 86   trainLoss: 1.5450e-02   valLoss:8.3573e-02  time: 1.75e-01\n",
      "epoch: 87   trainLoss: 9.5396e-03   valLoss:7.2587e-02  time: 1.76e-01\n",
      "epoch: 88   trainLoss: 1.0691e-02   valLoss:7.5507e-02  time: 1.81e-01\n",
      "epoch: 89   trainLoss: 5.9303e-03   valLoss:9.3231e-02  time: 1.73e-01\n",
      "epoch: 90   trainLoss: 7.9261e-03   valLoss:9.1636e-02  time: 1.75e-01\n",
      "epoch: 91   trainLoss: 7.6170e-03   valLoss:7.5744e-02  time: 1.82e-01\n",
      "epoch: 92   trainLoss: 5.0051e-03   valLoss:6.6762e-02  time: 1.74e-01\n",
      "epoch: 93   trainLoss: 4.4434e-03   valLoss:6.7137e-02  time: 1.74e-01\n",
      "epoch: 94   trainLoss: 4.8930e-03   valLoss:6.6411e-02  time: 1.75e-01\n",
      "epoch: 95   trainLoss: 3.2911e-03   valLoss:6.7884e-02  time: 1.79e-01\n",
      "epoch: 96   trainLoss: 3.3958e-03   valLoss:6.6737e-02  time: 1.74e-01\n",
      "epoch: 97   trainLoss: 2.5871e-03   valLoss:5.9854e-02  time: 1.73e-01\n",
      "epoch: 98   trainLoss: 2.3779e-03   valLoss:5.8442e-02  time: 1.83e-01\n",
      "epoch: 99   trainLoss: 2.5772e-03   valLoss:5.5713e-02  time: 1.87e-01\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.8437e+01   valLoss:3.9947e+01  time: 1.84e-01\n",
      "epoch: 1   trainLoss: 3.1763e+01   valLoss:3.6739e+01  time: 1.89e-01\n",
      "epoch: 2   trainLoss: 2.9248e+01   valLoss:3.1825e+01  time: 1.85e-01\n",
      "epoch: 3   trainLoss: 2.7399e+01   valLoss:2.7100e+01  time: 1.85e-01\n",
      "epoch: 4   trainLoss: 2.5799e+01   valLoss:2.2663e+01  time: 1.84e-01\n",
      "epoch: 5   trainLoss: 2.4321e+01   valLoss:2.2632e+01  time: 1.85e-01\n",
      "epoch: 6   trainLoss: 2.2932e+01   valLoss:2.0475e+01  time: 1.87e-01\n",
      "epoch: 7   trainLoss: 2.1645e+01   valLoss:2.2492e+01  time: 1.86e-01\n",
      "epoch: 8   trainLoss: 2.0386e+01   valLoss:2.0800e+01  time: 1.84e-01\n",
      "epoch: 9   trainLoss: 1.9020e+01   valLoss:1.8711e+01  time: 1.86e-01\n",
      "epoch: 10   trainLoss: 1.7821e+01   valLoss:1.7079e+01  time: 1.86e-01\n",
      "epoch: 11   trainLoss: 1.6714e+01   valLoss:1.3340e+01  time: 1.85e-01\n",
      "epoch: 12   trainLoss: 1.5506e+01   valLoss:1.0619e+01  time: 1.84e-01\n",
      "epoch: 13   trainLoss: 1.4294e+01   valLoss:1.1303e+01  time: 1.85e-01\n",
      "epoch: 14   trainLoss: 1.3223e+01   valLoss:9.3022e+00  time: 1.83e-01\n",
      "epoch: 15   trainLoss: 1.2061e+01   valLoss:8.7969e+00  time: 1.88e-01\n",
      "epoch: 16   trainLoss: 1.0980e+01   valLoss:9.0106e+00  time: 1.76e-01\n",
      "epoch: 17   trainLoss: 1.0010e+01   valLoss:6.2962e+00  time: 1.74e-01\n",
      "epoch: 18   trainLoss: 8.9223e+00   valLoss:6.5797e+00  time: 1.80e-01\n",
      "epoch: 19   trainLoss: 7.9857e+00   valLoss:6.2835e+00  time: 1.75e-01\n",
      "epoch: 20   trainLoss: 7.0518e+00   valLoss:5.1666e+00  time: 1.75e-01\n",
      "epoch: 21   trainLoss: 6.2193e+00   valLoss:5.3435e+00  time: 1.83e-01\n",
      "epoch: 22   trainLoss: 5.4331e+00   valLoss:4.1213e+00  time: 1.74e-01\n",
      "epoch: 23   trainLoss: 4.6790e+00   valLoss:3.7377e+00  time: 1.75e-01\n",
      "epoch: 24   trainLoss: 4.1829e+00   valLoss:4.9852e+00  time: 1.77e-01\n",
      "epoch: 25   trainLoss: 3.9576e+00   valLoss:3.8942e+00  time: 1.73e-01\n",
      "epoch: 26   trainLoss: 3.4582e+00   valLoss:2.2555e+00  time: 1.77e-01\n",
      "epoch: 27   trainLoss: 2.6865e+00   valLoss:2.9594e+00  time: 1.94e-01\n",
      "epoch: 28   trainLoss: 2.4773e+00   valLoss:2.3935e+00  time: 1.75e-01\n",
      "epoch: 29   trainLoss: 2.0356e+00   valLoss:1.9406e+00  time: 1.77e-01\n",
      "epoch: 30   trainLoss: 1.7996e+00   valLoss:2.3811e+00  time: 1.74e-01\n",
      "epoch: 31   trainLoss: 1.5810e+00   valLoss:3.5696e+00  time: 1.73e-01\n",
      "epoch: 32   trainLoss: 1.4463e+00   valLoss:2.2626e+00  time: 1.76e-01\n",
      "epoch: 33   trainLoss: 1.2886e+00   valLoss:1.8753e+00  time: 1.82e-01\n",
      "epoch: 34   trainLoss: 1.0673e+00   valLoss:2.1691e+00  time: 1.77e-01\n",
      "epoch: 35   trainLoss: 1.0044e+00   valLoss:2.4944e+00  time: 1.75e-01\n",
      "epoch: 36   trainLoss: 9.1583e-01   valLoss:2.2370e+00  time: 1.76e-01\n",
      "epoch: 37   trainLoss: 8.9769e-01   valLoss:2.1015e+00  time: 1.76e-01\n",
      "epoch: 38   trainLoss: 8.1585e-01   valLoss:1.4311e+00  time: 1.75e-01\n",
      "epoch: 39   trainLoss: 8.1646e-01   valLoss:1.4613e+00  time: 1.76e-01\n",
      "epoch: 40   trainLoss: 7.3835e-01   valLoss:1.2661e+00  time: 1.73e-01\n",
      "epoch: 41   trainLoss: 7.2760e-01   valLoss:1.1594e+00  time: 1.74e-01\n",
      "epoch: 42   trainLoss: 6.7644e-01   valLoss:1.0690e+00  time: 1.79e-01\n",
      "epoch: 43   trainLoss: 6.4987e-01   valLoss:1.4493e+00  time: 1.76e-01\n",
      "epoch: 44   trainLoss: 6.0565e-01   valLoss:1.0755e+00  time: 1.75e-01\n",
      "epoch: 45   trainLoss: 5.6926e-01   valLoss:9.0566e-01  time: 1.76e-01\n",
      "epoch: 46   trainLoss: 5.3014e-01   valLoss:9.0062e-01  time: 1.75e-01\n",
      "epoch: 47   trainLoss: 4.8391e-01   valLoss:8.8686e-01  time: 1.76e-01\n",
      "epoch: 48   trainLoss: 4.3305e-01   valLoss:6.8054e-01  time: 1.75e-01\n",
      "epoch: 49   trainLoss: 3.9436e-01   valLoss:6.8308e-01  time: 1.93e-01\n",
      "epoch: 50   trainLoss: 3.6308e-01   valLoss:8.3884e-01  time: 1.84e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51   trainLoss: 3.5904e-01   valLoss:6.9845e-01  time: 1.84e-01\n",
      "epoch: 52   trainLoss: 3.4725e-01   valLoss:7.0307e-01  time: 1.88e-01\n",
      "epoch: 53   trainLoss: 3.3774e-01   valLoss:6.4768e-01  time: 1.83e-01\n",
      "epoch: 54   trainLoss: 3.0794e-01   valLoss:6.4136e-01  time: 1.85e-01\n",
      "epoch: 55   trainLoss: 2.9666e-01   valLoss:6.8533e-01  time: 1.79e-01\n",
      "epoch: 56   trainLoss: 3.0818e-01   valLoss:4.7385e-01  time: 1.79e-01\n",
      "epoch: 57   trainLoss: 2.8505e-01   valLoss:6.4952e-01  time: 1.74e-01\n",
      "epoch: 58   trainLoss: 2.6477e-01   valLoss:7.1349e-01  time: 1.75e-01\n",
      "epoch: 59   trainLoss: 2.5783e-01   valLoss:4.7538e-01  time: 1.74e-01\n",
      "epoch: 60   trainLoss: 2.6574e-01   valLoss:5.5897e-01  time: 1.75e-01\n",
      "epoch: 61   trainLoss: 3.0464e-01   valLoss:5.5957e-01  time: 1.73e-01\n",
      "epoch: 62   trainLoss: 2.6286e-01   valLoss:5.4200e-01  time: 1.85e-01\n",
      "epoch: 63   trainLoss: 2.5643e-01   valLoss:8.2482e-01  time: 1.90e-01\n",
      "epoch: 64   trainLoss: 2.6296e-01   valLoss:5.9931e-01  time: 1.87e-01\n",
      "epoch: 65   trainLoss: 2.4880e-01   valLoss:4.4025e-01  time: 1.84e-01\n",
      "epoch: 66   trainLoss: 2.4291e-01   valLoss:5.4116e-01  time: 1.86e-01\n",
      "epoch: 67   trainLoss: 2.3731e-01   valLoss:6.4177e-01  time: 1.84e-01\n",
      "epoch: 68   trainLoss: 2.1625e-01   valLoss:6.8169e-01  time: 1.89e-01\n",
      "epoch: 69   trainLoss: 2.1544e-01   valLoss:7.4007e-01  time: 1.84e-01\n",
      "epoch: 70   trainLoss: 2.0974e-01   valLoss:8.7358e-01  time: 1.87e-01\n",
      "epoch: 71   trainLoss: 2.0842e-01   valLoss:1.1908e+00  time: 1.85e-01\n",
      "epoch: 72   trainLoss: 1.9143e-01   valLoss:1.3546e+00  time: 1.83e-01\n",
      "epoch: 73   trainLoss: 1.8691e-01   valLoss:1.2101e+00  time: 1.74e-01\n",
      "epoch: 74   trainLoss: 1.8176e-01   valLoss:1.1468e+00  time: 1.77e-01\n",
      "epoch: 75   trainLoss: 1.7823e-01   valLoss:8.7814e-01  time: 1.75e-01\n",
      "epoch: 76   trainLoss: 1.7395e-01   valLoss:7.8483e-01  time: 1.73e-01\n",
      "epoch: 77   trainLoss: 1.6637e-01   valLoss:6.8340e-01  time: 1.75e-01\n",
      "epoch: 78   trainLoss: 1.5928e-01   valLoss:5.5630e-01  time: 1.75e-01\n",
      "epoch: 79   trainLoss: 1.5466e-01   valLoss:4.4792e-01  time: 1.75e-01\n",
      "epoch: 80   trainLoss: 1.5205e-01   valLoss:4.3799e-01  time: 1.77e-01\n",
      "epoch: 81   trainLoss: 1.4475e-01   valLoss:4.1106e-01  time: 1.74e-01\n",
      "epoch: 82   trainLoss: 1.4061e-01   valLoss:2.9450e-01  time: 1.78e-01\n",
      "epoch: 83   trainLoss: 1.3458e-01   valLoss:2.3295e-01  time: 1.74e-01\n",
      "epoch: 84   trainLoss: 1.2991e-01   valLoss:2.1314e-01  time: 1.80e-01\n",
      "epoch: 85   trainLoss: 1.2502e-01   valLoss:1.7851e-01  time: 1.83e-01\n",
      "epoch: 86   trainLoss: 1.1907e-01   valLoss:1.5493e-01  time: 1.82e-01\n",
      "epoch: 87   trainLoss: 1.1465e-01   valLoss:1.4115e-01  time: 2.10e-01\n",
      "epoch: 88   trainLoss: 1.1017e-01   valLoss:1.5080e-01  time: 1.79e-01\n",
      "epoch: 89   trainLoss: 1.0602e-01   valLoss:1.5349e-01  time: 1.79e-01\n",
      "epoch: 90   trainLoss: 1.0201e-01   valLoss:1.4108e-01  time: 1.75e-01\n",
      "epoch: 91   trainLoss: 9.8905e-02   valLoss:1.4318e-01  time: 1.77e-01\n",
      "epoch: 92   trainLoss: 9.6079e-02   valLoss:1.2080e-01  time: 1.77e-01\n",
      "epoch: 93   trainLoss: 9.3130e-02   valLoss:1.6398e-01  time: 1.74e-01\n",
      "epoch: 94   trainLoss: 9.3831e-02   valLoss:1.2681e-01  time: 1.74e-01\n",
      "epoch: 95   trainLoss: 9.6653e-02   valLoss:3.3417e-01  time: 1.81e-01\n",
      "epoch: 96   trainLoss: 1.0698e-01   valLoss:1.7592e-01  time: 1.75e-01\n",
      "epoch: 97   trainLoss: 1.3068e-01   valLoss:8.2163e-01  time: 1.76e-01\n",
      "epoch: 98   trainLoss: 1.6386e-01   valLoss:2.4988e-01  time: 1.74e-01\n",
      "epoch: 99   trainLoss: 1.8450e-01   valLoss:8.8228e-01  time: 1.74e-01\n",
      "loading checkpoint 92\n",
      "trained 30 random forest models in 3.44 seconds\n",
      "loaded train set of size 4\n",
      "epoch: 0   trainLoss: 9.2618e-01   valLoss:9.5588e-01  time: 5.05e-02\n",
      "epoch: 1   trainLoss: 6.2161e-01   valLoss:9.5585e-01  time: 5.13e-02\n",
      "epoch: 2   trainLoss: 4.6672e-01   valLoss:9.4814e-01  time: 4.98e-02\n",
      "epoch: 3   trainLoss: 3.2962e-01   valLoss:9.4277e-01  time: 5.04e-02\n",
      "epoch: 4   trainLoss: 2.6768e-01   valLoss:9.3413e-01  time: 5.03e-02\n",
      "epoch: 5   trainLoss: 1.8966e-01   valLoss:9.2533e-01  time: 5.03e-02\n",
      "epoch: 6   trainLoss: 1.5035e-01   valLoss:9.1299e-01  time: 5.04e-02\n",
      "epoch: 7   trainLoss: 1.0550e-01   valLoss:9.0092e-01  time: 5.01e-02\n",
      "epoch: 8   trainLoss: 9.1321e-02   valLoss:8.8508e-01  time: 5.05e-02\n",
      "epoch: 9   trainLoss: 7.6592e-02   valLoss:8.7813e-01  time: 5.13e-02\n",
      "epoch: 10   trainLoss: 6.1426e-02   valLoss:8.8014e-01  time: 5.03e-02\n",
      "epoch: 11   trainLoss: 4.6634e-02   valLoss:8.8335e-01  time: 4.96e-02\n",
      "epoch: 12   trainLoss: 3.6847e-02   valLoss:8.8910e-01  time: 5.02e-02\n",
      "epoch: 13   trainLoss: 2.9207e-02   valLoss:8.8305e-01  time: 4.99e-02\n",
      "epoch: 14   trainLoss: 2.3306e-02   valLoss:8.6192e-01  time: 4.95e-02\n",
      "epoch: 15   trainLoss: 1.8699e-02   valLoss:8.3513e-01  time: 5.02e-02\n",
      "epoch: 16   trainLoss: 1.3797e-02   valLoss:8.0941e-01  time: 5.15e-02\n",
      "epoch: 17   trainLoss: 1.0984e-02   valLoss:7.7668e-01  time: 5.16e-02\n",
      "epoch: 18   trainLoss: 1.0078e-02   valLoss:7.4651e-01  time: 5.03e-02\n",
      "epoch: 19   trainLoss: 9.2381e-03   valLoss:7.1665e-01  time: 5.00e-02\n",
      "epoch: 20   trainLoss: 8.3155e-03   valLoss:6.8072e-01  time: 5.13e-02\n",
      "epoch: 21   trainLoss: 7.0038e-03   valLoss:6.2741e-01  time: 5.05e-02\n",
      "epoch: 22   trainLoss: 5.3774e-03   valLoss:5.7443e-01  time: 4.99e-02\n",
      "epoch: 23   trainLoss: 4.0302e-03   valLoss:5.0028e-01  time: 5.14e-02\n",
      "epoch: 24   trainLoss: 3.5616e-03   valLoss:4.1401e-01  time: 5.07e-02\n",
      "epoch: 25   trainLoss: 3.1709e-03   valLoss:3.2626e-01  time: 5.03e-02\n",
      "epoch: 26   trainLoss: 2.8687e-03   valLoss:2.4701e-01  time: 5.04e-02\n",
      "epoch: 27   trainLoss: 2.4551e-03   valLoss:1.8500e-01  time: 5.46e-02\n",
      "epoch: 28   trainLoss: 2.1760e-03   valLoss:1.4753e-01  time: 5.44e-02\n",
      "epoch: 29   trainLoss: 1.8576e-03   valLoss:1.2425e-01  time: 5.53e-02\n",
      "epoch: 30   trainLoss: 1.7797e-03   valLoss:1.1035e-01  time: 5.32e-02\n",
      "epoch: 31   trainLoss: 1.8369e-03   valLoss:9.7704e-02  time: 5.32e-02\n",
      "epoch: 32   trainLoss: 1.6118e-03   valLoss:8.9798e-02  time: 5.30e-02\n",
      "epoch: 33   trainLoss: 1.2602e-03   valLoss:8.6740e-02  time: 5.41e-02\n",
      "epoch: 34   trainLoss: 1.0111e-03   valLoss:8.5581e-02  time: 5.29e-02\n",
      "epoch: 35   trainLoss: 9.7976e-04   valLoss:8.2512e-02  time: 5.58e-02\n",
      "epoch: 36   trainLoss: 8.6606e-04   valLoss:7.8479e-02  time: 5.34e-02\n",
      "epoch: 37   trainLoss: 7.6922e-04   valLoss:7.5941e-02  time: 5.23e-02\n",
      "epoch: 38   trainLoss: 7.2565e-04   valLoss:7.5668e-02  time: 5.30e-02\n",
      "epoch: 39   trainLoss: 6.9878e-04   valLoss:7.5672e-02  time: 5.43e-02\n",
      "epoch: 40   trainLoss: 6.9509e-04   valLoss:7.5294e-02  time: 5.37e-02\n",
      "epoch: 41   trainLoss: 6.5725e-04   valLoss:7.4808e-02  time: 5.42e-02\n",
      "epoch: 42   trainLoss: 5.9585e-04   valLoss:7.5217e-02  time: 5.31e-02\n",
      "epoch: 43   trainLoss: 5.3448e-04   valLoss:7.6081e-02  time: 5.05e-02\n",
      "epoch: 44   trainLoss: 4.4758e-04   valLoss:7.6246e-02  time: 5.32e-02\n",
      "epoch: 45   trainLoss: 4.1329e-04   valLoss:7.5276e-02  time: 5.72e-02\n",
      "epoch: 46   trainLoss: 3.9124e-04   valLoss:7.3898e-02  time: 5.49e-02\n",
      "epoch: 47   trainLoss: 3.7155e-04   valLoss:7.2983e-02  time: 5.27e-02\n",
      "epoch: 48   trainLoss: 3.4214e-04   valLoss:7.2962e-02  time: 5.44e-02\n",
      "epoch: 49   trainLoss: 3.3022e-04   valLoss:7.2330e-02  time: 5.09e-02\n",
      "epoch: 50   trainLoss: 3.1998e-04   valLoss:7.0851e-02  time: 5.11e-02\n",
      "epoch: 51   trainLoss: 2.7915e-04   valLoss:7.0070e-02  time: 5.02e-02\n",
      "epoch: 52   trainLoss: 2.4953e-04   valLoss:7.0709e-02  time: 5.05e-02\n",
      "epoch: 53   trainLoss: 2.2470e-04   valLoss:7.1656e-02  time: 5.64e-02\n",
      "epoch: 54   trainLoss: 2.2105e-04   valLoss:7.1762e-02  time: 4.98e-02\n",
      "epoch: 55   trainLoss: 2.1601e-04   valLoss:7.2192e-02  time: 5.06e-02\n",
      "epoch: 56   trainLoss: 2.1042e-04   valLoss:7.3328e-02  time: 5.04e-02\n",
      "epoch: 57   trainLoss: 2.0121e-04   valLoss:7.3651e-02  time: 4.96e-02\n",
      "epoch: 58   trainLoss: 1.8977e-04   valLoss:7.3440e-02  time: 5.02e-02\n",
      "epoch: 59   trainLoss: 1.7381e-04   valLoss:7.4191e-02  time: 4.92e-02\n",
      "epoch: 60   trainLoss: 1.6572e-04   valLoss:7.5648e-02  time: 5.15e-02\n",
      "epoch: 61   trainLoss: 1.5392e-04   valLoss:7.6579e-02  time: 5.05e-02\n",
      "epoch: 62   trainLoss: 1.4429e-04   valLoss:7.8010e-02  time: 4.96e-02\n",
      "epoch: 63   trainLoss: 1.3583e-04   valLoss:7.9654e-02  time: 4.95e-02\n",
      "epoch: 64   trainLoss: 1.2842e-04   valLoss:7.9608e-02  time: 4.92e-02\n",
      "epoch: 65   trainLoss: 1.1426e-04   valLoss:7.9638e-02  time: 5.04e-02\n",
      "epoch: 66   trainLoss: 1.1388e-04   valLoss:8.0415e-02  time: 5.10e-02\n",
      "epoch: 67   trainLoss: 1.0769e-04   valLoss:8.0266e-02  time: 4.99e-02\n",
      "epoch: 68   trainLoss: 1.0003e-04   valLoss:8.0617e-02  time: 4.92e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69   trainLoss: 9.1062e-05   valLoss:8.1257e-02  time: 5.09e-02\n",
      "epoch: 70   trainLoss: 8.4850e-05   valLoss:8.1160e-02  time: 4.95e-02\n",
      "epoch: 71   trainLoss: 8.2007e-05   valLoss:8.1976e-02  time: 5.00e-02\n",
      "epoch: 72   trainLoss: 7.8732e-05   valLoss:8.1256e-02  time: 4.93e-02\n",
      "epoch: 73   trainLoss: 8.0517e-05   valLoss:8.2661e-02  time: 4.90e-02\n",
      "epoch: 74   trainLoss: 7.6344e-05   valLoss:8.0686e-02  time: 4.98e-02\n",
      "epoch: 75   trainLoss: 9.0884e-05   valLoss:8.2566e-02  time: 5.86e-02\n",
      "epoch: 76   trainLoss: 1.2991e-04   valLoss:7.8268e-02  time: 5.85e-02\n",
      "epoch: 77   trainLoss: 2.5883e-04   valLoss:8.2947e-02  time: 5.89e-02\n",
      "epoch: 78   trainLoss: 5.2321e-04   valLoss:7.3389e-02  time: 5.60e-02\n",
      "epoch: 79   trainLoss: 6.5966e-04   valLoss:7.8967e-02  time: 4.95e-02\n",
      "epoch: 80   trainLoss: 3.5839e-04   valLoss:7.5238e-02  time: 4.92e-02\n",
      "epoch: 81   trainLoss: 9.9657e-05   valLoss:7.0166e-02  time: 5.04e-02\n",
      "epoch: 82   trainLoss: 3.4369e-04   valLoss:7.3991e-02  time: 4.99e-02\n",
      "epoch: 83   trainLoss: 2.1794e-04   valLoss:7.2163e-02  time: 4.97e-02\n",
      "epoch: 84   trainLoss: 1.2664e-04   valLoss:6.8678e-02  time: 5.02e-02\n",
      "epoch: 85   trainLoss: 2.2936e-04   valLoss:7.1272e-02  time: 4.96e-02\n",
      "epoch: 86   trainLoss: 1.2433e-04   valLoss:7.1876e-02  time: 5.01e-02\n",
      "epoch: 87   trainLoss: 1.7339e-04   valLoss:7.0147e-02  time: 5.11e-02\n",
      "epoch: 88   trainLoss: 1.5151e-04   valLoss:7.1104e-02  time: 5.04e-02\n",
      "epoch: 89   trainLoss: 1.1595e-04   valLoss:7.1557e-02  time: 4.95e-02\n",
      "epoch: 90   trainLoss: 1.4456e-04   valLoss:7.1850e-02  time: 5.00e-02\n",
      "epoch: 91   trainLoss: 1.8042e-04   valLoss:7.1397e-02  time: 5.00e-02\n",
      "epoch: 92   trainLoss: 8.0459e-05   valLoss:7.0678e-02  time: 4.98e-02\n",
      "epoch: 93   trainLoss: 1.6663e-04   valLoss:7.1718e-02  time: 5.08e-02\n",
      "epoch: 94   trainLoss: 4.7699e-05   valLoss:7.3317e-02  time: 4.99e-02\n",
      "epoch: 95   trainLoss: 9.3237e-05   valLoss:7.3709e-02  time: 4.96e-02\n",
      "epoch: 96   trainLoss: 5.2221e-05   valLoss:7.3303e-02  time: 4.95e-02\n",
      "epoch: 97   trainLoss: 6.7826e-05   valLoss:7.4151e-02  time: 4.98e-02\n",
      "epoch: 98   trainLoss: 4.7336e-05   valLoss:7.5424e-02  time: 4.92e-02\n",
      "epoch: 99   trainLoss: 5.5797e-05   valLoss:7.5796e-02  time: 5.00e-02\n",
      "loading checkpoint 84\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.6343e+01   valLoss:4.9341e+01  time: 5.24e-02\n",
      "epoch: 1   trainLoss: 3.0341e+01   valLoss:3.7117e+01  time: 5.11e-02\n",
      "epoch: 2   trainLoss: 2.8245e+01   valLoss:2.5878e+01  time: 5.27e-02\n",
      "epoch: 3   trainLoss: 2.6573e+01   valLoss:2.0687e+01  time: 6.43e-02\n",
      "epoch: 4   trainLoss: 2.4906e+01   valLoss:1.7709e+01  time: 5.33e-02\n",
      "epoch: 5   trainLoss: 2.3508e+01   valLoss:1.6756e+01  time: 5.51e-02\n",
      "epoch: 6   trainLoss: 2.2215e+01   valLoss:1.5608e+01  time: 5.34e-02\n",
      "epoch: 7   trainLoss: 2.0960e+01   valLoss:1.4954e+01  time: 5.34e-02\n",
      "epoch: 8   trainLoss: 1.9700e+01   valLoss:1.4437e+01  time: 5.50e-02\n",
      "epoch: 9   trainLoss: 1.8481e+01   valLoss:1.2924e+01  time: 5.50e-02\n",
      "epoch: 10   trainLoss: 1.7285e+01   valLoss:1.2414e+01  time: 5.27e-02\n",
      "epoch: 11   trainLoss: 1.6095e+01   valLoss:1.2110e+01  time: 5.51e-02\n",
      "epoch: 12   trainLoss: 1.4939e+01   valLoss:1.0973e+01  time: 5.41e-02\n",
      "epoch: 13   trainLoss: 1.3806e+01   valLoss:9.6673e+00  time: 5.22e-02\n",
      "epoch: 14   trainLoss: 1.2769e+01   valLoss:8.7172e+00  time: 5.38e-02\n",
      "epoch: 15   trainLoss: 1.1882e+01   valLoss:8.2928e+00  time: 6.06e-02\n",
      "epoch: 16   trainLoss: 1.0863e+01   valLoss:7.1530e+00  time: 5.06e-02\n",
      "epoch: 17   trainLoss: 9.7934e+00   valLoss:5.8484e+00  time: 5.23e-02\n",
      "epoch: 18   trainLoss: 8.8362e+00   valLoss:4.6884e+00  time: 5.03e-02\n",
      "epoch: 19   trainLoss: 7.9295e+00   valLoss:4.1005e+00  time: 5.03e-02\n",
      "epoch: 20   trainLoss: 7.0895e+00   valLoss:3.6414e+00  time: 5.05e-02\n",
      "epoch: 21   trainLoss: 6.2844e+00   valLoss:3.4220e+00  time: 5.01e-02\n",
      "epoch: 22   trainLoss: 5.5780e+00   valLoss:3.1492e+00  time: 4.97e-02\n",
      "epoch: 23   trainLoss: 4.9640e+00   valLoss:2.9674e+00  time: 5.15e-02\n",
      "epoch: 24   trainLoss: 4.3592e+00   valLoss:2.7437e+00  time: 5.04e-02\n",
      "epoch: 25   trainLoss: 3.8499e+00   valLoss:2.5658e+00  time: 5.00e-02\n",
      "epoch: 26   trainLoss: 3.4020e+00   valLoss:2.5228e+00  time: 5.34e-02\n",
      "epoch: 27   trainLoss: 3.0131e+00   valLoss:2.4888e+00  time: 5.31e-02\n",
      "epoch: 28   trainLoss: 2.6742e+00   valLoss:2.4123e+00  time: 5.39e-02\n",
      "epoch: 29   trainLoss: 2.4063e+00   valLoss:2.3580e+00  time: 5.49e-02\n",
      "epoch: 30   trainLoss: 2.1786e+00   valLoss:2.2883e+00  time: 5.25e-02\n",
      "epoch: 31   trainLoss: 1.9913e+00   valLoss:2.1432e+00  time: 5.40e-02\n",
      "epoch: 32   trainLoss: 1.8406e+00   valLoss:2.0919e+00  time: 5.40e-02\n",
      "epoch: 33   trainLoss: 1.6955e+00   valLoss:1.8966e+00  time: 5.51e-02\n",
      "epoch: 34   trainLoss: 1.5730e+00   valLoss:1.6932e+00  time: 5.36e-02\n",
      "epoch: 35   trainLoss: 1.4554e+00   valLoss:1.4876e+00  time: 5.78e-02\n",
      "epoch: 36   trainLoss: 1.3561e+00   valLoss:1.3720e+00  time: 5.41e-02\n",
      "epoch: 37   trainLoss: 1.2728e+00   valLoss:1.3648e+00  time: 5.11e-02\n",
      "epoch: 38   trainLoss: 1.2024e+00   valLoss:1.3717e+00  time: 5.03e-02\n",
      "epoch: 39   trainLoss: 1.1413e+00   valLoss:1.4648e+00  time: 4.96e-02\n",
      "epoch: 40   trainLoss: 1.0887e+00   valLoss:1.5729e+00  time: 4.95e-02\n",
      "epoch: 41   trainLoss: 1.0503e+00   valLoss:1.7879e+00  time: 4.99e-02\n",
      "epoch: 42   trainLoss: 1.0827e+00   valLoss:1.6054e+00  time: 5.12e-02\n",
      "epoch: 43   trainLoss: 9.2320e-01   valLoss:1.5642e+00  time: 5.08e-02\n",
      "epoch: 44   trainLoss: 9.5728e-01   valLoss:1.3569e+00  time: 4.95e-02\n",
      "epoch: 45   trainLoss: 8.7729e-01   valLoss:1.1977e+00  time: 5.03e-02\n",
      "epoch: 46   trainLoss: 7.8773e-01   valLoss:8.7296e-01  time: 5.26e-02\n",
      "epoch: 47   trainLoss: 6.6685e-01   valLoss:7.6464e-01  time: 4.97e-02\n",
      "epoch: 48   trainLoss: 6.0220e-01   valLoss:8.2971e-01  time: 5.04e-02\n",
      "epoch: 49   trainLoss: 5.3539e-01   valLoss:9.1159e-01  time: 5.13e-02\n",
      "epoch: 50   trainLoss: 4.6385e-01   valLoss:9.9060e-01  time: 5.11e-02\n",
      "epoch: 51   trainLoss: 4.2991e-01   valLoss:7.3539e-01  time: 4.98e-02\n",
      "epoch: 52   trainLoss: 3.8083e-01   valLoss:6.2281e-01  time: 5.03e-02\n",
      "epoch: 53   trainLoss: 3.3708e-01   valLoss:5.2081e-01  time: 5.11e-02\n",
      "epoch: 54   trainLoss: 2.8697e-01   valLoss:4.6827e-01  time: 5.04e-02\n",
      "epoch: 55   trainLoss: 2.6819e-01   valLoss:4.5954e-01  time: 5.03e-02\n",
      "epoch: 56   trainLoss: 2.5976e-01   valLoss:4.2681e-01  time: 5.17e-02\n",
      "epoch: 57   trainLoss: 2.4617e-01   valLoss:3.8831e-01  time: 5.07e-02\n",
      "epoch: 58   trainLoss: 2.2837e-01   valLoss:3.4936e-01  time: 5.06e-02\n",
      "epoch: 59   trainLoss: 2.1143e-01   valLoss:3.5021e-01  time: 5.10e-02\n",
      "epoch: 60   trainLoss: 1.9944e-01   valLoss:3.9055e-01  time: 4.95e-02\n",
      "epoch: 61   trainLoss: 1.9076e-01   valLoss:4.0769e-01  time: 5.00e-02\n",
      "epoch: 62   trainLoss: 1.8540e-01   valLoss:4.0195e-01  time: 5.02e-02\n",
      "epoch: 63   trainLoss: 1.7604e-01   valLoss:4.1125e-01  time: 5.03e-02\n",
      "epoch: 64   trainLoss: 1.6897e-01   valLoss:4.3222e-01  time: 5.14e-02\n",
      "epoch: 65   trainLoss: 1.6294e-01   valLoss:4.2327e-01  time: 5.01e-02\n",
      "epoch: 66   trainLoss: 1.5566e-01   valLoss:3.8171e-01  time: 5.00e-02\n",
      "epoch: 67   trainLoss: 1.4921e-01   valLoss:3.5343e-01  time: 5.02e-02\n",
      "epoch: 68   trainLoss: 1.4163e-01   valLoss:3.1365e-01  time: 4.95e-02\n",
      "epoch: 69   trainLoss: 1.3485e-01   valLoss:2.7153e-01  time: 5.00e-02\n",
      "epoch: 70   trainLoss: 1.2853e-01   valLoss:2.7138e-01  time: 6.77e-02\n",
      "epoch: 71   trainLoss: 1.2254e-01   valLoss:2.9066e-01  time: 5.31e-02\n",
      "epoch: 72   trainLoss: 1.1686e-01   valLoss:2.9712e-01  time: 5.46e-02\n",
      "epoch: 73   trainLoss: 1.1093e-01   valLoss:2.9178e-01  time: 5.35e-02\n",
      "epoch: 74   trainLoss: 1.0641e-01   valLoss:3.1497e-01  time: 5.56e-02\n",
      "epoch: 75   trainLoss: 1.0377e-01   valLoss:3.5082e-01  time: 5.44e-02\n",
      "epoch: 76   trainLoss: 1.0019e-01   valLoss:3.8660e-01  time: 5.42e-02\n",
      "epoch: 77   trainLoss: 9.6226e-02   valLoss:4.5251e-01  time: 5.27e-02\n",
      "epoch: 78   trainLoss: 9.3681e-02   valLoss:5.3456e-01  time: 5.31e-02\n",
      "epoch: 79   trainLoss: 9.0429e-02   valLoss:5.9311e-01  time: 5.24e-02\n",
      "epoch: 80   trainLoss: 8.6920e-02   valLoss:6.3973e-01  time: 5.66e-02\n",
      "epoch: 81   trainLoss: 8.3289e-02   valLoss:6.8118e-01  time: 5.73e-02\n",
      "epoch: 82   trainLoss: 8.0321e-02   valLoss:7.0111e-01  time: 5.38e-02\n",
      "epoch: 83   trainLoss: 7.7244e-02   valLoss:7.2078e-01  time: 5.30e-02\n",
      "epoch: 84   trainLoss: 7.4124e-02   valLoss:7.5013e-01  time: 5.20e-02\n",
      "epoch: 85   trainLoss: 7.1165e-02   valLoss:7.4572e-01  time: 5.42e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 86   trainLoss: 6.8440e-02   valLoss:7.4346e-01  time: 5.26e-02\n",
      "epoch: 87   trainLoss: 6.6129e-02   valLoss:7.5190e-01  time: 5.32e-02\n",
      "epoch: 88   trainLoss: 6.3975e-02   valLoss:7.1794e-01  time: 5.41e-02\n",
      "epoch: 89   trainLoss: 6.1713e-02   valLoss:7.1245e-01  time: 5.34e-02\n",
      "epoch: 90   trainLoss: 5.9547e-02   valLoss:7.2301e-01  time: 5.48e-02\n",
      "epoch: 91   trainLoss: 5.7642e-02   valLoss:7.0205e-01  time: 5.42e-02\n",
      "epoch: 92   trainLoss: 5.5730e-02   valLoss:7.1862e-01  time: 5.22e-02\n",
      "epoch: 93   trainLoss: 5.3980e-02   valLoss:7.2304e-01  time: 5.41e-02\n",
      "epoch: 94   trainLoss: 5.2379e-02   valLoss:7.3164e-01  time: 5.19e-02\n",
      "epoch: 95   trainLoss: 5.1155e-02   valLoss:7.2640e-01  time: 5.06e-02\n",
      "epoch: 96   trainLoss: 5.0292e-02   valLoss:7.0233e-01  time: 5.08e-02\n",
      "epoch: 97   trainLoss: 5.0913e-02   valLoss:7.0901e-01  time: 5.00e-02\n",
      "epoch: 98   trainLoss: 5.5067e-02   valLoss:7.1196e-01  time: 5.12e-02\n",
      "epoch: 99   trainLoss: 7.1157e-02   valLoss:6.8462e-01  time: 5.14e-02\n",
      "loading checkpoint 70\n",
      "trained 30 random forest models in 3.47 seconds\n",
      "loaded train set of size 449\n",
      "epoch: 0   trainLoss: 9.9966e-01   valLoss:1.0266e+00  time: 3.67e+00\n",
      "epoch: 1   trainLoss: 7.6547e-01   valLoss:9.3356e-01  time: 3.82e+00\n",
      "epoch: 2   trainLoss: 6.0791e-01   valLoss:8.8757e-01  time: 3.70e+00\n",
      "epoch: 3   trainLoss: 5.2594e-01   valLoss:9.5897e-01  time: 3.67e+00\n",
      "epoch: 4   trainLoss: 4.3135e-01   valLoss:1.0005e+00  time: 3.89e+00\n",
      "epoch: 5   trainLoss: 3.8509e-01   valLoss:9.7092e-01  time: 3.75e+00\n",
      "epoch: 6   trainLoss: 3.4474e-01   valLoss:1.0099e+00  time: 3.62e+00\n",
      "epoch: 7   trainLoss: 2.7944e-01   valLoss:1.2049e+00  time: 3.66e+00\n",
      "epoch: 8   trainLoss: 2.4511e-01   valLoss:1.4258e+00  time: 3.79e+00\n",
      "epoch: 9   trainLoss: 2.1502e-01   valLoss:1.9722e+00  time: 3.64e+00\n",
      "epoch: 10   trainLoss: 1.9297e-01   valLoss:2.3135e+00  time: 3.62e+00\n",
      "epoch: 11   trainLoss: 1.8781e-01   valLoss:1.7646e+00  time: 3.78e+00\n",
      "epoch: 12   trainLoss: 1.5527e-01   valLoss:1.1445e+00  time: 3.77e+00\n",
      "epoch: 13   trainLoss: 1.3327e-01   valLoss:7.1071e-01  time: 3.79e+00\n",
      "epoch: 14   trainLoss: 1.1780e-01   valLoss:5.0063e-01  time: 3.76e+00\n",
      "epoch: 15   trainLoss: 1.0799e-01   valLoss:3.3884e-01  time: 3.62e+00\n",
      "epoch: 16   trainLoss: 9.1077e-02   valLoss:2.9783e-01  time: 3.76e+00\n",
      "epoch: 17   trainLoss: 9.3669e-02   valLoss:3.0099e-01  time: 3.87e+00\n",
      "epoch: 18   trainLoss: 9.0662e-02   valLoss:2.9709e-01  time: 3.55e+00\n",
      "epoch: 19   trainLoss: 7.0525e-02   valLoss:2.6331e-01  time: 3.67e+00\n",
      "epoch: 20   trainLoss: 6.1911e-02   valLoss:3.1957e-01  time: 3.77e+00\n",
      "epoch: 21   trainLoss: 7.7497e-02   valLoss:2.9085e-01  time: 3.80e+00\n",
      "epoch: 22   trainLoss: 5.9327e-02   valLoss:2.9176e-01  time: 3.68e+00\n",
      "epoch: 23   trainLoss: 5.8642e-02   valLoss:3.6072e-01  time: 3.72e+00\n",
      "epoch: 24   trainLoss: 5.2072e-02   valLoss:2.4023e-01  time: 3.79e+00\n",
      "epoch: 25   trainLoss: 5.0820e-02   valLoss:3.1333e-01  time: 3.64e+00\n",
      "epoch: 26   trainLoss: 4.1070e-02   valLoss:2.7419e-01  time: 3.86e+00\n",
      "epoch: 27   trainLoss: 4.0940e-02   valLoss:2.9624e-01  time: 3.76e+00\n",
      "epoch: 28   trainLoss: 4.1797e-02   valLoss:3.1782e-01  time: 3.70e+00\n",
      "epoch: 29   trainLoss: 3.7058e-02   valLoss:3.0246e-01  time: 3.57e+00\n",
      "epoch: 30   trainLoss: 4.0454e-02   valLoss:3.3836e-01  time: 3.63e+00\n",
      "epoch: 31   trainLoss: 3.1863e-02   valLoss:2.7731e-01  time: 3.57e+00\n",
      "epoch: 32   trainLoss: 2.9093e-02   valLoss:2.9731e-01  time: 3.60e+00\n",
      "epoch: 33   trainLoss: 3.0302e-02   valLoss:2.7441e-01  time: 3.62e+00\n",
      "epoch: 34   trainLoss: 3.0196e-02   valLoss:2.9859e-01  time: 3.74e+00\n",
      "epoch: 35   trainLoss: 2.7993e-02   valLoss:3.1864e-01  time: 3.60e+00\n",
      "epoch: 36   trainLoss: 2.7957e-02   valLoss:2.8851e-01  time: 3.62e+00\n",
      "epoch: 37   trainLoss: 2.2841e-02   valLoss:2.8256e-01  time: 3.62e+00\n",
      "epoch: 38   trainLoss: 3.8591e-02   valLoss:3.6933e-01  time: 3.58e+00\n",
      "epoch: 39   trainLoss: 3.6866e-02   valLoss:2.4810e-01  time: 3.73e+00\n",
      "epoch: 40   trainLoss: 3.0864e-02   valLoss:5.0583e-01  time: 3.63e+00\n",
      "epoch: 41   trainLoss: 2.5353e-02   valLoss:2.2810e-01  time: 3.80e+00\n",
      "epoch: 42   trainLoss: 3.2336e-02   valLoss:5.2660e-01  time: 3.62e+00\n",
      "epoch: 43   trainLoss: 3.8132e-02   valLoss:2.5637e-01  time: 3.69e+00\n",
      "epoch: 44   trainLoss: 4.1550e-02   valLoss:4.0446e-01  time: 3.58e+00\n",
      "epoch: 45   trainLoss: 3.5148e-02   valLoss:2.6608e-01  time: 3.79e+00\n",
      "epoch: 46   trainLoss: 3.4669e-02   valLoss:2.3568e-01  time: 3.73e+00\n",
      "epoch: 47   trainLoss: 2.6693e-02   valLoss:3.9175e-01  time: 3.68e+00\n",
      "epoch: 48   trainLoss: 3.3312e-02   valLoss:2.5773e-01  time: 3.82e+00\n",
      "epoch: 49   trainLoss: 2.8013e-02   valLoss:1.9666e-01  time: 3.85e+00\n",
      "epoch: 50   trainLoss: 2.5040e-02   valLoss:2.8998e-01  time: 3.57e+00\n",
      "epoch: 51   trainLoss: 2.4767e-02   valLoss:2.2561e-01  time: 3.68e+00\n",
      "epoch: 52   trainLoss: 3.4843e-02   valLoss:2.3629e-01  time: 3.91e+00\n",
      "epoch: 53   trainLoss: 4.6721e-02   valLoss:2.5889e-01  time: 3.58e+00\n",
      "epoch: 54   trainLoss: 4.5104e-02   valLoss:2.6437e-01  time: 3.61e+00\n",
      "epoch: 55   trainLoss: 3.8253e-02   valLoss:2.2970e-01  time: 3.55e+00\n",
      "epoch: 56   trainLoss: 3.9433e-02   valLoss:2.4984e-01  time: 3.64e+00\n",
      "epoch: 57   trainLoss: 2.6870e-02   valLoss:2.2976e-01  time: 3.65e+00\n",
      "epoch: 58   trainLoss: 3.1472e-02   valLoss:2.2725e-01  time: 3.61e+00\n",
      "epoch: 59   trainLoss: 2.4284e-02   valLoss:2.9299e-01  time: 3.62e+00\n",
      "epoch: 60   trainLoss: 2.9962e-02   valLoss:1.5817e-01  time: 3.59e+00\n",
      "epoch: 61   trainLoss: 2.0601e-02   valLoss:2.0646e-01  time: 3.67e+00\n",
      "epoch: 62   trainLoss: 1.9091e-02   valLoss:2.3353e-01  time: 3.65e+00\n",
      "epoch: 63   trainLoss: 3.5182e-02   valLoss:1.9808e-01  time: 3.84e+00\n",
      "epoch: 64   trainLoss: 2.9957e-02   valLoss:2.6939e-01  time: 3.85e+00\n",
      "epoch: 65   trainLoss: 2.6127e-02   valLoss:1.8451e-01  time: 3.61e+00\n",
      "epoch: 66   trainLoss: 4.9922e-02   valLoss:1.9246e-01  time: 3.72e+00\n",
      "epoch: 67   trainLoss: 3.0024e-02   valLoss:3.0976e-01  time: 3.79e+00\n",
      "epoch: 68   trainLoss: 2.6928e-02   valLoss:3.3266e-01  time: 3.67e+00\n",
      "epoch: 69   trainLoss: 2.7909e-02   valLoss:1.9214e-01  time: 3.84e+00\n",
      "epoch: 70   trainLoss: 2.9881e-02   valLoss:1.6017e-01  time: 3.82e+00\n",
      "epoch: 71   trainLoss: 2.5842e-02   valLoss:2.1054e-01  time: 3.70e+00\n",
      "epoch: 72   trainLoss: 2.4370e-02   valLoss:1.3727e-01  time: 3.82e+00\n",
      "epoch: 73   trainLoss: 3.0813e-02   valLoss:1.7321e-01  time: 3.76e+00\n",
      "epoch: 74   trainLoss: 2.7515e-02   valLoss:2.1175e-01  time: 3.78e+00\n",
      "epoch: 75   trainLoss: 2.1589e-02   valLoss:9.0277e-02  time: 3.81e+00\n",
      "epoch: 76   trainLoss: 2.2974e-02   valLoss:1.8259e-01  time: 3.54e+00\n",
      "epoch: 77   trainLoss: 2.0067e-02   valLoss:1.2931e-01  time: 3.68e+00\n",
      "epoch: 78   trainLoss: 2.1007e-02   valLoss:1.4555e-01  time: 3.70e+00\n",
      "epoch: 79   trainLoss: 1.6510e-02   valLoss:1.3174e-01  time: 3.79e+00\n",
      "epoch: 80   trainLoss: 1.5715e-02   valLoss:1.6789e-01  time: 3.74e+00\n",
      "epoch: 81   trainLoss: 1.7616e-02   valLoss:1.2014e-01  time: 3.80e+00\n",
      "epoch: 82   trainLoss: 1.4180e-02   valLoss:1.3287e-01  time: 3.69e+00\n",
      "epoch: 83   trainLoss: 1.4956e-02   valLoss:1.2502e-01  time: 3.81e+00\n",
      "epoch: 84   trainLoss: 1.7651e-02   valLoss:1.3064e-01  time: 3.54e+00\n",
      "epoch: 85   trainLoss: 2.7720e-02   valLoss:9.5449e-02  time: 3.56e+00\n",
      "epoch: 86   trainLoss: 2.0357e-02   valLoss:1.8672e-01  time: 3.70e+00\n",
      "epoch: 87   trainLoss: 1.4820e-02   valLoss:1.9013e-01  time: 3.83e+00\n",
      "epoch: 88   trainLoss: 1.6655e-02   valLoss:1.4105e-01  time: 3.63e+00\n",
      "epoch: 89   trainLoss: 1.7124e-02   valLoss:1.4888e-01  time: 4.05e+00\n",
      "epoch: 90   trainLoss: 1.5420e-02   valLoss:1.3801e-01  time: 3.75e+00\n",
      "epoch: 91   trainLoss: 1.6277e-02   valLoss:8.8917e-02  time: 3.73e+00\n",
      "epoch: 92   trainLoss: 2.1220e-02   valLoss:1.1889e-01  time: 3.78e+00\n",
      "epoch: 93   trainLoss: 2.3669e-02   valLoss:1.0602e-01  time: 3.83e+00\n",
      "epoch: 94   trainLoss: 1.6082e-02   valLoss:8.2839e-02  time: 3.78e+00\n",
      "epoch: 95   trainLoss: 1.9661e-02   valLoss:1.0712e-01  time: 3.78e+00\n",
      "epoch: 96   trainLoss: 1.6032e-02   valLoss:6.8137e-02  time: 3.81e+00\n",
      "epoch: 97   trainLoss: 1.4988e-02   valLoss:7.7695e-02  time: 3.60e+00\n",
      "epoch: 98   trainLoss: 1.3552e-02   valLoss:7.8394e-02  time: 3.61e+00\n",
      "epoch: 99   trainLoss: 1.8866e-02   valLoss:6.2640e-02  time: 3.77e+00\n",
      "loading checkpoint 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading restart file\n",
      "epoch: 0   trainLoss: 2.9688e+01   valLoss:2.8614e+01  time: 4.24e+00\n",
      "epoch: 1   trainLoss: 2.5261e+01   valLoss:2.9458e+01  time: 4.30e+00\n",
      "epoch: 2   trainLoss: 2.2552e+01   valLoss:2.7977e+01  time: 4.08e+00\n",
      "epoch: 3   trainLoss: 1.9697e+01   valLoss:2.4239e+01  time: 3.83e+00\n",
      "epoch: 4   trainLoss: 1.7220e+01   valLoss:1.9915e+01  time: 3.79e+00\n",
      "epoch: 5   trainLoss: 1.5094e+01   valLoss:1.8555e+01  time: 4.21e+00\n",
      "epoch: 6   trainLoss: 1.3384e+01   valLoss:2.1172e+01  time: 3.61e+00\n",
      "epoch: 7   trainLoss: 1.1171e+01   valLoss:1.9087e+01  time: 3.80e+00\n",
      "epoch: 8   trainLoss: 9.5422e+00   valLoss:1.6795e+01  time: 3.61e+00\n",
      "epoch: 9   trainLoss: 7.8521e+00   valLoss:1.3043e+01  time: 3.63e+00\n",
      "epoch: 10   trainLoss: 6.3766e+00   valLoss:1.0171e+01  time: 3.80e+00\n",
      "epoch: 11   trainLoss: 6.2081e+00   valLoss:8.7674e+00  time: 3.81e+00\n",
      "epoch: 12   trainLoss: 5.0874e+00   valLoss:4.6907e+00  time: 3.80e+00\n",
      "epoch: 13   trainLoss: 4.1239e+00   valLoss:4.8782e+00  time: 3.65e+00\n",
      "epoch: 14   trainLoss: 4.0016e+00   valLoss:5.8189e+00  time: 3.62e+00\n",
      "epoch: 15   trainLoss: 3.2731e+00   valLoss:6.1681e+00  time: 3.77e+00\n",
      "epoch: 16   trainLoss: 2.7767e+00   valLoss:5.0905e+00  time: 3.78e+00\n",
      "epoch: 17   trainLoss: 2.5425e+00   valLoss:8.2987e+00  time: 3.80e+00\n",
      "epoch: 18   trainLoss: 2.3512e+00   valLoss:6.4130e+00  time: 3.64e+00\n",
      "epoch: 19   trainLoss: 2.2155e+00   valLoss:1.8078e+01  time: 3.81e+00\n",
      "epoch: 20   trainLoss: 2.3508e+00   valLoss:3.5279e+00  time: 3.77e+00\n",
      "epoch: 21   trainLoss: 1.9726e+00   valLoss:3.0825e+00  time: 3.63e+00\n",
      "epoch: 22   trainLoss: 1.7430e+00   valLoss:2.1461e+00  time: 3.80e+00\n",
      "epoch: 23   trainLoss: 1.6617e+00   valLoss:2.8857e+00  time: 3.72e+00\n",
      "epoch: 24   trainLoss: 1.5995e+00   valLoss:2.5687e+00  time: 3.65e+00\n",
      "epoch: 25   trainLoss: 1.3560e+00   valLoss:2.1372e+00  time: 3.64e+00\n",
      "epoch: 26   trainLoss: 1.3425e+00   valLoss:1.6759e+00  time: 3.75e+00\n",
      "epoch: 27   trainLoss: 1.1478e+00   valLoss:1.4306e+00  time: 3.66e+00\n",
      "epoch: 28   trainLoss: 1.2360e+00   valLoss:1.7530e+00  time: 3.62e+00\n",
      "epoch: 29   trainLoss: 1.2207e+00   valLoss:5.8134e+00  time: 3.61e+00\n",
      "epoch: 30   trainLoss: 1.3882e+00   valLoss:4.5697e+00  time: 3.64e+00\n",
      "epoch: 31   trainLoss: 1.1722e+00   valLoss:4.4893e+00  time: 3.75e+00\n",
      "epoch: 32   trainLoss: 1.2355e+00   valLoss:3.3125e+00  time: 3.80e+00\n",
      "epoch: 33   trainLoss: 1.0863e+00   valLoss:2.9217e+00  time: 3.78e+00\n",
      "epoch: 34   trainLoss: 9.1715e-01   valLoss:2.3266e+00  time: 3.76e+00\n",
      "epoch: 35   trainLoss: 1.0827e+00   valLoss:1.5661e+00  time: 3.63e+00\n",
      "epoch: 36   trainLoss: 9.1735e-01   valLoss:2.5719e+00  time: 3.84e+00\n",
      "epoch: 37   trainLoss: 8.4983e-01   valLoss:2.2509e+00  time: 3.80e+00\n",
      "epoch: 38   trainLoss: 9.7384e-01   valLoss:2.3975e+00  time: 3.81e+00\n",
      "epoch: 39   trainLoss: 9.0163e-01   valLoss:1.1962e+00  time: 3.87e+00\n",
      "epoch: 40   trainLoss: 8.7818e-01   valLoss:1.4838e+00  time: 3.63e+00\n",
      "epoch: 41   trainLoss: 9.1130e-01   valLoss:2.2733e+00  time: 3.81e+00\n",
      "epoch: 42   trainLoss: 8.8730e-01   valLoss:1.3331e+00  time: 3.83e+00\n",
      "epoch: 43   trainLoss: 9.3807e-01   valLoss:1.4494e+00  time: 3.59e+00\n",
      "epoch: 44   trainLoss: 7.6616e-01   valLoss:1.3631e+00  time: 3.70e+00\n",
      "epoch: 45   trainLoss: 9.3861e-01   valLoss:1.1934e+00  time: 3.72e+00\n",
      "epoch: 46   trainLoss: 6.8504e-01   valLoss:2.1459e+00  time: 3.79e+00\n",
      "epoch: 47   trainLoss: 9.5507e-01   valLoss:1.1935e+00  time: 3.64e+00\n",
      "epoch: 48   trainLoss: 6.6058e-01   valLoss:1.0929e+00  time: 3.67e+00\n",
      "epoch: 49   trainLoss: 6.1953e-01   valLoss:8.9772e-01  time: 3.59e+00\n",
      "epoch: 50   trainLoss: 7.7330e-01   valLoss:1.3444e+00  time: 3.77e+00\n",
      "epoch: 51   trainLoss: 5.6893e-01   valLoss:7.8661e-01  time: 3.61e+00\n",
      "epoch: 52   trainLoss: 5.4428e-01   valLoss:1.1596e+00  time: 3.61e+00\n",
      "epoch: 53   trainLoss: 9.0170e-01   valLoss:1.4735e+00  time: 3.62e+00\n",
      "epoch: 54   trainLoss: 6.6361e-01   valLoss:8.2395e-01  time: 3.76e+00\n",
      "epoch: 55   trainLoss: 6.1948e-01   valLoss:7.4123e-01  time: 3.61e+00\n",
      "epoch: 56   trainLoss: 5.6235e-01   valLoss:7.2610e-01  time: 3.63e+00\n",
      "epoch: 57   trainLoss: 5.4985e-01   valLoss:1.1691e+00  time: 3.61e+00\n",
      "epoch: 58   trainLoss: 5.1786e-01   valLoss:2.1331e+00  time: 3.81e+00\n",
      "epoch: 59   trainLoss: 5.1074e-01   valLoss:2.5915e+00  time: 3.82e+00\n",
      "epoch: 60   trainLoss: 5.1009e-01   valLoss:1.2562e+00  time: 3.62e+00\n",
      "epoch: 61   trainLoss: 5.1106e-01   valLoss:1.8643e+00  time: 3.64e+00\n",
      "epoch: 62   trainLoss: 4.0949e-01   valLoss:3.3486e+00  time: 3.60e+00\n",
      "epoch: 63   trainLoss: 4.9397e-01   valLoss:1.9227e+00  time: 3.63e+00\n",
      "epoch: 64   trainLoss: 4.3632e-01   valLoss:2.7378e+00  time: 3.70e+00\n",
      "epoch: 65   trainLoss: 6.6149e-01   valLoss:2.1298e+00  time: 3.62e+00\n",
      "epoch: 66   trainLoss: 4.1077e-01   valLoss:2.1166e+00  time: 3.62e+00\n",
      "epoch: 67   trainLoss: 3.7203e-01   valLoss:1.6401e+00  time: 3.61e+00\n",
      "epoch: 68   trainLoss: 5.3847e-01   valLoss:2.1417e+00  time: 3.67e+00\n",
      "epoch: 69   trainLoss: 5.4208e-01   valLoss:5.6057e+00  time: 3.84e+00\n",
      "epoch: 70   trainLoss: 6.8752e-01   valLoss:7.0072e+00  time: 3.74e+00\n",
      "epoch: 71   trainLoss: 6.3487e-01   valLoss:1.1596e+01  time: 3.81e+00\n",
      "epoch: 72   trainLoss: 4.8787e-01   valLoss:1.0506e+01  time: 3.81e+00\n",
      "epoch: 73   trainLoss: 5.2048e-01   valLoss:1.0793e+01  time: 3.82e+00\n",
      "epoch: 74   trainLoss: 4.3262e-01   valLoss:6.3087e+00  time: 3.83e+00\n",
      "epoch: 75   trainLoss: 3.3147e-01   valLoss:4.2685e+00  time: 3.65e+00\n",
      "epoch: 76   trainLoss: 3.1642e-01   valLoss:2.9037e+00  time: 3.62e+00\n",
      "epoch: 77   trainLoss: 3.3101e-01   valLoss:1.8145e+00  time: 3.62e+00\n",
      "epoch: 78   trainLoss: 3.7609e-01   valLoss:1.5872e+00  time: 3.68e+00\n",
      "epoch: 79   trainLoss: 4.4572e-01   valLoss:1.1797e+00  time: 3.83e+00\n",
      "epoch: 80   trainLoss: 3.6269e-01   valLoss:1.3377e+00  time: 3.68e+00\n",
      "epoch: 81   trainLoss: 2.8088e-01   valLoss:1.0354e+00  time: 3.75e+00\n",
      "epoch: 82   trainLoss: 4.2421e-01   valLoss:1.5331e+00  time: 3.77e+00\n",
      "epoch: 83   trainLoss: 4.1355e-01   valLoss:1.3388e+00  time: 3.73e+00\n",
      "epoch: 84   trainLoss: 2.3591e-01   valLoss:5.5912e+00  time: 3.82e+00\n",
      "epoch: 85   trainLoss: 4.0686e-01   valLoss:5.2798e-01  time: 3.80e+00\n",
      "epoch: 86   trainLoss: 5.0277e-01   valLoss:6.2581e-01  time: 3.87e+00\n",
      "epoch: 87   trainLoss: 4.5005e-01   valLoss:1.3550e+00  time: 3.82e+00\n",
      "epoch: 88   trainLoss: 3.4192e-01   valLoss:1.9141e+00  time: 3.61e+00\n",
      "epoch: 89   trainLoss: 3.6006e-01   valLoss:3.6630e+00  time: 3.63e+00\n",
      "epoch: 90   trainLoss: 4.4718e-01   valLoss:6.6627e+00  time: 3.63e+00\n",
      "epoch: 91   trainLoss: 9.0915e-01   valLoss:5.0997e+00  time: 3.76e+00\n",
      "epoch: 92   trainLoss: 7.3155e-01   valLoss:3.7555e+00  time: 3.80e+00\n",
      "epoch: 93   trainLoss: 5.1570e-01   valLoss:6.3118e+00  time: 3.61e+00\n",
      "epoch: 94   trainLoss: 7.4818e-01   valLoss:3.4182e+00  time: 3.61e+00\n",
      "epoch: 95   trainLoss: 7.4709e-01   valLoss:6.7405e+00  time: 3.61e+00\n",
      "epoch: 96   trainLoss: 5.2275e-01   valLoss:6.4976e+00  time: 3.60e+00\n",
      "epoch: 97   trainLoss: 4.5556e-01   valLoss:4.2994e+00  time: 3.62e+00\n",
      "epoch: 98   trainLoss: 4.9054e-01   valLoss:4.1602e+00  time: 3.64e+00\n",
      "epoch: 99   trainLoss: 3.6102e-01   valLoss:3.5374e+00  time: 3.69e+00\n",
      "loading checkpoint 85\n",
      "trained 30 random forest models in 10.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.116020</td>\n",
       "      <td>0.969969</td>\n",
       "      <td>0.970118</td>\n",
       "      <td>0.859786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>0.123098</td>\n",
       "      <td>0.930629</td>\n",
       "      <td>0.947796</td>\n",
       "      <td>0.830341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.090039</td>\n",
       "      <td>0.946155</td>\n",
       "      <td>0.985602</td>\n",
       "      <td>0.867077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>0.921676</td>\n",
       "      <td>0.969216</td>\n",
       "      <td>0.844847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.045589</td>\n",
       "      <td>0.982229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984872</td>\n",
       "      <td>0.971596</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.883445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906463</td>\n",
       "      <td>0.852468</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>-0.062298</td>\n",
       "      <td>0.636615</td>\n",
       "      <td>-0.099011</td>\n",
       "      <td>-1.363379</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.028375</td>\n",
       "      <td>0.401506</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.679991</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>-1.273531</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.103450</td>\n",
       "      <td>0.988410</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.858695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.134394</td>\n",
       "      <td>0.894760</td>\n",
       "      <td>0.953532</td>\n",
       "      <td>0.806173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.076033</td>\n",
       "      <td>0.962339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967427</td>\n",
       "      <td>0.949307</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.177549</td>\n",
       "      <td>0.682126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756257</td>\n",
       "      <td>0.623823</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.179982</td>\n",
       "      <td>0.895609</td>\n",
       "      <td>0.979907</td>\n",
       "      <td>0.794434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.594724</td>\n",
       "      <td>0.760519</td>\n",
       "      <td>0.528574</td>\n",
       "      <td>-0.028610</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.112464</td>\n",
       "      <td>0.985691</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>0.874710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.185999</td>\n",
       "      <td>0.782280</td>\n",
       "      <td>0.843090</td>\n",
       "      <td>0.681770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.113320</td>\n",
       "      <td>0.887888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922671</td>\n",
       "      <td>0.875693</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.248540</td>\n",
       "      <td>0.464363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.552008</td>\n",
       "      <td>0.394289</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.236675</td>\n",
       "      <td>0.815571</td>\n",
       "      <td>0.925754</td>\n",
       "      <td>0.707271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.279860</td>\n",
       "      <td>0.642421</td>\n",
       "      <td>0.764840</td>\n",
       "      <td>0.559920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.988484</td>\n",
       "      <td>0.989712</td>\n",
       "      <td>0.881166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.143194</td>\n",
       "      <td>0.846093</td>\n",
       "      <td>0.904254</td>\n",
       "      <td>0.760050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>0.930226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948828</td>\n",
       "      <td>0.920758</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>0.217814</td>\n",
       "      <td>0.555371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.662096</td>\n",
       "      <td>0.528276</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.196468</td>\n",
       "      <td>0.839426</td>\n",
       "      <td>0.989344</td>\n",
       "      <td>0.671702</td>\n",
       "      <td>-1.086557</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.035355</td>\n",
       "      <td>0.452047</td>\n",
       "      <td>-0.642210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989524</td>\n",
       "      <td>-4.787792</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.097594</td>\n",
       "      <td>0.995855</td>\n",
       "      <td>0.998235</td>\n",
       "      <td>0.876995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>0.314788</td>\n",
       "      <td>0.423867</td>\n",
       "      <td>0.647423</td>\n",
       "      <td>0.267894</td>\n",
       "      <td>-0.302022</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>0.155727</td>\n",
       "      <td>0.833617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865235</td>\n",
       "      <td>0.731517</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>0.365152</td>\n",
       "      <td>0.264610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230196</td>\n",
       "      <td>-0.096280</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>0.210267</td>\n",
       "      <td>0.979214</td>\n",
       "      <td>0.983951</td>\n",
       "      <td>0.804955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.023347</td>\n",
       "      <td>0.360607</td>\n",
       "      <td>0.306081</td>\n",
       "      <td>0.440696</td>\n",
       "      <td>0.173626</td>\n",
       "      <td>-0.253506</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.890274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.224311</td>\n",
       "      <td>0.748556</td>\n",
       "      <td>0.749903</td>\n",
       "      <td>0.588884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>0.877132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905750</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.021402</td>\n",
       "      <td>0.297997</td>\n",
       "      <td>0.182627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312387</td>\n",
       "      <td>-0.087327</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.009610</td>\n",
       "      <td>0.186417</td>\n",
       "      <td>0.957829</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.754140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.029613</td>\n",
       "      <td>0.429541</td>\n",
       "      <td>-0.219908</td>\n",
       "      <td>0.442841</td>\n",
       "      <td>-0.213224</td>\n",
       "      <td>-1.850683</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.143963</td>\n",
       "      <td>0.995053</td>\n",
       "      <td>0.997183</td>\n",
       "      <td>0.882349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.027677</td>\n",
       "      <td>0.401612</td>\n",
       "      <td>-0.023768</td>\n",
       "      <td>0.326750</td>\n",
       "      <td>-0.097799</td>\n",
       "      <td>-1.446556</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.254387</td>\n",
       "      <td>0.865691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.710988</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.384686</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021673</td>\n",
       "      <td>-0.765306</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>0.204961</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.961784</td>\n",
       "      <td>0.799557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.220502</td>\n",
       "      <td>0.828093</td>\n",
       "      <td>0.907163</td>\n",
       "      <td>0.729519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.103358</td>\n",
       "      <td>0.973258</td>\n",
       "      <td>0.983559</td>\n",
       "      <td>0.867992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.118618</td>\n",
       "      <td>0.901584</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.822423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.060137</td>\n",
       "      <td>0.965108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973201</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.143631</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839371</td>\n",
       "      <td>0.728770</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2  minAggR2  \\\n",
       "0   0.000066  0.005145  0.116020  0.969969  0.970118   0.859786  0.000000   \n",
       "1   0.000126  0.005909  0.123098  0.930629  0.947796   0.830341  0.000000   \n",
       "2   0.000060  0.004432  0.090039  0.946155  0.985602   0.867077  0.000000   \n",
       "3   0.000102  0.005248  0.096943  0.921676  0.969216   0.844847  0.000000   \n",
       "4   0.000029  0.002645  0.045589  0.982229  1.000000   0.984872  0.971596   \n",
       "5   0.000191  0.006919  0.108247  0.883445  1.000000   0.906463  0.852468   \n",
       "6   0.001568  0.028773  0.404843 -0.062298  0.636615  -0.099011 -1.363379   \n",
       "7   0.001534  0.028375  0.401506  0.128784  0.679991   0.024153 -1.273531   \n",
       "8   0.000051  0.004035  0.103450  0.988410  0.996774   0.858695  0.000000   \n",
       "9   0.000169  0.006738  0.134394  0.894760  0.953532   0.806173  0.000000   \n",
       "10  0.000058  0.004151  0.076033  0.962339  1.000000   0.967427  0.949307   \n",
       "11  0.000492  0.011683  0.177549  0.682126  1.000000   0.756257  0.623823   \n",
       "12  0.000147  0.007899  0.179982  0.895609  0.979907   0.794434  0.000000   \n",
       "13  0.000704  0.014609  0.258789  0.594724  0.760519   0.528574 -0.028610   \n",
       "14  0.000029  0.003942  0.112464  0.985691  0.993392   0.874710  0.000000   \n",
       "15  0.000419  0.010248  0.185999  0.782280  0.843090   0.681770  0.000000   \n",
       "16  0.000121  0.006070  0.113320  0.887888  1.000000   0.922671  0.875693   \n",
       "17  0.000875  0.016395  0.248540  0.464363  1.000000   0.552008  0.394289   \n",
       "18  0.000329  0.012449  0.236675  0.815571  0.925754   0.707271  0.000000   \n",
       "19  0.000642  0.016082  0.279860  0.642421  0.764840   0.559920  0.000000   \n",
       "20  0.000029  0.003468  0.082520  0.988484  0.989712   0.881166  0.000000   \n",
       "21  0.000275  0.007875  0.143194  0.846093  0.904254   0.760050  0.000000   \n",
       "22  0.000094  0.005642  0.098860  0.930226  1.000000   0.948828  0.920758   \n",
       "23  0.000695  0.014376  0.217814  0.555371  1.000000   0.662096  0.528276   \n",
       "24  0.000316  0.010464  0.196468  0.839426  0.989344   0.671702 -1.086557   \n",
       "25  0.003178  0.035355  0.452047 -0.642210  0.000000  -0.989524 -4.787792   \n",
       "26  0.000018  0.002985  0.097594  0.995855  0.998235   0.876995  0.000000   \n",
       "27  0.001108  0.019115  0.314788  0.423867  0.647423   0.267894 -0.302022   \n",
       "28  0.000201  0.008569  0.155727  0.833617  1.000000   0.865235  0.731517   \n",
       "29  0.001439  0.025206  0.365152  0.264610  1.000000   0.230196 -0.096280   \n",
       "30  0.000184  0.009612  0.210267  0.979214  0.983951   0.804955  0.000000   \n",
       "31  0.001325  0.023347  0.360607  0.306081  0.440696   0.173626 -0.253506   \n",
       "32  0.000012  0.002383  0.084075  0.999611  0.999016   0.890274  0.000000   \n",
       "33  0.000557  0.012689  0.224311  0.748556  0.749903   0.588884  0.000000   \n",
       "34  0.000218  0.008944  0.163396  0.877132  1.000000   0.905750  0.848930   \n",
       "35  0.001323  0.021402  0.297997  0.182627  1.000000   0.312387 -0.087327   \n",
       "36  0.000238  0.009610  0.186417  0.957829  0.977029   0.754140  0.000000   \n",
       "37  0.002262  0.029613  0.429541 -0.219908  0.442841  -0.213224 -1.850683   \n",
       "38  0.000027  0.004120  0.143963  0.995053  0.997183   0.882349  0.000000   \n",
       "39  0.002011  0.027677  0.401612 -0.023768  0.326750  -0.097799 -1.446556   \n",
       "40  0.000383  0.013637  0.254387  0.865691  1.000000   0.830833  0.710988   \n",
       "41  0.001763  0.028615  0.384686  0.089023  1.000000   0.021673 -0.765306   \n",
       "42  0.000181  0.009778  0.204961  0.908847  0.961784   0.799557  0.000000   \n",
       "43  0.000318  0.011565  0.220502  0.828093  0.907163   0.729519  0.000000   \n",
       "44  0.000053  0.004701  0.103358  0.973258  0.983559   0.867992  0.000000   \n",
       "45  0.000146  0.006203  0.118618  0.901584  0.952072   0.822423  0.000000   \n",
       "46  0.000055  0.003633  0.060137  0.965108  1.000000   0.973201  0.959459   \n",
       "47  0.000338  0.009515  0.143631  0.770791  1.000000   0.839371  0.728770   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train         915  \n",
       "1               Fresh   Test         915  \n",
       "2   Transfer learning  Train         915  \n",
       "3   Transfer learning   Test         915  \n",
       "4       Random Forest  Train         915  \n",
       "5       Random Forest   Test         915  \n",
       "6               Fresh  Train         185  \n",
       "7               Fresh   Test         185  \n",
       "8   Transfer learning  Train         185  \n",
       "9   Transfer learning   Test         185  \n",
       "10      Random Forest  Train         185  \n",
       "11      Random Forest   Test         185  \n",
       "12              Fresh  Train          46  \n",
       "13              Fresh   Test          46  \n",
       "14  Transfer learning  Train          46  \n",
       "15  Transfer learning   Test          46  \n",
       "16      Random Forest  Train          46  \n",
       "17      Random Forest   Test          46  \n",
       "18              Fresh  Train          97  \n",
       "19              Fresh   Test          97  \n",
       "20  Transfer learning  Train          97  \n",
       "21  Transfer learning   Test          97  \n",
       "22      Random Forest  Train          97  \n",
       "23      Random Forest   Test          97  \n",
       "24              Fresh  Train           9  \n",
       "25              Fresh   Test           9  \n",
       "26  Transfer learning  Train           9  \n",
       "27  Transfer learning   Test           9  \n",
       "28      Random Forest  Train           9  \n",
       "29      Random Forest   Test           9  \n",
       "30              Fresh  Train          19  \n",
       "31              Fresh   Test          19  \n",
       "32  Transfer learning  Train          19  \n",
       "33  Transfer learning   Test          19  \n",
       "34      Random Forest  Train          19  \n",
       "35      Random Forest   Test          19  \n",
       "36              Fresh  Train           4  \n",
       "37              Fresh   Test           4  \n",
       "38  Transfer learning  Train           4  \n",
       "39  Transfer learning   Test           4  \n",
       "40      Random Forest  Train           4  \n",
       "41      Random Forest   Test           4  \n",
       "42              Fresh  Train         449  \n",
       "43              Fresh   Test         449  \n",
       "44  Transfer learning  Train         449  \n",
       "45  Transfer learning   Test         449  \n",
       "46      Random Forest  Train         449  \n",
       "47      Random Forest   Test         449  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataDirs = glob.glob(os.path.join(dataDir, 'design_7*/'))\n",
    "trainDataDirs.remove(testDir)\n",
    "\n",
    "allResults = []\n",
    "for trainDataDir in trainDataDirs:\n",
    "    trainDataUnfiltered = loadConmechGraphs(trainDataDir)\n",
    "    trainData = filterbyDispValue(trainDataUnfiltered, maxDispCutoff)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=epochs, \n",
    "                         saveDir=saveDir+f'{trainSize:05}/gcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Fresh'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Fresh'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile,\n",
    "                             epochs=epochs, \n",
    "                             saveDir=saveDir+f'{trainSize:05}/ptrGcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Transfer learning'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Transfer learning'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                     saveDir=saveDir+f'{trainSize:05}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['Model'] = 'Random Forest'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['Model'] = 'Random Forest'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.116020</td>\n",
       "      <td>0.969969</td>\n",
       "      <td>0.970118</td>\n",
       "      <td>0.859786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>0.123098</td>\n",
       "      <td>0.930629</td>\n",
       "      <td>0.947796</td>\n",
       "      <td>0.830341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.090039</td>\n",
       "      <td>0.946155</td>\n",
       "      <td>0.985602</td>\n",
       "      <td>0.867077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>0.921676</td>\n",
       "      <td>0.969216</td>\n",
       "      <td>0.844847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.045589</td>\n",
       "      <td>0.982229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984872</td>\n",
       "      <td>0.971596</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.883445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906463</td>\n",
       "      <td>0.852468</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.404843</td>\n",
       "      <td>-0.062298</td>\n",
       "      <td>0.636615</td>\n",
       "      <td>-0.099011</td>\n",
       "      <td>-1.363379</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.028375</td>\n",
       "      <td>0.401506</td>\n",
       "      <td>0.128784</td>\n",
       "      <td>0.679991</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>-1.273531</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.103450</td>\n",
       "      <td>0.988410</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.858695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.134394</td>\n",
       "      <td>0.894760</td>\n",
       "      <td>0.953532</td>\n",
       "      <td>0.806173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.076033</td>\n",
       "      <td>0.962339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967427</td>\n",
       "      <td>0.949307</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.177549</td>\n",
       "      <td>0.682126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756257</td>\n",
       "      <td>0.623823</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.179982</td>\n",
       "      <td>0.895609</td>\n",
       "      <td>0.979907</td>\n",
       "      <td>0.794434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.594724</td>\n",
       "      <td>0.760519</td>\n",
       "      <td>0.528574</td>\n",
       "      <td>-0.028610</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.112464</td>\n",
       "      <td>0.985691</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>0.874710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.185999</td>\n",
       "      <td>0.782280</td>\n",
       "      <td>0.843090</td>\n",
       "      <td>0.681770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.113320</td>\n",
       "      <td>0.887888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922671</td>\n",
       "      <td>0.875693</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.248540</td>\n",
       "      <td>0.464363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.552008</td>\n",
       "      <td>0.394289</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.236675</td>\n",
       "      <td>0.815571</td>\n",
       "      <td>0.925754</td>\n",
       "      <td>0.707271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.279860</td>\n",
       "      <td>0.642421</td>\n",
       "      <td>0.764840</td>\n",
       "      <td>0.559920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.988484</td>\n",
       "      <td>0.989712</td>\n",
       "      <td>0.881166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.143194</td>\n",
       "      <td>0.846093</td>\n",
       "      <td>0.904254</td>\n",
       "      <td>0.760050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>0.930226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948828</td>\n",
       "      <td>0.920758</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>0.217814</td>\n",
       "      <td>0.555371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.662096</td>\n",
       "      <td>0.528276</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.196468</td>\n",
       "      <td>0.839426</td>\n",
       "      <td>0.989344</td>\n",
       "      <td>0.671702</td>\n",
       "      <td>-1.086557</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.035355</td>\n",
       "      <td>0.452047</td>\n",
       "      <td>-0.642210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989524</td>\n",
       "      <td>-4.787792</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.097594</td>\n",
       "      <td>0.995855</td>\n",
       "      <td>0.998235</td>\n",
       "      <td>0.876995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>0.314788</td>\n",
       "      <td>0.423867</td>\n",
       "      <td>0.647423</td>\n",
       "      <td>0.267894</td>\n",
       "      <td>-0.302022</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>0.155727</td>\n",
       "      <td>0.833617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865235</td>\n",
       "      <td>0.731517</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>0.365152</td>\n",
       "      <td>0.264610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230196</td>\n",
       "      <td>-0.096280</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>0.210267</td>\n",
       "      <td>0.979214</td>\n",
       "      <td>0.983951</td>\n",
       "      <td>0.804955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.023347</td>\n",
       "      <td>0.360607</td>\n",
       "      <td>0.306081</td>\n",
       "      <td>0.440696</td>\n",
       "      <td>0.173626</td>\n",
       "      <td>-0.253506</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>0.890274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.224311</td>\n",
       "      <td>0.748556</td>\n",
       "      <td>0.749903</td>\n",
       "      <td>0.588884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>0.877132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905750</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.021402</td>\n",
       "      <td>0.297997</td>\n",
       "      <td>0.182627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312387</td>\n",
       "      <td>-0.087327</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.009610</td>\n",
       "      <td>0.186417</td>\n",
       "      <td>0.957829</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.754140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.029613</td>\n",
       "      <td>0.429541</td>\n",
       "      <td>-0.219908</td>\n",
       "      <td>0.442841</td>\n",
       "      <td>-0.213224</td>\n",
       "      <td>-1.850683</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.143963</td>\n",
       "      <td>0.995053</td>\n",
       "      <td>0.997183</td>\n",
       "      <td>0.882349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.027677</td>\n",
       "      <td>0.401612</td>\n",
       "      <td>-0.023768</td>\n",
       "      <td>0.326750</td>\n",
       "      <td>-0.097799</td>\n",
       "      <td>-1.446556</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.254387</td>\n",
       "      <td>0.865691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.710988</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.384686</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021673</td>\n",
       "      <td>-0.765306</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>0.204961</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.961784</td>\n",
       "      <td>0.799557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.220502</td>\n",
       "      <td>0.828093</td>\n",
       "      <td>0.907163</td>\n",
       "      <td>0.729519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.103358</td>\n",
       "      <td>0.973258</td>\n",
       "      <td>0.983559</td>\n",
       "      <td>0.867992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.118618</td>\n",
       "      <td>0.901584</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.822423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.060137</td>\n",
       "      <td>0.965108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973201</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.143631</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839371</td>\n",
       "      <td>0.728770</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2  minAggR2  \\\n",
       "0   0.000066  0.005145  0.116020  0.969969  0.970118   0.859786  0.000000   \n",
       "1   0.000126  0.005909  0.123098  0.930629  0.947796   0.830341  0.000000   \n",
       "2   0.000060  0.004432  0.090039  0.946155  0.985602   0.867077  0.000000   \n",
       "3   0.000102  0.005248  0.096943  0.921676  0.969216   0.844847  0.000000   \n",
       "4   0.000029  0.002645  0.045589  0.982229  1.000000   0.984872  0.971596   \n",
       "5   0.000191  0.006919  0.108247  0.883445  1.000000   0.906463  0.852468   \n",
       "6   0.001568  0.028773  0.404843 -0.062298  0.636615  -0.099011 -1.363379   \n",
       "7   0.001534  0.028375  0.401506  0.128784  0.679991   0.024153 -1.273531   \n",
       "8   0.000051  0.004035  0.103450  0.988410  0.996774   0.858695  0.000000   \n",
       "9   0.000169  0.006738  0.134394  0.894760  0.953532   0.806173  0.000000   \n",
       "10  0.000058  0.004151  0.076033  0.962339  1.000000   0.967427  0.949307   \n",
       "11  0.000492  0.011683  0.177549  0.682126  1.000000   0.756257  0.623823   \n",
       "12  0.000147  0.007899  0.179982  0.895609  0.979907   0.794434  0.000000   \n",
       "13  0.000704  0.014609  0.258789  0.594724  0.760519   0.528574 -0.028610   \n",
       "14  0.000029  0.003942  0.112464  0.985691  0.993392   0.874710  0.000000   \n",
       "15  0.000419  0.010248  0.185999  0.782280  0.843090   0.681770  0.000000   \n",
       "16  0.000121  0.006070  0.113320  0.887888  1.000000   0.922671  0.875693   \n",
       "17  0.000875  0.016395  0.248540  0.464363  1.000000   0.552008  0.394289   \n",
       "18  0.000329  0.012449  0.236675  0.815571  0.925754   0.707271  0.000000   \n",
       "19  0.000642  0.016082  0.279860  0.642421  0.764840   0.559920  0.000000   \n",
       "20  0.000029  0.003468  0.082520  0.988484  0.989712   0.881166  0.000000   \n",
       "21  0.000275  0.007875  0.143194  0.846093  0.904254   0.760050  0.000000   \n",
       "22  0.000094  0.005642  0.098860  0.930226  1.000000   0.948828  0.920758   \n",
       "23  0.000695  0.014376  0.217814  0.555371  1.000000   0.662096  0.528276   \n",
       "24  0.000316  0.010464  0.196468  0.839426  0.989344   0.671702 -1.086557   \n",
       "25  0.003178  0.035355  0.452047 -0.642210  0.000000  -0.989524 -4.787792   \n",
       "26  0.000018  0.002985  0.097594  0.995855  0.998235   0.876995  0.000000   \n",
       "27  0.001108  0.019115  0.314788  0.423867  0.647423   0.267894 -0.302022   \n",
       "28  0.000201  0.008569  0.155727  0.833617  1.000000   0.865235  0.731517   \n",
       "29  0.001439  0.025206  0.365152  0.264610  1.000000   0.230196 -0.096280   \n",
       "30  0.000184  0.009612  0.210267  0.979214  0.983951   0.804955  0.000000   \n",
       "31  0.001325  0.023347  0.360607  0.306081  0.440696   0.173626 -0.253506   \n",
       "32  0.000012  0.002383  0.084075  0.999611  0.999016   0.890274  0.000000   \n",
       "33  0.000557  0.012689  0.224311  0.748556  0.749903   0.588884  0.000000   \n",
       "34  0.000218  0.008944  0.163396  0.877132  1.000000   0.905750  0.848930   \n",
       "35  0.001323  0.021402  0.297997  0.182627  1.000000   0.312387 -0.087327   \n",
       "36  0.000238  0.009610  0.186417  0.957829  0.977029   0.754140  0.000000   \n",
       "37  0.002262  0.029613  0.429541 -0.219908  0.442841  -0.213224 -1.850683   \n",
       "38  0.000027  0.004120  0.143963  0.995053  0.997183   0.882349  0.000000   \n",
       "39  0.002011  0.027677  0.401612 -0.023768  0.326750  -0.097799 -1.446556   \n",
       "40  0.000383  0.013637  0.254387  0.865691  1.000000   0.830833  0.710988   \n",
       "41  0.001763  0.028615  0.384686  0.089023  1.000000   0.021673 -0.765306   \n",
       "42  0.000181  0.009778  0.204961  0.908847  0.961784   0.799557  0.000000   \n",
       "43  0.000318  0.011565  0.220502  0.828093  0.907163   0.729519  0.000000   \n",
       "44  0.000053  0.004701  0.103358  0.973258  0.983559   0.867992  0.000000   \n",
       "45  0.000146  0.006203  0.118618  0.901584  0.952072   0.822423  0.000000   \n",
       "46  0.000055  0.003633  0.060137  0.965108  1.000000   0.973201  0.959459   \n",
       "47  0.000338  0.009515  0.143631  0.770791  1.000000   0.839371  0.728770   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train         915  \n",
       "1               Fresh   Test         915  \n",
       "2   Transfer learning  Train         915  \n",
       "3   Transfer learning   Test         915  \n",
       "4       Random Forest  Train         915  \n",
       "5       Random Forest   Test         915  \n",
       "6               Fresh  Train         185  \n",
       "7               Fresh   Test         185  \n",
       "8   Transfer learning  Train         185  \n",
       "9   Transfer learning   Test         185  \n",
       "10      Random Forest  Train         185  \n",
       "11      Random Forest   Test         185  \n",
       "12              Fresh  Train          46  \n",
       "13              Fresh   Test          46  \n",
       "14  Transfer learning  Train          46  \n",
       "15  Transfer learning   Test          46  \n",
       "16      Random Forest  Train          46  \n",
       "17      Random Forest   Test          46  \n",
       "18              Fresh  Train          97  \n",
       "19              Fresh   Test          97  \n",
       "20  Transfer learning  Train          97  \n",
       "21  Transfer learning   Test          97  \n",
       "22      Random Forest  Train          97  \n",
       "23      Random Forest   Test          97  \n",
       "24              Fresh  Train           9  \n",
       "25              Fresh   Test           9  \n",
       "26  Transfer learning  Train           9  \n",
       "27  Transfer learning   Test           9  \n",
       "28      Random Forest  Train           9  \n",
       "29      Random Forest   Test           9  \n",
       "30              Fresh  Train          19  \n",
       "31              Fresh   Test          19  \n",
       "32  Transfer learning  Train          19  \n",
       "33  Transfer learning   Test          19  \n",
       "34      Random Forest  Train          19  \n",
       "35      Random Forest   Test          19  \n",
       "36              Fresh  Train           4  \n",
       "37              Fresh   Test           4  \n",
       "38  Transfer learning  Train           4  \n",
       "39  Transfer learning   Test           4  \n",
       "40      Random Forest  Train           4  \n",
       "41      Random Forest   Test           4  \n",
       "42              Fresh  Train         449  \n",
       "43              Fresh   Test         449  \n",
       "44  Transfer learning  Train         449  \n",
       "45  Transfer learning   Test         449  \n",
       "46      Random Forest  Train         449  \n",
       "47      Random Forest   Test         449  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(allResults)\n",
    "df = pd.read_csv('results/transferLrn_endloads_des7_01/testResults.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('Fresh', 'GCN')\n",
    "df = df.replace('Transfer learning', 'GCN with transfer learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-98b0ef0e63eb42a4a4abbd3fe6bbee53\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-98b0ef0e63eb42a4a4abbd3fe6bbee53\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-98b0ef0e63eb42a4a4abbd3fe6bbee53\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-42ed5c43a0859826bf07d00b8f9b5ce8\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Model\"}, {\"type\": \"quantitative\", \"field\": \"mse\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Train Size\", \"scale\": {\"type\": \"log\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".1e\"}, \"field\": \"mse\", \"title\": \"MSE\"}}, \"height\": 200, \"title\": \"Transfer learning accross load cases\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-42ed5c43a0859826bf07d00b8f9b5ce8\": [{\"mse\": 0.0001256786490557715, \"mae\": 0.005909198895096779, \"mre\": 0.12309765815734865, \"peakR2\": 0.9306291829731352, \"maxAggR2\": 0.9477964056611362, \"meanAggR2\": 0.8303405910353713, \"minAggR2\": 0.0, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 915}, {\"mse\": 0.00010210369509877636, \"mae\": 0.005247780587524176, \"mre\": 0.096942737698555, \"peakR2\": 0.9216764917339348, \"maxAggR2\": 0.9692164688272203, \"meanAggR2\": 0.8448473158403186, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 915}, {\"mse\": 0.00019058130168691202, \"mae\": 0.006918820911434421, \"mre\": 0.1082471024458406, \"peakR2\": 0.8834446446085601, \"maxAggR2\": 1.0, \"meanAggR2\": 0.906463094005476, \"minAggR2\": 0.8524678857585588, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 915}, {\"mse\": 0.0015344303101301193, \"mae\": 0.028375474736094475, \"mre\": 0.4015059173107147, \"peakR2\": 0.12878391828120486, \"maxAggR2\": 0.6799914271650228, \"meanAggR2\": 0.024153421565518852, \"minAggR2\": -1.273530838311967, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 185}, {\"mse\": 0.00016928603872656822, \"mae\": 0.0067375553771853456, \"mre\": 0.13439388573169708, \"peakR2\": 0.8947597914372536, \"maxAggR2\": 0.9535319170374436, \"meanAggR2\": 0.806172960024158, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 185}, {\"mse\": 0.0004918996540928559, \"mae\": 0.011683125428302377, \"mre\": 0.1775489892389112, \"peakR2\": 0.6821259018198294, \"maxAggR2\": 1.0, \"meanAggR2\": 0.7562572381247333, \"minAggR2\": 0.6238228170880985, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 185}, {\"mse\": 0.0007041643839329481, \"mae\": 0.014609269797801971, \"mre\": 0.25878918170928955, \"peakR2\": 0.5947240280220094, \"maxAggR2\": 0.7605194785131313, \"meanAggR2\": 0.5285738099821011, \"minAggR2\": -0.028610330322247583, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 46}, {\"mse\": 0.00041858921758830553, \"mae\": 0.010247831232845785, \"mre\": 0.18599942326545715, \"peakR2\": 0.7822803871679594, \"maxAggR2\": 0.8430899028211278, \"meanAggR2\": 0.6817697687434483, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 46}, {\"mse\": 0.0008749625361233459, \"mae\": 0.016394971615147767, \"mre\": 0.2485404074901001, \"peakR2\": 0.4643634604886828, \"maxAggR2\": 1.0, \"meanAggR2\": 0.5520076231704601, \"minAggR2\": 0.39428921242221904, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 46}, {\"mse\": 0.0006417802651412785, \"mae\": 0.016082115471363068, \"mre\": 0.2798598408699036, \"peakR2\": 0.6424211179370092, \"maxAggR2\": 0.7648404516623847, \"meanAggR2\": 0.559919804082698, \"minAggR2\": 0.0, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 97}, {\"mse\": 0.0002753332373686135, \"mae\": 0.007874848321080208, \"mre\": 0.1431935876607895, \"peakR2\": 0.8460925980425186, \"maxAggR2\": 0.9042538571675958, \"meanAggR2\": 0.7600498906452806, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 97}, {\"mse\": 0.0006947030272961537, \"mae\": 0.014376370480189844, \"mre\": 0.2178143516618095, \"peakR2\": 0.5553712243421931, \"maxAggR2\": 1.0, \"meanAggR2\": 0.6620962919415089, \"minAggR2\": 0.5282762630294153, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 97}, {\"mse\": 0.0031782451551407576, \"mae\": 0.03535537049174309, \"mre\": 0.4520472586154938, \"peakR2\": -0.6422099585816683, \"maxAggR2\": 0.0, \"meanAggR2\": -0.9895244893866746, \"minAggR2\": -4.787791513109514, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 0.0011075480142608283, \"mae\": 0.019114892929792404, \"mre\": 0.3147876858711243, \"peakR2\": 0.4238672675165153, \"maxAggR2\": 0.6474234063029444, \"meanAggR2\": 0.2678943752791555, \"minAggR2\": -0.3020220030159133, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 0.0014391396487947851, \"mae\": 0.025205892821836613, \"mre\": 0.3651519070509125, \"peakR2\": 0.26461011278411417, \"maxAggR2\": 1.0, \"meanAggR2\": 0.23019578731218554, \"minAggR2\": -0.09627970330660296, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 0.0013246779562905429, \"mae\": 0.023346515372395515, \"mre\": 0.3606074154376984, \"peakR2\": 0.3060813732271773, \"maxAggR2\": 0.44069604272638985, \"meanAggR2\": 0.17362609766877818, \"minAggR2\": -0.2535060742895556, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 19}, {\"mse\": 0.000557377003133297, \"mae\": 0.012689067982137205, \"mre\": 0.22431088984012604, \"peakR2\": 0.7485561697588756, \"maxAggR2\": 0.7499030715510815, \"meanAggR2\": 0.5888838315969257, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 19}, {\"mse\": 0.0013225317676061707, \"mae\": 0.0214016565386052, \"mre\": 0.2979969167782425, \"peakR2\": 0.18262659589005087, \"maxAggR2\": 1.0, \"meanAggR2\": 0.31238677406206306, \"minAggR2\": -0.08732686804647516, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 19}, {\"mse\": 0.002261706395074725, \"mae\": 0.029613457620143887, \"mre\": 0.4295410215854645, \"peakR2\": -0.21990835567035605, \"maxAggR2\": 0.4428407216201565, \"meanAggR2\": -0.21322392029136375, \"minAggR2\": -1.8506826059365655, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 0.0020113529171794653, \"mae\": 0.027677072212100032, \"mre\": 0.401611864566803, \"peakR2\": -0.02376848570475154, \"maxAggR2\": 0.3267500815618921, \"meanAggR2\": -0.09779865761056444, \"minAggR2\": -1.446556168233323, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 0.0017630501753962218, \"mae\": 0.028615033757927913, \"mre\": 0.38468644886403824, \"peakR2\": 0.08902251744676959, \"maxAggR2\": 1.0, \"meanAggR2\": 0.02167309282611026, \"minAggR2\": -0.7653058874278948, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 0.0003181106876581908, \"mae\": 0.011565256863832474, \"mre\": 0.22050225734710693, \"peakR2\": 0.8280926497723198, \"maxAggR2\": 0.9071632173556096, \"meanAggR2\": 0.7295187672529543, \"minAggR2\": 0.0, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 449}, {\"mse\": 0.00014642131282016635, \"mae\": 0.006202542688697577, \"mre\": 0.11861805617809296, \"peakR2\": 0.9015843691902882, \"maxAggR2\": 0.952072080548202, \"meanAggR2\": 0.8224229096396846, \"minAggR2\": 0.0, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 449}, {\"mse\": 0.0003379642096772886, \"mae\": 0.009515000937876, \"mre\": 0.14363127505800669, \"peakR2\": 0.7707913120824144, \"maxAggR2\": 1.0, \"meanAggR2\": 0.8393705431988624, \"minAggR2\": 0.7287700995070631, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 449}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df.Set=='Test']).mark_circle().encode(\n",
    "    x=alt.X('Train Size:Q', scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('mse:Q', title='MSE', axis=alt.Axis(format='.1e')),\n",
    "    color='Model',\n",
    "    tooltip=['Model', 'mse']\n",
    ").properties(width=400, height=200, title='Transfer learning accross load cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
