{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "sys.path.append('./models')\n",
    "from feastnetSurrogateModel import FeaStNet\n",
    "from pointRegressorSurrogateModel import PointRegressor\n",
    "\n",
    "sys.path.append('./readers')\n",
    "from loadGhGraphs import loadGhGraphs\n",
    "\n",
    "sys.path.append('./visualization')\n",
    "from altTrussViz import plotTruss, interactiveErrorPlot\n",
    "\n",
    "sys.path.append('./util')\n",
    "from gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.199516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.524026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.046981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67.325867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.199516\n",
       "std       2.524026\n",
       "min       0.006570\n",
       "25%       0.015425\n",
       "50%       0.024334\n",
       "75%       0.046981\n",
       "max      67.325867"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "testFile = os.path.join(dataDir, 'design_7_N_1000.csv')\n",
    "allGraphsUnfiltered = loadGhGraphs(testFile, NUM_DV=5)\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.028952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.014951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.021719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.038367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.097861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.028952\n",
       "std      0.019071\n",
       "min      0.006570\n",
       "25%      0.014951\n",
       "50%      0.021719\n",
       "75%      0.038367\n",
       "max      0.097861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_9\n",
      "loading design_6\n",
      "loading design_8\n",
      "loading design_5\n",
      "loaded 3600 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainFiles = glob.glob(os.path.join(dataDir, '*1000.csv'))\n",
    "pretrainFiles.remove(testFile)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainFile in pretrainFiles:\n",
    "    designName = pretrainFile.split('/')[-1].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    graphsUnfiltered = loadGhGraphs(pretrainFile, NUM_DV=5)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 9.0857e-01   valLoss:9.2913e-01  time: 5.29e+00\n",
      "epoch: 1   trainLoss: 7.1825e-01   valLoss:8.8634e-01  time: 4.94e+00\n",
      "epoch: 2   trainLoss: 5.9192e-01   valLoss:5.0052e-01  time: 4.95e+00\n",
      "epoch: 3   trainLoss: 4.8664e-01   valLoss:4.2644e-01  time: 4.95e+00\n",
      "epoch: 4   trainLoss: 4.1146e-01   valLoss:3.7613e-01  time: 4.99e+00\n",
      "epoch: 5   trainLoss: 3.5204e-01   valLoss:3.3834e-01  time: 5.01e+00\n",
      "epoch: 6   trainLoss: 3.1701e-01   valLoss:3.0320e-01  time: 5.02e+00\n",
      "epoch: 7   trainLoss: 2.8547e-01   valLoss:2.7650e-01  time: 4.99e+00\n",
      "epoch: 8   trainLoss: 2.5552e-01   valLoss:2.4898e-01  time: 5.09e+00\n",
      "epoch: 9   trainLoss: 2.3239e-01   valLoss:2.3264e-01  time: 4.99e+00\n",
      "epoch: 10   trainLoss: 2.1400e-01   valLoss:2.1519e-01  time: 4.98e+00\n",
      "epoch: 11   trainLoss: 1.9565e-01   valLoss:2.0955e-01  time: 4.98e+00\n",
      "epoch: 12   trainLoss: 1.7706e-01   valLoss:1.9891e-01  time: 5.01e+00\n",
      "epoch: 13   trainLoss: 1.6325e-01   valLoss:1.8650e-01  time: 4.99e+00\n",
      "epoch: 14   trainLoss: 1.5570e-01   valLoss:1.7608e-01  time: 5.00e+00\n",
      "epoch: 15   trainLoss: 1.5009e-01   valLoss:1.7233e-01  time: 4.98e+00\n",
      "epoch: 16   trainLoss: 1.3894e-01   valLoss:1.7883e-01  time: 4.98e+00\n",
      "epoch: 17   trainLoss: 1.3383e-01   valLoss:1.7225e-01  time: 5.00e+00\n",
      "epoch: 18   trainLoss: 1.2863e-01   valLoss:1.5090e-01  time: 4.99e+00\n",
      "epoch: 19   trainLoss: 1.2170e-01   valLoss:1.5736e-01  time: 4.98e+00\n",
      "epoch: 20   trainLoss: 1.2360e-01   valLoss:1.6195e-01  time: 4.98e+00\n",
      "epoch: 21   trainLoss: 1.1050e-01   valLoss:1.3359e-01  time: 4.97e+00\n",
      "epoch: 22   trainLoss: 1.0011e-01   valLoss:1.2515e-01  time: 5.00e+00\n",
      "epoch: 23   trainLoss: 1.0015e-01   valLoss:1.3714e-01  time: 5.00e+00\n",
      "epoch: 24   trainLoss: 9.8176e-02   valLoss:1.1581e-01  time: 5.03e+00\n",
      "epoch: 25   trainLoss: 9.4853e-02   valLoss:1.3203e-01  time: 4.97e+00\n",
      "epoch: 26   trainLoss: 9.4128e-02   valLoss:1.1831e-01  time: 5.07e+00\n",
      "epoch: 27   trainLoss: 8.6261e-02   valLoss:1.1650e-01  time: 4.99e+00\n",
      "epoch: 28   trainLoss: 8.6933e-02   valLoss:1.2546e-01  time: 4.98e+00\n",
      "epoch: 29   trainLoss: 8.7430e-02   valLoss:1.2012e-01  time: 4.99e+00\n",
      "epoch: 30   trainLoss: 8.4179e-02   valLoss:1.1669e-01  time: 5.03e+00\n",
      "epoch: 31   trainLoss: 8.2463e-02   valLoss:1.2200e-01  time: 5.02e+00\n",
      "epoch: 32   trainLoss: 8.3175e-02   valLoss:1.1927e-01  time: 5.01e+00\n",
      "epoch: 33   trainLoss: 7.8871e-02   valLoss:1.0242e-01  time: 4.98e+00\n",
      "epoch: 34   trainLoss: 7.5146e-02   valLoss:1.1462e-01  time: 5.01e+00\n",
      "epoch: 35   trainLoss: 7.4749e-02   valLoss:1.0045e-01  time: 5.10e+00\n",
      "epoch: 36   trainLoss: 7.5374e-02   valLoss:1.0411e-01  time: 5.01e+00\n",
      "epoch: 37   trainLoss: 7.2623e-02   valLoss:1.0687e-01  time: 5.00e+00\n",
      "epoch: 38   trainLoss: 7.2912e-02   valLoss:1.2179e-01  time: 4.98e+00\n",
      "epoch: 39   trainLoss: 7.1231e-02   valLoss:1.0786e-01  time: 5.00e+00\n",
      "epoch: 40   trainLoss: 7.0073e-02   valLoss:1.1054e-01  time: 5.02e+00\n",
      "epoch: 41   trainLoss: 7.8388e-02   valLoss:8.7636e-02  time: 5.01e+00\n",
      "epoch: 42   trainLoss: 6.5088e-02   valLoss:1.0069e-01  time: 4.99e+00\n",
      "epoch: 43   trainLoss: 6.5064e-02   valLoss:9.6115e-02  time: 5.00e+00\n",
      "epoch: 44   trainLoss: 6.1435e-02   valLoss:8.6558e-02  time: 5.14e+00\n",
      "epoch: 45   trainLoss: 6.0656e-02   valLoss:9.1462e-02  time: 5.02e+00\n",
      "epoch: 46   trainLoss: 6.2548e-02   valLoss:8.2815e-02  time: 5.06e+00\n",
      "epoch: 47   trainLoss: 6.0797e-02   valLoss:7.8470e-02  time: 5.07e+00\n",
      "epoch: 48   trainLoss: 5.9522e-02   valLoss:9.4994e-02  time: 5.05e+00\n",
      "epoch: 49   trainLoss: 6.2874e-02   valLoss:9.2549e-02  time: 5.01e+00\n",
      "epoch: 50   trainLoss: 5.9828e-02   valLoss:8.7183e-02  time: 5.02e+00\n",
      "epoch: 51   trainLoss: 5.8606e-02   valLoss:7.5831e-02  time: 5.03e+00\n",
      "epoch: 52   trainLoss: 6.3379e-02   valLoss:8.3891e-02  time: 5.07e+00\n",
      "epoch: 53   trainLoss: 5.4066e-02   valLoss:8.7253e-02  time: 5.16e+00\n",
      "epoch: 54   trainLoss: 5.5609e-02   valLoss:7.9616e-02  time: 5.06e+00\n",
      "epoch: 55   trainLoss: 5.4183e-02   valLoss:7.6838e-02  time: 5.05e+00\n",
      "epoch: 56   trainLoss: 5.2819e-02   valLoss:8.2488e-02  time: 5.07e+00\n",
      "epoch: 57   trainLoss: 5.4394e-02   valLoss:8.2749e-02  time: 5.14e+00\n",
      "epoch: 58   trainLoss: 5.1387e-02   valLoss:6.1288e-02  time: 5.03e+00\n",
      "epoch: 59   trainLoss: 5.2608e-02   valLoss:7.9942e-02  time: 4.97e+00\n",
      "epoch: 60   trainLoss: 5.3819e-02   valLoss:7.4363e-02  time: 4.99e+00\n",
      "epoch: 61   trainLoss: 4.9024e-02   valLoss:7.3280e-02  time: 4.95e+00\n",
      "epoch: 62   trainLoss: 5.0061e-02   valLoss:7.6223e-02  time: 5.09e+00\n",
      "epoch: 63   trainLoss: 5.2924e-02   valLoss:7.4738e-02  time: 4.94e+00\n",
      "epoch: 64   trainLoss: 5.3054e-02   valLoss:6.9865e-02  time: 5.04e+00\n",
      "epoch: 65   trainLoss: 4.9727e-02   valLoss:7.5395e-02  time: 4.90e+00\n",
      "epoch: 66   trainLoss: 4.8600e-02   valLoss:7.3916e-02  time: 4.96e+00\n",
      "epoch: 67   trainLoss: 4.7506e-02   valLoss:6.6676e-02  time: 4.91e+00\n",
      "epoch: 68   trainLoss: 4.4842e-02   valLoss:7.0259e-02  time: 4.90e+00\n",
      "epoch: 69   trainLoss: 5.4413e-02   valLoss:5.9558e-02  time: 4.92e+00\n",
      "epoch: 70   trainLoss: 4.6489e-02   valLoss:6.3190e-02  time: 4.91e+00\n",
      "epoch: 71   trainLoss: 4.6747e-02   valLoss:6.9321e-02  time: 5.00e+00\n",
      "epoch: 72   trainLoss: 4.4258e-02   valLoss:5.4907e-02  time: 4.91e+00\n",
      "epoch: 73   trainLoss: 4.6226e-02   valLoss:6.9774e-02  time: 4.88e+00\n",
      "epoch: 74   trainLoss: 5.0908e-02   valLoss:7.6725e-02  time: 4.89e+00\n",
      "epoch: 75   trainLoss: 4.6855e-02   valLoss:7.1044e-02  time: 4.90e+00\n",
      "epoch: 76   trainLoss: 4.6851e-02   valLoss:6.5295e-02  time: 4.87e+00\n",
      "epoch: 77   trainLoss: 4.8095e-02   valLoss:6.7056e-02  time: 4.87e+00\n",
      "epoch: 78   trainLoss: 5.9364e-02   valLoss:7.8470e-02  time: 4.87e+00\n",
      "epoch: 79   trainLoss: 4.5566e-02   valLoss:6.9960e-02  time: 4.91e+00\n",
      "epoch: 80   trainLoss: 4.7451e-02   valLoss:5.8456e-02  time: 4.90e+00\n",
      "epoch: 81   trainLoss: 4.6137e-02   valLoss:6.3628e-02  time: 4.95e+00\n",
      "epoch: 82   trainLoss: 4.8846e-02   valLoss:7.8189e-02  time: 4.88e+00\n",
      "epoch: 83   trainLoss: 4.0095e-02   valLoss:6.4404e-02  time: 4.90e+00\n",
      "epoch: 84   trainLoss: 3.9391e-02   valLoss:5.6457e-02  time: 4.87e+00\n",
      "epoch: 85   trainLoss: 4.2078e-02   valLoss:4.8805e-02  time: 4.86e+00\n",
      "epoch: 86   trainLoss: 4.2816e-02   valLoss:7.0659e-02  time: 4.84e+00\n",
      "epoch: 87   trainLoss: 4.0478e-02   valLoss:5.3190e-02  time: 4.84e+00\n",
      "epoch: 88   trainLoss: 4.3439e-02   valLoss:7.0764e-02  time: 4.83e+00\n",
      "epoch: 89   trainLoss: 4.3254e-02   valLoss:5.3272e-02  time: 4.85e+00\n",
      "epoch: 90   trainLoss: 4.1287e-02   valLoss:6.1122e-02  time: 4.84e+00\n",
      "epoch: 91   trainLoss: 4.5943e-02   valLoss:5.8230e-02  time: 4.99e+00\n",
      "epoch: 92   trainLoss: 3.6826e-02   valLoss:5.1011e-02  time: 4.88e+00\n",
      "epoch: 93   trainLoss: 4.2593e-02   valLoss:5.5292e-02  time: 4.87e+00\n",
      "epoch: 94   trainLoss: 3.9445e-02   valLoss:4.8850e-02  time: 4.85e+00\n",
      "epoch: 95   trainLoss: 3.8728e-02   valLoss:5.6088e-02  time: 4.90e+00\n",
      "epoch: 96   trainLoss: 3.8614e-02   valLoss:5.0143e-02  time: 4.93e+00\n",
      "epoch: 97   trainLoss: 3.5896e-02   valLoss:5.8696e-02  time: 4.90e+00\n",
      "epoch: 98   trainLoss: 3.7828e-02   valLoss:5.2069e-02  time: 4.89e+00\n",
      "epoch: 99   trainLoss: 3.8893e-02   valLoss:5.6802e-02  time: 4.90e+00\n",
      "loading checkpoint 85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-31f4b996dbd040669e98e3e814d4a637\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-31f4b996dbd040669e98e3e814d4a637\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-31f4b996dbd040669e98e3e814d4a637\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-702ad2ff17b8f47a0d2ec7e138734fe3\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-702ad2ff17b8f47a0d2ec7e138734fe3\": [{\"train\": 0.9085740049680074, \"val\": 0.9291322387892891, \"epoch\": 0}, {\"train\": 0.7182507415612539, \"val\": 0.886335687863606, \"epoch\": 1}, {\"train\": 0.591922735174497, \"val\": 0.5005183708664306, \"epoch\": 2}, {\"train\": 0.4866370012362798, \"val\": 0.42644165832963254, \"epoch\": 3}, {\"train\": 0.4114631364742915, \"val\": 0.37612878016230683, \"epoch\": 4}, {\"train\": 0.352035087843736, \"val\": 0.33834391540709746, \"epoch\": 5}, {\"train\": 0.3170146991809209, \"val\": 0.30319684422919874, \"epoch\": 6}, {\"train\": 0.285472109913826, \"val\": 0.2765035581809503, \"epoch\": 7}, {\"train\": 0.2555213086307049, \"val\": 0.2489828081594573, \"epoch\": 8}, {\"train\": 0.23239212358991304, \"val\": 0.23264209561187912, \"epoch\": 9}, {\"train\": 0.21400463953614235, \"val\": 0.21518945201489798, \"epoch\": 10}, {\"train\": 0.1956473315755526, \"val\": 0.20954504568664425, \"epoch\": 11}, {\"train\": 0.17706244563062987, \"val\": 0.19891402349997037, \"epoch\": 12}, {\"train\": 0.163253802806139, \"val\": 0.18649910448326004, \"epoch\": 13}, {\"train\": 0.15569836335877577, \"val\": 0.17607702759646432, \"epoch\": 14}, {\"train\": 0.15008876410623392, \"val\": 0.17232860202461275, \"epoch\": 15}, {\"train\": 0.13893551751971245, \"val\": 0.17883377353815982, \"epoch\": 16}, {\"train\": 0.13383011581997076, \"val\": 0.17225296735392745, \"epoch\": 17}, {\"train\": 0.12862678430974483, \"val\": 0.15090360820759088, \"epoch\": 18}, {\"train\": 0.12169828327993552, \"val\": 0.157360877587322, \"epoch\": 19}, {\"train\": 0.12359557611246903, \"val\": 0.1619519355568897, \"epoch\": 20}, {\"train\": 0.11050174261132877, \"val\": 0.13358900610236796, \"epoch\": 21}, {\"train\": 0.10010555448631446, \"val\": 0.12514745214462486, \"epoch\": 22}, {\"train\": 0.1001480259001255, \"val\": 0.13714167383344222, \"epoch\": 23}, {\"train\": 0.09817633343239625, \"val\": 0.11580988210367246, \"epoch\": 24}, {\"train\": 0.09485348748664062, \"val\": 0.13202977691587336, \"epoch\": 25}, {\"train\": 0.09412758859495322, \"val\": 0.11830782809562085, \"epoch\": 26}, {\"train\": 0.08626142796128988, \"val\": 0.116499308387197, \"epoch\": 27}, {\"train\": 0.08693293606241544, \"val\": 0.1254556142380116, \"epoch\": 28}, {\"train\": 0.08743025735020638, \"val\": 0.12012265087320918, \"epoch\": 29}, {\"train\": 0.08417857500414054, \"val\": 0.11668729062226635, \"epoch\": 30}, {\"train\": 0.08246324770152569, \"val\": 0.12199610168830043, \"epoch\": 31}, {\"train\": 0.08317485110213359, \"val\": 0.11926744463249903, \"epoch\": 32}, {\"train\": 0.07887143238137166, \"val\": 0.1024173351203057, \"epoch\": 33}, {\"train\": 0.0751463146880269, \"val\": 0.11461770039017277, \"epoch\": 34}, {\"train\": 0.0747491242364049, \"val\": 0.1004458273295313, \"epoch\": 35}, {\"train\": 0.07537358440458775, \"val\": 0.10411108398327121, \"epoch\": 36}, {\"train\": 0.07262266272058089, \"val\": 0.10686819380002648, \"epoch\": 37}, {\"train\": 0.07291221494476001, \"val\": 0.12179361592128839, \"epoch\": 38}, {\"train\": 0.07123071948687236, \"val\": 0.10785826562032863, \"epoch\": 39}, {\"train\": 0.07007292627046506, \"val\": 0.11053944403107313, \"epoch\": 40}, {\"train\": 0.07838797786583503, \"val\": 0.08763614443035934, \"epoch\": 41}, {\"train\": 0.06508782412856817, \"val\": 0.10069463400006363, \"epoch\": 42}, {\"train\": 0.06506350977967183, \"val\": 0.09611473827433117, \"epoch\": 43}, {\"train\": 0.06143500500669082, \"val\": 0.08655817916129578, \"epoch\": 44}, {\"train\": 0.06065579286466042, \"val\": 0.09146219067301395, \"epoch\": 45}, {\"train\": 0.06254809939612944, \"val\": 0.0828146518456008, \"epoch\": 46}, {\"train\": 0.060797158939143024, \"val\": 0.07847021938672427, \"epoch\": 47}, {\"train\": 0.05952173316230377, \"val\": 0.09499417408886883, \"epoch\": 48}, {\"train\": 0.06287403653065364, \"val\": 0.09254926061063694, \"epoch\": 49}, {\"train\": 0.05982802094270786, \"val\": 0.08718332343226022, \"epoch\": 50}, {\"train\": 0.0586063734566172, \"val\": 0.07583123117693942, \"epoch\": 51}, {\"train\": 0.06337912598003943, \"val\": 0.08389101205458348, \"epoch\": 52}, {\"train\": 0.05406558824082216, \"val\": 0.0872531443349241, \"epoch\": 53}, {\"train\": 0.05560859075436989, \"val\": 0.07961551633327164, \"epoch\": 54}, {\"train\": 0.054183349634210266, \"val\": 0.07683800107808093, \"epoch\": 55}, {\"train\": 0.052819351044793926, \"val\": 0.08248795113500415, \"epoch\": 56}, {\"train\": 0.054394462456305824, \"val\": 0.08274906021521289, \"epoch\": 57}, {\"train\": 0.05138659322013458, \"val\": 0.06128784610773437, \"epoch\": 58}, {\"train\": 0.05260754811267058, \"val\": 0.07994178441866231, \"epoch\": 59}, {\"train\": 0.05381935679664215, \"val\": 0.07436312853996814, \"epoch\": 60}, {\"train\": 0.04902404608825842, \"val\": 0.07328032396725137, \"epoch\": 61}, {\"train\": 0.050060722045600414, \"val\": 0.07622256799734681, \"epoch\": 62}, {\"train\": 0.05292419716715813, \"val\": 0.07473848444146657, \"epoch\": 63}, {\"train\": 0.05305449509372314, \"val\": 0.06986492083006952, \"epoch\": 64}, {\"train\": 0.049727250511447586, \"val\": 0.07539510487048473, \"epoch\": 65}, {\"train\": 0.048599629352490105, \"val\": 0.07391582490634863, \"epoch\": 66}, {\"train\": 0.04750595645358165, \"val\": 0.06667590322322212, \"epoch\": 67}, {\"train\": 0.04484197497367859, \"val\": 0.07025894187692622, \"epoch\": 68}, {\"train\": 0.054413133300840855, \"val\": 0.05955802588486458, \"epoch\": 69}, {\"train\": 0.04648903695245584, \"val\": 0.06318950208714577, \"epoch\": 70}, {\"train\": 0.04674672707915306, \"val\": 0.06932076981110084, \"epoch\": 71}, {\"train\": 0.04425768398990234, \"val\": 0.05490701390253552, \"epoch\": 72}, {\"train\": 0.04622626335670551, \"val\": 0.06977413647648602, \"epoch\": 73}, {\"train\": 0.050908115381995835, \"val\": 0.07672524797445578, \"epoch\": 74}, {\"train\": 0.04685481172055006, \"val\": 0.07104368898715754, \"epoch\": 75}, {\"train\": 0.046851117784778275, \"val\": 0.06529474768839362, \"epoch\": 76}, {\"train\": 0.04809534549713135, \"val\": 0.06705581349971773, \"epoch\": 77}, {\"train\": 0.05936438745508591, \"val\": 0.07847014787322325, \"epoch\": 78}, {\"train\": 0.04556647269055247, \"val\": 0.06996023920774287, \"epoch\": 79}, {\"train\": 0.047450667868057884, \"val\": 0.05845564963938496, \"epoch\": 80}, {\"train\": 0.04613652235517899, \"val\": 0.06362773501917858, \"epoch\": 81}, {\"train\": 0.048845996148884296, \"val\": 0.07818867903185525, \"epoch\": 82}, {\"train\": 0.04009507643058896, \"val\": 0.06440440295113216, \"epoch\": 83}, {\"train\": 0.039391429318735995, \"val\": 0.056457309635287084, \"epoch\": 84}, {\"train\": 0.042078004529078804, \"val\": 0.04880450364887818, \"epoch\": 85}, {\"train\": 0.04281553719192743, \"val\": 0.07065937085491088, \"epoch\": 86}, {\"train\": 0.04047807569925984, \"val\": 0.05318955307054609, \"epoch\": 87}, {\"train\": 0.04343924609323343, \"val\": 0.07076369371088394, \"epoch\": 88}, {\"train\": 0.04325395077466965, \"val\": 0.053272026149487055, \"epoch\": 89}, {\"train\": 0.04128651351978382, \"val\": 0.06112188734135728, \"epoch\": 90}, {\"train\": 0.0459430826207002, \"val\": 0.05823043052676237, \"epoch\": 91}, {\"train\": 0.0368258369465669, \"val\": 0.05101086703458956, \"epoch\": 92}, {\"train\": 0.04259270646919807, \"val\": 0.05529157781318106, \"epoch\": 93}, {\"train\": 0.03944472270086408, \"val\": 0.04885038473294116, \"epoch\": 94}, {\"train\": 0.03872779912004868, \"val\": 0.05608760647807719, \"epoch\": 95}, {\"train\": 0.03861416441698869, \"val\": 0.050143196801137595, \"epoch\": 96}, {\"train\": 0.03589599020779133, \"val\": 0.05869577192546179, \"epoch\": 97}, {\"train\": 0.03782805552085241, \"val\": 0.05206862607457744, \"epoch\": 98}, {\"train\": 0.03889335536708435, \"val\": 0.056801686596871286, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = './results/transferLrn_des7_01/'\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                         epochs=100, \n",
    "                         batch_size=256, \n",
    "                         flatten=True, \n",
    "                         logTrans=False, \n",
    "                         ssTrans=True, \n",
    "                         saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.113388</td>\n",
       "      <td>0.93765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.171898</td>\n",
       "      <td>0.66472</td>\n",
       "      <td>0.879246</td>\n",
       "      <td>-0.739851</td>\n",
       "      <td>-21.937541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre   peakR2  maxAggR2  meanAggR2   minAggR2\n",
       "train  0.000026  0.003139  0.113388  0.93765       NaN        NaN        NaN\n",
       "test   0.000043  0.004044  0.171898  0.66472  0.879246  -0.739851 -21.937541"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-36b31bd7d079>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-36b31bd7d079>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    epochs=3,\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "trainDataFiles = glob.glob(os.path.join(dataDir, 'design_7*.csv'))\n",
    "trainDataFiles.remove(testFile)\n",
    "\n",
    "allResults = []\n",
    "for trainDataFile in trainDataFiles:\n",
    "    trainData = loadGhGraphs(trainDataFile, NUM_DV=5)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=3, \n",
    "                         batch_size=256, \n",
    "                         flatten=True, \n",
    "                         logTrans=False, \n",
    "                         ssTrans=True, \n",
    "                         saveDir=saveDir+f'{trainSize}/gcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['model'] = 'Fresh'\n",
    "    trainRes['set'] = 'Train'\n",
    "    trainRes['train size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['model'] = 'Fresh'\n",
    "    testRes['set'] = 'Test'\n",
    "    testRes['train size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile\n",
    "                             epochs=3, \n",
    "                             batch_size=256, \n",
    "                             flatten=True, \n",
    "                             logTrans=False, \n",
    "                             ssTrans=True, \n",
    "                             saveDir=saveDir+f'{trainSize}/ptrGcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['model'] = 'Transfer learning'\n",
    "    trainRes['set'] = 'Train'\n",
    "    trainRes['train size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['model'] = 'Transfer learning'\n",
    "    testRes['set'] = 'Test'\n",
    "    testRes['train size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                     flatten=False, \n",
    "                     logTrans=False, \n",
    "                     ssTrans=True, \n",
    "                     saveDir=saveDir+f'{trainSize}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['model'] = 'Random Forest'\n",
    "    trainRes['set'] = 'Train'\n",
    "    trainRes['train size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['model'] = 'Random Forest'\n",
    "    testRes['set'] = 'Test'\n",
    "    testRes['train size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(allResults)\n",
    "df[df.set=='Test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
