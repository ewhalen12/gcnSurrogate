{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from gcnSurrogate.models.feastnetSurrogateModel import FeaStNet\n",
    "from gcnSurrogate.models.pointRegressorSurrogateModel import PointRegressor\n",
    "from gcnSurrogate.readers.loadConmechGraphs import loadConmechGraphs\n",
    "from gcnSurrogate.visualization.altTrussViz import plotTruss, interactiveErrorPlot\n",
    "from gcnSurrogate.util.gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.027434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.022934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.028404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.065064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.027434\n",
       "std       0.005930\n",
       "min       0.022934\n",
       "25%       0.023780\n",
       "50%       0.025250\n",
       "75%       0.028404\n",
       "max       0.065064"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataDir = \"data/2D_Truss_v1.3/conmech/\"\n",
    "# testDir = os.path.join(dataDir, 'design_7_N_1000/')\n",
    "# allGraphsUnfiltered = loadConmechGraphs(testDir)\n",
    "\n",
    "# dataDir = \"data/endLoadsv1.0/conmech/\"\n",
    "# testDir = os.path.join(dataDir, 'design_7_N_1000/')\n",
    "# allGraphsUnfiltered = loadConmechGraphs(testDir)\n",
    "\n",
    "dataDir = \"data/tower1.0/conmech/\"\n",
    "testDir = os.path.join(dataDir, 'design_5_N_1000/')\n",
    "allGraphsUnfiltered = loadConmechGraphs(testDir, loadDims=[0])\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.025836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.022934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.027176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.034477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.025836\n",
       "std      0.002804\n",
       "min      0.022934\n",
       "25%      0.023687\n",
       "50%      0.024775\n",
       "75%      0.027176\n",
       "max      0.034477"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max().item()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from data/2D_Truss_v1.3/conmech/design_7_N_1000/\n",
      "loaded 900 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainDirs = glob.glob('data/2D_Truss_v1.3/conmech/design_7_N_1000/')\n",
    "\n",
    "# pretrainDirs = glob.glob(os.path.join(dataDir, '*1000/'))\n",
    "# pretrainDirs.remove(testDir)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainDir in pretrainDirs:\n",
    "    designName = pretrainDir.split('/')[-2].split('_N')[0]\n",
    "    print(f'loading from {pretrainDir}')\n",
    "    graphsUnfiltered = loadConmechGraphs(pretrainDir)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 9.3096e-01   valLoss:8.6974e-01  time: 2.16e+00\n",
      "epoch: 1   trainLoss: 6.8632e-01   valLoss:8.5991e-01  time: 1.77e+00\n",
      "epoch: 2   trainLoss: 5.3786e-01   valLoss:9.4513e-01  time: 1.78e+00\n",
      "epoch: 3   trainLoss: 4.4543e-01   valLoss:1.7662e+00  time: 1.79e+00\n",
      "epoch: 4   trainLoss: 3.6750e-01   valLoss:4.4537e+00  time: 1.77e+00\n",
      "epoch: 5   trainLoss: 3.0831e-01   valLoss:8.1462e+00  time: 1.77e+00\n",
      "epoch: 6   trainLoss: 2.5948e-01   valLoss:9.3518e+00  time: 1.76e+00\n",
      "epoch: 7   trainLoss: 2.3203e-01   valLoss:5.0187e+00  time: 1.76e+00\n",
      "epoch: 8   trainLoss: 1.9679e-01   valLoss:2.5170e+00  time: 1.78e+00\n",
      "epoch: 9   trainLoss: 1.8080e-01   valLoss:1.2414e+00  time: 1.78e+00\n",
      "epoch: 10   trainLoss: 1.6973e-01   valLoss:1.0225e+00  time: 1.77e+00\n",
      "epoch: 11   trainLoss: 1.4591e-01   valLoss:6.2598e-01  time: 1.78e+00\n",
      "epoch: 12   trainLoss: 1.4978e-01   valLoss:7.4784e-01  time: 1.76e+00\n",
      "epoch: 13   trainLoss: 1.6399e-01   valLoss:1.1204e+00  time: 1.75e+00\n",
      "epoch: 14   trainLoss: 1.3983e-01   valLoss:8.3557e-01  time: 1.75e+00\n",
      "epoch: 15   trainLoss: 1.4117e-01   valLoss:6.9016e-01  time: 1.76e+00\n",
      "epoch: 16   trainLoss: 1.1898e-01   valLoss:7.1225e-01  time: 1.75e+00\n",
      "epoch: 17   trainLoss: 1.0699e-01   valLoss:1.0645e+00  time: 1.75e+00\n",
      "epoch: 18   trainLoss: 1.2501e-01   valLoss:5.7372e-01  time: 1.85e+00\n",
      "epoch: 19   trainLoss: 1.0227e-01   valLoss:7.3836e-01  time: 1.75e+00\n",
      "epoch: 20   trainLoss: 9.2086e-02   valLoss:7.3106e-01  time: 1.76e+00\n",
      "epoch: 21   trainLoss: 8.5453e-02   valLoss:6.5725e-01  time: 1.74e+00\n",
      "epoch: 22   trainLoss: 8.2485e-02   valLoss:7.1334e-01  time: 1.76e+00\n",
      "epoch: 23   trainLoss: 6.7562e-02   valLoss:9.3445e-01  time: 1.80e+00\n",
      "epoch: 24   trainLoss: 7.0080e-02   valLoss:1.3728e+00  time: 1.76e+00\n",
      "epoch: 25   trainLoss: 6.9548e-02   valLoss:8.3885e-01  time: 1.75e+00\n",
      "epoch: 26   trainLoss: 6.3663e-02   valLoss:3.9993e-01  time: 1.77e+00\n",
      "epoch: 27   trainLoss: 5.9292e-02   valLoss:6.5135e-01  time: 1.71e+00\n",
      "epoch: 28   trainLoss: 6.3936e-02   valLoss:4.6039e-01  time: 1.71e+00\n",
      "epoch: 29   trainLoss: 5.1700e-02   valLoss:5.6387e-01  time: 1.72e+00\n",
      "epoch: 30   trainLoss: 4.3205e-02   valLoss:3.7556e-01  time: 1.71e+00\n",
      "epoch: 31   trainLoss: 3.9709e-02   valLoss:4.5405e-01  time: 1.73e+00\n",
      "epoch: 32   trainLoss: 4.2796e-02   valLoss:3.7932e-01  time: 1.74e+00\n",
      "epoch: 33   trainLoss: 5.4271e-02   valLoss:4.9933e-01  time: 1.71e+00\n",
      "epoch: 34   trainLoss: 6.0978e-02   valLoss:3.0722e-01  time: 1.74e+00\n",
      "epoch: 35   trainLoss: 5.5979e-02   valLoss:3.1034e-01  time: 1.71e+00\n",
      "epoch: 36   trainLoss: 4.8572e-02   valLoss:2.6805e-01  time: 1.72e+00\n",
      "epoch: 37   trainLoss: 4.8246e-02   valLoss:3.6622e-01  time: 1.76e+00\n",
      "epoch: 38   trainLoss: 5.2099e-02   valLoss:3.7549e-01  time: 1.72e+00\n",
      "epoch: 39   trainLoss: 5.3263e-02   valLoss:3.4563e-01  time: 1.72e+00\n",
      "epoch: 40   trainLoss: 7.2438e-02   valLoss:5.4172e-01  time: 1.76e+00\n",
      "epoch: 41   trainLoss: 8.3024e-02   valLoss:2.5981e-01  time: 1.72e+00\n",
      "epoch: 42   trainLoss: 8.2128e-02   valLoss:4.3723e-01  time: 1.73e+00\n",
      "epoch: 43   trainLoss: 6.6087e-02   valLoss:3.6768e-01  time: 1.73e+00\n",
      "epoch: 44   trainLoss: 5.6384e-02   valLoss:5.7223e-01  time: 1.72e+00\n",
      "epoch: 45   trainLoss: 5.4145e-02   valLoss:3.0931e-01  time: 1.71e+00\n",
      "epoch: 46   trainLoss: 5.1343e-02   valLoss:2.3255e-01  time: 1.71e+00\n",
      "epoch: 47   trainLoss: 5.6909e-02   valLoss:3.2911e-01  time: 1.71e+00\n",
      "epoch: 48   trainLoss: 4.0141e-02   valLoss:2.2943e-01  time: 1.79e+00\n",
      "epoch: 49   trainLoss: 3.2282e-02   valLoss:2.6870e-01  time: 1.77e+00\n",
      "epoch: 50   trainLoss: 4.0464e-02   valLoss:3.0239e-01  time: 1.72e+00\n",
      "epoch: 51   trainLoss: 4.7144e-02   valLoss:2.0361e-01  time: 1.72e+00\n",
      "epoch: 52   trainLoss: 3.6123e-02   valLoss:5.3463e-01  time: 1.73e+00\n",
      "epoch: 53   trainLoss: 3.4903e-02   valLoss:2.2320e-01  time: 1.72e+00\n",
      "epoch: 54   trainLoss: 3.8843e-02   valLoss:1.8256e-01  time: 1.72e+00\n",
      "epoch: 55   trainLoss: 3.9330e-02   valLoss:3.3747e-01  time: 1.71e+00\n",
      "epoch: 56   trainLoss: 3.0398e-02   valLoss:2.0996e-01  time: 1.74e+00\n",
      "epoch: 57   trainLoss: 3.8545e-02   valLoss:1.8281e-01  time: 1.77e+00\n",
      "epoch: 58   trainLoss: 3.6751e-02   valLoss:2.2189e-01  time: 1.70e+00\n",
      "epoch: 59   trainLoss: 3.0306e-02   valLoss:2.6753e-01  time: 1.73e+00\n",
      "epoch: 60   trainLoss: 3.3945e-02   valLoss:7.8159e-01  time: 1.71e+00\n",
      "epoch: 61   trainLoss: 3.8661e-02   valLoss:8.5631e-01  time: 1.71e+00\n",
      "epoch: 62   trainLoss: 3.8281e-02   valLoss:9.9589e-01  time: 1.72e+00\n",
      "epoch: 63   trainLoss: 3.7689e-02   valLoss:5.8133e-01  time: 1.71e+00\n",
      "epoch: 64   trainLoss: 3.9763e-02   valLoss:3.9062e-01  time: 1.72e+00\n",
      "epoch: 65   trainLoss: 4.4097e-02   valLoss:1.9174e-01  time: 1.75e+00\n",
      "epoch: 66   trainLoss: 3.7059e-02   valLoss:1.7719e-01  time: 1.75e+00\n",
      "epoch: 67   trainLoss: 3.1951e-02   valLoss:1.8480e-01  time: 1.74e+00\n",
      "epoch: 68   trainLoss: 3.8487e-02   valLoss:1.8313e-01  time: 1.74e+00\n",
      "epoch: 69   trainLoss: 3.6608e-02   valLoss:2.5985e-01  time: 1.74e+00\n",
      "epoch: 70   trainLoss: 4.5245e-02   valLoss:1.8775e-01  time: 1.75e+00\n",
      "epoch: 71   trainLoss: 4.4845e-02   valLoss:2.8420e-01  time: 1.75e+00\n",
      "epoch: 72   trainLoss: 3.5789e-02   valLoss:1.7335e-01  time: 1.74e+00\n",
      "epoch: 73   trainLoss: 4.1424e-02   valLoss:1.8516e-01  time: 1.74e+00\n",
      "epoch: 74   trainLoss: 3.6333e-02   valLoss:1.7928e-01  time: 1.74e+00\n",
      "epoch: 75   trainLoss: 3.5042e-02   valLoss:1.9220e-01  time: 1.75e+00\n",
      "epoch: 76   trainLoss: 3.2936e-02   valLoss:2.2963e-01  time: 1.74e+00\n",
      "epoch: 77   trainLoss: 3.4202e-02   valLoss:1.6641e-01  time: 1.75e+00\n",
      "epoch: 78   trainLoss: 3.5143e-02   valLoss:1.9864e-01  time: 1.76e+00\n",
      "epoch: 79   trainLoss: 4.2948e-02   valLoss:2.0947e-01  time: 1.75e+00\n",
      "epoch: 80   trainLoss: 4.2209e-02   valLoss:4.2907e-01  time: 1.78e+00\n",
      "epoch: 81   trainLoss: 4.7147e-02   valLoss:2.7397e-01  time: 1.75e+00\n",
      "epoch: 82   trainLoss: 6.9331e-02   valLoss:2.5290e-01  time: 1.76e+00\n",
      "epoch: 83   trainLoss: 7.5758e-02   valLoss:6.6170e-01  time: 1.74e+00\n",
      "epoch: 84   trainLoss: 6.4752e-02   valLoss:1.6880e-01  time: 1.74e+00\n",
      "epoch: 85   trainLoss: 5.4471e-02   valLoss:1.7003e-01  time: 1.74e+00\n",
      "epoch: 86   trainLoss: 6.1391e-02   valLoss:2.1467e-01  time: 1.74e+00\n",
      "epoch: 87   trainLoss: 5.3723e-02   valLoss:3.0886e-01  time: 1.74e+00\n",
      "epoch: 88   trainLoss: 4.0794e-02   valLoss:1.3758e-01  time: 1.76e+00\n",
      "epoch: 89   trainLoss: 3.7455e-02   valLoss:1.7319e-01  time: 1.76e+00\n",
      "epoch: 90   trainLoss: 3.1495e-02   valLoss:2.2877e-01  time: 1.75e+00\n",
      "epoch: 91   trainLoss: 3.3627e-02   valLoss:3.2064e-01  time: 1.73e+00\n",
      "epoch: 92   trainLoss: 2.8093e-02   valLoss:2.8345e-01  time: 1.74e+00\n",
      "epoch: 93   trainLoss: 3.2584e-02   valLoss:1.8460e-01  time: 1.74e+00\n",
      "epoch: 94   trainLoss: 3.5447e-02   valLoss:1.6795e-01  time: 1.75e+00\n",
      "epoch: 95   trainLoss: 4.5966e-02   valLoss:3.4143e-01  time: 1.75e+00\n",
      "epoch: 96   trainLoss: 4.9577e-02   valLoss:1.5448e-01  time: 1.74e+00\n",
      "epoch: 97   trainLoss: 4.8488e-02   valLoss:1.8206e-01  time: 1.74e+00\n",
      "epoch: 98   trainLoss: 4.4138e-02   valLoss:1.2069e-01  time: 1.73e+00\n",
      "epoch: 99   trainLoss: 3.4268e-02   valLoss:1.2660e-01  time: 1.76e+00\n",
      "loading checkpoint 98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-212c9ea118b44fa8972206a4a831f2cb\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-212c9ea118b44fa8972206a4a831f2cb\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-212c9ea118b44fa8972206a4a831f2cb\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3da13fc644522c6e52e5cecbeb83f237\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-3da13fc644522c6e52e5cecbeb83f237\": [{\"train\": 0.9309606750806173, \"val\": 0.8697372146226742, \"epoch\": 0}, {\"train\": 0.6863244970639547, \"val\": 0.8599144057819137, \"epoch\": 1}, {\"train\": 0.5378566781679789, \"val\": 0.9451305510269271, \"epoch\": 2}, {\"train\": 0.4454311231772105, \"val\": 1.7661816674250144, \"epoch\": 3}, {\"train\": 0.36749932169914246, \"val\": 4.45367580431479, \"epoch\": 4}, {\"train\": 0.3083060383796692, \"val\": 8.146163159829598, \"epoch\": 5}, {\"train\": 0.2594817876815796, \"val\": 9.351810893306025, \"epoch\": 6}, {\"train\": 0.23203311363855997, \"val\": 5.018725718833782, \"epoch\": 7}, {\"train\": 0.1967941770950953, \"val\": 2.516973388857312, \"epoch\": 8}, {\"train\": 0.1807971398035685, \"val\": 1.2413982030970079, \"epoch\": 9}, {\"train\": 0.16973412533601126, \"val\": 1.0225345040637035, \"epoch\": 10}, {\"train\": 0.14591124653816223, \"val\": 0.6259816814765886, \"epoch\": 11}, {\"train\": 0.14978023866812387, \"val\": 0.7478405514663017, \"epoch\": 12}, {\"train\": 0.16398764650026956, \"val\": 1.1204435753325621, \"epoch\": 13}, {\"train\": 0.13983494540055594, \"val\": 0.8355655906652962, \"epoch\": 14}, {\"train\": 0.14116731782754263, \"val\": 0.6901641655092438, \"epoch\": 15}, {\"train\": 0.11897745976845424, \"val\": 0.7122478366863948, \"epoch\": 16}, {\"train\": 0.10699411233266194, \"val\": 1.0644845674987193, \"epoch\": 17}, {\"train\": 0.1250105376044909, \"val\": 0.5737228720552392, \"epoch\": 18}, {\"train\": 0.10226564357678096, \"val\": 0.7383645855373254, \"epoch\": 19}, {\"train\": 0.09208582838376363, \"val\": 0.7310640437873426, \"epoch\": 20}, {\"train\": 0.08545317004124324, \"val\": 0.6572475953303554, \"epoch\": 21}, {\"train\": 0.08248488729198773, \"val\": 0.7133449429439174, \"epoch\": 22}, {\"train\": 0.06756235162417094, \"val\": 0.9344502581214463, \"epoch\": 23}, {\"train\": 0.07007956008116405, \"val\": 1.372798554226756, \"epoch\": 24}, {\"train\": 0.06954807043075562, \"val\": 0.8388499787146294, \"epoch\": 25}, {\"train\": 0.06366257990399997, \"val\": 0.399933467970954, \"epoch\": 26}, {\"train\": 0.059292408327261605, \"val\": 0.6513480943800123, \"epoch\": 27}, {\"train\": 0.06393630554278691, \"val\": 0.4603874861918114, \"epoch\": 28}, {\"train\": 0.05170036728183428, \"val\": 0.5638732300726352, \"epoch\": 29}, {\"train\": 0.04320503647128741, \"val\": 0.3755641490496971, \"epoch\": 30}, {\"train\": 0.03970919425288836, \"val\": 0.4540545132252629, \"epoch\": 31}, {\"train\": 0.04279563948512077, \"val\": 0.37931562737320307, \"epoch\": 32}, {\"train\": 0.05427146330475807, \"val\": 0.49933102337160595, \"epoch\": 33}, {\"train\": 0.0609783964852492, \"val\": 0.3072177848951132, \"epoch\": 34}, {\"train\": 0.05597888554135958, \"val\": 0.3103386694082507, \"epoch\": 35}, {\"train\": 0.04857151582837105, \"val\": 0.2680458279878453, \"epoch\": 36}, {\"train\": 0.04824597512682279, \"val\": 0.36621804384997597, \"epoch\": 37}, {\"train\": 0.05209900811314583, \"val\": 0.37549361761797356, \"epoch\": 38}, {\"train\": 0.05326284343997637, \"val\": 0.34562501667619305, \"epoch\": 39}, {\"train\": 0.07243750120202701, \"val\": 0.5417232635396498, \"epoch\": 40}, {\"train\": 0.0830243577559789, \"val\": 0.2598096889064268, \"epoch\": 41}, {\"train\": 0.08212752019365628, \"val\": 0.43723043214391777, \"epoch\": 42}, {\"train\": 0.06608684360980988, \"val\": 0.3676778533944377, \"epoch\": 43}, {\"train\": 0.05638419215877851, \"val\": 0.5722331294307003, \"epoch\": 44}, {\"train\": 0.054145291447639465, \"val\": 0.30930659650928444, \"epoch\": 45}, {\"train\": 0.05134306848049164, \"val\": 0.23255398976444094, \"epoch\": 46}, {\"train\": 0.05690938731034597, \"val\": 0.32911039448722645, \"epoch\": 47}, {\"train\": 0.040141439686218895, \"val\": 0.22943453284808332, \"epoch\": 48}, {\"train\": 0.032281674444675446, \"val\": 0.26870269505996947, \"epoch\": 49}, {\"train\": 0.04046351710955302, \"val\": 0.3023873002682295, \"epoch\": 50}, {\"train\": 0.047143651793400444, \"val\": 0.20361480847070063, \"epoch\": 51}, {\"train\": 0.036122847348451614, \"val\": 0.5346278002209686, \"epoch\": 52}, {\"train\": 0.03490329161286354, \"val\": 0.22319632267786396, \"epoch\": 53}, {\"train\": 0.03884280348817507, \"val\": 0.18255799333047534, \"epoch\": 54}, {\"train\": 0.03932994852463404, \"val\": 0.3374707504486044, \"epoch\": 55}, {\"train\": 0.03039766972263654, \"val\": 0.20996304401369006, \"epoch\": 56}, {\"train\": 0.03854533160726229, \"val\": 0.18281119438578133, \"epoch\": 57}, {\"train\": 0.036751000210642815, \"val\": 0.22189455577344805, \"epoch\": 58}, {\"train\": 0.030305565645297367, \"val\": 0.26752886755598915, \"epoch\": 59}, {\"train\": 0.033944783732295036, \"val\": 0.7815948571871828, \"epoch\": 60}, {\"train\": 0.03866069515546163, \"val\": 0.8563128096361955, \"epoch\": 61}, {\"train\": 0.038281156991918884, \"val\": 0.9958906971470073, \"epoch\": 62}, {\"train\": 0.03768910219271978, \"val\": 0.581334038492706, \"epoch\": 63}, {\"train\": 0.03976286326845487, \"val\": 0.3906201603098048, \"epoch\": 64}, {\"train\": 0.044097318003575005, \"val\": 0.19173621846431935, \"epoch\": 65}, {\"train\": 0.03705935552716255, \"val\": 0.17719094230000068, \"epoch\": 66}, {\"train\": 0.03195141814649105, \"val\": 0.18479634637564973, \"epoch\": 67}, {\"train\": 0.0384872326006492, \"val\": 0.18312991917547253, \"epoch\": 68}, {\"train\": 0.036607650419076286, \"val\": 0.2598517889467378, \"epoch\": 69}, {\"train\": 0.04524542142947515, \"val\": 0.1877483508170203, \"epoch\": 70}, {\"train\": 0.044845289861162506, \"val\": 0.28420339059084654, \"epoch\": 71}, {\"train\": 0.03578884154558182, \"val\": 0.17334835919103136, \"epoch\": 72}, {\"train\": 0.041424249609311424, \"val\": 0.1851593532702989, \"epoch\": 73}, {\"train\": 0.036332650731007256, \"val\": 0.17927935684306753, \"epoch\": 74}, {\"train\": 0.03504179852704207, \"val\": 0.19220368725213188, \"epoch\": 75}, {\"train\": 0.032936024790008865, \"val\": 0.22962722377161737, \"epoch\": 76}, {\"train\": 0.03420158475637436, \"val\": 0.16640655730885487, \"epoch\": 77}, {\"train\": 0.03514283336699009, \"val\": 0.1986400704899872, \"epoch\": 78}, {\"train\": 0.04294799889127413, \"val\": 0.20947281636504664, \"epoch\": 79}, {\"train\": 0.0422087957461675, \"val\": 0.42907061086346704, \"epoch\": 80}, {\"train\": 0.04714748760064443, \"val\": 0.2739667573460826, \"epoch\": 81}, {\"train\": 0.0693311554690202, \"val\": 0.25290359539083307, \"epoch\": 82}, {\"train\": 0.07575754324595134, \"val\": 0.6617040085295837, \"epoch\": 83}, {\"train\": 0.0647524781525135, \"val\": 0.168802377809253, \"epoch\": 84}, {\"train\": 0.05447077378630638, \"val\": 0.1700310799051766, \"epoch\": 85}, {\"train\": 0.06139060730735461, \"val\": 0.21467183682446678, \"epoch\": 86}, {\"train\": 0.05372312789162, \"val\": 0.3088619892106012, \"epoch\": 87}, {\"train\": 0.04079391434788704, \"val\": 0.13758045642947156, \"epoch\": 88}, {\"train\": 0.03745545571049055, \"val\": 0.1731901455739582, \"epoch\": 89}, {\"train\": 0.03149483414987723, \"val\": 0.22876801594608911, \"epoch\": 90}, {\"train\": 0.033627054343620934, \"val\": 0.3206418898991413, \"epoch\": 91}, {\"train\": 0.0280934888869524, \"val\": 0.28344601503814815, \"epoch\": 92}, {\"train\": 0.032584323858221374, \"val\": 0.18460498292137076, \"epoch\": 93}, {\"train\": 0.035446859896183014, \"val\": 0.16795110898954724, \"epoch\": 94}, {\"train\": 0.04596604903539022, \"val\": 0.3414260360377806, \"epoch\": 95}, {\"train\": 0.049576759338378906, \"val\": 0.15447645807052376, \"epoch\": 96}, {\"train\": 0.048487755159536995, \"val\": 0.18205824868270645, \"epoch\": 97}, {\"train\": 0.04413845265905062, \"val\": 0.12068978986288938, \"epoch\": 98}, {\"train\": 0.03426793962717056, \"val\": 0.12659621181449404, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = 'results/transferLrn_des7_tower5_01/'\n",
    "epochs = 100\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                            epochs=epochs,\n",
    "                            saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.100537</td>\n",
       "      <td>7.403459e-01</td>\n",
       "      <td>0.949667</td>\n",
       "      <td>7.878571e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1.470075</td>\n",
       "      <td>0.867565</td>\n",
       "      <td>0.966870</td>\n",
       "      <td>-1.687669e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.368448e+06</td>\n",
       "      <td>-1.241156e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre        peakR2  maxAggR2     meanAggR2  \\\n",
       "train  0.000011  0.001990  0.100537  7.403459e-01  0.949667  7.878571e-01   \n",
       "test   1.470075  0.867565  0.966870 -1.687669e+06  0.000000 -3.368448e+06   \n",
       "\n",
       "           minAggR2  \n",
       "train  0.000000e+00  \n",
       "test  -1.241156e+07  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from data/tower1.0/conmech/design_5_N_200/\n",
      "loaded train set of size 180\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 8.0157e-01   valLoss:9.0456e-01  time: 1.43e+00\n",
      "epoch: 1   trainLoss: 5.2345e-01   valLoss:8.9783e-01  time: 1.43e+00\n",
      "epoch: 2   trainLoss: 3.4602e-01   valLoss:8.8903e-01  time: 1.42e+00\n",
      "epoch: 3   trainLoss: 2.3784e-01   valLoss:8.7072e-01  time: 1.39e+00\n",
      "epoch: 4   trainLoss: 1.5705e-01   valLoss:8.5016e-01  time: 1.44e+00\n",
      "epoch: 5   trainLoss: 1.1786e-01   valLoss:8.3249e-01  time: 1.46e+00\n",
      "epoch: 6   trainLoss: 9.4067e-02   valLoss:8.2349e-01  time: 1.43e+00\n",
      "epoch: 7   trainLoss: 7.7052e-02   valLoss:8.1461e-01  time: 1.42e+00\n",
      "epoch: 8   trainLoss: 7.1516e-02   valLoss:7.9984e-01  time: 1.38e+00\n",
      "epoch: 9   trainLoss: 7.0579e-02   valLoss:7.4647e-01  time: 1.38e+00\n",
      "epoch: 10   trainLoss: 6.8106e-02   valLoss:6.6452e-01  time: 1.39e+00\n",
      "epoch: 11   trainLoss: 6.2419e-02   valLoss:6.3430e-01  time: 1.40e+00\n",
      "epoch: 12   trainLoss: 5.5915e-02   valLoss:6.1678e-01  time: 1.46e+00\n",
      "epoch: 13   trainLoss: 4.7946e-02   valLoss:5.7943e-01  time: 1.38e+00\n",
      "epoch: 14   trainLoss: 4.0932e-02   valLoss:5.2033e-01  time: 1.37e+00\n",
      "epoch: 15   trainLoss: 3.6207e-02   valLoss:4.5808e-01  time: 1.38e+00\n",
      "epoch: 16   trainLoss: 3.3586e-02   valLoss:4.0424e-01  time: 1.43e+00\n",
      "epoch: 17   trainLoss: 3.2888e-02   valLoss:3.5239e-01  time: 1.44e+00\n",
      "epoch: 18   trainLoss: 3.2438e-02   valLoss:3.0457e-01  time: 1.40e+00\n",
      "epoch: 19   trainLoss: 3.1930e-02   valLoss:2.5575e-01  time: 1.39e+00\n",
      "epoch: 20   trainLoss: 3.1152e-02   valLoss:2.0579e-01  time: 1.37e+00\n",
      "epoch: 21   trainLoss: 2.9458e-02   valLoss:1.6981e-01  time: 1.37e+00\n",
      "epoch: 22   trainLoss: 2.7810e-02   valLoss:1.5750e-01  time: 1.43e+00\n",
      "epoch: 23   trainLoss: 2.6182e-02   valLoss:1.4311e-01  time: 1.45e+00\n",
      "epoch: 24   trainLoss: 2.4767e-02   valLoss:1.1784e-01  time: 1.39e+00\n",
      "epoch: 25   trainLoss: 2.3568e-02   valLoss:9.2238e-02  time: 1.43e+00\n",
      "epoch: 26   trainLoss: 2.2762e-02   valLoss:7.9626e-02  time: 1.43e+00\n",
      "epoch: 27   trainLoss: 2.2179e-02   valLoss:7.2015e-02  time: 1.44e+00\n",
      "epoch: 28   trainLoss: 2.1588e-02   valLoss:6.2043e-02  time: 1.38e+00\n",
      "epoch: 29   trainLoss: 2.0985e-02   valLoss:5.3782e-02  time: 1.38e+00\n",
      "epoch: 30   trainLoss: 2.0209e-02   valLoss:4.9325e-02  time: 1.38e+00\n",
      "epoch: 31   trainLoss: 1.9347e-02   valLoss:4.7570e-02  time: 1.42e+00\n",
      "epoch: 32   trainLoss: 1.8534e-02   valLoss:4.6518e-02  time: 1.38e+00\n",
      "epoch: 33   trainLoss: 1.7774e-02   valLoss:4.4176e-02  time: 1.43e+00\n",
      "epoch: 34   trainLoss: 1.7017e-02   valLoss:4.3135e-02  time: 1.41e+00\n",
      "epoch: 35   trainLoss: 1.6268e-02   valLoss:4.1689e-02  time: 1.42e+00\n",
      "epoch: 36   trainLoss: 1.5710e-02   valLoss:3.9600e-02  time: 1.39e+00\n",
      "epoch: 37   trainLoss: 1.5253e-02   valLoss:3.9412e-02  time: 1.42e+00\n",
      "epoch: 38   trainLoss: 1.4908e-02   valLoss:3.7936e-02  time: 1.42e+00\n",
      "epoch: 39   trainLoss: 1.4510e-02   valLoss:3.7171e-02  time: 1.43e+00\n",
      "epoch: 40   trainLoss: 1.4070e-02   valLoss:3.6122e-02  time: 1.40e+00\n",
      "epoch: 41   trainLoss: 1.3731e-02   valLoss:3.4592e-02  time: 1.43e+00\n",
      "epoch: 42   trainLoss: 1.3450e-02   valLoss:3.3942e-02  time: 1.43e+00\n",
      "epoch: 43   trainLoss: 1.3142e-02   valLoss:3.3318e-02  time: 1.38e+00\n",
      "epoch: 44   trainLoss: 1.2818e-02   valLoss:3.2922e-02  time: 1.39e+00\n",
      "epoch: 45   trainLoss: 1.2512e-02   valLoss:3.3220e-02  time: 1.38e+00\n",
      "epoch: 46   trainLoss: 1.2199e-02   valLoss:3.2597e-02  time: 1.38e+00\n",
      "epoch: 47   trainLoss: 1.1868e-02   valLoss:3.3136e-02  time: 1.38e+00\n",
      "epoch: 48   trainLoss: 1.1543e-02   valLoss:3.3640e-02  time: 1.41e+00\n",
      "epoch: 49   trainLoss: 1.1275e-02   valLoss:3.4268e-02  time: 1.42e+00\n",
      "epoch: 50   trainLoss: 1.1061e-02   valLoss:3.4543e-02  time: 1.41e+00\n",
      "epoch: 51   trainLoss: 1.0883e-02   valLoss:3.5297e-02  time: 1.42e+00\n",
      "epoch: 52   trainLoss: 1.0712e-02   valLoss:3.5043e-02  time: 1.41e+00\n",
      "epoch: 53   trainLoss: 1.0518e-02   valLoss:3.5960e-02  time: 1.43e+00\n",
      "epoch: 54   trainLoss: 1.0308e-02   valLoss:3.5778e-02  time: 1.43e+00\n",
      "epoch: 55   trainLoss: 1.0092e-02   valLoss:3.7356e-02  time: 1.43e+00\n",
      "epoch: 56   trainLoss: 9.9197e-03   valLoss:3.6070e-02  time: 1.42e+00\n",
      "epoch: 57   trainLoss: 1.0012e-02   valLoss:4.1335e-02  time: 1.40e+00\n",
      "epoch: 58   trainLoss: 1.0864e-02   valLoss:3.5826e-02  time: 1.38e+00\n",
      "epoch: 59   trainLoss: 1.3202e-02   valLoss:4.3994e-02  time: 1.43e+00\n",
      "epoch: 60   trainLoss: 1.3065e-02   valLoss:3.7992e-02  time: 1.42e+00\n",
      "epoch: 61   trainLoss: 9.6209e-03   valLoss:4.0140e-02  time: 1.40e+00\n",
      "epoch: 62   trainLoss: 1.2008e-02   valLoss:4.1970e-02  time: 1.38e+00\n",
      "epoch: 63   trainLoss: 1.2260e-02   valLoss:4.4848e-02  time: 1.43e+00\n",
      "epoch: 64   trainLoss: 9.7283e-03   valLoss:3.9585e-02  time: 1.42e+00\n",
      "epoch: 65   trainLoss: 1.0467e-02   valLoss:4.2007e-02  time: 1.44e+00\n",
      "epoch: 66   trainLoss: 8.2368e-03   valLoss:4.8803e-02  time: 1.42e+00\n",
      "epoch: 67   trainLoss: 9.5284e-03   valLoss:4.3925e-02  time: 1.37e+00\n",
      "epoch: 68   trainLoss: 7.7547e-03   valLoss:4.6262e-02  time: 1.38e+00\n",
      "epoch: 69   trainLoss: 8.2891e-03   valLoss:5.6218e-02  time: 1.39e+00\n",
      "epoch: 70   trainLoss: 7.3737e-03   valLoss:5.6074e-02  time: 1.39e+00\n",
      "epoch: 71   trainLoss: 7.2992e-03   valLoss:5.0955e-02  time: 1.38e+00\n",
      "epoch: 72   trainLoss: 7.0423e-03   valLoss:5.2589e-02  time: 1.37e+00\n",
      "epoch: 73   trainLoss: 6.4241e-03   valLoss:5.7602e-02  time: 1.39e+00\n",
      "epoch: 74   trainLoss: 6.6644e-03   valLoss:6.0785e-02  time: 1.37e+00\n",
      "epoch: 75   trainLoss: 6.1424e-03   valLoss:6.0956e-02  time: 1.37e+00\n",
      "epoch: 76   trainLoss: 5.8837e-03   valLoss:6.3331e-02  time: 1.39e+00\n",
      "epoch: 77   trainLoss: 5.6745e-03   valLoss:6.2680e-02  time: 1.40e+00\n",
      "epoch: 78   trainLoss: 5.5306e-03   valLoss:5.8270e-02  time: 1.45e+00\n",
      "epoch: 79   trainLoss: 5.3849e-03   valLoss:6.3977e-02  time: 1.37e+00\n",
      "epoch: 80   trainLoss: 5.0630e-03   valLoss:6.6585e-02  time: 1.38e+00\n",
      "epoch: 81   trainLoss: 4.7686e-03   valLoss:6.5909e-02  time: 1.38e+00\n",
      "epoch: 82   trainLoss: 4.5460e-03   valLoss:6.7140e-02  time: 1.38e+00\n",
      "epoch: 83   trainLoss: 4.1866e-03   valLoss:6.9680e-02  time: 1.38e+00\n",
      "epoch: 84   trainLoss: 4.0117e-03   valLoss:6.9556e-02  time: 1.41e+00\n",
      "epoch: 85   trainLoss: 3.7045e-03   valLoss:6.9689e-02  time: 1.43e+00\n",
      "epoch: 86   trainLoss: 3.4781e-03   valLoss:7.5967e-02  time: 1.42e+00\n",
      "epoch: 87   trainLoss: 3.4819e-03   valLoss:7.5203e-02  time: 1.43e+00\n",
      "epoch: 88   trainLoss: 4.0892e-03   valLoss:8.6156e-02  time: 1.42e+00\n",
      "epoch: 89   trainLoss: 6.9025e-03   valLoss:7.6028e-02  time: 1.42e+00\n",
      "epoch: 90   trainLoss: 1.1848e-02   valLoss:7.7227e-02  time: 1.43e+00\n",
      "epoch: 91   trainLoss: 8.2152e-03   valLoss:6.4945e-02  time: 1.43e+00\n",
      "epoch: 92   trainLoss: 2.5384e-03   valLoss:5.2720e-02  time: 1.43e+00\n",
      "epoch: 93   trainLoss: 5.3559e-03   valLoss:5.5530e-02  time: 1.43e+00\n",
      "epoch: 94   trainLoss: 2.6508e-03   valLoss:6.7750e-02  time: 1.38e+00\n",
      "epoch: 95   trainLoss: 3.2664e-03   valLoss:5.8008e-02  time: 1.37e+00\n",
      "epoch: 96   trainLoss: 2.4089e-03   valLoss:4.8150e-02  time: 1.39e+00\n",
      "epoch: 97   trainLoss: 2.1403e-03   valLoss:5.2728e-02  time: 1.37e+00\n",
      "epoch: 98   trainLoss: 2.0725e-03   valLoss:5.8289e-02  time: 1.41e+00\n",
      "epoch: 99   trainLoss: 1.5310e-03   valLoss:5.5744e-02  time: 1.42e+00\n",
      "loading checkpoint 46\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.3785e+00   valLoss:1.4232e+03  time: 1.48e+00\n",
      "epoch: 1   trainLoss: 3.4445e-01   valLoss:4.7880e+02  time: 1.48e+00\n",
      "epoch: 2   trainLoss: 3.3218e-01   valLoss:1.0876e+02  time: 1.44e+00\n",
      "epoch: 3   trainLoss: 1.8588e-01   valLoss:1.1505e+01  time: 1.42e+00\n",
      "epoch: 4   trainLoss: 1.2515e-01   valLoss:2.3304e+00  time: 1.38e+00\n",
      "epoch: 5   trainLoss: 8.6245e-02   valLoss:5.2875e-01  time: 1.39e+00\n",
      "epoch: 6   trainLoss: 8.5989e-02   valLoss:3.2886e-01  time: 1.38e+00\n",
      "epoch: 7   trainLoss: 8.6548e-02   valLoss:2.2489e-01  time: 1.39e+00\n",
      "epoch: 8   trainLoss: 7.8256e-02   valLoss:1.6952e-01  time: 1.43e+00\n",
      "epoch: 9   trainLoss: 7.2463e-02   valLoss:1.3436e-01  time: 1.42e+00\n",
      "epoch: 10   trainLoss: 7.1249e-02   valLoss:1.0669e-01  time: 1.43e+00\n",
      "epoch: 11   trainLoss: 6.8550e-02   valLoss:8.4696e-02  time: 1.43e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12   trainLoss: 6.2915e-02   valLoss:7.1970e-02  time: 1.39e+00\n",
      "epoch: 13   trainLoss: 5.8072e-02   valLoss:6.9960e-02  time: 1.38e+00\n",
      "epoch: 14   trainLoss: 5.5700e-02   valLoss:7.1944e-02  time: 1.39e+00\n",
      "epoch: 15   trainLoss: 5.3955e-02   valLoss:6.9553e-02  time: 1.43e+00\n",
      "epoch: 16   trainLoss: 5.1224e-02   valLoss:6.2372e-02  time: 1.43e+00\n",
      "epoch: 17   trainLoss: 4.8053e-02   valLoss:5.4488e-02  time: 1.42e+00\n",
      "epoch: 18   trainLoss: 4.5773e-02   valLoss:4.8779e-02  time: 1.43e+00\n",
      "epoch: 19   trainLoss: 4.4404e-02   valLoss:4.5629e-02  time: 1.43e+00\n",
      "epoch: 20   trainLoss: 4.3002e-02   valLoss:4.4109e-02  time: 1.40e+00\n",
      "epoch: 21   trainLoss: 4.1030e-02   valLoss:4.3841e-02  time: 1.41e+00\n",
      "epoch: 22   trainLoss: 3.8961e-02   valLoss:4.4307e-02  time: 1.41e+00\n",
      "epoch: 23   trainLoss: 3.7310e-02   valLoss:4.4409e-02  time: 1.41e+00\n",
      "epoch: 24   trainLoss: 3.5989e-02   valLoss:4.3476e-02  time: 1.42e+00\n",
      "epoch: 25   trainLoss: 3.4650e-02   valLoss:4.1400e-02  time: 1.43e+00\n",
      "epoch: 26   trainLoss: 3.3064e-02   valLoss:3.8735e-02  time: 1.43e+00\n",
      "epoch: 27   trainLoss: 3.1331e-02   valLoss:3.6111e-02  time: 1.41e+00\n",
      "epoch: 28   trainLoss: 2.9751e-02   valLoss:3.3826e-02  time: 1.41e+00\n",
      "epoch: 29   trainLoss: 2.8299e-02   valLoss:3.2077e-02  time: 1.42e+00\n",
      "epoch: 30   trainLoss: 2.6810e-02   valLoss:3.1078e-02  time: 1.41e+00\n",
      "epoch: 31   trainLoss: 2.5411e-02   valLoss:3.0378e-02  time: 1.43e+00\n",
      "epoch: 32   trainLoss: 2.4204e-02   valLoss:2.9296e-02  time: 1.42e+00\n",
      "epoch: 33   trainLoss: 2.3119e-02   valLoss:2.7558e-02  time: 1.42e+00\n",
      "epoch: 34   trainLoss: 2.2048e-02   valLoss:2.5790e-02  time: 1.42e+00\n",
      "epoch: 35   trainLoss: 2.1102e-02   valLoss:2.4636e-02  time: 1.42e+00\n",
      "epoch: 36   trainLoss: 2.0602e-02   valLoss:2.4080e-02  time: 1.44e+00\n",
      "epoch: 37   trainLoss: 1.9930e-02   valLoss:2.4046e-02  time: 1.44e+00\n",
      "epoch: 38   trainLoss: 1.9140e-02   valLoss:2.4015e-02  time: 1.43e+00\n",
      "epoch: 39   trainLoss: 1.8656e-02   valLoss:2.2810e-02  time: 1.42e+00\n",
      "epoch: 40   trainLoss: 1.7968e-02   valLoss:2.1008e-02  time: 1.42e+00\n",
      "epoch: 41   trainLoss: 1.7262e-02   valLoss:1.9766e-02  time: 1.42e+00\n",
      "epoch: 42   trainLoss: 1.6847e-02   valLoss:1.9020e-02  time: 1.42e+00\n",
      "epoch: 43   trainLoss: 1.6183e-02   valLoss:1.8577e-02  time: 1.42e+00\n",
      "epoch: 44   trainLoss: 1.5411e-02   valLoss:1.8290e-02  time: 1.43e+00\n",
      "epoch: 45   trainLoss: 1.4818e-02   valLoss:1.7723e-02  time: 1.44e+00\n",
      "epoch: 46   trainLoss: 1.4078e-02   valLoss:1.7160e-02  time: 1.42e+00\n",
      "epoch: 47   trainLoss: 1.3378e-02   valLoss:1.6666e-02  time: 1.42e+00\n",
      "epoch: 48   trainLoss: 1.2625e-02   valLoss:1.5980e-02  time: 1.43e+00\n",
      "epoch: 49   trainLoss: 1.1723e-02   valLoss:1.5210e-02  time: 1.43e+00\n",
      "epoch: 50   trainLoss: 1.0919e-02   valLoss:1.4309e-02  time: 1.43e+00\n",
      "epoch: 51   trainLoss: 1.0150e-02   valLoss:1.3198e-02  time: 1.43e+00\n",
      "epoch: 52   trainLoss: 9.3279e-03   valLoss:1.2231e-02  time: 1.42e+00\n",
      "epoch: 53   trainLoss: 8.5996e-03   valLoss:1.1336e-02  time: 1.43e+00\n",
      "epoch: 54   trainLoss: 7.7799e-03   valLoss:1.0471e-02  time: 1.44e+00\n",
      "epoch: 55   trainLoss: 6.9915e-03   valLoss:9.6403e-03  time: 1.43e+00\n",
      "epoch: 56   trainLoss: 6.2819e-03   valLoss:8.9951e-03  time: 1.42e+00\n",
      "epoch: 57   trainLoss: 5.4960e-03   valLoss:8.4525e-03  time: 1.42e+00\n",
      "epoch: 58   trainLoss: 4.7943e-03   valLoss:8.0154e-03  time: 1.44e+00\n",
      "epoch: 59   trainLoss: 4.2407e-03   valLoss:7.8606e-03  time: 1.43e+00\n",
      "epoch: 60   trainLoss: 4.0369e-03   valLoss:8.0611e-03  time: 1.43e+00\n",
      "epoch: 61   trainLoss: 4.8762e-03   valLoss:9.8136e-03  time: 1.43e+00\n",
      "epoch: 62   trainLoss: 8.2596e-03   valLoss:6.9528e-03  time: 1.44e+00\n",
      "epoch: 63   trainLoss: 3.9380e-03   valLoss:8.1529e-03  time: 1.43e+00\n",
      "epoch: 64   trainLoss: 4.6070e-03   valLoss:6.2416e-03  time: 1.46e+00\n",
      "epoch: 65   trainLoss: 2.2135e-03   valLoss:6.7068e-03  time: 1.43e+00\n",
      "epoch: 66   trainLoss: 3.5009e-03   valLoss:6.3278e-03  time: 1.44e+00\n",
      "epoch: 67   trainLoss: 1.9644e-03   valLoss:6.6628e-03  time: 1.42e+00\n",
      "epoch: 68   trainLoss: 2.0040e-03   valLoss:5.9942e-03  time: 1.43e+00\n",
      "epoch: 69   trainLoss: 2.0310e-03   valLoss:4.6934e-03  time: 1.42e+00\n",
      "epoch: 70   trainLoss: 1.6141e-03   valLoss:3.9688e-03  time: 1.43e+00\n",
      "epoch: 71   trainLoss: 1.3462e-03   valLoss:4.0335e-03  time: 1.43e+00\n",
      "epoch: 72   trainLoss: 1.3275e-03   valLoss:4.5171e-03  time: 1.42e+00\n",
      "epoch: 73   trainLoss: 1.3356e-03   valLoss:4.6989e-03  time: 1.42e+00\n",
      "epoch: 74   trainLoss: 1.2546e-03   valLoss:4.1758e-03  time: 1.45e+00\n",
      "epoch: 75   trainLoss: 9.7727e-04   valLoss:3.7764e-03  time: 1.42e+00\n",
      "epoch: 76   trainLoss: 9.1611e-04   valLoss:3.5700e-03  time: 1.43e+00\n",
      "epoch: 77   trainLoss: 9.4737e-04   valLoss:3.4288e-03  time: 1.44e+00\n",
      "epoch: 78   trainLoss: 9.3621e-04   valLoss:2.8239e-03  time: 1.43e+00\n",
      "epoch: 79   trainLoss: 7.5801e-04   valLoss:2.4101e-03  time: 1.42e+00\n",
      "epoch: 80   trainLoss: 6.7456e-04   valLoss:2.4748e-03  time: 1.43e+00\n",
      "epoch: 81   trainLoss: 6.7522e-04   valLoss:2.4820e-03  time: 1.43e+00\n",
      "epoch: 82   trainLoss: 6.6107e-04   valLoss:2.3084e-03  time: 1.44e+00\n",
      "epoch: 83   trainLoss: 5.9369e-04   valLoss:2.3220e-03  time: 1.43e+00\n",
      "epoch: 84   trainLoss: 5.4945e-04   valLoss:2.5983e-03  time: 1.41e+00\n",
      "epoch: 85   trainLoss: 5.5327e-04   valLoss:2.6751e-03  time: 1.45e+00\n",
      "epoch: 86   trainLoss: 5.1188e-04   valLoss:2.5977e-03  time: 1.42e+00\n",
      "epoch: 87   trainLoss: 4.8184e-04   valLoss:2.7521e-03  time: 1.43e+00\n",
      "epoch: 88   trainLoss: 4.6299e-04   valLoss:2.8487e-03  time: 1.44e+00\n",
      "epoch: 89   trainLoss: 4.5938e-04   valLoss:2.6954e-03  time: 1.42e+00\n",
      "epoch: 90   trainLoss: 4.0866e-04   valLoss:2.6872e-03  time: 1.43e+00\n",
      "epoch: 91   trainLoss: 3.8293e-04   valLoss:2.7161e-03  time: 1.42e+00\n",
      "epoch: 92   trainLoss: 3.8687e-04   valLoss:2.5619e-03  time: 1.43e+00\n",
      "epoch: 93   trainLoss: 3.6788e-04   valLoss:2.5214e-03  time: 1.44e+00\n",
      "epoch: 94   trainLoss: 3.3239e-04   valLoss:2.6216e-03  time: 1.42e+00\n",
      "epoch: 95   trainLoss: 3.2752e-04   valLoss:2.5885e-03  time: 1.42e+00\n",
      "epoch: 96   trainLoss: 3.2259e-04   valLoss:2.6998e-03  time: 1.43e+00\n",
      "epoch: 97   trainLoss: 3.0473e-04   valLoss:2.7088e-03  time: 1.43e+00\n",
      "epoch: 98   trainLoss: 2.8440e-04   valLoss:2.6214e-03  time: 1.44e+00\n",
      "epoch: 99   trainLoss: 2.8869e-04   valLoss:2.7069e-03  time: 1.44e+00\n",
      "loading checkpoint 82\n",
      "trained 24 random forest models in 4.60 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_500/\n",
      "loaded train set of size 445\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 8.6942e-01   valLoss:1.0266e+00  time: 3.62e+00\n",
      "epoch: 1   trainLoss: 4.6869e-01   valLoss:9.3649e-01  time: 3.53e+00\n",
      "epoch: 2   trainLoss: 2.1715e-01   valLoss:8.2699e-01  time: 3.54e+00\n",
      "epoch: 3   trainLoss: 1.2161e-01   valLoss:7.4530e-01  time: 3.62e+00\n",
      "epoch: 4   trainLoss: 9.2420e-02   valLoss:7.0390e-01  time: 3.52e+00\n",
      "epoch: 5   trainLoss: 8.2502e-02   valLoss:6.5748e-01  time: 3.55e+00\n",
      "epoch: 6   trainLoss: 7.1062e-02   valLoss:6.4398e-01  time: 3.54e+00\n",
      "epoch: 7   trainLoss: 5.9812e-02   valLoss:6.2271e-01  time: 3.52e+00\n",
      "epoch: 8   trainLoss: 4.9152e-02   valLoss:5.7413e-01  time: 3.53e+00\n",
      "epoch: 9   trainLoss: 4.2853e-02   valLoss:4.5018e-01  time: 3.53e+00\n",
      "epoch: 10   trainLoss: 3.8101e-02   valLoss:2.7270e-01  time: 3.62e+00\n",
      "epoch: 11   trainLoss: 3.4885e-02   valLoss:1.5377e-01  time: 3.55e+00\n",
      "epoch: 12   trainLoss: 3.1341e-02   valLoss:8.0704e-02  time: 3.53e+00\n",
      "epoch: 13   trainLoss: 3.0979e-02   valLoss:5.6521e-02  time: 3.53e+00\n",
      "epoch: 14   trainLoss: 2.9530e-02   valLoss:5.2297e-02  time: 3.55e+00\n",
      "epoch: 15   trainLoss: 2.5462e-02   valLoss:5.3694e-02  time: 3.55e+00\n",
      "epoch: 16   trainLoss: 2.3231e-02   valLoss:7.3827e-02  time: 3.55e+00\n",
      "epoch: 17   trainLoss: 2.1980e-02   valLoss:5.9744e-02  time: 3.54e+00\n",
      "epoch: 18   trainLoss: 2.0139e-02   valLoss:8.5407e-02  time: 3.56e+00\n",
      "epoch: 19   trainLoss: 2.0177e-02   valLoss:5.3013e-02  time: 3.56e+00\n",
      "epoch: 20   trainLoss: 1.8455e-02   valLoss:5.8035e-02  time: 3.56e+00\n",
      "epoch: 21   trainLoss: 1.5719e-02   valLoss:5.9233e-02  time: 3.55e+00\n",
      "epoch: 22   trainLoss: 1.4159e-02   valLoss:3.4510e-02  time: 3.54e+00\n",
      "epoch: 23   trainLoss: 1.1969e-02   valLoss:4.4118e-02  time: 3.62e+00\n",
      "epoch: 24   trainLoss: 1.0734e-02   valLoss:3.3376e-02  time: 3.51e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25   trainLoss: 9.4180e-03   valLoss:3.1269e-02  time: 3.53e+00\n",
      "epoch: 26   trainLoss: 7.9601e-03   valLoss:3.0167e-02  time: 3.52e+00\n",
      "epoch: 27   trainLoss: 8.3642e-03   valLoss:2.7795e-02  time: 3.54e+00\n",
      "epoch: 28   trainLoss: 6.1520e-03   valLoss:2.6368e-02  time: 3.56e+00\n",
      "epoch: 29   trainLoss: 4.3842e-03   valLoss:2.1943e-02  time: 3.54e+00\n",
      "epoch: 30   trainLoss: 3.7459e-03   valLoss:2.4606e-02  time: 3.55e+00\n",
      "epoch: 31   trainLoss: 2.9943e-03   valLoss:1.8426e-02  time: 3.52e+00\n",
      "epoch: 32   trainLoss: 2.7931e-03   valLoss:1.6684e-02  time: 3.54e+00\n",
      "epoch: 33   trainLoss: 2.8996e-03   valLoss:1.8241e-02  time: 3.53e+00\n",
      "epoch: 34   trainLoss: 2.1274e-03   valLoss:1.5234e-02  time: 3.52e+00\n",
      "epoch: 35   trainLoss: 1.6969e-03   valLoss:1.3791e-02  time: 3.53e+00\n",
      "epoch: 36   trainLoss: 1.4995e-03   valLoss:1.2894e-02  time: 3.61e+00\n",
      "epoch: 37   trainLoss: 1.9659e-03   valLoss:1.2898e-02  time: 3.55e+00\n",
      "epoch: 38   trainLoss: 2.2946e-03   valLoss:1.8662e-02  time: 3.53e+00\n",
      "epoch: 39   trainLoss: 3.1304e-03   valLoss:1.5002e-02  time: 3.54e+00\n",
      "epoch: 40   trainLoss: 4.5025e-03   valLoss:1.5785e-02  time: 3.50e+00\n",
      "epoch: 41   trainLoss: 5.3997e-03   valLoss:1.1809e-02  time: 3.53e+00\n",
      "epoch: 42   trainLoss: 3.2776e-03   valLoss:1.0638e-02  time: 3.54e+00\n",
      "epoch: 43   trainLoss: 3.4920e-03   valLoss:1.5160e-02  time: 3.55e+00\n",
      "epoch: 44   trainLoss: 3.2406e-03   valLoss:1.0577e-02  time: 3.53e+00\n",
      "epoch: 45   trainLoss: 1.9778e-03   valLoss:1.1146e-02  time: 3.53e+00\n",
      "epoch: 46   trainLoss: 1.6864e-03   valLoss:1.1993e-02  time: 3.51e+00\n",
      "epoch: 47   trainLoss: 1.2952e-03   valLoss:9.8867e-03  time: 3.50e+00\n",
      "epoch: 48   trainLoss: 1.8301e-03   valLoss:1.1466e-02  time: 3.54e+00\n",
      "epoch: 49   trainLoss: 9.5720e-04   valLoss:1.2586e-02  time: 3.62e+00\n",
      "epoch: 50   trainLoss: 1.4171e-03   valLoss:1.1356e-02  time: 3.51e+00\n",
      "epoch: 51   trainLoss: 1.3973e-03   valLoss:1.0603e-02  time: 3.47e+00\n",
      "epoch: 52   trainLoss: 2.0607e-03   valLoss:1.1678e-02  time: 3.48e+00\n",
      "epoch: 53   trainLoss: 1.5850e-03   valLoss:1.1176e-02  time: 3.50e+00\n",
      "epoch: 54   trainLoss: 1.2660e-03   valLoss:1.0316e-02  time: 3.53e+00\n",
      "epoch: 55   trainLoss: 1.2097e-03   valLoss:1.1373e-02  time: 3.55e+00\n",
      "epoch: 56   trainLoss: 2.0883e-03   valLoss:1.1307e-02  time: 3.53e+00\n",
      "epoch: 57   trainLoss: 1.2255e-03   valLoss:9.0127e-03  time: 3.55e+00\n",
      "epoch: 58   trainLoss: 1.9297e-03   valLoss:1.3281e-02  time: 3.50e+00\n",
      "epoch: 59   trainLoss: 4.6616e-03   valLoss:9.9166e-03  time: 3.50e+00\n",
      "epoch: 60   trainLoss: 2.2363e-03   valLoss:9.4752e-03  time: 3.50e+00\n",
      "epoch: 61   trainLoss: 2.0184e-03   valLoss:1.0458e-02  time: 3.51e+00\n",
      "epoch: 62   trainLoss: 2.0249e-03   valLoss:8.3178e-03  time: 3.60e+00\n",
      "epoch: 63   trainLoss: 1.3046e-03   valLoss:8.0439e-03  time: 3.49e+00\n",
      "epoch: 64   trainLoss: 2.4586e-03   valLoss:1.4684e-02  time: 3.50e+00\n",
      "epoch: 65   trainLoss: 3.8902e-03   valLoss:7.4094e-03  time: 3.49e+00\n",
      "epoch: 66   trainLoss: 2.9397e-03   valLoss:9.0907e-03  time: 3.51e+00\n",
      "epoch: 67   trainLoss: 2.2528e-03   valLoss:6.6307e-03  time: 3.48e+00\n",
      "epoch: 68   trainLoss: 2.6996e-03   valLoss:9.4454e-03  time: 3.49e+00\n",
      "epoch: 69   trainLoss: 1.7910e-03   valLoss:6.0218e-03  time: 3.46e+00\n",
      "epoch: 70   trainLoss: 2.3897e-03   valLoss:8.9847e-03  time: 3.47e+00\n",
      "epoch: 71   trainLoss: 1.0248e-03   valLoss:6.7202e-03  time: 3.46e+00\n",
      "epoch: 72   trainLoss: 1.3557e-03   valLoss:1.3291e-02  time: 3.47e+00\n",
      "epoch: 73   trainLoss: 1.2236e-03   valLoss:6.2148e-03  time: 3.45e+00\n",
      "epoch: 74   trainLoss: 1.7219e-03   valLoss:1.3104e-02  time: 3.46e+00\n",
      "epoch: 75   trainLoss: 1.4947e-03   valLoss:7.1726e-03  time: 3.55e+00\n",
      "epoch: 76   trainLoss: 1.1961e-03   valLoss:1.2205e-02  time: 3.47e+00\n",
      "epoch: 77   trainLoss: 1.8676e-03   valLoss:7.3710e-03  time: 3.46e+00\n",
      "epoch: 78   trainLoss: 3.1177e-03   valLoss:1.3670e-02  time: 3.46e+00\n",
      "epoch: 79   trainLoss: 3.8273e-03   valLoss:1.0656e-02  time: 3.35e+00\n",
      "epoch: 80   trainLoss: 2.8564e-03   valLoss:8.7323e-03  time: 3.49e+00\n",
      "epoch: 81   trainLoss: 3.3174e-03   valLoss:9.9135e-03  time: 3.48e+00\n",
      "epoch: 82   trainLoss: 2.1082e-03   valLoss:8.4736e-03  time: 3.53e+00\n",
      "epoch: 83   trainLoss: 1.5189e-03   valLoss:1.2531e-02  time: 3.46e+00\n",
      "epoch: 84   trainLoss: 1.6847e-03   valLoss:8.9350e-03  time: 3.48e+00\n",
      "epoch: 85   trainLoss: 1.5954e-03   valLoss:1.4836e-02  time: 3.47e+00\n",
      "epoch: 86   trainLoss: 1.6781e-03   valLoss:9.7604e-03  time: 3.47e+00\n",
      "epoch: 87   trainLoss: 1.8947e-03   valLoss:1.1492e-02  time: 3.45e+00\n",
      "epoch: 88   trainLoss: 1.7052e-03   valLoss:1.3450e-02  time: 3.55e+00\n",
      "epoch: 89   trainLoss: 1.1537e-03   valLoss:1.2555e-02  time: 3.48e+00\n",
      "epoch: 90   trainLoss: 8.1862e-04   valLoss:1.4302e-02  time: 3.39e+00\n",
      "epoch: 91   trainLoss: 1.8263e-03   valLoss:1.3244e-02  time: 3.36e+00\n",
      "epoch: 92   trainLoss: 8.4320e-04   valLoss:1.3237e-02  time: 3.37e+00\n",
      "epoch: 93   trainLoss: 9.4686e-04   valLoss:1.5911e-02  time: 3.44e+00\n",
      "epoch: 94   trainLoss: 1.6770e-03   valLoss:1.1608e-02  time: 3.48e+00\n",
      "epoch: 95   trainLoss: 1.2458e-03   valLoss:1.4725e-02  time: 3.44e+00\n",
      "epoch: 96   trainLoss: 2.2377e-03   valLoss:2.4814e-02  time: 3.46e+00\n",
      "epoch: 97   trainLoss: 2.8219e-03   valLoss:1.1639e-02  time: 3.46e+00\n",
      "epoch: 98   trainLoss: 1.5439e-03   valLoss:1.3441e-02  time: 3.47e+00\n",
      "epoch: 99   trainLoss: 1.4553e-03   valLoss:9.0160e-03  time: 3.36e+00\n",
      "loading checkpoint 69\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 8.6326e-01   valLoss:4.3182e+02  time: 3.59e+00\n",
      "epoch: 1   trainLoss: 2.6502e-01   valLoss:1.8943e+01  time: 3.50e+00\n",
      "epoch: 2   trainLoss: 9.4099e-02   valLoss:7.4147e-01  time: 3.48e+00\n",
      "epoch: 3   trainLoss: 6.9616e-02   valLoss:1.9219e-01  time: 3.43e+00\n",
      "epoch: 4   trainLoss: 6.5789e-02   valLoss:1.1845e-01  time: 3.37e+00\n",
      "epoch: 5   trainLoss: 5.8083e-02   valLoss:7.7382e-02  time: 3.44e+00\n",
      "epoch: 6   trainLoss: 4.7541e-02   valLoss:5.0508e-02  time: 3.39e+00\n",
      "epoch: 7   trainLoss: 4.0777e-02   valLoss:4.7708e-02  time: 3.36e+00\n",
      "epoch: 8   trainLoss: 3.7629e-02   valLoss:5.1744e-02  time: 3.43e+00\n",
      "epoch: 9   trainLoss: 3.4646e-02   valLoss:4.3216e-02  time: 3.48e+00\n",
      "epoch: 10   trainLoss: 3.1251e-02   valLoss:3.5058e-02  time: 3.43e+00\n",
      "epoch: 11   trainLoss: 2.8285e-02   valLoss:3.2360e-02  time: 3.44e+00\n",
      "epoch: 12   trainLoss: 2.6649e-02   valLoss:2.9010e-02  time: 3.44e+00\n",
      "epoch: 13   trainLoss: 2.5554e-02   valLoss:2.6826e-02  time: 3.36e+00\n",
      "epoch: 14   trainLoss: 2.5767e-02   valLoss:2.5897e-02  time: 3.37e+00\n",
      "epoch: 15   trainLoss: 2.3987e-02   valLoss:2.6128e-02  time: 3.40e+00\n",
      "epoch: 16   trainLoss: 2.3911e-02   valLoss:2.3064e-02  time: 3.36e+00\n",
      "epoch: 17   trainLoss: 2.1982e-02   valLoss:2.2389e-02  time: 3.40e+00\n",
      "epoch: 18   trainLoss: 2.0891e-02   valLoss:2.3423e-02  time: 3.47e+00\n",
      "epoch: 19   trainLoss: 2.0325e-02   valLoss:2.1167e-02  time: 3.47e+00\n",
      "epoch: 20   trainLoss: 1.8760e-02   valLoss:1.9849e-02  time: 3.38e+00\n",
      "epoch: 21   trainLoss: 1.8183e-02   valLoss:1.8909e-02  time: 3.44e+00\n",
      "epoch: 22   trainLoss: 1.7239e-02   valLoss:1.9433e-02  time: 3.38e+00\n",
      "epoch: 23   trainLoss: 1.5713e-02   valLoss:1.8464e-02  time: 3.46e+00\n",
      "epoch: 24   trainLoss: 1.3726e-02   valLoss:1.7630e-02  time: 3.37e+00\n",
      "epoch: 25   trainLoss: 1.2159e-02   valLoss:1.6626e-02  time: 3.46e+00\n",
      "epoch: 26   trainLoss: 1.0643e-02   valLoss:1.4358e-02  time: 3.39e+00\n",
      "epoch: 27   trainLoss: 9.5349e-03   valLoss:1.2928e-02  time: 3.41e+00\n",
      "epoch: 28   trainLoss: 8.3313e-03   valLoss:1.1294e-02  time: 3.46e+00\n",
      "epoch: 29   trainLoss: 7.8128e-03   valLoss:1.1397e-02  time: 3.47e+00\n",
      "epoch: 30   trainLoss: 6.7346e-03   valLoss:9.3955e-03  time: 3.47e+00\n",
      "epoch: 31   trainLoss: 7.7655e-03   valLoss:1.1088e-02  time: 3.45e+00\n",
      "epoch: 32   trainLoss: 9.7527e-03   valLoss:7.9571e-03  time: 3.47e+00\n",
      "epoch: 33   trainLoss: 9.2816e-03   valLoss:6.2024e-03  time: 3.47e+00\n",
      "epoch: 34   trainLoss: 7.3887e-03   valLoss:1.4066e-02  time: 3.46e+00\n",
      "epoch: 35   trainLoss: 7.3343e-03   valLoss:6.8823e-03  time: 3.49e+00\n",
      "epoch: 36   trainLoss: 5.5778e-03   valLoss:7.0282e-03  time: 3.48e+00\n",
      "epoch: 37   trainLoss: 4.8701e-03   valLoss:6.8580e-03  time: 3.48e+00\n",
      "epoch: 38   trainLoss: 2.9681e-03   valLoss:4.8280e-03  time: 3.58e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39   trainLoss: 3.0557e-03   valLoss:4.2022e-03  time: 3.50e+00\n",
      "epoch: 40   trainLoss: 2.7812e-03   valLoss:4.3370e-03  time: 3.39e+00\n",
      "epoch: 41   trainLoss: 1.2637e-03   valLoss:2.2937e-03  time: 3.36e+00\n",
      "epoch: 42   trainLoss: 1.9341e-03   valLoss:2.8682e-03  time: 3.39e+00\n",
      "epoch: 43   trainLoss: 1.0900e-03   valLoss:3.8998e-03  time: 3.46e+00\n",
      "epoch: 44   trainLoss: 1.0719e-03   valLoss:2.2800e-03  time: 3.47e+00\n",
      "epoch: 45   trainLoss: 1.0674e-03   valLoss:2.3108e-03  time: 3.49e+00\n",
      "epoch: 46   trainLoss: 9.6114e-04   valLoss:3.6627e-03  time: 3.48e+00\n",
      "epoch: 47   trainLoss: 1.1081e-03   valLoss:2.3981e-03  time: 3.47e+00\n",
      "epoch: 48   trainLoss: 7.6299e-04   valLoss:1.9374e-03  time: 3.52e+00\n",
      "epoch: 49   trainLoss: 6.7079e-04   valLoss:2.5439e-03  time: 3.48e+00\n",
      "epoch: 50   trainLoss: 6.4509e-04   valLoss:2.5374e-03  time: 3.48e+00\n",
      "epoch: 51   trainLoss: 5.8198e-04   valLoss:2.1978e-03  time: 3.55e+00\n",
      "epoch: 52   trainLoss: 1.1200e-03   valLoss:2.2416e-03  time: 3.46e+00\n",
      "epoch: 53   trainLoss: 4.3668e-04   valLoss:1.9838e-03  time: 3.50e+00\n",
      "epoch: 54   trainLoss: 6.9981e-04   valLoss:2.8412e-03  time: 3.48e+00\n",
      "epoch: 55   trainLoss: 5.0484e-04   valLoss:3.0178e-03  time: 3.47e+00\n",
      "epoch: 56   trainLoss: 3.4078e-04   valLoss:2.6332e-03  time: 3.48e+00\n",
      "epoch: 57   trainLoss: 4.3687e-04   valLoss:2.4845e-03  time: 3.49e+00\n",
      "epoch: 58   trainLoss: 3.9222e-04   valLoss:2.7925e-03  time: 3.53e+00\n",
      "epoch: 59   trainLoss: 5.6572e-04   valLoss:2.3510e-03  time: 3.47e+00\n",
      "epoch: 60   trainLoss: 5.0903e-04   valLoss:2.2538e-03  time: 3.47e+00\n",
      "epoch: 61   trainLoss: 6.6306e-04   valLoss:2.9322e-03  time: 3.47e+00\n",
      "epoch: 62   trainLoss: 5.9576e-04   valLoss:2.3553e-03  time: 3.49e+00\n",
      "epoch: 63   trainLoss: 1.0696e-03   valLoss:2.9599e-03  time: 3.48e+00\n",
      "epoch: 64   trainLoss: 7.8332e-04   valLoss:2.6341e-03  time: 3.55e+00\n",
      "epoch: 65   trainLoss: 8.3374e-04   valLoss:3.0458e-03  time: 3.47e+00\n",
      "epoch: 66   trainLoss: 5.7223e-04   valLoss:3.1219e-03  time: 3.50e+00\n",
      "epoch: 67   trainLoss: 1.2312e-03   valLoss:2.9721e-03  time: 3.49e+00\n",
      "epoch: 68   trainLoss: 4.7228e-04   valLoss:3.8442e-03  time: 3.47e+00\n",
      "epoch: 69   trainLoss: 8.7157e-04   valLoss:2.5871e-03  time: 3.46e+00\n",
      "epoch: 70   trainLoss: 9.5258e-04   valLoss:3.1996e-03  time: 3.45e+00\n",
      "epoch: 71   trainLoss: 1.4927e-03   valLoss:3.5052e-03  time: 3.48e+00\n",
      "epoch: 72   trainLoss: 9.9387e-04   valLoss:2.7623e-03  time: 3.51e+00\n",
      "epoch: 73   trainLoss: 6.0072e-04   valLoss:3.3857e-03  time: 3.47e+00\n",
      "epoch: 74   trainLoss: 1.1720e-03   valLoss:2.8814e-03  time: 3.49e+00\n",
      "epoch: 75   trainLoss: 9.0179e-04   valLoss:2.8475e-03  time: 3.38e+00\n",
      "epoch: 76   trainLoss: 7.7690e-04   valLoss:2.8459e-03  time: 3.48e+00\n",
      "epoch: 77   trainLoss: 6.1961e-04   valLoss:2.4874e-03  time: 3.54e+00\n",
      "epoch: 78   trainLoss: 8.8666e-04   valLoss:2.9609e-03  time: 3.48e+00\n",
      "epoch: 79   trainLoss: 5.6208e-04   valLoss:2.8800e-03  time: 3.43e+00\n",
      "epoch: 80   trainLoss: 7.2090e-04   valLoss:2.9152e-03  time: 3.45e+00\n",
      "epoch: 81   trainLoss: 6.4810e-04   valLoss:2.9368e-03  time: 3.49e+00\n",
      "epoch: 82   trainLoss: 7.8491e-04   valLoss:2.9635e-03  time: 3.42e+00\n",
      "epoch: 83   trainLoss: 1.3010e-03   valLoss:5.3821e-03  time: 3.45e+00\n",
      "epoch: 84   trainLoss: 1.2810e-03   valLoss:3.2452e-03  time: 3.44e+00\n",
      "epoch: 85   trainLoss: 1.0438e-03   valLoss:3.4054e-03  time: 3.43e+00\n",
      "epoch: 86   trainLoss: 8.3169e-04   valLoss:3.1594e-03  time: 3.45e+00\n",
      "epoch: 87   trainLoss: 5.2326e-04   valLoss:3.1785e-03  time: 3.47e+00\n",
      "epoch: 88   trainLoss: 1.1383e-03   valLoss:3.9743e-03  time: 3.47e+00\n",
      "epoch: 89   trainLoss: 9.2487e-04   valLoss:3.6971e-03  time: 3.46e+00\n",
      "epoch: 90   trainLoss: 1.4466e-03   valLoss:4.5613e-03  time: 3.58e+00\n",
      "epoch: 91   trainLoss: 2.0080e-03   valLoss:5.7520e-03  time: 3.49e+00\n",
      "epoch: 92   trainLoss: 2.6708e-03   valLoss:5.1910e-03  time: 3.52e+00\n",
      "epoch: 93   trainLoss: 3.4678e-03   valLoss:4.9485e-03  time: 3.48e+00\n",
      "epoch: 94   trainLoss: 2.5707e-03   valLoss:3.1155e-03  time: 3.48e+00\n",
      "epoch: 95   trainLoss: 1.3572e-03   valLoss:2.9003e-03  time: 3.48e+00\n",
      "epoch: 96   trainLoss: 1.1391e-03   valLoss:3.7110e-03  time: 3.47e+00\n",
      "epoch: 97   trainLoss: 1.2304e-03   valLoss:3.2124e-03  time: 3.46e+00\n",
      "epoch: 98   trainLoss: 1.2181e-03   valLoss:4.8588e-03  time: 3.46e+00\n",
      "epoch: 99   trainLoss: 1.1090e-03   valLoss:3.7626e-03  time: 3.46e+00\n",
      "loading checkpoint 48\n",
      "trained 24 random forest models in 8.27 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_10/\n",
      "loaded train set of size 9\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.2157e+00   valLoss:1.0740e+00  time: 8.86e-02\n",
      "epoch: 1   trainLoss: 9.6967e-01   valLoss:1.0708e+00  time: 9.06e-02\n",
      "epoch: 2   trainLoss: 8.2732e-01   valLoss:1.0685e+00  time: 8.98e-02\n",
      "epoch: 3   trainLoss: 6.7977e-01   valLoss:1.0505e+00  time: 9.18e-02\n",
      "epoch: 4   trainLoss: 5.3669e-01   valLoss:1.0315e+00  time: 9.47e-02\n",
      "epoch: 5   trainLoss: 4.0661e-01   valLoss:1.0057e+00  time: 9.84e-02\n",
      "epoch: 6   trainLoss: 3.0856e-01   valLoss:9.6748e-01  time: 8.92e-02\n",
      "epoch: 7   trainLoss: 2.3067e-01   valLoss:9.1825e-01  time: 9.16e-02\n",
      "epoch: 8   trainLoss: 1.8289e-01   valLoss:8.7709e-01  time: 8.93e-02\n",
      "epoch: 9   trainLoss: 1.3526e-01   valLoss:8.3640e-01  time: 8.91e-02\n",
      "epoch: 10   trainLoss: 1.1112e-01   valLoss:7.9693e-01  time: 8.73e-02\n",
      "epoch: 11   trainLoss: 9.9688e-02   valLoss:7.5862e-01  time: 8.88e-02\n",
      "epoch: 12   trainLoss: 9.7349e-02   valLoss:7.3055e-01  time: 8.87e-02\n",
      "epoch: 13   trainLoss: 9.6940e-02   valLoss:7.0933e-01  time: 8.86e-02\n",
      "epoch: 14   trainLoss: 9.4386e-02   valLoss:6.7036e-01  time: 8.80e-02\n",
      "epoch: 15   trainLoss: 9.0391e-02   valLoss:6.2268e-01  time: 8.82e-02\n",
      "epoch: 16   trainLoss: 8.3325e-02   valLoss:5.7992e-01  time: 9.01e-02\n",
      "epoch: 17   trainLoss: 7.8933e-02   valLoss:5.4907e-01  time: 8.84e-02\n",
      "epoch: 18   trainLoss: 7.3681e-02   valLoss:5.1628e-01  time: 8.71e-02\n",
      "epoch: 19   trainLoss: 6.7921e-02   valLoss:4.7613e-01  time: 8.72e-02\n",
      "epoch: 20   trainLoss: 6.2421e-02   valLoss:4.2115e-01  time: 8.94e-02\n",
      "epoch: 21   trainLoss: 5.8011e-02   valLoss:3.5793e-01  time: 8.82e-02\n",
      "epoch: 22   trainLoss: 5.3732e-02   valLoss:3.0426e-01  time: 8.73e-02\n",
      "epoch: 23   trainLoss: 4.9936e-02   valLoss:2.6093e-01  time: 8.80e-02\n",
      "epoch: 24   trainLoss: 4.7756e-02   valLoss:2.2943e-01  time: 8.96e-02\n",
      "epoch: 25   trainLoss: 4.6149e-02   valLoss:2.1140e-01  time: 8.76e-02\n",
      "epoch: 26   trainLoss: 4.4585e-02   valLoss:1.9798e-01  time: 8.78e-02\n",
      "epoch: 27   trainLoss: 4.2992e-02   valLoss:1.8208e-01  time: 8.95e-02\n",
      "epoch: 28   trainLoss: 4.1817e-02   valLoss:1.6691e-01  time: 8.98e-02\n",
      "epoch: 29   trainLoss: 4.0630e-02   valLoss:1.5398e-01  time: 8.97e-02\n",
      "epoch: 30   trainLoss: 3.9186e-02   valLoss:1.4140e-01  time: 8.86e-02\n",
      "epoch: 31   trainLoss: 3.7849e-02   valLoss:1.3118e-01  time: 9.22e-02\n",
      "epoch: 32   trainLoss: 3.6624e-02   valLoss:1.2369e-01  time: 8.93e-02\n",
      "epoch: 33   trainLoss: 3.5429e-02   valLoss:1.1767e-01  time: 8.97e-02\n",
      "epoch: 34   trainLoss: 3.4775e-02   valLoss:1.1247e-01  time: 8.89e-02\n",
      "epoch: 35   trainLoss: 3.4099e-02   valLoss:1.0855e-01  time: 8.97e-02\n",
      "epoch: 36   trainLoss: 3.3614e-02   valLoss:1.0507e-01  time: 8.88e-02\n",
      "epoch: 37   trainLoss: 3.3188e-02   valLoss:1.0282e-01  time: 9.05e-02\n",
      "epoch: 38   trainLoss: 3.2919e-02   valLoss:1.0203e-01  time: 8.89e-02\n",
      "epoch: 39   trainLoss: 3.2839e-02   valLoss:1.0270e-01  time: 9.10e-02\n",
      "epoch: 40   trainLoss: 3.2728e-02   valLoss:1.0430e-01  time: 8.90e-02\n",
      "epoch: 41   trainLoss: 3.2487e-02   valLoss:1.0611e-01  time: 8.97e-02\n",
      "epoch: 42   trainLoss: 3.2286e-02   valLoss:1.0777e-01  time: 8.90e-02\n",
      "epoch: 43   trainLoss: 3.2066e-02   valLoss:1.0956e-01  time: 8.87e-02\n",
      "epoch: 44   trainLoss: 3.1856e-02   valLoss:1.1151e-01  time: 8.84e-02\n",
      "epoch: 45   trainLoss: 3.1705e-02   valLoss:1.1370e-01  time: 8.90e-02\n",
      "epoch: 46   trainLoss: 3.1534e-02   valLoss:1.1656e-01  time: 8.81e-02\n",
      "epoch: 47   trainLoss: 3.1474e-02   valLoss:1.1907e-01  time: 8.87e-02\n",
      "epoch: 48   trainLoss: 3.1285e-02   valLoss:1.2103e-01  time: 8.87e-02\n",
      "epoch: 49   trainLoss: 3.0996e-02   valLoss:1.2290e-01  time: 8.82e-02\n",
      "epoch: 50   trainLoss: 3.0979e-02   valLoss:1.2519e-01  time: 8.96e-02\n",
      "epoch: 51   trainLoss: 3.1791e-02   valLoss:1.2630e-01  time: 8.94e-02\n",
      "epoch: 52   trainLoss: 3.1443e-02   valLoss:1.2729e-01  time: 9.00e-02\n",
      "epoch: 53   trainLoss: 3.0267e-02   valLoss:1.2758e-01  time: 8.91e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54   trainLoss: 3.0039e-02   valLoss:1.2635e-01  time: 8.66e-02\n",
      "epoch: 55   trainLoss: 2.9980e-02   valLoss:1.2272e-01  time: 8.67e-02\n",
      "epoch: 56   trainLoss: 3.0481e-02   valLoss:1.2233e-01  time: 8.70e-02\n",
      "epoch: 57   trainLoss: 3.0366e-02   valLoss:1.2433e-01  time: 8.72e-02\n",
      "epoch: 58   trainLoss: 2.8833e-02   valLoss:1.2264e-01  time: 8.67e-02\n",
      "epoch: 59   trainLoss: 2.9182e-02   valLoss:1.1842e-01  time: 8.78e-02\n",
      "epoch: 60   trainLoss: 2.8352e-02   valLoss:1.1662e-01  time: 8.73e-02\n",
      "epoch: 61   trainLoss: 2.8113e-02   valLoss:1.1972e-01  time: 8.90e-02\n",
      "epoch: 62   trainLoss: 2.7261e-02   valLoss:1.2143e-01  time: 9.07e-02\n",
      "epoch: 63   trainLoss: 2.6778e-02   valLoss:1.1943e-01  time: 9.15e-02\n",
      "epoch: 64   trainLoss: 2.6885e-02   valLoss:1.2168e-01  time: 9.18e-02\n",
      "epoch: 65   trainLoss: 2.7143e-02   valLoss:1.1702e-01  time: 8.96e-02\n",
      "epoch: 66   trainLoss: 2.7102e-02   valLoss:1.1962e-01  time: 8.88e-02\n",
      "epoch: 67   trainLoss: 2.7318e-02   valLoss:1.1849e-01  time: 8.93e-02\n",
      "epoch: 68   trainLoss: 2.6412e-02   valLoss:1.1851e-01  time: 9.25e-02\n",
      "epoch: 69   trainLoss: 2.6781e-02   valLoss:1.1794e-01  time: 8.92e-02\n",
      "epoch: 70   trainLoss: 2.6266e-02   valLoss:1.1652e-01  time: 9.90e-02\n",
      "epoch: 71   trainLoss: 2.6516e-02   valLoss:1.1617e-01  time: 8.99e-02\n",
      "epoch: 72   trainLoss: 2.6073e-02   valLoss:1.1523e-01  time: 8.99e-02\n",
      "epoch: 73   trainLoss: 2.6241e-02   valLoss:1.1260e-01  time: 9.28e-02\n",
      "epoch: 74   trainLoss: 2.6202e-02   valLoss:1.1378e-01  time: 8.92e-02\n",
      "epoch: 75   trainLoss: 2.6189e-02   valLoss:1.1459e-01  time: 8.98e-02\n",
      "epoch: 76   trainLoss: 2.5988e-02   valLoss:1.1628e-01  time: 8.91e-02\n",
      "epoch: 77   trainLoss: 2.5940e-02   valLoss:1.1685e-01  time: 8.92e-02\n",
      "epoch: 78   trainLoss: 2.5877e-02   valLoss:1.1562e-01  time: 8.72e-02\n",
      "epoch: 79   trainLoss: 2.5689e-02   valLoss:1.1456e-01  time: 8.69e-02\n",
      "epoch: 80   trainLoss: 2.5664e-02   valLoss:1.1452e-01  time: 8.66e-02\n",
      "epoch: 81   trainLoss: 2.5584e-02   valLoss:1.1474e-01  time: 8.96e-02\n",
      "epoch: 82   trainLoss: 2.5405e-02   valLoss:1.1501e-01  time: 8.87e-02\n",
      "epoch: 83   trainLoss: 2.5266e-02   valLoss:1.1613e-01  time: 8.73e-02\n",
      "epoch: 84   trainLoss: 2.5103e-02   valLoss:1.1593e-01  time: 8.92e-02\n",
      "epoch: 85   trainLoss: 2.4841e-02   valLoss:1.1554e-01  time: 8.83e-02\n",
      "epoch: 86   trainLoss: 2.4625e-02   valLoss:1.1601e-01  time: 8.68e-02\n",
      "epoch: 87   trainLoss: 2.4370e-02   valLoss:1.1711e-01  time: 8.88e-02\n",
      "epoch: 88   trainLoss: 2.4060e-02   valLoss:1.1901e-01  time: 8.68e-02\n",
      "epoch: 89   trainLoss: 2.3819e-02   valLoss:1.1821e-01  time: 8.59e-02\n",
      "epoch: 90   trainLoss: 2.3635e-02   valLoss:1.1906e-01  time: 8.70e-02\n",
      "epoch: 91   trainLoss: 2.3438e-02   valLoss:1.1757e-01  time: 8.76e-02\n",
      "epoch: 92   trainLoss: 2.3044e-02   valLoss:1.2063e-01  time: 8.61e-02\n",
      "epoch: 93   trainLoss: 2.2695e-02   valLoss:1.2097e-01  time: 8.85e-02\n",
      "epoch: 94   trainLoss: 2.2539e-02   valLoss:1.2202e-01  time: 8.70e-02\n",
      "epoch: 95   trainLoss: 2.2999e-02   valLoss:1.1740e-01  time: 9.29e-02\n",
      "epoch: 96   trainLoss: 2.4268e-02   valLoss:1.3931e-01  time: 8.90e-02\n",
      "epoch: 97   trainLoss: 2.6684e-02   valLoss:1.2031e-01  time: 9.70e-02\n",
      "epoch: 98   trainLoss: 2.3978e-02   valLoss:1.0120e-01  time: 9.06e-02\n",
      "epoch: 99   trainLoss: 2.5358e-02   valLoss:1.0869e-01  time: 9.06e-02\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.2844e+00   valLoss:9.5375e+02  time: 9.09e-02\n",
      "epoch: 1   trainLoss: 4.8855e-01   valLoss:1.8512e+02  time: 9.05e-02\n",
      "epoch: 2   trainLoss: 2.3284e-01   valLoss:3.9206e+01  time: 9.01e-02\n",
      "epoch: 3   trainLoss: 1.3727e-01   valLoss:6.8385e+00  time: 1.08e-01\n",
      "epoch: 4   trainLoss: 9.1238e-02   valLoss:1.0199e+00  time: 8.94e-02\n",
      "epoch: 5   trainLoss: 8.0098e-02   valLoss:5.7738e-01  time: 9.00e-02\n",
      "epoch: 6   trainLoss: 7.6982e-02   valLoss:3.3565e-01  time: 9.19e-02\n",
      "epoch: 7   trainLoss: 7.1643e-02   valLoss:1.9810e-01  time: 9.23e-02\n",
      "epoch: 8   trainLoss: 6.8404e-02   valLoss:1.2913e-01  time: 8.97e-02\n",
      "epoch: 9   trainLoss: 6.4141e-02   valLoss:8.5676e-02  time: 8.95e-02\n",
      "epoch: 10   trainLoss: 5.8631e-02   valLoss:6.8869e-02  time: 8.95e-02\n",
      "epoch: 11   trainLoss: 5.4283e-02   valLoss:6.5780e-02  time: 9.11e-02\n",
      "epoch: 12   trainLoss: 5.0364e-02   valLoss:6.5943e-02  time: 8.97e-02\n",
      "epoch: 13   trainLoss: 4.6235e-02   valLoss:6.8506e-02  time: 8.90e-02\n",
      "epoch: 14   trainLoss: 4.3141e-02   valLoss:6.9190e-02  time: 8.86e-02\n",
      "epoch: 15   trainLoss: 4.1504e-02   valLoss:6.8191e-02  time: 8.85e-02\n",
      "epoch: 16   trainLoss: 4.0373e-02   valLoss:6.7098e-02  time: 8.91e-02\n",
      "epoch: 17   trainLoss: 3.9431e-02   valLoss:6.5889e-02  time: 8.93e-02\n",
      "epoch: 18   trainLoss: 3.8858e-02   valLoss:6.2731e-02  time: 8.88e-02\n",
      "epoch: 19   trainLoss: 3.8330e-02   valLoss:5.7601e-02  time: 8.94e-02\n",
      "epoch: 20   trainLoss: 3.7434e-02   valLoss:5.2043e-02  time: 8.94e-02\n",
      "epoch: 21   trainLoss: 3.6182e-02   valLoss:4.7998e-02  time: 9.01e-02\n",
      "epoch: 22   trainLoss: 3.4568e-02   valLoss:4.5762e-02  time: 8.94e-02\n",
      "epoch: 23   trainLoss: 3.2753e-02   valLoss:4.5367e-02  time: 8.95e-02\n",
      "epoch: 24   trainLoss: 3.1177e-02   valLoss:4.5702e-02  time: 8.99e-02\n",
      "epoch: 25   trainLoss: 3.0070e-02   valLoss:4.5595e-02  time: 8.87e-02\n",
      "epoch: 26   trainLoss: 2.9210e-02   valLoss:4.4466e-02  time: 8.93e-02\n",
      "epoch: 27   trainLoss: 2.8443e-02   valLoss:4.2818e-02  time: 8.96e-02\n",
      "epoch: 28   trainLoss: 2.7897e-02   valLoss:4.1281e-02  time: 8.90e-02\n",
      "epoch: 29   trainLoss: 2.7322e-02   valLoss:3.9569e-02  time: 8.90e-02\n",
      "epoch: 30   trainLoss: 2.6572e-02   valLoss:3.7062e-02  time: 8.92e-02\n",
      "epoch: 31   trainLoss: 2.5712e-02   valLoss:3.4420e-02  time: 8.89e-02\n",
      "epoch: 32   trainLoss: 2.4800e-02   valLoss:3.2281e-02  time: 8.87e-02\n",
      "epoch: 33   trainLoss: 2.3919e-02   valLoss:3.0657e-02  time: 8.99e-02\n",
      "epoch: 34   trainLoss: 2.3136e-02   valLoss:2.9089e-02  time: 9.24e-02\n",
      "epoch: 35   trainLoss: 2.2368e-02   valLoss:2.7586e-02  time: 8.99e-02\n",
      "epoch: 36   trainLoss: 2.1562e-02   valLoss:2.6389e-02  time: 9.09e-02\n",
      "epoch: 37   trainLoss: 2.0706e-02   valLoss:2.5729e-02  time: 9.12e-02\n",
      "epoch: 38   trainLoss: 1.9753e-02   valLoss:2.5718e-02  time: 9.16e-02\n",
      "epoch: 39   trainLoss: 1.8781e-02   valLoss:2.6052e-02  time: 9.04e-02\n",
      "epoch: 40   trainLoss: 1.7892e-02   valLoss:2.6131e-02  time: 8.93e-02\n",
      "epoch: 41   trainLoss: 1.7176e-02   valLoss:2.5707e-02  time: 8.91e-02\n",
      "epoch: 42   trainLoss: 1.6618e-02   valLoss:2.4420e-02  time: 8.97e-02\n",
      "epoch: 43   trainLoss: 1.6206e-02   valLoss:2.2461e-02  time: 9.31e-02\n",
      "epoch: 44   trainLoss: 1.5851e-02   valLoss:2.0630e-02  time: 9.21e-02\n",
      "epoch: 45   trainLoss: 1.5477e-02   valLoss:1.9499e-02  time: 8.98e-02\n",
      "epoch: 46   trainLoss: 1.5115e-02   valLoss:1.8722e-02  time: 9.05e-02\n",
      "epoch: 47   trainLoss: 1.4740e-02   valLoss:1.8228e-02  time: 9.36e-02\n",
      "epoch: 48   trainLoss: 1.4381e-02   valLoss:1.8050e-02  time: 9.14e-02\n",
      "epoch: 49   trainLoss: 1.4057e-02   valLoss:1.7974e-02  time: 8.83e-02\n",
      "epoch: 50   trainLoss: 1.3710e-02   valLoss:1.7945e-02  time: 8.84e-02\n",
      "epoch: 51   trainLoss: 1.3334e-02   valLoss:1.8022e-02  time: 9.18e-02\n",
      "epoch: 52   trainLoss: 1.2929e-02   valLoss:1.8172e-02  time: 8.95e-02\n",
      "epoch: 53   trainLoss: 1.2470e-02   valLoss:1.8097e-02  time: 8.68e-02\n",
      "epoch: 54   trainLoss: 1.1980e-02   valLoss:1.7876e-02  time: 9.81e-02\n",
      "epoch: 55   trainLoss: 1.1379e-02   valLoss:1.7848e-02  time: 9.11e-02\n",
      "epoch: 56   trainLoss: 1.0711e-02   valLoss:1.7931e-02  time: 9.13e-02\n",
      "epoch: 57   trainLoss: 9.9990e-03   valLoss:1.7665e-02  time: 9.02e-02\n",
      "epoch: 58   trainLoss: 9.2538e-03   valLoss:1.7056e-02  time: 8.97e-02\n",
      "epoch: 59   trainLoss: 8.4940e-03   valLoss:1.6764e-02  time: 9.01e-02\n",
      "epoch: 60   trainLoss: 7.7507e-03   valLoss:1.7062e-02  time: 9.30e-02\n",
      "epoch: 61   trainLoss: 7.0879e-03   valLoss:1.7092e-02  time: 8.99e-02\n",
      "epoch: 62   trainLoss: 6.4968e-03   valLoss:1.6491e-02  time: 8.98e-02\n",
      "epoch: 63   trainLoss: 5.9333e-03   valLoss:1.6571e-02  time: 9.06e-02\n",
      "epoch: 64   trainLoss: 5.4441e-03   valLoss:1.7012e-02  time: 8.92e-02\n",
      "epoch: 65   trainLoss: 4.8797e-03   valLoss:1.5847e-02  time: 8.92e-02\n",
      "epoch: 66   trainLoss: 4.0487e-03   valLoss:1.5687e-02  time: 8.96e-02\n",
      "epoch: 67   trainLoss: 4.0959e-03   valLoss:1.7005e-02  time: 9.18e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68   trainLoss: 3.5958e-03   valLoss:1.4356e-02  time: 8.93e-02\n",
      "epoch: 69   trainLoss: 3.0538e-03   valLoss:1.5150e-02  time: 8.91e-02\n",
      "epoch: 70   trainLoss: 2.8351e-03   valLoss:1.5783e-02  time: 9.22e-02\n",
      "epoch: 71   trainLoss: 2.3419e-03   valLoss:1.4074e-02  time: 8.98e-02\n",
      "epoch: 72   trainLoss: 2.5381e-03   valLoss:1.3684e-02  time: 8.97e-02\n",
      "epoch: 73   trainLoss: 2.0211e-03   valLoss:1.1058e-02  time: 9.13e-02\n",
      "epoch: 74   trainLoss: 1.7098e-03   valLoss:9.1225e-03  time: 9.12e-02\n",
      "epoch: 75   trainLoss: 1.3722e-03   valLoss:7.7825e-03  time: 9.23e-02\n",
      "epoch: 76   trainLoss: 1.3111e-03   valLoss:6.5160e-03  time: 9.13e-02\n",
      "epoch: 77   trainLoss: 1.1106e-03   valLoss:6.9396e-03  time: 9.13e-02\n",
      "epoch: 78   trainLoss: 8.7623e-04   valLoss:7.3824e-03  time: 9.19e-02\n",
      "epoch: 79   trainLoss: 8.2110e-04   valLoss:7.3017e-03  time: 9.55e-02\n",
      "epoch: 80   trainLoss: 6.6993e-04   valLoss:7.5767e-03  time: 9.10e-02\n",
      "epoch: 81   trainLoss: 5.4434e-04   valLoss:8.0777e-03  time: 9.05e-02\n",
      "epoch: 82   trainLoss: 5.4058e-04   valLoss:8.1369e-03  time: 9.15e-02\n",
      "epoch: 83   trainLoss: 3.9683e-04   valLoss:8.6610e-03  time: 9.14e-02\n",
      "epoch: 84   trainLoss: 4.6579e-04   valLoss:8.9004e-03  time: 9.15e-02\n",
      "epoch: 85   trainLoss: 3.3573e-04   valLoss:8.8886e-03  time: 9.07e-02\n",
      "epoch: 86   trainLoss: 3.7009e-04   valLoss:8.8219e-03  time: 9.05e-02\n",
      "epoch: 87   trainLoss: 3.0319e-04   valLoss:8.6149e-03  time: 9.06e-02\n",
      "epoch: 88   trainLoss: 2.7015e-04   valLoss:8.3042e-03  time: 9.12e-02\n",
      "epoch: 89   trainLoss: 2.9003e-04   valLoss:7.7004e-03  time: 9.08e-02\n",
      "epoch: 90   trainLoss: 2.2630e-04   valLoss:7.6003e-03  time: 1.04e-01\n",
      "epoch: 91   trainLoss: 2.3767e-04   valLoss:7.5050e-03  time: 9.28e-02\n",
      "epoch: 92   trainLoss: 1.9239e-04   valLoss:7.1102e-03  time: 9.34e-02\n",
      "epoch: 93   trainLoss: 1.8783e-04   valLoss:6.8314e-03  time: 9.22e-02\n",
      "epoch: 94   trainLoss: 1.5697e-04   valLoss:7.1010e-03  time: 9.07e-02\n",
      "epoch: 95   trainLoss: 1.5096e-04   valLoss:7.2281e-03  time: 9.19e-02\n",
      "epoch: 96   trainLoss: 1.2825e-04   valLoss:7.1378e-03  time: 9.09e-02\n",
      "epoch: 97   trainLoss: 1.1614e-04   valLoss:7.3214e-03  time: 8.99e-02\n",
      "epoch: 98   trainLoss: 1.0117e-04   valLoss:7.6746e-03  time: 9.08e-02\n",
      "epoch: 99   trainLoss: 9.6820e-05   valLoss:7.4309e-03  time: 9.16e-02\n",
      "loading checkpoint 76\n",
      "trained 24 random forest models in 2.73 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_100/\n",
      "loaded train set of size 92\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 9.3879e-01   valLoss:9.6050e-01  time: 7.74e-01\n",
      "epoch: 1   trainLoss: 6.8950e-01   valLoss:9.4932e-01  time: 7.65e-01\n",
      "epoch: 2   trainLoss: 5.1119e-01   valLoss:9.3196e-01  time: 7.51e-01\n",
      "epoch: 3   trainLoss: 3.4998e-01   valLoss:9.1759e-01  time: 7.49e-01\n",
      "epoch: 4   trainLoss: 2.4970e-01   valLoss:8.9441e-01  time: 7.58e-01\n",
      "epoch: 5   trainLoss: 1.7172e-01   valLoss:8.7974e-01  time: 7.69e-01\n",
      "epoch: 6   trainLoss: 1.2052e-01   valLoss:8.6669e-01  time: 7.43e-01\n",
      "epoch: 7   trainLoss: 9.4717e-02   valLoss:8.5232e-01  time: 7.41e-01\n",
      "epoch: 8   trainLoss: 8.9543e-02   valLoss:8.3301e-01  time: 7.53e-01\n",
      "epoch: 9   trainLoss: 9.0302e-02   valLoss:8.1243e-01  time: 7.45e-01\n",
      "epoch: 10   trainLoss: 8.9635e-02   valLoss:7.7612e-01  time: 7.48e-01\n",
      "epoch: 11   trainLoss: 8.3973e-02   valLoss:7.2993e-01  time: 7.60e-01\n",
      "epoch: 12   trainLoss: 7.8822e-02   valLoss:6.7648e-01  time: 7.43e-01\n",
      "epoch: 13   trainLoss: 7.1791e-02   valLoss:6.2388e-01  time: 7.40e-01\n",
      "epoch: 14   trainLoss: 6.2878e-02   valLoss:5.5466e-01  time: 7.42e-01\n",
      "epoch: 15   trainLoss: 5.6255e-02   valLoss:4.9619e-01  time: 7.44e-01\n",
      "epoch: 16   trainLoss: 5.2350e-02   valLoss:4.5018e-01  time: 7.47e-01\n",
      "epoch: 17   trainLoss: 4.8952e-02   valLoss:4.0571e-01  time: 7.39e-01\n",
      "epoch: 18   trainLoss: 4.5909e-02   valLoss:3.6226e-01  time: 7.24e-01\n",
      "epoch: 19   trainLoss: 4.3571e-02   valLoss:3.2427e-01  time: 7.35e-01\n",
      "epoch: 20   trainLoss: 4.1462e-02   valLoss:2.8478e-01  time: 7.44e-01\n",
      "epoch: 21   trainLoss: 3.9280e-02   valLoss:2.4858e-01  time: 7.30e-01\n",
      "epoch: 22   trainLoss: 3.7488e-02   valLoss:2.1696e-01  time: 7.39e-01\n",
      "epoch: 23   trainLoss: 3.6025e-02   valLoss:1.8395e-01  time: 7.43e-01\n",
      "epoch: 24   trainLoss: 3.4367e-02   valLoss:1.5334e-01  time: 7.26e-01\n",
      "epoch: 25   trainLoss: 3.2971e-02   valLoss:1.3210e-01  time: 7.07e-01\n",
      "epoch: 26   trainLoss: 3.1396e-02   valLoss:1.2122e-01  time: 7.30e-01\n",
      "epoch: 27   trainLoss: 2.9503e-02   valLoss:1.1559e-01  time: 7.28e-01\n",
      "epoch: 28   trainLoss: 2.8029e-02   valLoss:1.1208e-01  time: 7.37e-01\n",
      "epoch: 29   trainLoss: 2.6630e-02   valLoss:1.1046e-01  time: 7.23e-01\n",
      "epoch: 30   trainLoss: 2.5538e-02   valLoss:1.0980e-01  time: 7.33e-01\n",
      "epoch: 31   trainLoss: 2.4334e-02   valLoss:1.0988e-01  time: 7.24e-01\n",
      "epoch: 32   trainLoss: 2.3164e-02   valLoss:1.0935e-01  time: 7.26e-01\n",
      "epoch: 33   trainLoss: 2.2268e-02   valLoss:1.0751e-01  time: 7.06e-01\n",
      "epoch: 34   trainLoss: 2.1025e-02   valLoss:1.0669e-01  time: 7.16e-01\n",
      "epoch: 35   trainLoss: 1.9569e-02   valLoss:1.0715e-01  time: 7.18e-01\n",
      "epoch: 36   trainLoss: 1.8693e-02   valLoss:1.0753e-01  time: 7.15e-01\n",
      "epoch: 37   trainLoss: 1.7432e-02   valLoss:1.0866e-01  time: 7.17e-01\n",
      "epoch: 38   trainLoss: 1.6750e-02   valLoss:1.1052e-01  time: 7.23e-01\n",
      "epoch: 39   trainLoss: 1.5741e-02   valLoss:1.1110e-01  time: 7.28e-01\n",
      "epoch: 40   trainLoss: 1.5154e-02   valLoss:1.0958e-01  time: 7.20e-01\n",
      "epoch: 41   trainLoss: 1.4292e-02   valLoss:1.0762e-01  time: 7.24e-01\n",
      "epoch: 42   trainLoss: 1.3719e-02   valLoss:1.0630e-01  time: 7.21e-01\n",
      "epoch: 43   trainLoss: 1.2827e-02   valLoss:1.0511e-01  time: 7.16e-01\n",
      "epoch: 44   trainLoss: 1.2231e-02   valLoss:1.0350e-01  time: 7.15e-01\n",
      "epoch: 45   trainLoss: 1.1495e-02   valLoss:1.0303e-01  time: 7.17e-01\n",
      "epoch: 46   trainLoss: 1.0916e-02   valLoss:1.0454e-01  time: 7.18e-01\n",
      "epoch: 47   trainLoss: 1.0254e-02   valLoss:1.0582e-01  time: 7.19e-01\n",
      "epoch: 48   trainLoss: 9.7356e-03   valLoss:1.0637e-01  time: 7.16e-01\n",
      "epoch: 49   trainLoss: 9.2366e-03   valLoss:1.0910e-01  time: 7.18e-01\n",
      "epoch: 50   trainLoss: 8.7620e-03   valLoss:1.1238e-01  time: 7.25e-01\n",
      "epoch: 51   trainLoss: 8.2075e-03   valLoss:1.1292e-01  time: 7.33e-01\n",
      "epoch: 52   trainLoss: 7.6258e-03   valLoss:1.1274e-01  time: 7.07e-01\n",
      "epoch: 53   trainLoss: 7.1046e-03   valLoss:1.1425e-01  time: 7.06e-01\n",
      "epoch: 54   trainLoss: 6.5240e-03   valLoss:1.1516e-01  time: 7.03e-01\n",
      "epoch: 55   trainLoss: 5.9008e-03   valLoss:1.1605e-01  time: 7.04e-01\n",
      "epoch: 56   trainLoss: 5.2894e-03   valLoss:1.1837e-01  time: 7.05e-01\n",
      "epoch: 57   trainLoss: 4.7457e-03   valLoss:1.1969e-01  time: 6.99e-01\n",
      "epoch: 58   trainLoss: 4.2320e-03   valLoss:1.2175e-01  time: 7.08e-01\n",
      "epoch: 59   trainLoss: 3.7939e-03   valLoss:1.2012e-01  time: 7.04e-01\n",
      "epoch: 60   trainLoss: 3.3664e-03   valLoss:1.2111e-01  time: 7.01e-01\n",
      "epoch: 61   trainLoss: 3.0599e-03   valLoss:1.1424e-01  time: 7.28e-01\n",
      "epoch: 62   trainLoss: 3.8956e-03   valLoss:1.2095e-01  time: 7.08e-01\n",
      "epoch: 63   trainLoss: 8.5339e-03   valLoss:1.0722e-01  time: 7.22e-01\n",
      "epoch: 64   trainLoss: 5.9328e-03   valLoss:1.0351e-01  time: 7.24e-01\n",
      "epoch: 65   trainLoss: 3.9679e-03   valLoss:1.0451e-01  time: 7.27e-01\n",
      "epoch: 66   trainLoss: 3.5780e-03   valLoss:1.0170e-01  time: 7.02e-01\n",
      "epoch: 67   trainLoss: 4.0899e-03   valLoss:9.8345e-02  time: 7.01e-01\n",
      "epoch: 68   trainLoss: 1.9546e-03   valLoss:9.9024e-02  time: 7.09e-01\n",
      "epoch: 69   trainLoss: 3.2975e-03   valLoss:9.6693e-02  time: 6.99e-01\n",
      "epoch: 70   trainLoss: 2.0803e-03   valLoss:9.4332e-02  time: 7.08e-01\n",
      "epoch: 71   trainLoss: 2.0293e-03   valLoss:9.4672e-02  time: 7.07e-01\n",
      "epoch: 72   trainLoss: 1.3870e-03   valLoss:9.4467e-02  time: 7.40e-01\n",
      "epoch: 73   trainLoss: 1.9488e-03   valLoss:9.0713e-02  time: 7.25e-01\n",
      "epoch: 74   trainLoss: 1.2516e-03   valLoss:9.2088e-02  time: 7.31e-01\n",
      "epoch: 75   trainLoss: 1.2663e-03   valLoss:9.6647e-02  time: 7.25e-01\n",
      "epoch: 76   trainLoss: 1.2233e-03   valLoss:9.5513e-02  time: 7.04e-01\n",
      "epoch: 77   trainLoss: 1.0794e-03   valLoss:9.2037e-02  time: 7.22e-01\n",
      "epoch: 78   trainLoss: 1.0856e-03   valLoss:9.2767e-02  time: 7.25e-01\n",
      "epoch: 79   trainLoss: 7.4521e-04   valLoss:9.3585e-02  time: 7.23e-01\n",
      "epoch: 80   trainLoss: 9.6907e-04   valLoss:9.0812e-02  time: 7.05e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 81   trainLoss: 7.7320e-04   valLoss:8.8915e-02  time: 7.06e-01\n",
      "epoch: 82   trainLoss: 6.9539e-04   valLoss:8.9796e-02  time: 7.02e-01\n",
      "epoch: 83   trainLoss: 6.8533e-04   valLoss:8.8964e-02  time: 7.06e-01\n",
      "epoch: 84   trainLoss: 5.5526e-04   valLoss:8.7279e-02  time: 7.02e-01\n",
      "epoch: 85   trainLoss: 6.5107e-04   valLoss:8.7750e-02  time: 6.99e-01\n",
      "epoch: 86   trainLoss: 4.7396e-04   valLoss:8.6778e-02  time: 7.05e-01\n",
      "epoch: 87   trainLoss: 4.8492e-04   valLoss:8.4371e-02  time: 7.01e-01\n",
      "epoch: 88   trainLoss: 5.2269e-04   valLoss:8.5982e-02  time: 7.05e-01\n",
      "epoch: 89   trainLoss: 4.1484e-04   valLoss:8.6132e-02  time: 7.00e-01\n",
      "epoch: 90   trainLoss: 4.5271e-04   valLoss:8.5799e-02  time: 7.01e-01\n",
      "epoch: 91   trainLoss: 5.2255e-04   valLoss:8.6014e-02  time: 7.12e-01\n",
      "epoch: 92   trainLoss: 5.4832e-04   valLoss:8.8054e-02  time: 7.20e-01\n",
      "epoch: 93   trainLoss: 1.0747e-03   valLoss:8.2592e-02  time: 7.04e-01\n",
      "epoch: 94   trainLoss: 3.0896e-03   valLoss:9.3478e-02  time: 7.10e-01\n",
      "epoch: 95   trainLoss: 6.7465e-03   valLoss:8.1875e-02  time: 7.42e-01\n",
      "epoch: 96   trainLoss: 9.6145e-03   valLoss:8.5801e-02  time: 7.14e-01\n",
      "epoch: 97   trainLoss: 2.3950e-03   valLoss:8.6227e-02  time: 7.23e-01\n",
      "epoch: 98   trainLoss: 4.0179e-03   valLoss:7.3796e-02  time: 7.25e-01\n",
      "epoch: 99   trainLoss: 3.1522e-03   valLoss:7.2478e-02  time: 7.24e-01\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.4079e+00   valLoss:1.5733e+03  time: 7.51e-01\n",
      "epoch: 1   trainLoss: 3.1426e-01   valLoss:6.5183e+02  time: 7.52e-01\n",
      "epoch: 2   trainLoss: 4.0325e-01   valLoss:1.5733e+02  time: 7.20e-01\n",
      "epoch: 3   trainLoss: 1.9361e-01   valLoss:2.3346e+01  time: 7.22e-01\n",
      "epoch: 4   trainLoss: 9.2129e-02   valLoss:3.7105e+00  time: 7.17e-01\n",
      "epoch: 5   trainLoss: 8.5772e-02   valLoss:1.3878e+00  time: 7.13e-01\n",
      "epoch: 6   trainLoss: 7.9157e-02   valLoss:5.3847e-01  time: 7.07e-01\n",
      "epoch: 7   trainLoss: 6.7838e-02   valLoss:2.7756e-01  time: 7.19e-01\n",
      "epoch: 8   trainLoss: 6.6067e-02   valLoss:1.6426e-01  time: 7.37e-01\n",
      "epoch: 9   trainLoss: 6.5390e-02   valLoss:1.1590e-01  time: 7.26e-01\n",
      "epoch: 10   trainLoss: 6.0672e-02   valLoss:9.8533e-02  time: 7.04e-01\n",
      "epoch: 11   trainLoss: 5.4714e-02   valLoss:8.9093e-02  time: 7.05e-01\n",
      "epoch: 12   trainLoss: 5.0348e-02   valLoss:7.9504e-02  time: 7.10e-01\n",
      "epoch: 13   trainLoss: 4.7392e-02   valLoss:6.9714e-02  time: 7.04e-01\n",
      "epoch: 14   trainLoss: 4.3965e-02   valLoss:6.2637e-02  time: 7.06e-01\n",
      "epoch: 15   trainLoss: 4.1709e-02   valLoss:5.8177e-02  time: 7.06e-01\n",
      "epoch: 16   trainLoss: 4.0805e-02   valLoss:5.4088e-02  time: 7.14e-01\n",
      "epoch: 17   trainLoss: 3.9461e-02   valLoss:5.1581e-02  time: 7.06e-01\n",
      "epoch: 18   trainLoss: 3.7452e-02   valLoss:5.0425e-02  time: 7.11e-01\n",
      "epoch: 19   trainLoss: 3.5467e-02   valLoss:4.7836e-02  time: 7.12e-01\n",
      "epoch: 20   trainLoss: 3.3630e-02   valLoss:4.3503e-02  time: 7.15e-01\n",
      "epoch: 21   trainLoss: 3.1548e-02   valLoss:4.1034e-02  time: 7.07e-01\n",
      "epoch: 22   trainLoss: 3.0096e-02   valLoss:4.0266e-02  time: 7.03e-01\n",
      "epoch: 23   trainLoss: 2.9223e-02   valLoss:3.9341e-02  time: 7.29e-01\n",
      "epoch: 24   trainLoss: 2.8319e-02   valLoss:3.7089e-02  time: 7.17e-01\n",
      "epoch: 25   trainLoss: 2.7152e-02   valLoss:3.4486e-02  time: 7.03e-01\n",
      "epoch: 26   trainLoss: 2.6159e-02   valLoss:3.2503e-02  time: 7.03e-01\n",
      "epoch: 27   trainLoss: 2.5210e-02   valLoss:3.1006e-02  time: 7.08e-01\n",
      "epoch: 28   trainLoss: 2.4294e-02   valLoss:2.9452e-02  time: 7.08e-01\n",
      "epoch: 29   trainLoss: 2.3698e-02   valLoss:2.7820e-02  time: 7.03e-01\n",
      "epoch: 30   trainLoss: 2.3226e-02   valLoss:2.6270e-02  time: 7.09e-01\n",
      "epoch: 31   trainLoss: 2.2565e-02   valLoss:2.5157e-02  time: 7.05e-01\n",
      "epoch: 32   trainLoss: 2.2027e-02   valLoss:2.4617e-02  time: 7.04e-01\n",
      "epoch: 33   trainLoss: 2.1675e-02   valLoss:2.4722e-02  time: 7.25e-01\n",
      "epoch: 34   trainLoss: 2.1235e-02   valLoss:2.5112e-02  time: 7.08e-01\n",
      "epoch: 35   trainLoss: 2.0755e-02   valLoss:2.5214e-02  time: 7.11e-01\n",
      "epoch: 36   trainLoss: 2.0319e-02   valLoss:2.4845e-02  time: 7.07e-01\n",
      "epoch: 37   trainLoss: 1.9860e-02   valLoss:2.4216e-02  time: 7.25e-01\n",
      "epoch: 38   trainLoss: 1.9420e-02   valLoss:2.3513e-02  time: 7.24e-01\n",
      "epoch: 39   trainLoss: 1.8990e-02   valLoss:2.3061e-02  time: 7.24e-01\n",
      "epoch: 40   trainLoss: 1.8533e-02   valLoss:2.2948e-02  time: 7.17e-01\n",
      "epoch: 41   trainLoss: 1.8132e-02   valLoss:2.2839e-02  time: 7.22e-01\n",
      "epoch: 42   trainLoss: 1.7744e-02   valLoss:2.2374e-02  time: 7.13e-01\n",
      "epoch: 43   trainLoss: 1.7251e-02   valLoss:2.1815e-02  time: 7.08e-01\n",
      "epoch: 44   trainLoss: 1.6699e-02   valLoss:2.1610e-02  time: 7.06e-01\n",
      "epoch: 45   trainLoss: 1.6102e-02   valLoss:2.1556e-02  time: 7.06e-01\n",
      "epoch: 46   trainLoss: 1.5487e-02   valLoss:2.1274e-02  time: 7.03e-01\n",
      "epoch: 47   trainLoss: 1.4916e-02   valLoss:2.0543e-02  time: 7.22e-01\n",
      "epoch: 48   trainLoss: 1.4337e-02   valLoss:1.9577e-02  time: 7.25e-01\n",
      "epoch: 49   trainLoss: 1.3724e-02   valLoss:1.8710e-02  time: 7.02e-01\n",
      "epoch: 50   trainLoss: 1.3119e-02   valLoss:1.7947e-02  time: 7.22e-01\n",
      "epoch: 51   trainLoss: 1.2531e-02   valLoss:1.7321e-02  time: 7.29e-01\n",
      "epoch: 52   trainLoss: 1.1964e-02   valLoss:1.6699e-02  time: 7.30e-01\n",
      "epoch: 53   trainLoss: 1.1386e-02   valLoss:1.5954e-02  time: 7.29e-01\n",
      "epoch: 54   trainLoss: 1.0754e-02   valLoss:1.5452e-02  time: 7.26e-01\n",
      "epoch: 55   trainLoss: 1.0078e-02   valLoss:1.5334e-02  time: 7.21e-01\n",
      "epoch: 56   trainLoss: 9.3759e-03   valLoss:1.5466e-02  time: 7.21e-01\n",
      "epoch: 57   trainLoss: 8.6887e-03   valLoss:1.5471e-02  time: 7.20e-01\n",
      "epoch: 58   trainLoss: 8.0342e-03   valLoss:1.5024e-02  time: 7.20e-01\n",
      "epoch: 59   trainLoss: 7.4184e-03   valLoss:1.4501e-02  time: 7.06e-01\n",
      "epoch: 60   trainLoss: 6.8623e-03   valLoss:1.4529e-02  time: 7.01e-01\n",
      "epoch: 61   trainLoss: 6.3166e-03   valLoss:1.4092e-02  time: 7.39e-01\n",
      "epoch: 62   trainLoss: 5.8635e-03   valLoss:1.4395e-02  time: 7.13e-01\n",
      "epoch: 63   trainLoss: 5.6920e-03   valLoss:1.2967e-02  time: 7.07e-01\n",
      "epoch: 64   trainLoss: 6.8206e-03   valLoss:1.7611e-02  time: 7.05e-01\n",
      "epoch: 65   trainLoss: 1.2147e-02   valLoss:1.1879e-02  time: 7.06e-01\n",
      "epoch: 66   trainLoss: 5.0727e-03   valLoss:1.1812e-02  time: 7.22e-01\n",
      "epoch: 67   trainLoss: 6.6865e-03   valLoss:1.0448e-02  time: 7.30e-01\n",
      "epoch: 68   trainLoss: 6.6350e-03   valLoss:8.9603e-03  time: 7.28e-01\n",
      "epoch: 69   trainLoss: 4.4588e-03   valLoss:1.2014e-02  time: 7.27e-01\n",
      "epoch: 70   trainLoss: 4.0168e-03   valLoss:1.2175e-02  time: 7.35e-01\n",
      "epoch: 71   trainLoss: 3.3588e-03   valLoss:9.3406e-03  time: 7.24e-01\n",
      "epoch: 72   trainLoss: 2.7801e-03   valLoss:5.7524e-03  time: 7.23e-01\n",
      "epoch: 73   trainLoss: 2.1202e-03   valLoss:4.2973e-03  time: 7.31e-01\n",
      "epoch: 74   trainLoss: 2.6304e-03   valLoss:3.7419e-03  time: 7.20e-01\n",
      "epoch: 75   trainLoss: 1.8891e-03   valLoss:5.2730e-03  time: 7.31e-01\n",
      "epoch: 76   trainLoss: 1.4385e-03   valLoss:6.4095e-03  time: 7.22e-01\n",
      "epoch: 77   trainLoss: 1.8887e-03   valLoss:5.2640e-03  time: 7.30e-01\n",
      "epoch: 78   trainLoss: 1.4441e-03   valLoss:4.4063e-03  time: 7.24e-01\n",
      "epoch: 79   trainLoss: 1.2919e-03   valLoss:4.2666e-03  time: 7.29e-01\n",
      "epoch: 80   trainLoss: 1.2112e-03   valLoss:4.4378e-03  time: 7.25e-01\n",
      "epoch: 81   trainLoss: 9.4311e-04   valLoss:4.6415e-03  time: 7.27e-01\n",
      "epoch: 82   trainLoss: 1.1620e-03   valLoss:4.0346e-03  time: 7.39e-01\n",
      "epoch: 83   trainLoss: 9.8064e-04   valLoss:3.5392e-03  time: 7.28e-01\n",
      "epoch: 84   trainLoss: 8.1173e-04   valLoss:3.9545e-03  time: 7.31e-01\n",
      "epoch: 85   trainLoss: 9.4749e-04   valLoss:4.4663e-03  time: 7.31e-01\n",
      "epoch: 86   trainLoss: 6.2628e-04   valLoss:5.2145e-03  time: 7.23e-01\n",
      "epoch: 87   trainLoss: 7.9673e-04   valLoss:4.8207e-03  time: 7.21e-01\n",
      "epoch: 88   trainLoss: 6.1531e-04   valLoss:4.1978e-03  time: 7.28e-01\n",
      "epoch: 89   trainLoss: 6.1281e-04   valLoss:3.8243e-03  time: 7.36e-01\n",
      "epoch: 90   trainLoss: 5.8943e-04   valLoss:4.0841e-03  time: 7.34e-01\n",
      "epoch: 91   trainLoss: 5.2843e-04   valLoss:4.3658e-03  time: 7.30e-01\n",
      "epoch: 92   trainLoss: 4.8417e-04   valLoss:4.2933e-03  time: 7.29e-01\n",
      "epoch: 93   trainLoss: 5.0315e-04   valLoss:3.8585e-03  time: 7.23e-01\n",
      "epoch: 94   trainLoss: 3.9314e-04   valLoss:3.8839e-03  time: 7.22e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 95   trainLoss: 4.9073e-04   valLoss:4.0812e-03  time: 7.22e-01\n",
      "epoch: 96   trainLoss: 3.6026e-04   valLoss:4.5003e-03  time: 7.21e-01\n",
      "epoch: 97   trainLoss: 4.2416e-04   valLoss:4.2768e-03  time: 7.25e-01\n",
      "epoch: 98   trainLoss: 3.3061e-04   valLoss:3.9024e-03  time: 7.23e-01\n",
      "epoch: 99   trainLoss: 3.4546e-04   valLoss:3.7754e-03  time: 7.30e-01\n",
      "loading checkpoint 83\n",
      "trained 24 random forest models in 3.47 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_50/\n",
      "loaded train set of size 47\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 9.1671e-01   valLoss:9.7218e-01  time: 3.90e-01\n",
      "epoch: 1   trainLoss: 6.3223e-01   valLoss:9.6083e-01  time: 3.95e-01\n",
      "epoch: 2   trainLoss: 4.4562e-01   valLoss:9.4886e-01  time: 3.90e-01\n",
      "epoch: 3   trainLoss: 2.9254e-01   valLoss:9.2474e-01  time: 3.88e-01\n",
      "epoch: 4   trainLoss: 1.9070e-01   valLoss:9.0124e-01  time: 3.97e-01\n",
      "epoch: 5   trainLoss: 1.2525e-01   valLoss:8.6257e-01  time: 3.89e-01\n",
      "epoch: 6   trainLoss: 9.6981e-02   valLoss:8.1948e-01  time: 3.90e-01\n",
      "epoch: 7   trainLoss: 8.4708e-02   valLoss:7.6822e-01  time: 3.90e-01\n",
      "epoch: 8   trainLoss: 8.2329e-02   valLoss:7.1807e-01  time: 3.90e-01\n",
      "epoch: 9   trainLoss: 7.6583e-02   valLoss:6.6837e-01  time: 3.88e-01\n",
      "epoch: 10   trainLoss: 6.8421e-02   valLoss:6.1492e-01  time: 3.89e-01\n",
      "epoch: 11   trainLoss: 6.0179e-02   valLoss:5.6294e-01  time: 3.88e-01\n",
      "epoch: 12   trainLoss: 5.4744e-02   valLoss:5.1501e-01  time: 3.91e-01\n",
      "epoch: 13   trainLoss: 5.1183e-02   valLoss:4.6383e-01  time: 3.90e-01\n",
      "epoch: 14   trainLoss: 4.6660e-02   valLoss:4.0151e-01  time: 3.96e-01\n",
      "epoch: 15   trainLoss: 4.2732e-02   valLoss:3.4521e-01  time: 3.86e-01\n",
      "epoch: 16   trainLoss: 3.9186e-02   valLoss:2.9087e-01  time: 3.90e-01\n",
      "epoch: 17   trainLoss: 3.6409e-02   valLoss:2.4566e-01  time: 3.90e-01\n",
      "epoch: 18   trainLoss: 3.3893e-02   valLoss:2.0685e-01  time: 3.89e-01\n",
      "epoch: 19   trainLoss: 3.2534e-02   valLoss:1.7237e-01  time: 3.93e-01\n",
      "epoch: 20   trainLoss: 3.1397e-02   valLoss:1.4066e-01  time: 3.93e-01\n",
      "epoch: 21   trainLoss: 3.0639e-02   valLoss:1.1250e-01  time: 3.87e-01\n",
      "epoch: 22   trainLoss: 2.9919e-02   valLoss:8.9166e-02  time: 3.90e-01\n",
      "epoch: 23   trainLoss: 2.8723e-02   valLoss:7.8112e-02  time: 3.87e-01\n",
      "epoch: 24   trainLoss: 2.8042e-02   valLoss:7.3464e-02  time: 3.92e-01\n",
      "epoch: 25   trainLoss: 2.7244e-02   valLoss:6.8988e-02  time: 3.88e-01\n",
      "epoch: 26   trainLoss: 2.6315e-02   valLoss:6.7476e-02  time: 3.88e-01\n",
      "epoch: 27   trainLoss: 2.5554e-02   valLoss:6.4804e-02  time: 3.90e-01\n",
      "epoch: 28   trainLoss: 2.4950e-02   valLoss:6.1788e-02  time: 3.99e-01\n",
      "epoch: 29   trainLoss: 2.4512e-02   valLoss:6.2976e-02  time: 3.88e-01\n",
      "epoch: 30   trainLoss: 2.4087e-02   valLoss:6.4795e-02  time: 3.90e-01\n",
      "epoch: 31   trainLoss: 2.3951e-02   valLoss:6.3231e-02  time: 3.91e-01\n",
      "epoch: 32   trainLoss: 2.3432e-02   valLoss:6.4336e-02  time: 3.91e-01\n",
      "epoch: 33   trainLoss: 2.2902e-02   valLoss:6.9561e-02  time: 3.88e-01\n",
      "epoch: 34   trainLoss: 2.2477e-02   valLoss:6.8468e-02  time: 3.89e-01\n",
      "epoch: 35   trainLoss: 2.2048e-02   valLoss:6.1163e-02  time: 3.93e-01\n",
      "epoch: 36   trainLoss: 2.1637e-02   valLoss:5.9332e-02  time: 3.87e-01\n",
      "epoch: 37   trainLoss: 2.1330e-02   valLoss:6.0012e-02  time: 3.88e-01\n",
      "epoch: 38   trainLoss: 2.1069e-02   valLoss:5.6734e-02  time: 3.88e-01\n",
      "epoch: 39   trainLoss: 2.0765e-02   valLoss:5.8342e-02  time: 3.95e-01\n",
      "epoch: 40   trainLoss: 2.0484e-02   valLoss:6.0313e-02  time: 4.11e-01\n",
      "epoch: 41   trainLoss: 2.0275e-02   valLoss:5.5070e-02  time: 3.99e-01\n",
      "epoch: 42   trainLoss: 2.0020e-02   valLoss:5.5187e-02  time: 3.94e-01\n",
      "epoch: 43   trainLoss: 1.9664e-02   valLoss:5.4730e-02  time: 3.85e-01\n",
      "epoch: 44   trainLoss: 1.9353e-02   valLoss:5.1591e-02  time: 3.98e-01\n",
      "epoch: 45   trainLoss: 1.9058e-02   valLoss:5.3641e-02  time: 4.10e-01\n",
      "epoch: 46   trainLoss: 1.8805e-02   valLoss:4.9840e-02  time: 3.96e-01\n",
      "epoch: 47   trainLoss: 1.8472e-02   valLoss:4.9866e-02  time: 3.94e-01\n",
      "epoch: 48   trainLoss: 1.8152e-02   valLoss:4.9499e-02  time: 3.90e-01\n",
      "epoch: 49   trainLoss: 1.7859e-02   valLoss:4.7697e-02  time: 3.95e-01\n",
      "epoch: 50   trainLoss: 1.7552e-02   valLoss:4.8975e-02  time: 3.89e-01\n",
      "epoch: 51   trainLoss: 1.7173e-02   valLoss:4.6212e-02  time: 3.99e-01\n",
      "epoch: 52   trainLoss: 1.6768e-02   valLoss:4.5950e-02  time: 3.90e-01\n",
      "epoch: 53   trainLoss: 1.6314e-02   valLoss:4.3864e-02  time: 4.12e-01\n",
      "epoch: 54   trainLoss: 1.5814e-02   valLoss:4.5141e-02  time: 3.88e-01\n",
      "epoch: 55   trainLoss: 1.5286e-02   valLoss:4.3294e-02  time: 4.05e-01\n",
      "epoch: 56   trainLoss: 1.4764e-02   valLoss:4.5509e-02  time: 4.07e-01\n",
      "epoch: 57   trainLoss: 1.4428e-02   valLoss:4.1118e-02  time: 3.96e-01\n",
      "epoch: 58   trainLoss: 1.5500e-02   valLoss:5.9096e-02  time: 3.95e-01\n",
      "epoch: 59   trainLoss: 2.0408e-02   valLoss:4.2983e-02  time: 3.89e-01\n",
      "epoch: 60   trainLoss: 2.2928e-02   valLoss:4.6235e-02  time: 3.93e-01\n",
      "epoch: 61   trainLoss: 1.4186e-02   valLoss:4.5373e-02  time: 3.96e-01\n",
      "epoch: 62   trainLoss: 1.6120e-02   valLoss:3.2156e-02  time: 3.98e-01\n",
      "epoch: 63   trainLoss: 1.3224e-02   valLoss:3.4350e-02  time: 3.91e-01\n",
      "epoch: 64   trainLoss: 1.3747e-02   valLoss:3.6140e-02  time: 3.87e-01\n",
      "epoch: 65   trainLoss: 1.1953e-02   valLoss:3.5754e-02  time: 3.89e-01\n",
      "epoch: 66   trainLoss: 1.2250e-02   valLoss:2.9211e-02  time: 4.05e-01\n",
      "epoch: 67   trainLoss: 1.1339e-02   valLoss:2.7559e-02  time: 3.88e-01\n",
      "epoch: 68   trainLoss: 1.1027e-02   valLoss:3.1142e-02  time: 3.89e-01\n",
      "epoch: 69   trainLoss: 1.0308e-02   valLoss:3.2029e-02  time: 3.89e-01\n",
      "epoch: 70   trainLoss: 1.0397e-02   valLoss:2.6597e-02  time: 3.86e-01\n",
      "epoch: 71   trainLoss: 9.2654e-03   valLoss:2.4565e-02  time: 3.94e-01\n",
      "epoch: 72   trainLoss: 9.5520e-03   valLoss:2.5615e-02  time: 3.89e-01\n",
      "epoch: 73   trainLoss: 8.6117e-03   valLoss:2.6049e-02  time: 3.88e-01\n",
      "epoch: 74   trainLoss: 8.7281e-03   valLoss:2.2597e-02  time: 3.86e-01\n",
      "epoch: 75   trainLoss: 8.0052e-03   valLoss:2.1014e-02  time: 3.89e-01\n",
      "epoch: 76   trainLoss: 8.1597e-03   valLoss:2.0691e-02  time: 3.87e-01\n",
      "epoch: 77   trainLoss: 7.3037e-03   valLoss:2.2160e-02  time: 3.88e-01\n",
      "epoch: 78   trainLoss: 7.1256e-03   valLoss:2.1251e-02  time: 3.96e-01\n",
      "epoch: 79   trainLoss: 6.6434e-03   valLoss:1.9813e-02  time: 3.89e-01\n",
      "epoch: 80   trainLoss: 6.3967e-03   valLoss:1.9214e-02  time: 3.87e-01\n",
      "epoch: 81   trainLoss: 6.1739e-03   valLoss:1.9741e-02  time: 3.89e-01\n",
      "epoch: 82   trainLoss: 6.0567e-03   valLoss:1.9639e-02  time: 3.92e-01\n",
      "epoch: 83   trainLoss: 5.8605e-03   valLoss:1.9000e-02  time: 3.88e-01\n",
      "epoch: 84   trainLoss: 5.6849e-03   valLoss:1.8690e-02  time: 3.83e-01\n",
      "epoch: 85   trainLoss: 5.3455e-03   valLoss:1.8890e-02  time: 3.80e-01\n",
      "epoch: 86   trainLoss: 5.0401e-03   valLoss:1.8869e-02  time: 3.86e-01\n",
      "epoch: 87   trainLoss: 4.6718e-03   valLoss:1.8927e-02  time: 3.94e-01\n",
      "epoch: 88   trainLoss: 4.0051e-03   valLoss:1.8983e-02  time: 3.85e-01\n",
      "epoch: 89   trainLoss: 3.3639e-03   valLoss:1.8755e-02  time: 3.91e-01\n",
      "epoch: 90   trainLoss: 2.4057e-03   valLoss:1.8752e-02  time: 4.03e-01\n",
      "epoch: 91   trainLoss: 2.0973e-03   valLoss:2.0109e-02  time: 3.93e-01\n",
      "epoch: 92   trainLoss: 5.9959e-03   valLoss:1.8563e-02  time: 3.91e-01\n",
      "epoch: 93   trainLoss: 3.7610e-03   valLoss:2.0720e-02  time: 3.94e-01\n",
      "epoch: 94   trainLoss: 3.5997e-03   valLoss:2.1998e-02  time: 3.88e-01\n",
      "epoch: 95   trainLoss: 2.8875e-03   valLoss:1.8869e-02  time: 3.90e-01\n",
      "epoch: 96   trainLoss: 2.5166e-03   valLoss:1.7274e-02  time: 3.87e-01\n",
      "epoch: 97   trainLoss: 2.4077e-03   valLoss:1.5685e-02  time: 3.89e-01\n",
      "epoch: 98   trainLoss: 2.1030e-03   valLoss:1.6534e-02  time: 3.90e-01\n",
      "epoch: 99   trainLoss: 2.0345e-03   valLoss:1.5099e-02  time: 3.94e-01\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.3590e+00   valLoss:9.4729e+02  time: 3.94e-01\n",
      "epoch: 1   trainLoss: 3.2956e-01   valLoss:3.1353e+02  time: 3.90e-01\n",
      "epoch: 2   trainLoss: 3.5967e-01   valLoss:7.1812e+01  time: 3.90e-01\n",
      "epoch: 3   trainLoss: 1.1630e-01   valLoss:8.9177e+00  time: 3.91e-01\n",
      "epoch: 4   trainLoss: 1.1782e-01   valLoss:1.5759e+00  time: 4.07e-01\n",
      "epoch: 5   trainLoss: 7.8018e-02   valLoss:4.4486e-01  time: 3.99e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6   trainLoss: 7.4281e-02   valLoss:1.9998e-01  time: 3.91e-01\n",
      "epoch: 7   trainLoss: 6.7570e-02   valLoss:1.3814e-01  time: 3.92e-01\n",
      "epoch: 8   trainLoss: 5.9732e-02   valLoss:1.1502e-01  time: 3.89e-01\n",
      "epoch: 9   trainLoss: 5.9435e-02   valLoss:1.0532e-01  time: 3.91e-01\n",
      "epoch: 10   trainLoss: 5.8090e-02   valLoss:1.0327e-01  time: 3.91e-01\n",
      "epoch: 11   trainLoss: 5.3707e-02   valLoss:1.0427e-01  time: 3.91e-01\n",
      "epoch: 12   trainLoss: 4.9566e-02   valLoss:9.8981e-02  time: 3.90e-01\n",
      "epoch: 13   trainLoss: 4.6399e-02   valLoss:8.4771e-02  time: 3.91e-01\n",
      "epoch: 14   trainLoss: 4.3045e-02   valLoss:6.9169e-02  time: 3.92e-01\n",
      "epoch: 15   trainLoss: 3.9436e-02   valLoss:5.7476e-02  time: 3.90e-01\n",
      "epoch: 16   trainLoss: 3.6589e-02   valLoss:5.0684e-02  time: 3.92e-01\n",
      "epoch: 17   trainLoss: 3.4100e-02   valLoss:4.7163e-02  time: 3.90e-01\n",
      "epoch: 18   trainLoss: 3.1613e-02   valLoss:4.6862e-02  time: 3.91e-01\n",
      "epoch: 19   trainLoss: 2.9851e-02   valLoss:4.8294e-02  time: 3.90e-01\n",
      "epoch: 20   trainLoss: 2.9122e-02   valLoss:4.8161e-02  time: 3.89e-01\n",
      "epoch: 21   trainLoss: 2.8798e-02   valLoss:4.4875e-02  time: 3.91e-01\n",
      "epoch: 22   trainLoss: 2.7982e-02   valLoss:4.0590e-02  time: 3.92e-01\n",
      "epoch: 23   trainLoss: 2.6630e-02   valLoss:3.7639e-02  time: 3.88e-01\n",
      "epoch: 24   trainLoss: 2.5249e-02   valLoss:3.5728e-02  time: 4.07e-01\n",
      "epoch: 25   trainLoss: 2.4011e-02   valLoss:3.4177e-02  time: 3.94e-01\n",
      "epoch: 26   trainLoss: 2.3078e-02   valLoss:3.2946e-02  time: 3.91e-01\n",
      "epoch: 27   trainLoss: 2.2703e-02   valLoss:3.1816e-02  time: 3.91e-01\n",
      "epoch: 28   trainLoss: 2.2574e-02   valLoss:2.9927e-02  time: 3.90e-01\n",
      "epoch: 29   trainLoss: 2.2055e-02   valLoss:2.7684e-02  time: 3.96e-01\n",
      "epoch: 30   trainLoss: 2.1206e-02   valLoss:2.6072e-02  time: 3.92e-01\n",
      "epoch: 31   trainLoss: 2.0506e-02   valLoss:2.5083e-02  time: 3.91e-01\n",
      "epoch: 32   trainLoss: 1.9893e-02   valLoss:2.4300e-02  time: 3.92e-01\n",
      "epoch: 33   trainLoss: 1.9057e-02   valLoss:2.3822e-02  time: 3.92e-01\n",
      "epoch: 34   trainLoss: 1.8136e-02   valLoss:2.3896e-02  time: 3.88e-01\n",
      "epoch: 35   trainLoss: 1.7399e-02   valLoss:2.4200e-02  time: 4.07e-01\n",
      "epoch: 36   trainLoss: 1.6760e-02   valLoss:2.4404e-02  time: 3.95e-01\n",
      "epoch: 37   trainLoss: 1.6096e-02   valLoss:2.4205e-02  time: 3.95e-01\n",
      "epoch: 38   trainLoss: 1.5370e-02   valLoss:2.3100e-02  time: 3.90e-01\n",
      "epoch: 39   trainLoss: 1.4542e-02   valLoss:2.1608e-02  time: 4.05e-01\n",
      "epoch: 40   trainLoss: 1.3646e-02   valLoss:1.9960e-02  time: 3.93e-01\n",
      "epoch: 41   trainLoss: 1.2828e-02   valLoss:1.8484e-02  time: 3.93e-01\n",
      "epoch: 42   trainLoss: 1.2079e-02   valLoss:1.7025e-02  time: 3.97e-01\n",
      "epoch: 43   trainLoss: 1.1327e-02   valLoss:1.5749e-02  time: 3.93e-01\n",
      "epoch: 44   trainLoss: 1.0654e-02   valLoss:1.4803e-02  time: 3.95e-01\n",
      "epoch: 45   trainLoss: 1.0117e-02   valLoss:1.3967e-02  time: 4.00e-01\n",
      "epoch: 46   trainLoss: 9.6278e-03   valLoss:1.3202e-02  time: 3.94e-01\n",
      "epoch: 47   trainLoss: 9.2241e-03   valLoss:1.2749e-02  time: 3.91e-01\n",
      "epoch: 48   trainLoss: 8.9866e-03   valLoss:1.2545e-02  time: 3.93e-01\n",
      "epoch: 49   trainLoss: 8.8228e-03   valLoss:1.2540e-02  time: 3.96e-01\n",
      "epoch: 50   trainLoss: 8.6755e-03   valLoss:1.2508e-02  time: 3.93e-01\n",
      "epoch: 51   trainLoss: 8.5474e-03   valLoss:1.2206e-02  time: 3.95e-01\n",
      "epoch: 52   trainLoss: 8.3767e-03   valLoss:1.1550e-02  time: 3.92e-01\n",
      "epoch: 53   trainLoss: 8.1311e-03   valLoss:1.0834e-02  time: 3.91e-01\n",
      "epoch: 54   trainLoss: 7.8499e-03   valLoss:1.0352e-02  time: 3.96e-01\n",
      "epoch: 55   trainLoss: 7.5083e-03   valLoss:1.0122e-02  time: 3.95e-01\n",
      "epoch: 56   trainLoss: 7.0718e-03   valLoss:1.0049e-02  time: 3.92e-01\n",
      "epoch: 57   trainLoss: 6.5699e-03   valLoss:9.9190e-03  time: 3.99e-01\n",
      "epoch: 58   trainLoss: 6.0200e-03   valLoss:9.7475e-03  time: 3.91e-01\n",
      "epoch: 59   trainLoss: 5.4309e-03   valLoss:9.5054e-03  time: 4.03e-01\n",
      "epoch: 60   trainLoss: 4.8131e-03   valLoss:9.2208e-03  time: 4.02e-01\n",
      "epoch: 61   trainLoss: 4.1921e-03   valLoss:8.8558e-03  time: 3.97e-01\n",
      "epoch: 62   trainLoss: 3.5820e-03   valLoss:8.3491e-03  time: 3.95e-01\n",
      "epoch: 63   trainLoss: 3.0338e-03   valLoss:7.3655e-03  time: 3.95e-01\n",
      "epoch: 64   trainLoss: 2.5650e-03   valLoss:6.1834e-03  time: 3.95e-01\n",
      "epoch: 65   trainLoss: 2.1908e-03   valLoss:5.2768e-03  time: 3.92e-01\n",
      "epoch: 66   trainLoss: 1.9373e-03   valLoss:4.7344e-03  time: 3.95e-01\n",
      "epoch: 67   trainLoss: 1.7147e-03   valLoss:4.3454e-03  time: 3.98e-01\n",
      "epoch: 68   trainLoss: 1.5277e-03   valLoss:4.0219e-03  time: 4.03e-01\n",
      "epoch: 69   trainLoss: 1.3986e-03   valLoss:3.7790e-03  time: 3.93e-01\n",
      "epoch: 70   trainLoss: 1.3012e-03   valLoss:3.6406e-03  time: 4.02e-01\n",
      "epoch: 71   trainLoss: 1.2044e-03   valLoss:3.5963e-03  time: 3.91e-01\n",
      "epoch: 72   trainLoss: 1.0878e-03   valLoss:3.6971e-03  time: 3.91e-01\n",
      "epoch: 73   trainLoss: 9.8420e-04   valLoss:3.8996e-03  time: 3.96e-01\n",
      "epoch: 74   trainLoss: 8.9177e-04   valLoss:4.0208e-03  time: 3.90e-01\n",
      "epoch: 75   trainLoss: 7.9779e-04   valLoss:4.0251e-03  time: 3.89e-01\n",
      "epoch: 76   trainLoss: 7.0834e-04   valLoss:4.1030e-03  time: 3.91e-01\n",
      "epoch: 77   trainLoss: 6.4812e-04   valLoss:4.2355e-03  time: 3.94e-01\n",
      "epoch: 78   trainLoss: 6.1477e-04   valLoss:4.2130e-03  time: 3.90e-01\n",
      "epoch: 79   trainLoss: 5.8566e-04   valLoss:3.9946e-03  time: 3.89e-01\n",
      "epoch: 80   trainLoss: 5.4353e-04   valLoss:3.7665e-03  time: 3.88e-01\n",
      "epoch: 81   trainLoss: 4.9328e-04   valLoss:3.6035e-03  time: 3.90e-01\n",
      "epoch: 82   trainLoss: 4.4646e-04   valLoss:3.3975e-03  time: 3.91e-01\n",
      "epoch: 83   trainLoss: 4.0998e-04   valLoss:3.2302e-03  time: 3.97e-01\n",
      "epoch: 84   trainLoss: 3.8991e-04   valLoss:3.2529e-03  time: 4.06e-01\n",
      "epoch: 85   trainLoss: 3.7779e-04   valLoss:3.4001e-03  time: 3.99e-01\n",
      "epoch: 86   trainLoss: 3.6388e-04   valLoss:3.5255e-03  time: 3.89e-01\n",
      "epoch: 87   trainLoss: 3.4151e-04   valLoss:3.6776e-03  time: 3.95e-01\n",
      "epoch: 88   trainLoss: 3.1769e-04   valLoss:3.8600e-03  time: 3.93e-01\n",
      "epoch: 89   trainLoss: 3.0114e-04   valLoss:3.8958e-03  time: 3.90e-01\n",
      "epoch: 90   trainLoss: 2.8879e-04   valLoss:3.7540e-03  time: 3.90e-01\n",
      "epoch: 91   trainLoss: 2.7571e-04   valLoss:3.6307e-03  time: 3.92e-01\n",
      "epoch: 92   trainLoss: 2.6633e-04   valLoss:3.5760e-03  time: 3.90e-01\n",
      "epoch: 93   trainLoss: 2.5887e-04   valLoss:3.5214e-03  time: 3.94e-01\n",
      "epoch: 94   trainLoss: 2.4574e-04   valLoss:3.5250e-03  time: 4.14e-01\n",
      "epoch: 95   trainLoss: 2.2977e-04   valLoss:3.6429e-03  time: 3.90e-01\n",
      "epoch: 96   trainLoss: 2.2027e-04   valLoss:3.7899e-03  time: 3.90e-01\n",
      "epoch: 97   trainLoss: 2.1530e-04   valLoss:3.9210e-03  time: 3.90e-01\n",
      "epoch: 98   trainLoss: 2.0684e-04   valLoss:4.0218e-03  time: 3.92e-01\n",
      "epoch: 99   trainLoss: 1.9619e-04   valLoss:4.0435e-03  time: 3.91e-01\n",
      "loading checkpoint 83\n",
      "trained 24 random forest models in 3.03 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_5/\n",
      "loaded train set of size 4\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.0259e+00   valLoss:9.2756e-01  time: 5.17e-02\n",
      "epoch: 1   trainLoss: 7.7349e-01   valLoss:9.2395e-01  time: 5.14e-02\n",
      "epoch: 2   trainLoss: 5.9147e-01   valLoss:9.1455e-01  time: 5.00e-02\n",
      "epoch: 3   trainLoss: 4.5081e-01   valLoss:8.9543e-01  time: 4.95e-02\n",
      "epoch: 4   trainLoss: 3.5800e-01   valLoss:8.7540e-01  time: 4.97e-02\n",
      "epoch: 5   trainLoss: 2.8170e-01   valLoss:8.4168e-01  time: 4.97e-02\n",
      "epoch: 6   trainLoss: 2.2080e-01   valLoss:8.1381e-01  time: 4.93e-02\n",
      "epoch: 7   trainLoss: 1.7448e-01   valLoss:7.8430e-01  time: 5.08e-02\n",
      "epoch: 8   trainLoss: 1.2311e-01   valLoss:7.5067e-01  time: 5.00e-02\n",
      "epoch: 9   trainLoss: 1.1111e-01   valLoss:7.1895e-01  time: 4.97e-02\n",
      "epoch: 10   trainLoss: 9.7255e-02   valLoss:7.1205e-01  time: 4.92e-02\n",
      "epoch: 11   trainLoss: 9.6631e-02   valLoss:7.0825e-01  time: 5.07e-02\n",
      "epoch: 12   trainLoss: 9.2648e-02   valLoss:7.0102e-01  time: 4.92e-02\n",
      "epoch: 13   trainLoss: 9.2991e-02   valLoss:6.9320e-01  time: 4.96e-02\n",
      "epoch: 14   trainLoss: 8.7843e-02   valLoss:6.6114e-01  time: 5.10e-02\n",
      "epoch: 15   trainLoss: 8.4563e-02   valLoss:5.9836e-01  time: 5.65e-02\n",
      "epoch: 16   trainLoss: 7.7572e-02   valLoss:5.1718e-01  time: 4.95e-02\n",
      "epoch: 17   trainLoss: 6.9609e-02   valLoss:4.4409e-01  time: 4.93e-02\n",
      "epoch: 18   trainLoss: 6.2536e-02   valLoss:3.7495e-01  time: 4.92e-02\n",
      "epoch: 19   trainLoss: 5.7035e-02   valLoss:3.2218e-01  time: 4.96e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20   trainLoss: 5.3099e-02   valLoss:2.7592e-01  time: 4.99e-02\n",
      "epoch: 21   trainLoss: 5.0306e-02   valLoss:2.2647e-01  time: 4.90e-02\n",
      "epoch: 22   trainLoss: 4.8171e-02   valLoss:1.8688e-01  time: 4.91e-02\n",
      "epoch: 23   trainLoss: 4.6720e-02   valLoss:1.5336e-01  time: 5.16e-02\n",
      "epoch: 24   trainLoss: 4.5838e-02   valLoss:1.3322e-01  time: 4.93e-02\n",
      "epoch: 25   trainLoss: 4.4934e-02   valLoss:1.1465e-01  time: 4.95e-02\n",
      "epoch: 26   trainLoss: 4.4002e-02   valLoss:1.0768e-01  time: 5.03e-02\n",
      "epoch: 27   trainLoss: 4.3136e-02   valLoss:1.0338e-01  time: 4.97e-02\n",
      "epoch: 28   trainLoss: 4.1827e-02   valLoss:9.7967e-02  time: 4.96e-02\n",
      "epoch: 29   trainLoss: 4.0639e-02   valLoss:9.2692e-02  time: 4.97e-02\n",
      "epoch: 30   trainLoss: 3.9342e-02   valLoss:8.8203e-02  time: 4.94e-02\n",
      "epoch: 31   trainLoss: 3.8174e-02   valLoss:8.9926e-02  time: 4.92e-02\n",
      "epoch: 32   trainLoss: 3.7183e-02   valLoss:9.1908e-02  time: 4.92e-02\n",
      "epoch: 33   trainLoss: 3.6331e-02   valLoss:9.1286e-02  time: 5.00e-02\n",
      "epoch: 34   trainLoss: 3.5772e-02   valLoss:8.6219e-02  time: 4.90e-02\n",
      "epoch: 35   trainLoss: 3.5199e-02   valLoss:7.8500e-02  time: 4.97e-02\n",
      "epoch: 36   trainLoss: 3.4858e-02   valLoss:7.2179e-02  time: 4.95e-02\n",
      "epoch: 37   trainLoss: 3.4684e-02   valLoss:6.7738e-02  time: 4.93e-02\n",
      "epoch: 38   trainLoss: 3.4528e-02   valLoss:6.4959e-02  time: 4.90e-02\n",
      "epoch: 39   trainLoss: 3.4399e-02   valLoss:6.3541e-02  time: 4.93e-02\n",
      "epoch: 40   trainLoss: 3.4293e-02   valLoss:6.2434e-02  time: 4.89e-02\n",
      "epoch: 41   trainLoss: 3.4796e-02   valLoss:6.5007e-02  time: 4.95e-02\n",
      "epoch: 42   trainLoss: 3.5976e-02   valLoss:6.5215e-02  time: 4.97e-02\n",
      "epoch: 43   trainLoss: 3.5381e-02   valLoss:6.3745e-02  time: 4.91e-02\n",
      "epoch: 44   trainLoss: 3.4973e-02   valLoss:6.3551e-02  time: 4.97e-02\n",
      "epoch: 45   trainLoss: 3.4814e-02   valLoss:6.4177e-02  time: 4.93e-02\n",
      "epoch: 46   trainLoss: 3.4656e-02   valLoss:6.4922e-02  time: 5.06e-02\n",
      "epoch: 47   trainLoss: 3.4588e-02   valLoss:6.6741e-02  time: 4.94e-02\n",
      "epoch: 48   trainLoss: 3.4431e-02   valLoss:6.9144e-02  time: 4.84e-02\n",
      "epoch: 49   trainLoss: 3.4354e-02   valLoss:7.1687e-02  time: 4.91e-02\n",
      "epoch: 50   trainLoss: 3.4194e-02   valLoss:7.4213e-02  time: 4.81e-02\n",
      "epoch: 51   trainLoss: 3.4200e-02   valLoss:7.7024e-02  time: 4.83e-02\n",
      "epoch: 52   trainLoss: 3.4221e-02   valLoss:7.9634e-02  time: 4.85e-02\n",
      "epoch: 53   trainLoss: 3.4102e-02   valLoss:8.1337e-02  time: 4.80e-02\n",
      "epoch: 54   trainLoss: 3.4039e-02   valLoss:8.3304e-02  time: 4.83e-02\n",
      "epoch: 55   trainLoss: 3.4007e-02   valLoss:8.6073e-02  time: 4.80e-02\n",
      "epoch: 56   trainLoss: 3.4018e-02   valLoss:8.9210e-02  time: 4.82e-02\n",
      "epoch: 57   trainLoss: 3.4000e-02   valLoss:9.2466e-02  time: 4.84e-02\n",
      "epoch: 58   trainLoss: 3.3908e-02   valLoss:9.5314e-02  time: 4.75e-02\n",
      "epoch: 59   trainLoss: 3.3886e-02   valLoss:9.7325e-02  time: 4.89e-02\n",
      "epoch: 60   trainLoss: 3.3898e-02   valLoss:9.8168e-02  time: 4.94e-02\n",
      "epoch: 61   trainLoss: 3.3895e-02   valLoss:9.8719e-02  time: 4.82e-02\n",
      "epoch: 62   trainLoss: 3.3827e-02   valLoss:9.9391e-02  time: 4.92e-02\n",
      "epoch: 63   trainLoss: 3.3781e-02   valLoss:1.0052e-01  time: 4.78e-02\n",
      "epoch: 64   trainLoss: 3.3812e-02   valLoss:1.0212e-01  time: 4.85e-02\n",
      "epoch: 65   trainLoss: 3.3818e-02   valLoss:1.0381e-01  time: 4.79e-02\n",
      "epoch: 66   trainLoss: 3.3788e-02   valLoss:1.0565e-01  time: 4.80e-02\n",
      "epoch: 67   trainLoss: 3.3756e-02   valLoss:1.0691e-01  time: 4.84e-02\n",
      "epoch: 68   trainLoss: 3.3741e-02   valLoss:1.0807e-01  time: 4.79e-02\n",
      "epoch: 69   trainLoss: 3.3723e-02   valLoss:1.0902e-01  time: 4.79e-02\n",
      "epoch: 70   trainLoss: 3.3715e-02   valLoss:1.0959e-01  time: 4.92e-02\n",
      "epoch: 71   trainLoss: 3.3699e-02   valLoss:1.0972e-01  time: 4.81e-02\n",
      "epoch: 72   trainLoss: 3.3681e-02   valLoss:1.0975e-01  time: 4.85e-02\n",
      "epoch: 73   trainLoss: 3.3674e-02   valLoss:1.1019e-01  time: 4.81e-02\n",
      "epoch: 74   trainLoss: 3.3655e-02   valLoss:1.1093e-01  time: 5.11e-02\n",
      "epoch: 75   trainLoss: 3.3628e-02   valLoss:1.1163e-01  time: 4.79e-02\n",
      "epoch: 76   trainLoss: 3.3606e-02   valLoss:1.1212e-01  time: 4.75e-02\n",
      "epoch: 77   trainLoss: 3.3597e-02   valLoss:1.1240e-01  time: 4.84e-02\n",
      "epoch: 78   trainLoss: 3.3576e-02   valLoss:1.1234e-01  time: 4.85e-02\n",
      "epoch: 79   trainLoss: 3.3551e-02   valLoss:1.1235e-01  time: 4.78e-02\n",
      "epoch: 80   trainLoss: 3.3526e-02   valLoss:1.1278e-01  time: 4.90e-02\n",
      "epoch: 81   trainLoss: 3.3500e-02   valLoss:1.1328e-01  time: 4.81e-02\n",
      "epoch: 82   trainLoss: 3.3461e-02   valLoss:1.1338e-01  time: 4.81e-02\n",
      "epoch: 83   trainLoss: 3.3427e-02   valLoss:1.1290e-01  time: 4.78e-02\n",
      "epoch: 84   trainLoss: 3.3386e-02   valLoss:1.1239e-01  time: 4.78e-02\n",
      "epoch: 85   trainLoss: 3.3330e-02   valLoss:1.1254e-01  time: 4.84e-02\n",
      "epoch: 86   trainLoss: 3.3248e-02   valLoss:1.1310e-01  time: 4.82e-02\n",
      "epoch: 87   trainLoss: 3.3152e-02   valLoss:1.1387e-01  time: 4.88e-02\n",
      "epoch: 88   trainLoss: 3.3037e-02   valLoss:1.1459e-01  time: 4.82e-02\n",
      "epoch: 89   trainLoss: 3.2892e-02   valLoss:1.1390e-01  time: 4.80e-02\n",
      "epoch: 90   trainLoss: 3.2690e-02   valLoss:1.1289e-01  time: 4.90e-02\n",
      "epoch: 91   trainLoss: 3.2401e-02   valLoss:1.1255e-01  time: 4.85e-02\n",
      "epoch: 92   trainLoss: 3.1900e-02   valLoss:1.1418e-01  time: 4.83e-02\n",
      "epoch: 93   trainLoss: 3.1362e-02   valLoss:1.1613e-01  time: 4.83e-02\n",
      "epoch: 94   trainLoss: 3.0614e-02   valLoss:1.1742e-01  time: 4.98e-02\n",
      "epoch: 95   trainLoss: 2.9766e-02   valLoss:1.1707e-01  time: 4.87e-02\n",
      "epoch: 96   trainLoss: 2.9197e-02   valLoss:1.2009e-01  time: 4.80e-02\n",
      "epoch: 97   trainLoss: 2.8553e-02   valLoss:1.2031e-01  time: 4.87e-02\n",
      "epoch: 98   trainLoss: 2.7743e-02   valLoss:1.1839e-01  time: 4.83e-02\n",
      "epoch: 99   trainLoss: 2.8432e-02   valLoss:1.2532e-01  time: 4.76e-02\n",
      "loading checkpoint 40\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.5503e+00   valLoss:6.8433e+02  time: 5.26e-02\n",
      "epoch: 1   trainLoss: 3.4335e-01   valLoss:2.6600e+02  time: 4.97e-02\n",
      "epoch: 2   trainLoss: 2.8883e-01   valLoss:5.3631e+01  time: 4.97e-02\n",
      "epoch: 3   trainLoss: 2.1702e-01   valLoss:1.6492e+01  time: 5.02e-02\n",
      "epoch: 4   trainLoss: 1.1837e-01   valLoss:6.1071e+00  time: 4.93e-02\n",
      "epoch: 5   trainLoss: 9.2619e-02   valLoss:1.0539e+00  time: 5.02e-02\n",
      "epoch: 6   trainLoss: 8.4185e-02   valLoss:7.3926e-01  time: 5.11e-02\n",
      "epoch: 7   trainLoss: 8.1354e-02   valLoss:4.2546e-01  time: 5.17e-02\n",
      "epoch: 8   trainLoss: 7.2666e-02   valLoss:2.4317e-01  time: 4.94e-02\n",
      "epoch: 9   trainLoss: 6.8607e-02   valLoss:1.5884e-01  time: 4.94e-02\n",
      "epoch: 10   trainLoss: 6.7536e-02   valLoss:1.2637e-01  time: 4.90e-02\n",
      "epoch: 11   trainLoss: 6.0724e-02   valLoss:1.0956e-01  time: 4.92e-02\n",
      "epoch: 12   trainLoss: 5.7329e-02   valLoss:9.7547e-02  time: 4.92e-02\n",
      "epoch: 13   trainLoss: 5.4901e-02   valLoss:8.7807e-02  time: 4.98e-02\n",
      "epoch: 14   trainLoss: 5.1504e-02   valLoss:7.8538e-02  time: 4.94e-02\n",
      "epoch: 15   trainLoss: 4.9691e-02   valLoss:7.3175e-02  time: 4.95e-02\n",
      "epoch: 16   trainLoss: 4.9132e-02   valLoss:6.8586e-02  time: 4.94e-02\n",
      "epoch: 17   trainLoss: 4.7470e-02   valLoss:6.2656e-02  time: 4.98e-02\n",
      "epoch: 18   trainLoss: 4.6420e-02   valLoss:6.0783e-02  time: 4.99e-02\n",
      "epoch: 19   trainLoss: 4.6282e-02   valLoss:6.1940e-02  time: 6.00e-02\n",
      "epoch: 20   trainLoss: 4.5569e-02   valLoss:6.4903e-02  time: 5.02e-02\n",
      "epoch: 21   trainLoss: 4.4951e-02   valLoss:6.8072e-02  time: 5.19e-02\n",
      "epoch: 22   trainLoss: 4.4675e-02   valLoss:6.9850e-02  time: 4.98e-02\n",
      "epoch: 23   trainLoss: 4.4359e-02   valLoss:7.0037e-02  time: 5.49e-02\n",
      "epoch: 24   trainLoss: 4.3667e-02   valLoss:6.8906e-02  time: 5.05e-02\n",
      "epoch: 25   trainLoss: 4.3055e-02   valLoss:6.7415e-02  time: 4.97e-02\n",
      "epoch: 26   trainLoss: 4.2504e-02   valLoss:6.6003e-02  time: 4.95e-02\n",
      "epoch: 27   trainLoss: 4.1930e-02   valLoss:6.5023e-02  time: 4.96e-02\n",
      "epoch: 28   trainLoss: 4.1339e-02   valLoss:6.4052e-02  time: 5.07e-02\n",
      "epoch: 29   trainLoss: 4.0853e-02   valLoss:6.2925e-02  time: 5.04e-02\n",
      "epoch: 30   trainLoss: 4.0288e-02   valLoss:6.2779e-02  time: 4.98e-02\n",
      "epoch: 31   trainLoss: 3.9598e-02   valLoss:6.3305e-02  time: 5.01e-02\n",
      "epoch: 32   trainLoss: 3.8971e-02   valLoss:6.3034e-02  time: 5.02e-02\n",
      "epoch: 33   trainLoss: 3.8000e-02   valLoss:6.3610e-02  time: 4.94e-02\n",
      "epoch: 34   trainLoss: 3.6924e-02   valLoss:6.4844e-02  time: 4.96e-02\n",
      "epoch: 35   trainLoss: 3.5280e-02   valLoss:6.6106e-02  time: 4.96e-02\n",
      "epoch: 36   trainLoss: 3.3307e-02   valLoss:6.5461e-02  time: 4.91e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37   trainLoss: 3.0980e-02   valLoss:6.3763e-02  time: 5.00e-02\n",
      "epoch: 38   trainLoss: 2.8868e-02   valLoss:6.2129e-02  time: 4.95e-02\n",
      "epoch: 39   trainLoss: 2.7485e-02   valLoss:6.1561e-02  time: 5.12e-02\n",
      "epoch: 40   trainLoss: 2.7044e-02   valLoss:5.6371e-02  time: 4.94e-02\n",
      "epoch: 41   trainLoss: 2.9784e-02   valLoss:5.6149e-02  time: 4.99e-02\n",
      "epoch: 42   trainLoss: 2.8990e-02   valLoss:5.4492e-02  time: 4.91e-02\n",
      "epoch: 43   trainLoss: 2.7945e-02   valLoss:5.0125e-02  time: 5.02e-02\n",
      "epoch: 44   trainLoss: 2.6433e-02   valLoss:4.5280e-02  time: 5.03e-02\n",
      "epoch: 45   trainLoss: 2.5340e-02   valLoss:4.2577e-02  time: 4.95e-02\n",
      "epoch: 46   trainLoss: 2.4313e-02   valLoss:4.0959e-02  time: 4.99e-02\n",
      "epoch: 47   trainLoss: 2.4539e-02   valLoss:4.6175e-02  time: 5.00e-02\n",
      "epoch: 48   trainLoss: 2.8119e-02   valLoss:4.3260e-02  time: 4.90e-02\n",
      "epoch: 49   trainLoss: 2.3071e-02   valLoss:4.7298e-02  time: 4.91e-02\n",
      "epoch: 50   trainLoss: 2.3764e-02   valLoss:4.7037e-02  time: 4.92e-02\n",
      "epoch: 51   trainLoss: 2.0843e-02   valLoss:4.4729e-02  time: 4.92e-02\n",
      "epoch: 52   trainLoss: 2.0748e-02   valLoss:4.1537e-02  time: 5.01e-02\n",
      "epoch: 53   trainLoss: 1.8465e-02   valLoss:4.0680e-02  time: 4.92e-02\n",
      "epoch: 54   trainLoss: 1.8236e-02   valLoss:4.2694e-02  time: 5.08e-02\n",
      "epoch: 55   trainLoss: 1.7716e-02   valLoss:4.7001e-02  time: 4.86e-02\n",
      "epoch: 56   trainLoss: 1.6626e-02   valLoss:5.1328e-02  time: 4.76e-02\n",
      "epoch: 57   trainLoss: 1.6259e-02   valLoss:5.2202e-02  time: 4.82e-02\n",
      "epoch: 58   trainLoss: 1.5959e-02   valLoss:4.9654e-02  time: 4.79e-02\n",
      "epoch: 59   trainLoss: 1.4984e-02   valLoss:4.6729e-02  time: 4.79e-02\n",
      "epoch: 60   trainLoss: 1.4475e-02   valLoss:4.5559e-02  time: 4.80e-02\n",
      "epoch: 61   trainLoss: 1.4340e-02   valLoss:4.6203e-02  time: 4.94e-02\n",
      "epoch: 62   trainLoss: 1.3813e-02   valLoss:4.7590e-02  time: 4.93e-02\n",
      "epoch: 63   trainLoss: 1.3020e-02   valLoss:4.8522e-02  time: 4.91e-02\n",
      "epoch: 64   trainLoss: 1.2388e-02   valLoss:4.7991e-02  time: 4.91e-02\n",
      "epoch: 65   trainLoss: 1.1707e-02   valLoss:4.6596e-02  time: 4.98e-02\n",
      "epoch: 66   trainLoss: 1.0887e-02   valLoss:4.5572e-02  time: 4.95e-02\n",
      "epoch: 67   trainLoss: 1.0125e-02   valLoss:4.5603e-02  time: 4.93e-02\n",
      "epoch: 68   trainLoss: 9.5679e-03   valLoss:4.6571e-02  time: 4.89e-02\n",
      "epoch: 69   trainLoss: 9.1357e-03   valLoss:4.6780e-02  time: 4.90e-02\n",
      "epoch: 70   trainLoss: 8.7896e-03   valLoss:4.5632e-02  time: 4.88e-02\n",
      "epoch: 71   trainLoss: 8.5300e-03   valLoss:4.4084e-02  time: 4.92e-02\n",
      "epoch: 72   trainLoss: 8.2559e-03   valLoss:4.2710e-02  time: 4.93e-02\n",
      "epoch: 73   trainLoss: 8.0915e-03   valLoss:4.1348e-02  time: 4.90e-02\n",
      "epoch: 74   trainLoss: 7.8605e-03   valLoss:4.0641e-02  time: 4.90e-02\n",
      "epoch: 75   trainLoss: 7.6951e-03   valLoss:4.1261e-02  time: 4.94e-02\n",
      "epoch: 76   trainLoss: 7.5170e-03   valLoss:4.2575e-02  time: 4.93e-02\n",
      "epoch: 77   trainLoss: 7.3074e-03   valLoss:4.3687e-02  time: 4.92e-02\n",
      "epoch: 78   trainLoss: 7.1489e-03   valLoss:4.4393e-02  time: 4.94e-02\n",
      "epoch: 79   trainLoss: 6.9605e-03   valLoss:4.4579e-02  time: 4.91e-02\n",
      "epoch: 80   trainLoss: 6.6704e-03   valLoss:4.4035e-02  time: 4.91e-02\n",
      "epoch: 81   trainLoss: 6.3256e-03   valLoss:4.3230e-02  time: 4.92e-02\n",
      "epoch: 82   trainLoss: 5.9024e-03   valLoss:4.2453e-02  time: 4.96e-02\n",
      "epoch: 83   trainLoss: 5.4166e-03   valLoss:4.1398e-02  time: 4.91e-02\n",
      "epoch: 84   trainLoss: 4.8365e-03   valLoss:3.9607e-02  time: 4.93e-02\n",
      "epoch: 85   trainLoss: 4.0717e-03   valLoss:3.8686e-02  time: 4.99e-02\n",
      "epoch: 86   trainLoss: 3.1410e-03   valLoss:3.9147e-02  time: 5.00e-02\n",
      "epoch: 87   trainLoss: 2.1756e-03   valLoss:3.9927e-02  time: 4.92e-02\n",
      "epoch: 88   trainLoss: 1.4083e-03   valLoss:3.9394e-02  time: 4.93e-02\n",
      "epoch: 89   trainLoss: 9.5744e-04   valLoss:3.7612e-02  time: 4.92e-02\n",
      "epoch: 90   trainLoss: 8.4495e-04   valLoss:3.5055e-02  time: 4.94e-02\n",
      "epoch: 91   trainLoss: 1.5862e-03   valLoss:3.6524e-02  time: 4.92e-02\n",
      "epoch: 92   trainLoss: 2.0109e-03   valLoss:3.3705e-02  time: 4.96e-02\n",
      "epoch: 93   trainLoss: 2.8915e-03   valLoss:3.1381e-02  time: 4.96e-02\n",
      "epoch: 94   trainLoss: 1.8860e-03   valLoss:2.5729e-02  time: 4.93e-02\n",
      "epoch: 95   trainLoss: 1.1277e-03   valLoss:2.9153e-02  time: 4.98e-02\n",
      "epoch: 96   trainLoss: 1.0557e-03   valLoss:3.0356e-02  time: 4.90e-02\n",
      "epoch: 97   trainLoss: 9.4550e-04   valLoss:2.4902e-02  time: 4.89e-02\n",
      "epoch: 98   trainLoss: 8.0490e-04   valLoss:2.2287e-02  time: 4.91e-02\n",
      "epoch: 99   trainLoss: 7.4341e-04   valLoss:2.2894e-02  time: 4.90e-02\n",
      "loading checkpoint 98\n",
      "trained 24 random forest models in 2.66 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_1000_v2/\n",
      "loaded train set of size 909\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 7.3732e-01   valLoss:1.0131e+00  time: 7.20e+00\n",
      "epoch: 1   trainLoss: 2.1188e-01   valLoss:8.3378e-01  time: 7.21e+00\n",
      "epoch: 2   trainLoss: 8.2608e-02   valLoss:6.0159e-01  time: 7.13e+00\n",
      "epoch: 3   trainLoss: 7.6080e-02   valLoss:3.3977e-01  time: 7.09e+00\n",
      "epoch: 4   trainLoss: 5.5774e-02   valLoss:1.7873e-01  time: 7.19e+00\n",
      "epoch: 5   trainLoss: 5.1061e-02   valLoss:1.4929e-01  time: 7.11e+00\n",
      "epoch: 6   trainLoss: 4.3023e-02   valLoss:1.4987e-01  time: 7.11e+00\n",
      "epoch: 7   trainLoss: 3.5531e-02   valLoss:1.2104e-01  time: 7.21e+00\n",
      "epoch: 8   trainLoss: 3.4812e-02   valLoss:1.2368e-01  time: 7.10e+00\n",
      "epoch: 9   trainLoss: 3.6474e-02   valLoss:1.1251e-01  time: 7.15e+00\n",
      "epoch: 10   trainLoss: 3.3988e-02   valLoss:9.8401e-02  time: 7.14e+00\n",
      "epoch: 11   trainLoss: 3.3305e-02   valLoss:1.0564e-01  time: 7.12e+00\n",
      "epoch: 12   trainLoss: 3.0824e-02   valLoss:9.6565e-02  time: 7.11e+00\n",
      "epoch: 13   trainLoss: 2.9728e-02   valLoss:9.4587e-02  time: 7.14e+00\n",
      "epoch: 14   trainLoss: 2.8054e-02   valLoss:9.0499e-02  time: 7.19e+00\n",
      "epoch: 15   trainLoss: 2.5956e-02   valLoss:8.3195e-02  time: 7.13e+00\n",
      "epoch: 16   trainLoss: 2.3525e-02   valLoss:9.4252e-02  time: 7.15e+00\n",
      "epoch: 17   trainLoss: 2.2078e-02   valLoss:7.5553e-02  time: 7.11e+00\n",
      "epoch: 18   trainLoss: 2.1471e-02   valLoss:7.2297e-02  time: 7.14e+00\n",
      "epoch: 19   trainLoss: 2.0122e-02   valLoss:6.5889e-02  time: 7.09e+00\n",
      "epoch: 20   trainLoss: 1.9060e-02   valLoss:7.7867e-02  time: 7.16e+00\n",
      "epoch: 21   trainLoss: 1.7184e-02   valLoss:8.1817e-02  time: 7.06e+00\n",
      "epoch: 22   trainLoss: 1.7962e-02   valLoss:7.2699e-02  time: 7.14e+00\n",
      "epoch: 23   trainLoss: 1.7558e-02   valLoss:6.5165e-02  time: 7.04e+00\n",
      "epoch: 24   trainLoss: 1.5823e-02   valLoss:5.0243e-02  time: 7.10e+00\n",
      "epoch: 25   trainLoss: 1.4985e-02   valLoss:4.1982e-02  time: 7.03e+00\n",
      "epoch: 26   trainLoss: 1.4614e-02   valLoss:4.4386e-02  time: 7.11e+00\n",
      "epoch: 27   trainLoss: 1.3815e-02   valLoss:5.8816e-02  time: 7.20e+00\n",
      "epoch: 28   trainLoss: 1.3425e-02   valLoss:7.4335e-02  time: 7.11e+00\n",
      "epoch: 29   trainLoss: 9.1925e-03   valLoss:3.9574e-02  time: 7.07e+00\n",
      "epoch: 30   trainLoss: 3.6389e-03   valLoss:3.4885e-02  time: 7.12e+00\n",
      "epoch: 31   trainLoss: 2.4687e-03   valLoss:4.1366e-02  time: 7.16e+00\n",
      "epoch: 32   trainLoss: 2.3428e-03   valLoss:2.9220e-02  time: 7.10e+00\n",
      "epoch: 33   trainLoss: 2.3127e-03   valLoss:2.7032e-02  time: 7.18e+00\n",
      "epoch: 34   trainLoss: 3.1450e-03   valLoss:2.7727e-02  time: 7.12e+00\n",
      "epoch: 35   trainLoss: 2.2536e-03   valLoss:3.7562e-02  time: 7.11e+00\n",
      "epoch: 36   trainLoss: 1.9811e-03   valLoss:3.8385e-02  time: 7.07e+00\n",
      "epoch: 37   trainLoss: 1.7900e-03   valLoss:3.2469e-02  time: 7.10e+00\n",
      "epoch: 38   trainLoss: 2.1282e-03   valLoss:2.5602e-02  time: 7.11e+00\n",
      "epoch: 39   trainLoss: 1.5834e-03   valLoss:3.1012e-02  time: 7.11e+00\n",
      "epoch: 40   trainLoss: 1.4332e-03   valLoss:3.4366e-02  time: 7.18e+00\n",
      "epoch: 41   trainLoss: 1.4213e-03   valLoss:3.1384e-02  time: 7.12e+00\n",
      "epoch: 42   trainLoss: 2.0690e-03   valLoss:3.4461e-02  time: 7.10e+00\n",
      "epoch: 43   trainLoss: 1.6505e-03   valLoss:2.7898e-02  time: 7.07e+00\n",
      "epoch: 44   trainLoss: 1.6413e-03   valLoss:2.7559e-02  time: 7.10e+00\n",
      "epoch: 45   trainLoss: 1.1715e-03   valLoss:3.1477e-02  time: 7.12e+00\n",
      "epoch: 46   trainLoss: 1.4423e-03   valLoss:2.3147e-02  time: 7.21e+00\n",
      "epoch: 47   trainLoss: 2.4909e-03   valLoss:2.2180e-02  time: 7.13e+00\n",
      "epoch: 48   trainLoss: 1.9772e-03   valLoss:2.6122e-02  time: 7.15e+00\n",
      "epoch: 49   trainLoss: 2.2046e-03   valLoss:1.9600e-02  time: 7.09e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50   trainLoss: 1.3361e-03   valLoss:2.6183e-02  time: 7.09e+00\n",
      "epoch: 51   trainLoss: 1.2734e-03   valLoss:2.2011e-02  time: 7.11e+00\n",
      "epoch: 52   trainLoss: 2.5431e-03   valLoss:2.1636e-02  time: 7.14e+00\n",
      "epoch: 53   trainLoss: 1.2726e-03   valLoss:1.5585e-02  time: 7.20e+00\n",
      "epoch: 54   trainLoss: 1.4002e-03   valLoss:1.3951e-02  time: 7.11e+00\n",
      "epoch: 55   trainLoss: 2.5334e-03   valLoss:1.4936e-02  time: 7.10e+00\n",
      "epoch: 56   trainLoss: 1.5687e-03   valLoss:1.7395e-02  time: 7.06e+00\n",
      "epoch: 57   trainLoss: 2.0158e-03   valLoss:2.2874e-02  time: 7.03e+00\n",
      "epoch: 58   trainLoss: 1.1628e-03   valLoss:2.3256e-02  time: 7.11e+00\n",
      "epoch: 59   trainLoss: 1.2983e-03   valLoss:1.9713e-02  time: 7.12e+00\n",
      "epoch: 60   trainLoss: 1.5794e-03   valLoss:1.7587e-02  time: 7.07e+00\n",
      "epoch: 61   trainLoss: 1.8241e-03   valLoss:1.8205e-02  time: 7.13e+00\n",
      "epoch: 62   trainLoss: 3.0274e-03   valLoss:1.6327e-02  time: 7.13e+00\n",
      "epoch: 63   trainLoss: 1.7494e-03   valLoss:1.5605e-02  time: 7.08e+00\n",
      "epoch: 64   trainLoss: 1.4948e-03   valLoss:1.3522e-02  time: 7.12e+00\n",
      "epoch: 65   trainLoss: 1.6252e-03   valLoss:1.2569e-02  time: 7.06e+00\n",
      "epoch: 66   trainLoss: 2.0522e-03   valLoss:8.7454e-03  time: 7.17e+00\n",
      "epoch: 67   trainLoss: 2.0130e-03   valLoss:9.4437e-03  time: 7.10e+00\n",
      "epoch: 68   trainLoss: 2.3917e-03   valLoss:1.0745e-02  time: 7.15e+00\n",
      "epoch: 69   trainLoss: 2.0772e-03   valLoss:9.4246e-03  time: 7.10e+00\n",
      "epoch: 70   trainLoss: 1.6970e-03   valLoss:1.3104e-02  time: 7.08e+00\n",
      "epoch: 71   trainLoss: 1.4426e-03   valLoss:8.8594e-03  time: 7.13e+00\n",
      "epoch: 72   trainLoss: 1.9126e-03   valLoss:1.0594e-02  time: 7.22e+00\n",
      "epoch: 73   trainLoss: 1.3992e-03   valLoss:7.8052e-03  time: 7.11e+00\n",
      "epoch: 74   trainLoss: 1.0790e-03   valLoss:1.0493e-02  time: 7.10e+00\n",
      "epoch: 75   trainLoss: 8.7561e-04   valLoss:1.1959e-02  time: 7.10e+00\n",
      "epoch: 76   trainLoss: 9.5690e-04   valLoss:1.5434e-02  time: 7.08e+00\n",
      "epoch: 77   trainLoss: 6.9817e-04   valLoss:1.3873e-02  time: 7.11e+00\n",
      "epoch: 78   trainLoss: 9.6212e-04   valLoss:1.2580e-02  time: 7.16e+00\n",
      "epoch: 79   trainLoss: 1.6015e-03   valLoss:1.3666e-02  time: 7.15e+00\n",
      "epoch: 80   trainLoss: 1.5793e-03   valLoss:2.1169e-02  time: 7.10e+00\n",
      "epoch: 81   trainLoss: 2.8402e-03   valLoss:1.8696e-02  time: 7.07e+00\n",
      "epoch: 82   trainLoss: 3.3961e-03   valLoss:1.3131e-02  time: 7.11e+00\n",
      "epoch: 83   trainLoss: 4.4654e-03   valLoss:3.8760e-03  time: 7.11e+00\n",
      "epoch: 84   trainLoss: 2.9117e-03   valLoss:4.9239e-03  time: 7.14e+00\n",
      "epoch: 85   trainLoss: 1.6169e-03   valLoss:6.7274e-03  time: 7.18e+00\n",
      "epoch: 86   trainLoss: 1.4806e-03   valLoss:1.9871e-03  time: 7.17e+00\n",
      "epoch: 87   trainLoss: 1.9285e-03   valLoss:1.1753e-02  time: 7.13e+00\n",
      "epoch: 88   trainLoss: 1.9305e-03   valLoss:1.3049e-02  time: 7.17e+00\n",
      "epoch: 89   trainLoss: 1.0354e-03   valLoss:6.7928e-03  time: 7.14e+00\n",
      "epoch: 90   trainLoss: 1.1375e-03   valLoss:7.7580e-03  time: 7.05e+00\n",
      "epoch: 91   trainLoss: 1.2398e-03   valLoss:6.7736e-03  time: 7.15e+00\n",
      "epoch: 92   trainLoss: 2.6810e-03   valLoss:7.7351e-03  time: 7.10e+00\n",
      "epoch: 93   trainLoss: 2.6193e-03   valLoss:6.7759e-03  time: 7.00e+00\n",
      "epoch: 94   trainLoss: 1.3236e-03   valLoss:4.7840e-03  time: 7.13e+00\n",
      "epoch: 95   trainLoss: 1.1668e-03   valLoss:6.7599e-03  time: 7.01e+00\n",
      "epoch: 96   trainLoss: 1.0322e-03   valLoss:8.5373e-03  time: 7.06e+00\n",
      "epoch: 97   trainLoss: 7.1045e-04   valLoss:7.6509e-03  time: 7.07e+00\n",
      "epoch: 98   trainLoss: 4.9838e-04   valLoss:1.0741e-02  time: 7.10e+00\n",
      "epoch: 99   trainLoss: 8.4924e-04   valLoss:5.7977e-03  time: 7.17e+00\n",
      "loading checkpoint 86\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 5.9360e-01   valLoss:1.2336e+01  time: 7.10e+00\n",
      "epoch: 1   trainLoss: 1.0690e-01   valLoss:2.2061e-01  time: 7.12e+00\n",
      "epoch: 2   trainLoss: 6.3144e-02   valLoss:9.4348e-02  time: 7.17e+00\n",
      "epoch: 3   trainLoss: 4.9058e-02   valLoss:5.7944e-02  time: 7.14e+00\n",
      "epoch: 4   trainLoss: 3.9858e-02   valLoss:4.8653e-02  time: 7.10e+00\n",
      "epoch: 5   trainLoss: 3.4246e-02   valLoss:3.8895e-02  time: 7.08e+00\n",
      "epoch: 6   trainLoss: 2.7791e-02   valLoss:3.0041e-02  time: 7.14e+00\n",
      "epoch: 7   trainLoss: 2.3373e-02   valLoss:2.6657e-02  time: 7.10e+00\n",
      "epoch: 8   trainLoss: 2.0643e-02   valLoss:2.2153e-02  time: 7.06e+00\n",
      "epoch: 9   trainLoss: 1.8175e-02   valLoss:1.8904e-02  time: 7.16e+00\n",
      "epoch: 10   trainLoss: 1.6072e-02   valLoss:2.0905e-02  time: 7.20e+00\n",
      "epoch: 11   trainLoss: 1.4237e-02   valLoss:1.5940e-02  time: 7.10e+00\n",
      "epoch: 12   trainLoss: 1.1465e-02   valLoss:1.6123e-02  time: 7.11e+00\n",
      "epoch: 13   trainLoss: 9.1007e-03   valLoss:1.2046e-02  time: 7.18e+00\n",
      "epoch: 14   trainLoss: 5.7356e-03   valLoss:7.1496e-03  time: 7.14e+00\n",
      "epoch: 15   trainLoss: 3.8255e-03   valLoss:7.9633e-03  time: 7.32e+00\n",
      "epoch: 16   trainLoss: 2.9587e-03   valLoss:7.8512e-03  time: 7.21e+00\n",
      "epoch: 17   trainLoss: 1.7174e-03   valLoss:6.3383e-03  time: 7.06e+00\n",
      "epoch: 18   trainLoss: 1.3472e-03   valLoss:5.4975e-03  time: 7.12e+00\n",
      "epoch: 19   trainLoss: 1.3592e-03   valLoss:3.7804e-03  time: 7.09e+00\n",
      "epoch: 20   trainLoss: 1.4607e-03   valLoss:3.2004e-03  time: 7.15e+00\n",
      "epoch: 21   trainLoss: 1.1143e-03   valLoss:4.3760e-03  time: 7.15e+00\n",
      "epoch: 22   trainLoss: 9.7111e-04   valLoss:3.8724e-03  time: 7.15e+00\n",
      "epoch: 23   trainLoss: 1.2280e-03   valLoss:2.7322e-03  time: 7.25e+00\n",
      "epoch: 24   trainLoss: 7.8607e-04   valLoss:3.0234e-03  time: 7.12e+00\n",
      "epoch: 25   trainLoss: 8.2951e-04   valLoss:2.8264e-03  time: 7.13e+00\n",
      "epoch: 26   trainLoss: 5.7723e-04   valLoss:2.7424e-03  time: 7.11e+00\n",
      "epoch: 27   trainLoss: 5.3047e-04   valLoss:3.2299e-03  time: 7.12e+00\n",
      "epoch: 28   trainLoss: 1.3010e-03   valLoss:3.1015e-03  time: 7.13e+00\n",
      "epoch: 29   trainLoss: 1.0351e-03   valLoss:2.7162e-03  time: 7.24e+00\n",
      "epoch: 30   trainLoss: 1.1672e-03   valLoss:2.9593e-03  time: 7.20e+00\n",
      "epoch: 31   trainLoss: 1.2469e-03   valLoss:4.0038e-03  time: 7.12e+00\n",
      "epoch: 32   trainLoss: 1.5079e-03   valLoss:2.0636e-03  time: 7.11e+00\n",
      "epoch: 33   trainLoss: 6.3030e-04   valLoss:2.5790e-03  time: 7.11e+00\n",
      "epoch: 34   trainLoss: 1.0103e-03   valLoss:5.3436e-03  time: 7.10e+00\n",
      "epoch: 35   trainLoss: 2.0028e-03   valLoss:3.4234e-03  time: 7.07e+00\n",
      "epoch: 36   trainLoss: 2.3092e-03   valLoss:3.7927e-03  time: 7.19e+00\n",
      "epoch: 37   trainLoss: 1.3924e-03   valLoss:3.4108e-03  time: 7.13e+00\n",
      "epoch: 38   trainLoss: 1.1007e-03   valLoss:2.9422e-03  time: 7.14e+00\n",
      "epoch: 39   trainLoss: 1.1442e-03   valLoss:2.5722e-03  time: 7.09e+00\n",
      "epoch: 40   trainLoss: 7.6587e-04   valLoss:3.4138e-03  time: 7.13e+00\n",
      "epoch: 41   trainLoss: 6.6297e-04   valLoss:2.4996e-03  time: 7.13e+00\n",
      "epoch: 42   trainLoss: 7.9904e-04   valLoss:2.4494e-03  time: 7.18e+00\n",
      "epoch: 43   trainLoss: 8.2727e-04   valLoss:4.1873e-03  time: 7.06e+00\n",
      "epoch: 44   trainLoss: 1.4809e-03   valLoss:4.6686e-03  time: 7.11e+00\n",
      "epoch: 45   trainLoss: 2.1300e-03   valLoss:3.2853e-03  time: 7.14e+00\n",
      "epoch: 46   trainLoss: 1.0063e-03   valLoss:2.8234e-03  time: 7.08e+00\n",
      "epoch: 47   trainLoss: 7.9839e-04   valLoss:3.0721e-03  time: 7.13e+00\n",
      "epoch: 48   trainLoss: 1.0101e-03   valLoss:3.5071e-03  time: 7.15e+00\n",
      "epoch: 49   trainLoss: 6.5597e-04   valLoss:3.7002e-03  time: 7.20e+00\n",
      "epoch: 50   trainLoss: 7.7317e-04   valLoss:3.7731e-03  time: 7.14e+00\n",
      "epoch: 51   trainLoss: 4.9150e-04   valLoss:3.9565e-03  time: 7.12e+00\n",
      "epoch: 52   trainLoss: 6.4870e-04   valLoss:3.1740e-03  time: 7.05e+00\n",
      "epoch: 53   trainLoss: 6.8426e-04   valLoss:2.7224e-03  time: 7.14e+00\n",
      "epoch: 54   trainLoss: 7.6236e-04   valLoss:2.6105e-03  time: 7.12e+00\n",
      "epoch: 55   trainLoss: 1.5206e-03   valLoss:3.5353e-03  time: 7.20e+00\n",
      "epoch: 56   trainLoss: 2.5869e-03   valLoss:5.7692e-03  time: 7.10e+00\n",
      "epoch: 57   trainLoss: 2.1596e-03   valLoss:3.4635e-03  time: 7.07e+00\n",
      "epoch: 58   trainLoss: 1.4563e-03   valLoss:2.9003e-03  time: 7.14e+00\n",
      "epoch: 59   trainLoss: 9.8873e-04   valLoss:5.6819e-03  time: 7.12e+00\n",
      "epoch: 60   trainLoss: 1.6962e-03   valLoss:3.2230e-03  time: 7.11e+00\n",
      "epoch: 61   trainLoss: 9.4128e-04   valLoss:2.7901e-03  time: 7.15e+00\n",
      "epoch: 62   trainLoss: 1.1567e-03   valLoss:1.6255e-03  time: 7.24e+00\n",
      "epoch: 63   trainLoss: 6.7504e-04   valLoss:3.7300e-03  time: 7.12e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 64   trainLoss: 5.7664e-04   valLoss:2.9955e-03  time: 7.13e+00\n",
      "epoch: 65   trainLoss: 1.1024e-03   valLoss:2.0612e-03  time: 7.04e+00\n",
      "epoch: 66   trainLoss: 1.2813e-03   valLoss:4.7559e-03  time: 7.11e+00\n",
      "epoch: 67   trainLoss: 1.0686e-03   valLoss:3.3877e-03  time: 7.13e+00\n",
      "epoch: 68   trainLoss: 7.0961e-04   valLoss:2.9002e-03  time: 7.19e+00\n",
      "epoch: 69   trainLoss: 7.5887e-04   valLoss:2.4624e-03  time: 7.07e+00\n",
      "epoch: 70   trainLoss: 8.8057e-04   valLoss:3.2198e-03  time: 7.04e+00\n",
      "epoch: 71   trainLoss: 1.3809e-03   valLoss:3.0058e-03  time: 7.08e+00\n",
      "epoch: 72   trainLoss: 1.1400e-03   valLoss:3.4794e-03  time: 7.05e+00\n",
      "epoch: 73   trainLoss: 7.1897e-04   valLoss:2.5653e-03  time: 7.11e+00\n",
      "epoch: 74   trainLoss: 5.8114e-04   valLoss:2.3366e-03  time: 7.19e+00\n",
      "epoch: 75   trainLoss: 5.3605e-04   valLoss:3.5257e-03  time: 7.21e+00\n",
      "epoch: 76   trainLoss: 6.7255e-04   valLoss:5.0574e-03  time: 7.12e+00\n",
      "epoch: 77   trainLoss: 8.6647e-04   valLoss:4.2512e-03  time: 7.17e+00\n",
      "epoch: 78   trainLoss: 7.7811e-04   valLoss:5.5703e-03  time: 7.17e+00\n",
      "epoch: 79   trainLoss: 1.7083e-03   valLoss:5.3005e-03  time: 7.14e+00\n",
      "epoch: 80   trainLoss: 1.4896e-03   valLoss:3.8815e-03  time: 7.17e+00\n",
      "epoch: 81   trainLoss: 1.0543e-03   valLoss:3.3711e-03  time: 7.20e+00\n",
      "epoch: 82   trainLoss: 7.8924e-04   valLoss:1.8285e-03  time: 7.15e+00\n",
      "epoch: 83   trainLoss: 6.0951e-04   valLoss:2.5538e-03  time: 7.07e+00\n",
      "epoch: 84   trainLoss: 5.1877e-04   valLoss:2.3499e-03  time: 7.05e+00\n",
      "epoch: 85   trainLoss: 9.5379e-04   valLoss:2.9157e-03  time: 7.08e+00\n",
      "epoch: 86   trainLoss: 1.5340e-03   valLoss:2.5846e-03  time: 7.03e+00\n",
      "epoch: 87   trainLoss: 1.1505e-03   valLoss:2.8003e-03  time: 7.10e+00\n",
      "epoch: 88   trainLoss: 9.1965e-04   valLoss:3.8183e-03  time: 7.22e+00\n",
      "epoch: 89   trainLoss: 6.8546e-04   valLoss:1.5115e-03  time: 7.06e+00\n",
      "epoch: 90   trainLoss: 6.3329e-04   valLoss:2.6436e-03  time: 7.08e+00\n",
      "epoch: 91   trainLoss: 7.7835e-04   valLoss:2.9918e-03  time: 7.03e+00\n",
      "epoch: 92   trainLoss: 1.0653e-03   valLoss:3.5350e-03  time: 7.10e+00\n",
      "epoch: 93   trainLoss: 1.8660e-03   valLoss:3.9607e-03  time: 7.15e+00\n",
      "epoch: 94   trainLoss: 2.1408e-03   valLoss:3.9326e-03  time: 7.22e+00\n",
      "epoch: 95   trainLoss: 1.4175e-03   valLoss:3.2356e-03  time: 7.14e+00\n",
      "epoch: 96   trainLoss: 9.9864e-04   valLoss:4.0696e-03  time: 7.14e+00\n",
      "epoch: 97   trainLoss: 6.6144e-04   valLoss:1.9554e-03  time: 7.09e+00\n",
      "epoch: 98   trainLoss: 4.4743e-04   valLoss:1.4015e-03  time: 7.11e+00\n",
      "epoch: 99   trainLoss: 3.8880e-04   valLoss:2.4300e-03  time: 7.09e+00\n",
      "loading checkpoint 98\n",
      "trained 24 random forest models in 15.80 seconds\n",
      "reading from data/tower1.0/conmech/design_5_N_20/\n",
      "loaded train set of size 18\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.1956e+00   valLoss:1.0694e+00  time: 1.63e-01\n",
      "epoch: 1   trainLoss: 9.3846e-01   valLoss:1.0356e+00  time: 1.63e-01\n",
      "epoch: 2   trainLoss: 7.4375e-01   valLoss:1.0064e+00  time: 1.61e-01\n",
      "epoch: 3   trainLoss: 5.7358e-01   valLoss:9.6143e-01  time: 1.62e-01\n",
      "epoch: 4   trainLoss: 4.3637e-01   valLoss:9.0066e-01  time: 1.62e-01\n",
      "epoch: 5   trainLoss: 3.3090e-01   valLoss:8.3257e-01  time: 1.66e-01\n",
      "epoch: 6   trainLoss: 2.5185e-01   valLoss:7.7240e-01  time: 1.62e-01\n",
      "epoch: 7   trainLoss: 1.8512e-01   valLoss:7.2235e-01  time: 1.68e-01\n",
      "epoch: 8   trainLoss: 1.4983e-01   valLoss:6.8687e-01  time: 1.65e-01\n",
      "epoch: 9   trainLoss: 1.1492e-01   valLoss:6.6041e-01  time: 1.62e-01\n",
      "epoch: 10   trainLoss: 9.9373e-02   valLoss:6.3740e-01  time: 1.68e-01\n",
      "epoch: 11   trainLoss: 9.1174e-02   valLoss:6.1575e-01  time: 1.63e-01\n",
      "epoch: 12   trainLoss: 8.7714e-02   valLoss:5.9555e-01  time: 1.62e-01\n",
      "epoch: 13   trainLoss: 8.4505e-02   valLoss:5.6764e-01  time: 1.66e-01\n",
      "epoch: 14   trainLoss: 8.2345e-02   valLoss:5.4091e-01  time: 1.62e-01\n",
      "epoch: 15   trainLoss: 8.0732e-02   valLoss:5.2816e-01  time: 1.65e-01\n",
      "epoch: 16   trainLoss: 7.8740e-02   valLoss:5.1525e-01  time: 1.63e-01\n",
      "epoch: 17   trainLoss: 7.3415e-02   valLoss:4.8820e-01  time: 1.62e-01\n",
      "epoch: 18   trainLoss: 6.7883e-02   valLoss:4.4151e-01  time: 1.64e-01\n",
      "epoch: 19   trainLoss: 6.3867e-02   valLoss:3.8468e-01  time: 1.62e-01\n",
      "epoch: 20   trainLoss: 6.1228e-02   valLoss:3.2277e-01  time: 1.66e-01\n",
      "epoch: 21   trainLoss: 5.6911e-02   valLoss:2.7898e-01  time: 1.64e-01\n",
      "epoch: 22   trainLoss: 5.5344e-02   valLoss:2.5677e-01  time: 1.64e-01\n",
      "epoch: 23   trainLoss: 5.3662e-02   valLoss:2.5179e-01  time: 1.65e-01\n",
      "epoch: 24   trainLoss: 5.2900e-02   valLoss:2.3818e-01  time: 1.64e-01\n",
      "epoch: 25   trainLoss: 5.2029e-02   valLoss:2.0359e-01  time: 1.67e-01\n",
      "epoch: 26   trainLoss: 5.0770e-02   valLoss:1.6882e-01  time: 1.63e-01\n",
      "epoch: 27   trainLoss: 4.9254e-02   valLoss:1.4523e-01  time: 1.64e-01\n",
      "epoch: 28   trainLoss: 4.7442e-02   valLoss:1.3036e-01  time: 1.64e-01\n",
      "epoch: 29   trainLoss: 4.5570e-02   valLoss:1.1858e-01  time: 1.63e-01\n",
      "epoch: 30   trainLoss: 4.4042e-02   valLoss:1.1316e-01  time: 1.66e-01\n",
      "epoch: 31   trainLoss: 4.2993e-02   valLoss:1.1233e-01  time: 1.64e-01\n",
      "epoch: 32   trainLoss: 4.2112e-02   valLoss:1.1498e-01  time: 1.63e-01\n",
      "epoch: 33   trainLoss: 4.1036e-02   valLoss:1.1877e-01  time: 1.67e-01\n",
      "epoch: 34   trainLoss: 3.9831e-02   valLoss:1.2115e-01  time: 1.66e-01\n",
      "epoch: 35   trainLoss: 3.8663e-02   valLoss:1.2918e-01  time: 1.65e-01\n",
      "epoch: 36   trainLoss: 3.7257e-02   valLoss:1.2966e-01  time: 1.62e-01\n",
      "epoch: 37   trainLoss: 3.5568e-02   valLoss:1.3770e-01  time: 1.66e-01\n",
      "epoch: 38   trainLoss: 3.3635e-02   valLoss:1.3547e-01  time: 1.63e-01\n",
      "epoch: 39   trainLoss: 3.1845e-02   valLoss:1.6514e-01  time: 1.61e-01\n",
      "epoch: 40   trainLoss: 3.3830e-02   valLoss:1.0216e-01  time: 1.64e-01\n",
      "epoch: 41   trainLoss: 4.2336e-02   valLoss:1.4449e-01  time: 1.64e-01\n",
      "epoch: 42   trainLoss: 2.9491e-02   valLoss:1.9863e-01  time: 1.64e-01\n",
      "epoch: 43   trainLoss: 3.3105e-02   valLoss:1.1434e-01  time: 1.68e-01\n",
      "epoch: 44   trainLoss: 2.8064e-02   valLoss:8.6541e-02  time: 1.62e-01\n",
      "epoch: 45   trainLoss: 2.5993e-02   valLoss:8.4179e-02  time: 1.64e-01\n",
      "epoch: 46   trainLoss: 2.6188e-02   valLoss:8.3111e-02  time: 1.67e-01\n",
      "epoch: 47   trainLoss: 2.3095e-02   valLoss:8.4114e-02  time: 1.63e-01\n",
      "epoch: 48   trainLoss: 2.2304e-02   valLoss:8.4226e-02  time: 1.65e-01\n",
      "epoch: 49   trainLoss: 2.1451e-02   valLoss:8.1538e-02  time: 1.63e-01\n",
      "epoch: 50   trainLoss: 2.0972e-02   valLoss:7.9329e-02  time: 1.64e-01\n",
      "epoch: 51   trainLoss: 2.0061e-02   valLoss:7.9149e-02  time: 1.65e-01\n",
      "epoch: 52   trainLoss: 1.9933e-02   valLoss:7.9881e-02  time: 1.63e-01\n",
      "epoch: 53   trainLoss: 1.9146e-02   valLoss:8.1455e-02  time: 1.65e-01\n",
      "epoch: 54   trainLoss: 1.8690e-02   valLoss:8.3149e-02  time: 1.67e-01\n",
      "epoch: 55   trainLoss: 1.7857e-02   valLoss:8.2299e-02  time: 1.62e-01\n",
      "epoch: 56   trainLoss: 1.7035e-02   valLoss:7.9639e-02  time: 1.64e-01\n",
      "epoch: 57   trainLoss: 1.6659e-02   valLoss:8.7108e-02  time: 1.66e-01\n",
      "epoch: 58   trainLoss: 1.4999e-02   valLoss:9.2295e-02  time: 1.65e-01\n",
      "epoch: 59   trainLoss: 1.4040e-02   valLoss:9.0269e-02  time: 1.63e-01\n",
      "epoch: 60   trainLoss: 1.3382e-02   valLoss:9.8850e-02  time: 1.65e-01\n",
      "epoch: 61   trainLoss: 1.2494e-02   valLoss:8.6654e-02  time: 1.62e-01\n",
      "epoch: 62   trainLoss: 1.2616e-02   valLoss:9.5877e-02  time: 1.64e-01\n",
      "epoch: 63   trainLoss: 1.2409e-02   valLoss:7.4450e-02  time: 1.64e-01\n",
      "epoch: 64   trainLoss: 1.0436e-02   valLoss:7.2019e-02  time: 1.63e-01\n",
      "epoch: 65   trainLoss: 7.4226e-03   valLoss:7.5171e-02  time: 1.66e-01\n",
      "epoch: 66   trainLoss: 5.7304e-03   valLoss:7.1390e-02  time: 1.65e-01\n",
      "epoch: 67   trainLoss: 4.4536e-03   valLoss:7.0648e-02  time: 1.64e-01\n",
      "epoch: 68   trainLoss: 3.5915e-03   valLoss:7.3272e-02  time: 1.65e-01\n",
      "epoch: 69   trainLoss: 3.4059e-03   valLoss:7.0962e-02  time: 1.65e-01\n",
      "epoch: 70   trainLoss: 3.0569e-03   valLoss:7.3750e-02  time: 1.64e-01\n",
      "epoch: 71   trainLoss: 3.2563e-03   valLoss:6.9232e-02  time: 1.64e-01\n",
      "epoch: 72   trainLoss: 5.2900e-03   valLoss:7.0636e-02  time: 1.64e-01\n",
      "epoch: 73   trainLoss: 7.0041e-03   valLoss:5.5885e-02  time: 1.63e-01\n",
      "epoch: 74   trainLoss: 8.7809e-03   valLoss:6.0246e-02  time: 1.66e-01\n",
      "epoch: 75   trainLoss: 3.1243e-03   valLoss:5.8250e-02  time: 1.64e-01\n",
      "epoch: 76   trainLoss: 3.1945e-03   valLoss:5.5715e-02  time: 1.64e-01\n",
      "epoch: 77   trainLoss: 4.7918e-03   valLoss:6.4358e-02  time: 1.65e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 78   trainLoss: 2.1546e-03   valLoss:6.6875e-02  time: 1.65e-01\n",
      "epoch: 79   trainLoss: 2.7053e-03   valLoss:5.8923e-02  time: 1.62e-01\n",
      "epoch: 80   trainLoss: 2.1536e-03   valLoss:5.4863e-02  time: 1.64e-01\n",
      "epoch: 81   trainLoss: 1.5685e-03   valLoss:5.3319e-02  time: 1.65e-01\n",
      "epoch: 82   trainLoss: 1.8736e-03   valLoss:4.8656e-02  time: 1.62e-01\n",
      "epoch: 83   trainLoss: 1.5453e-03   valLoss:4.6031e-02  time: 1.63e-01\n",
      "epoch: 84   trainLoss: 1.6466e-03   valLoss:4.6693e-02  time: 1.63e-01\n",
      "epoch: 85   trainLoss: 8.6944e-04   valLoss:5.0016e-02  time: 1.65e-01\n",
      "epoch: 86   trainLoss: 1.1344e-03   valLoss:4.9260e-02  time: 1.64e-01\n",
      "epoch: 87   trainLoss: 8.6539e-04   valLoss:4.8484e-02  time: 1.68e-01\n",
      "epoch: 88   trainLoss: 1.1156e-03   valLoss:4.8509e-02  time: 1.60e-01\n",
      "epoch: 89   trainLoss: 7.8766e-04   valLoss:4.5456e-02  time: 1.65e-01\n",
      "epoch: 90   trainLoss: 8.0288e-04   valLoss:3.9761e-02  time: 1.61e-01\n",
      "epoch: 91   trainLoss: 5.9060e-04   valLoss:3.8091e-02  time: 1.61e-01\n",
      "epoch: 92   trainLoss: 6.0073e-04   valLoss:4.0583e-02  time: 1.61e-01\n",
      "epoch: 93   trainLoss: 5.4867e-04   valLoss:4.1535e-02  time: 1.61e-01\n",
      "epoch: 94   trainLoss: 4.8206e-04   valLoss:4.0907e-02  time: 1.62e-01\n",
      "epoch: 95   trainLoss: 4.8298e-04   valLoss:4.3464e-02  time: 1.62e-01\n",
      "epoch: 96   trainLoss: 3.4384e-04   valLoss:4.5943e-02  time: 1.58e-01\n",
      "epoch: 97   trainLoss: 4.4396e-04   valLoss:4.3060e-02  time: 1.59e-01\n",
      "epoch: 98   trainLoss: 3.2737e-04   valLoss:4.0938e-02  time: 1.58e-01\n",
      "epoch: 99   trainLoss: 3.2955e-04   valLoss:4.1647e-02  time: 1.58e-01\n",
      "loading checkpoint 91\n",
      "loading restart file\n",
      "train model: flatten: True\n",
      "train model: self.flatten: True\n",
      "fitSS: self.flatten: True\n",
      "epoch: 0   trainLoss: 1.4882e+00   valLoss:1.1982e+03  time: 1.62e-01\n",
      "epoch: 1   trainLoss: 3.4723e-01   valLoss:4.3866e+02  time: 1.63e-01\n",
      "epoch: 2   trainLoss: 3.5420e-01   valLoss:1.0737e+02  time: 1.62e-01\n",
      "epoch: 3   trainLoss: 2.0622e-01   valLoss:1.6950e+01  time: 1.69e-01\n",
      "epoch: 4   trainLoss: 1.0053e-01   valLoss:3.1926e+00  time: 1.62e-01\n",
      "epoch: 5   trainLoss: 8.3720e-02   valLoss:8.6020e-01  time: 1.62e-01\n",
      "epoch: 6   trainLoss: 7.6274e-02   valLoss:4.0405e-01  time: 1.63e-01\n",
      "epoch: 7   trainLoss: 7.2206e-02   valLoss:2.4547e-01  time: 1.62e-01\n",
      "epoch: 8   trainLoss: 7.0359e-02   valLoss:1.7060e-01  time: 1.65e-01\n",
      "epoch: 9   trainLoss: 6.4990e-02   valLoss:1.6716e-01  time: 1.62e-01\n",
      "epoch: 10   trainLoss: 6.1590e-02   valLoss:1.3848e-01  time: 1.69e-01\n",
      "epoch: 11   trainLoss: 5.9325e-02   valLoss:1.0807e-01  time: 1.65e-01\n",
      "epoch: 12   trainLoss: 5.4767e-02   valLoss:8.7597e-02  time: 1.66e-01\n",
      "epoch: 13   trainLoss: 5.0844e-02   valLoss:7.4354e-02  time: 1.62e-01\n",
      "epoch: 14   trainLoss: 4.8600e-02   valLoss:6.5072e-02  time: 1.63e-01\n",
      "epoch: 15   trainLoss: 4.5959e-02   valLoss:6.1171e-02  time: 1.62e-01\n",
      "epoch: 16   trainLoss: 4.3568e-02   valLoss:6.0620e-02  time: 1.62e-01\n",
      "epoch: 17   trainLoss: 4.2403e-02   valLoss:6.0078e-02  time: 1.61e-01\n",
      "epoch: 18   trainLoss: 4.1000e-02   valLoss:5.7632e-02  time: 1.61e-01\n",
      "epoch: 19   trainLoss: 3.9057e-02   valLoss:5.5559e-02  time: 1.61e-01\n",
      "epoch: 20   trainLoss: 3.7650e-02   valLoss:5.4529e-02  time: 1.75e-01\n",
      "epoch: 21   trainLoss: 3.6259e-02   valLoss:5.5034e-02  time: 1.63e-01\n",
      "epoch: 22   trainLoss: 3.5051e-02   valLoss:5.5255e-02  time: 1.62e-01\n",
      "epoch: 23   trainLoss: 3.4675e-02   valLoss:5.2109e-02  time: 1.61e-01\n",
      "epoch: 24   trainLoss: 3.4000e-02   valLoss:4.6089e-02  time: 1.63e-01\n",
      "epoch: 25   trainLoss: 3.2582e-02   valLoss:4.1225e-02  time: 1.62e-01\n",
      "epoch: 26   trainLoss: 3.1340e-02   valLoss:3.8274e-02  time: 1.62e-01\n",
      "epoch: 27   trainLoss: 3.0345e-02   valLoss:3.5811e-02  time: 1.61e-01\n",
      "epoch: 28   trainLoss: 2.9285e-02   valLoss:3.3934e-02  time: 1.62e-01\n",
      "epoch: 29   trainLoss: 2.8783e-02   valLoss:3.2586e-02  time: 1.63e-01\n",
      "epoch: 30   trainLoss: 2.8307e-02   valLoss:3.1242e-02  time: 1.82e-01\n",
      "epoch: 31   trainLoss: 2.7652e-02   valLoss:3.0534e-02  time: 1.59e-01\n",
      "epoch: 32   trainLoss: 2.7199e-02   valLoss:3.0388e-02  time: 1.60e-01\n",
      "epoch: 33   trainLoss: 2.6725e-02   valLoss:3.0848e-02  time: 1.58e-01\n",
      "epoch: 34   trainLoss: 2.6266e-02   valLoss:3.1813e-02  time: 1.57e-01\n",
      "epoch: 35   trainLoss: 2.6029e-02   valLoss:3.2218e-02  time: 1.59e-01\n",
      "epoch: 36   trainLoss: 2.5906e-02   valLoss:3.1671e-02  time: 1.59e-01\n",
      "epoch: 37   trainLoss: 2.5711e-02   valLoss:3.1098e-02  time: 1.58e-01\n",
      "epoch: 38   trainLoss: 2.5641e-02   valLoss:3.0655e-02  time: 1.59e-01\n",
      "epoch: 39   trainLoss: 2.5450e-02   valLoss:3.0340e-02  time: 1.57e-01\n",
      "epoch: 40   trainLoss: 2.5202e-02   valLoss:2.9696e-02  time: 1.58e-01\n",
      "epoch: 41   trainLoss: 2.5080e-02   valLoss:2.8679e-02  time: 1.62e-01\n",
      "epoch: 42   trainLoss: 2.4958e-02   valLoss:2.8027e-02  time: 1.58e-01\n",
      "epoch: 43   trainLoss: 2.4859e-02   valLoss:2.8069e-02  time: 1.60e-01\n",
      "epoch: 44   trainLoss: 2.4743e-02   valLoss:2.8368e-02  time: 1.58e-01\n",
      "epoch: 45   trainLoss: 2.4534e-02   valLoss:2.8568e-02  time: 1.59e-01\n",
      "epoch: 46   trainLoss: 2.4324e-02   valLoss:2.8510e-02  time: 1.58e-01\n",
      "epoch: 47   trainLoss: 2.4155e-02   valLoss:2.8273e-02  time: 1.58e-01\n",
      "epoch: 48   trainLoss: 2.3972e-02   valLoss:2.8209e-02  time: 1.58e-01\n",
      "epoch: 49   trainLoss: 2.3785e-02   valLoss:2.8268e-02  time: 1.57e-01\n",
      "epoch: 50   trainLoss: 2.3616e-02   valLoss:2.8342e-02  time: 1.59e-01\n",
      "epoch: 51   trainLoss: 2.3412e-02   valLoss:2.8485e-02  time: 1.58e-01\n",
      "epoch: 52   trainLoss: 2.3191e-02   valLoss:2.8657e-02  time: 1.58e-01\n",
      "epoch: 53   trainLoss: 2.2949e-02   valLoss:2.8820e-02  time: 1.58e-01\n",
      "epoch: 54   trainLoss: 2.2662e-02   valLoss:2.8931e-02  time: 1.58e-01\n",
      "epoch: 55   trainLoss: 2.2363e-02   valLoss:2.8811e-02  time: 1.57e-01\n",
      "epoch: 56   trainLoss: 2.1973e-02   valLoss:2.8714e-02  time: 1.58e-01\n",
      "epoch: 57   trainLoss: 2.1507e-02   valLoss:2.8710e-02  time: 1.58e-01\n",
      "epoch: 58   trainLoss: 2.0968e-02   valLoss:2.8270e-02  time: 1.64e-01\n",
      "epoch: 59   trainLoss: 2.0290e-02   valLoss:2.7667e-02  time: 1.62e-01\n",
      "epoch: 60   trainLoss: 1.9414e-02   valLoss:2.7299e-02  time: 1.62e-01\n",
      "epoch: 61   trainLoss: 1.8360e-02   valLoss:2.7380e-02  time: 1.62e-01\n",
      "epoch: 62   trainLoss: 1.7124e-02   valLoss:2.7309e-02  time: 1.61e-01\n",
      "epoch: 63   trainLoss: 1.5744e-02   valLoss:2.7084e-02  time: 1.61e-01\n",
      "epoch: 64   trainLoss: 1.4364e-02   valLoss:2.7388e-02  time: 1.61e-01\n",
      "epoch: 65   trainLoss: 1.3121e-02   valLoss:2.7576e-02  time: 1.61e-01\n",
      "epoch: 66   trainLoss: 1.2082e-02   valLoss:2.7401e-02  time: 1.61e-01\n",
      "epoch: 67   trainLoss: 1.1292e-02   valLoss:2.5540e-02  time: 1.62e-01\n",
      "epoch: 68   trainLoss: 1.0869e-02   valLoss:2.5011e-02  time: 1.62e-01\n",
      "epoch: 69   trainLoss: 1.1100e-02   valLoss:2.2751e-02  time: 1.61e-01\n",
      "epoch: 70   trainLoss: 1.1202e-02   valLoss:2.1141e-02  time: 1.61e-01\n",
      "epoch: 71   trainLoss: 1.1350e-02   valLoss:1.6986e-02  time: 1.62e-01\n",
      "epoch: 72   trainLoss: 9.8011e-03   valLoss:1.5508e-02  time: 1.60e-01\n",
      "epoch: 73   trainLoss: 1.0082e-02   valLoss:1.4339e-02  time: 1.63e-01\n",
      "epoch: 74   trainLoss: 9.1587e-03   valLoss:1.4993e-02  time: 1.58e-01\n",
      "epoch: 75   trainLoss: 9.3455e-03   valLoss:1.4133e-02  time: 1.58e-01\n",
      "epoch: 76   trainLoss: 8.4203e-03   valLoss:1.3127e-02  time: 1.62e-01\n",
      "epoch: 77   trainLoss: 8.4895e-03   valLoss:1.3808e-02  time: 1.59e-01\n",
      "epoch: 78   trainLoss: 8.0382e-03   valLoss:1.1810e-02  time: 1.60e-01\n",
      "epoch: 79   trainLoss: 7.6501e-03   valLoss:1.3386e-02  time: 1.60e-01\n",
      "epoch: 80   trainLoss: 7.3203e-03   valLoss:1.2119e-02  time: 1.58e-01\n",
      "epoch: 81   trainLoss: 6.6975e-03   valLoss:1.1900e-02  time: 1.57e-01\n",
      "epoch: 82   trainLoss: 6.3676e-03   valLoss:1.2647e-02  time: 1.61e-01\n",
      "epoch: 83   trainLoss: 6.1857e-03   valLoss:1.1965e-02  time: 1.58e-01\n",
      "epoch: 84   trainLoss: 5.8185e-03   valLoss:1.0040e-02  time: 1.58e-01\n",
      "epoch: 85   trainLoss: 6.0610e-03   valLoss:1.4063e-02  time: 1.60e-01\n",
      "epoch: 86   trainLoss: 7.4405e-03   valLoss:1.1542e-02  time: 1.59e-01\n",
      "epoch: 87   trainLoss: 1.0326e-02   valLoss:1.0251e-02  time: 1.58e-01\n",
      "epoch: 88   trainLoss: 5.4968e-03   valLoss:1.3687e-02  time: 1.60e-01\n",
      "epoch: 89   trainLoss: 7.5359e-03   valLoss:1.1074e-02  time: 1.58e-01\n",
      "epoch: 90   trainLoss: 5.1474e-03   valLoss:1.1149e-02  time: 1.58e-01\n",
      "epoch: 91   trainLoss: 5.5347e-03   valLoss:1.2175e-02  time: 1.58e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92   trainLoss: 5.4118e-03   valLoss:1.2118e-02  time: 1.57e-01\n",
      "epoch: 93   trainLoss: 4.5079e-03   valLoss:1.1905e-02  time: 1.57e-01\n",
      "epoch: 94   trainLoss: 4.6104e-03   valLoss:1.1016e-02  time: 1.62e-01\n",
      "epoch: 95   trainLoss: 4.4962e-03   valLoss:1.0728e-02  time: 1.59e-01\n",
      "epoch: 96   trainLoss: 3.9407e-03   valLoss:1.2022e-02  time: 1.60e-01\n",
      "epoch: 97   trainLoss: 3.6472e-03   valLoss:1.3584e-02  time: 1.58e-01\n",
      "epoch: 98   trainLoss: 3.4581e-03   valLoss:1.4598e-02  time: 1.59e-01\n",
      "epoch: 99   trainLoss: 3.1927e-03   valLoss:1.4142e-02  time: 1.58e-01\n",
      "loading checkpoint 84\n",
      "trained 24 random forest models in 2.76 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.660634e-06</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.079675</td>\n",
       "      <td>0.673650</td>\n",
       "      <td>0.576182</td>\n",
       "      <td>-13.553525</td>\n",
       "      <td>-108.045485</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.031741e-03</td>\n",
       "      <td>0.022531</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>-755.943496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1824.017334</td>\n",
       "      <td>-9924.245758</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.884177e-07</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.947683</td>\n",
       "      <td>0.979292</td>\n",
       "      <td>0.370694</td>\n",
       "      <td>-0.839688</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220445e-05</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>-3.028061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-57.299070</td>\n",
       "      <td>-276.002546</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.212383e-09</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>0.991977</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.297604e-08</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.989085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981748</td>\n",
       "      <td>0.959730</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.916766e-07</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.031258</td>\n",
       "      <td>0.837433</td>\n",
       "      <td>0.841597</td>\n",
       "      <td>-1.160613</td>\n",
       "      <td>-18.464501</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.824250e-04</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.338595</td>\n",
       "      <td>-107.680914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-410.452558</td>\n",
       "      <td>-2639.883435</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.581865e-07</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>0.951290</td>\n",
       "      <td>0.965992</td>\n",
       "      <td>0.459569</td>\n",
       "      <td>-1.157768</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.574461e-05</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.196118</td>\n",
       "      <td>0.295736</td>\n",
       "      <td>0.383904</td>\n",
       "      <td>-72.745692</td>\n",
       "      <td>-407.906966</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.191794e-09</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.999366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.997467</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.390658e-09</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.995058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992980</td>\n",
       "      <td>0.981827</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.076576e-06</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.120836</td>\n",
       "      <td>-1.885063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-20.174543</td>\n",
       "      <td>-138.859218</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.645967e-02</td>\n",
       "      <td>0.108885</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>-6529.935799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-65863.508984</td>\n",
       "      <td>-316316.061695</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.200493e-07</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.749644</td>\n",
       "      <td>0.950518</td>\n",
       "      <td>-3.363083</td>\n",
       "      <td>-34.796888</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.301280e-05</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.204735</td>\n",
       "      <td>-6.353722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-61.949078</td>\n",
       "      <td>-495.073811</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.675630e-08</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.876270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933749</td>\n",
       "      <td>0.878432</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.054910e-07</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.697931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698409</td>\n",
       "      <td>0.284956</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.960677e-06</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.080866</td>\n",
       "      <td>-2.844588</td>\n",
       "      <td>0.567935</td>\n",
       "      <td>-8.459198</td>\n",
       "      <td>-65.232206</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.193004e-04</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.504556</td>\n",
       "      <td>-68.214723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3902.891968</td>\n",
       "      <td>-34491.497420</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.910660e-07</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>0.908692</td>\n",
       "      <td>0.177742</td>\n",
       "      <td>-1.866629</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.144119e-05</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.151473</td>\n",
       "      <td>-5.178070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41.156471</td>\n",
       "      <td>-443.268347</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.954749e-09</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.996824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>0.984915</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.565904e-08</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.982923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969177</td>\n",
       "      <td>0.936244</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.268256e-06</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.686853</td>\n",
       "      <td>0.683126</td>\n",
       "      <td>-1.560371</td>\n",
       "      <td>-13.763742</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.171260e-05</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.267157</td>\n",
       "      <td>-30.596243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-313.186802</td>\n",
       "      <td>-2594.850341</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.713254e-07</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.900817</td>\n",
       "      <td>0.954794</td>\n",
       "      <td>0.343524</td>\n",
       "      <td>-0.709498</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.603618e-05</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.201086</td>\n",
       "      <td>-24.250164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-77.072700</td>\n",
       "      <td>-365.735228</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.426482e-08</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.992217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>0.976529</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.843102e-08</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.967751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946369</td>\n",
       "      <td>0.865053</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.602252e-06</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.103689</td>\n",
       "      <td>-1.153199</td>\n",
       "      <td>0.551921</td>\n",
       "      <td>-42.139572</td>\n",
       "      <td>-276.934450</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.054442e-02</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.822251</td>\n",
       "      <td>-4949.722858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25901.793605</td>\n",
       "      <td>-131472.776737</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.642824e-06</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.048987</td>\n",
       "      <td>-3.420992</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>-4.789272</td>\n",
       "      <td>-28.433425</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.891860e-05</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.197501</td>\n",
       "      <td>-27.705239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.772709</td>\n",
       "      <td>-117.636579</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.479980e-08</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>0.818203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883461</td>\n",
       "      <td>0.750508</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.955223e-07</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.184766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432342</td>\n",
       "      <td>-0.052897</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.611182e-07</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.877649</td>\n",
       "      <td>0.964932</td>\n",
       "      <td>0.383509</td>\n",
       "      <td>-6.295137</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.179941e-05</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>-2.717348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.161340</td>\n",
       "      <td>-207.867096</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.136385e-07</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>0.907977</td>\n",
       "      <td>0.985209</td>\n",
       "      <td>0.648775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.942709e-06</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.090346</td>\n",
       "      <td>-0.902966</td>\n",
       "      <td>0.406240</td>\n",
       "      <td>-14.647808</td>\n",
       "      <td>-138.789016</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6.322944e-10</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.998682</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.379187e-09</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.997107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995964</td>\n",
       "      <td>0.989259</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3.184538e-06</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>0.277988</td>\n",
       "      <td>0.464438</td>\n",
       "      <td>-16.090131</td>\n",
       "      <td>-192.428279</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.390252e-03</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.784932</td>\n",
       "      <td>-1770.434508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41253.389966</td>\n",
       "      <td>-238117.364956</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.393706e-07</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.884327</td>\n",
       "      <td>0.939190</td>\n",
       "      <td>-6.088165</td>\n",
       "      <td>-94.359605</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.030112e-05</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>-35.044213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-48.687522</td>\n",
       "      <td>-411.241520</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.002128e-08</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.989647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959859</td>\n",
       "      <td>0.910077</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.234846e-07</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.011253</td>\n",
       "      <td>0.941031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862967</td>\n",
       "      <td>0.680570</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse       mae       mre       peakR2  maxAggR2     meanAggR2  \\\n",
       "0   2.660634e-06  0.001215  0.079675     0.673650  0.576182    -13.553525   \n",
       "1   1.031741e-03  0.022531  0.481587  -755.943496  0.000000  -1824.017334   \n",
       "2   1.884177e-07  0.000335  0.021375     0.947683  0.979292      0.370694   \n",
       "3   1.220445e-05  0.002801  0.163308    -3.028061  0.000000    -57.299070   \n",
       "4   3.212383e-09  0.000028  0.001506     0.998414  1.000000      0.997109   \n",
       "5   2.297604e-08  0.000073  0.003909     0.989085  1.000000      0.981748   \n",
       "6   4.916766e-07  0.000520  0.031258     0.837433  0.841597     -1.160613   \n",
       "7   1.824250e-04  0.010227  0.338595  -107.680914  0.000000   -410.452558   \n",
       "8   1.581865e-07  0.000303  0.019524     0.951290  0.965992      0.459569   \n",
       "9   1.574461e-05  0.003146  0.196118     0.295736  0.383904    -72.745692   \n",
       "10  1.191794e-09  0.000017  0.000914     0.999366  1.000000      0.999002   \n",
       "11  8.390658e-09  0.000046  0.002416     0.995058  1.000000      0.992980   \n",
       "12  8.076576e-06  0.002033  0.120836    -1.885063  0.000000    -20.174543   \n",
       "13  1.645967e-02  0.108885  0.843348 -6529.935799  0.000000 -65863.508984   \n",
       "14  5.200493e-07  0.000530  0.033506     0.749644  0.950518     -3.363083   \n",
       "15  2.301280e-05  0.003744  0.204735    -6.353722  0.000000    -61.949078   \n",
       "16  9.675630e-08  0.000159  0.007925     0.876270  1.000000      0.933749   \n",
       "17  4.054910e-07  0.000315  0.015321     0.697931  1.000000      0.698409   \n",
       "18  5.960677e-06  0.001482  0.080866    -2.844588  0.567935     -8.459198   \n",
       "19  5.193004e-04  0.018067  0.504556   -68.214723  0.000000  -3902.891968   \n",
       "20  2.910660e-07  0.000413  0.024989     0.868221  0.908692      0.177742   \n",
       "21  1.144119e-05  0.002553  0.151473    -5.178070  0.000000    -41.156471   \n",
       "22  6.954749e-09  0.000041  0.002185     0.996824  1.000000      0.994072   \n",
       "23  3.565904e-08  0.000096  0.005099     0.982923  1.000000      0.969177   \n",
       "24  1.268256e-06  0.000829  0.051854     0.686853  0.683126     -1.560371   \n",
       "25  9.171260e-05  0.006767  0.267157   -30.596243  0.000000   -313.186802   \n",
       "26  2.713254e-07  0.000387  0.023301     0.900817  0.954794      0.343524   \n",
       "27  4.603618e-05  0.004701  0.201086   -24.250164  0.000000    -77.072700   \n",
       "28  1.426482e-08  0.000059  0.003119     0.992217  1.000000      0.990386   \n",
       "29  5.843102e-08  0.000135  0.007209     0.967751  1.000000      0.946369   \n",
       "30  4.602252e-06  0.001582  0.103689    -1.153199  0.551921    -42.139572   \n",
       "31  1.054442e-02  0.087341  0.822251 -4949.722858  0.000000 -25901.793605   \n",
       "32  1.642824e-06  0.000872  0.048987    -3.420992  0.920875     -4.789272   \n",
       "33  3.891860e-05  0.004225  0.197501   -27.705239  0.000000    -38.772709   \n",
       "34  6.479980e-08  0.000161  0.008175     0.818203  1.000000      0.883461   \n",
       "35  9.955223e-07  0.000472  0.023015     0.184766  1.000000      0.432342   \n",
       "36  1.611182e-07  0.000254  0.014151     0.877649  0.964932      0.383509   \n",
       "37  1.179941e-05  0.002673  0.152432    -2.717348  0.000000    -38.161340   \n",
       "38  1.136385e-07  0.000235  0.013086     0.907977  0.985209      0.648775   \n",
       "39  3.942709e-06  0.001496  0.090346    -0.902966  0.406240    -14.647808   \n",
       "40  6.322944e-10  0.000012  0.000640     0.999632  1.000000      0.999436   \n",
       "41  4.379187e-09  0.000032  0.001688     0.997107  1.000000      0.995964   \n",
       "42  3.184538e-06  0.001275  0.076007     0.277988  0.464438    -16.090131   \n",
       "43  6.390252e-03  0.072734  0.784932 -1770.434508  0.000000 -41253.389966   \n",
       "44  8.393706e-07  0.000634  0.042089     0.884327  0.939190     -6.088165   \n",
       "45  4.030112e-05  0.003847  0.163396   -35.044213  0.000000    -48.687522   \n",
       "46  4.002128e-08  0.000109  0.005799     0.989647  1.000000      0.959859   \n",
       "47  1.234846e-07  0.000210  0.011253     0.941031  1.000000      0.862967   \n",
       "\n",
       "         minAggR2              Model    Set  Train Size  \n",
       "0     -108.045485              Fresh  Train         180  \n",
       "1    -9924.245758              Fresh   Test         180  \n",
       "2       -0.839688  Transfer learning  Train         180  \n",
       "3     -276.002546  Transfer learning   Test         180  \n",
       "4        0.991977      Random Forest  Train         180  \n",
       "5        0.959730      Random Forest   Test         180  \n",
       "6      -18.464501              Fresh  Train         445  \n",
       "7    -2639.883435              Fresh   Test         445  \n",
       "8       -1.157768  Transfer learning  Train         445  \n",
       "9     -407.906966  Transfer learning   Test         445  \n",
       "10       0.997467      Random Forest  Train         445  \n",
       "11       0.981827      Random Forest   Test         445  \n",
       "12    -138.859218              Fresh  Train           9  \n",
       "13 -316316.061695              Fresh   Test           9  \n",
       "14     -34.796888  Transfer learning  Train           9  \n",
       "15    -495.073811  Transfer learning   Test           9  \n",
       "16       0.878432      Random Forest  Train           9  \n",
       "17       0.284956      Random Forest   Test           9  \n",
       "18     -65.232206              Fresh  Train          92  \n",
       "19  -34491.497420              Fresh   Test          92  \n",
       "20      -1.866629  Transfer learning  Train          92  \n",
       "21    -443.268347  Transfer learning   Test          92  \n",
       "22       0.984915      Random Forest  Train          92  \n",
       "23       0.936244      Random Forest   Test          92  \n",
       "24     -13.763742              Fresh  Train          47  \n",
       "25   -2594.850341              Fresh   Test          47  \n",
       "26      -0.709498  Transfer learning  Train          47  \n",
       "27    -365.735228  Transfer learning   Test          47  \n",
       "28       0.976529      Random Forest  Train          47  \n",
       "29       0.865053      Random Forest   Test          47  \n",
       "30    -276.934450              Fresh  Train           4  \n",
       "31 -131472.776737              Fresh   Test           4  \n",
       "32     -28.433425  Transfer learning  Train           4  \n",
       "33    -117.636579  Transfer learning   Test           4  \n",
       "34       0.750508      Random Forest  Train           4  \n",
       "35      -0.052897      Random Forest   Test           4  \n",
       "36      -6.295137              Fresh  Train         909  \n",
       "37    -207.867096              Fresh   Test         909  \n",
       "38       0.000000  Transfer learning  Train         909  \n",
       "39    -138.789016  Transfer learning   Test         909  \n",
       "40       0.998682      Random Forest  Train         909  \n",
       "41       0.989259      Random Forest   Test         909  \n",
       "42    -192.428279              Fresh  Train          18  \n",
       "43 -238117.364956              Fresh   Test          18  \n",
       "44     -94.359605  Transfer learning  Train          18  \n",
       "45    -411.241520  Transfer learning   Test          18  \n",
       "46       0.910077      Random Forest  Train          18  \n",
       "47       0.680570      Random Forest   Test          18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainDataDirs = glob.glob(os.path.join(dataDir, 'design_7*/'))\n",
    "trainDataDirs = glob.glob(os.path.join(dataDir, '*/'))\n",
    "trainDataDirs.remove(testDir)\n",
    "\n",
    "allResults = []\n",
    "for trainDataDir in trainDataDirs:\n",
    "    trainDataUnfiltered = loadConmechGraphs(trainDataDir)\n",
    "    trainData = filterbyDispValue(trainDataUnfiltered, maxDispCutoff)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'reading from {trainDataDir}')\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=epochs, \n",
    "                         saveDir=saveDir+f'{trainSize:05}/gcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Fresh'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Fresh'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile,\n",
    "                             epochs=epochs, \n",
    "                             saveDir=saveDir+f'{trainSize:05}/ptrGcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Transfer learning'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Transfer learning'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                  useXFeatures=False,\n",
    "                  saveDir=saveDir+f'{trainSize:05}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['Model'] = 'Random Forest'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['Model'] = 'Random Forest'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.660634e-06</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.079675</td>\n",
       "      <td>0.673650</td>\n",
       "      <td>0.576182</td>\n",
       "      <td>-13.553525</td>\n",
       "      <td>-108.045485</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.031741e-03</td>\n",
       "      <td>0.022531</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>-755.943496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1824.017334</td>\n",
       "      <td>-9924.245758</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.884177e-07</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.947683</td>\n",
       "      <td>0.979292</td>\n",
       "      <td>0.370694</td>\n",
       "      <td>-0.839688</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220445e-05</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>-3.028061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-57.299070</td>\n",
       "      <td>-276.002546</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.212383e-09</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>0.991977</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.297604e-08</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.989085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981748</td>\n",
       "      <td>0.959730</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.916766e-07</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.031258</td>\n",
       "      <td>0.837433</td>\n",
       "      <td>0.841597</td>\n",
       "      <td>-1.160613</td>\n",
       "      <td>-18.464501</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.824250e-04</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.338595</td>\n",
       "      <td>-107.680914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-410.452558</td>\n",
       "      <td>-2639.883435</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.581865e-07</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>0.951290</td>\n",
       "      <td>0.965992</td>\n",
       "      <td>0.459569</td>\n",
       "      <td>-1.157768</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.574461e-05</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.196118</td>\n",
       "      <td>0.295736</td>\n",
       "      <td>0.383904</td>\n",
       "      <td>-72.745692</td>\n",
       "      <td>-407.906966</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.191794e-09</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.999366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.997467</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.390658e-09</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.995058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992980</td>\n",
       "      <td>0.981827</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.076576e-06</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.120836</td>\n",
       "      <td>-1.885063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-20.174543</td>\n",
       "      <td>-138.859218</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.645967e-02</td>\n",
       "      <td>0.108885</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>-6529.935799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-65863.508984</td>\n",
       "      <td>-316316.061695</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.200493e-07</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.033506</td>\n",
       "      <td>0.749644</td>\n",
       "      <td>0.950518</td>\n",
       "      <td>-3.363083</td>\n",
       "      <td>-34.796888</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.301280e-05</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.204735</td>\n",
       "      <td>-6.353722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-61.949078</td>\n",
       "      <td>-495.073811</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9.675630e-08</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.876270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933749</td>\n",
       "      <td>0.878432</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.054910e-07</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.697931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698409</td>\n",
       "      <td>0.284956</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.960677e-06</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.080866</td>\n",
       "      <td>-2.844588</td>\n",
       "      <td>0.567935</td>\n",
       "      <td>-8.459198</td>\n",
       "      <td>-65.232206</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.193004e-04</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.504556</td>\n",
       "      <td>-68.214723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3902.891968</td>\n",
       "      <td>-34491.497420</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.910660e-07</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>0.908692</td>\n",
       "      <td>0.177742</td>\n",
       "      <td>-1.866629</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.144119e-05</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.151473</td>\n",
       "      <td>-5.178070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41.156471</td>\n",
       "      <td>-443.268347</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.954749e-09</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.996824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>0.984915</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.565904e-08</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.982923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969177</td>\n",
       "      <td>0.936244</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.268256e-06</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.686853</td>\n",
       "      <td>0.683126</td>\n",
       "      <td>-1.560371</td>\n",
       "      <td>-13.763742</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.171260e-05</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.267157</td>\n",
       "      <td>-30.596243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-313.186802</td>\n",
       "      <td>-2594.850341</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.713254e-07</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.900817</td>\n",
       "      <td>0.954794</td>\n",
       "      <td>0.343524</td>\n",
       "      <td>-0.709498</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.603618e-05</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.201086</td>\n",
       "      <td>-24.250164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-77.072700</td>\n",
       "      <td>-365.735228</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.426482e-08</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.992217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>0.976529</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.843102e-08</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.967751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946369</td>\n",
       "      <td>0.865053</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.602252e-06</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.103689</td>\n",
       "      <td>-1.153199</td>\n",
       "      <td>0.551921</td>\n",
       "      <td>-42.139572</td>\n",
       "      <td>-276.934450</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.054442e-02</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.822251</td>\n",
       "      <td>-4949.722858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25901.793605</td>\n",
       "      <td>-131472.776737</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.642824e-06</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.048987</td>\n",
       "      <td>-3.420992</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>-4.789272</td>\n",
       "      <td>-28.433425</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.891860e-05</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.197501</td>\n",
       "      <td>-27.705239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.772709</td>\n",
       "      <td>-117.636579</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.479980e-08</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>0.818203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883461</td>\n",
       "      <td>0.750508</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.955223e-07</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.184766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432342</td>\n",
       "      <td>-0.052897</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.611182e-07</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.877649</td>\n",
       "      <td>0.964932</td>\n",
       "      <td>0.383509</td>\n",
       "      <td>-6.295137</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.179941e-05</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>-2.717348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.161340</td>\n",
       "      <td>-207.867096</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.136385e-07</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>0.907977</td>\n",
       "      <td>0.985209</td>\n",
       "      <td>0.648775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.942709e-06</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.090346</td>\n",
       "      <td>-0.902966</td>\n",
       "      <td>0.406240</td>\n",
       "      <td>-14.647808</td>\n",
       "      <td>-138.789016</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6.322944e-10</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.998682</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.379187e-09</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.997107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995964</td>\n",
       "      <td>0.989259</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3.184538e-06</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>0.277988</td>\n",
       "      <td>0.464438</td>\n",
       "      <td>-16.090131</td>\n",
       "      <td>-192.428279</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.390252e-03</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.784932</td>\n",
       "      <td>-1770.434508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41253.389966</td>\n",
       "      <td>-238117.364956</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.393706e-07</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.884327</td>\n",
       "      <td>0.939190</td>\n",
       "      <td>-6.088165</td>\n",
       "      <td>-94.359605</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.030112e-05</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>-35.044213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-48.687522</td>\n",
       "      <td>-411.241520</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.002128e-08</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.989647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959859</td>\n",
       "      <td>0.910077</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.234846e-07</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.011253</td>\n",
       "      <td>0.941031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862967</td>\n",
       "      <td>0.680570</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse       mae       mre       peakR2  maxAggR2     meanAggR2  \\\n",
       "0   2.660634e-06  0.001215  0.079675     0.673650  0.576182    -13.553525   \n",
       "1   1.031741e-03  0.022531  0.481587  -755.943496  0.000000  -1824.017334   \n",
       "2   1.884177e-07  0.000335  0.021375     0.947683  0.979292      0.370694   \n",
       "3   1.220445e-05  0.002801  0.163308    -3.028061  0.000000    -57.299070   \n",
       "4   3.212383e-09  0.000028  0.001506     0.998414  1.000000      0.997109   \n",
       "5   2.297604e-08  0.000073  0.003909     0.989085  1.000000      0.981748   \n",
       "6   4.916766e-07  0.000520  0.031258     0.837433  0.841597     -1.160613   \n",
       "7   1.824250e-04  0.010227  0.338595  -107.680914  0.000000   -410.452558   \n",
       "8   1.581865e-07  0.000303  0.019524     0.951290  0.965992      0.459569   \n",
       "9   1.574461e-05  0.003146  0.196118     0.295736  0.383904    -72.745692   \n",
       "10  1.191794e-09  0.000017  0.000914     0.999366  1.000000      0.999002   \n",
       "11  8.390658e-09  0.000046  0.002416     0.995058  1.000000      0.992980   \n",
       "12  8.076576e-06  0.002033  0.120836    -1.885063  0.000000    -20.174543   \n",
       "13  1.645967e-02  0.108885  0.843348 -6529.935799  0.000000 -65863.508984   \n",
       "14  5.200493e-07  0.000530  0.033506     0.749644  0.950518     -3.363083   \n",
       "15  2.301280e-05  0.003744  0.204735    -6.353722  0.000000    -61.949078   \n",
       "16  9.675630e-08  0.000159  0.007925     0.876270  1.000000      0.933749   \n",
       "17  4.054910e-07  0.000315  0.015321     0.697931  1.000000      0.698409   \n",
       "18  5.960677e-06  0.001482  0.080866    -2.844588  0.567935     -8.459198   \n",
       "19  5.193004e-04  0.018067  0.504556   -68.214723  0.000000  -3902.891968   \n",
       "20  2.910660e-07  0.000413  0.024989     0.868221  0.908692      0.177742   \n",
       "21  1.144119e-05  0.002553  0.151473    -5.178070  0.000000    -41.156471   \n",
       "22  6.954749e-09  0.000041  0.002185     0.996824  1.000000      0.994072   \n",
       "23  3.565904e-08  0.000096  0.005099     0.982923  1.000000      0.969177   \n",
       "24  1.268256e-06  0.000829  0.051854     0.686853  0.683126     -1.560371   \n",
       "25  9.171260e-05  0.006767  0.267157   -30.596243  0.000000   -313.186802   \n",
       "26  2.713254e-07  0.000387  0.023301     0.900817  0.954794      0.343524   \n",
       "27  4.603618e-05  0.004701  0.201086   -24.250164  0.000000    -77.072700   \n",
       "28  1.426482e-08  0.000059  0.003119     0.992217  1.000000      0.990386   \n",
       "29  5.843102e-08  0.000135  0.007209     0.967751  1.000000      0.946369   \n",
       "30  4.602252e-06  0.001582  0.103689    -1.153199  0.551921    -42.139572   \n",
       "31  1.054442e-02  0.087341  0.822251 -4949.722858  0.000000 -25901.793605   \n",
       "32  1.642824e-06  0.000872  0.048987    -3.420992  0.920875     -4.789272   \n",
       "33  3.891860e-05  0.004225  0.197501   -27.705239  0.000000    -38.772709   \n",
       "34  6.479980e-08  0.000161  0.008175     0.818203  1.000000      0.883461   \n",
       "35  9.955223e-07  0.000472  0.023015     0.184766  1.000000      0.432342   \n",
       "36  1.611182e-07  0.000254  0.014151     0.877649  0.964932      0.383509   \n",
       "37  1.179941e-05  0.002673  0.152432    -2.717348  0.000000    -38.161340   \n",
       "38  1.136385e-07  0.000235  0.013086     0.907977  0.985209      0.648775   \n",
       "39  3.942709e-06  0.001496  0.090346    -0.902966  0.406240    -14.647808   \n",
       "40  6.322944e-10  0.000012  0.000640     0.999632  1.000000      0.999436   \n",
       "41  4.379187e-09  0.000032  0.001688     0.997107  1.000000      0.995964   \n",
       "42  3.184538e-06  0.001275  0.076007     0.277988  0.464438    -16.090131   \n",
       "43  6.390252e-03  0.072734  0.784932 -1770.434508  0.000000 -41253.389966   \n",
       "44  8.393706e-07  0.000634  0.042089     0.884327  0.939190     -6.088165   \n",
       "45  4.030112e-05  0.003847  0.163396   -35.044213  0.000000    -48.687522   \n",
       "46  4.002128e-08  0.000109  0.005799     0.989647  1.000000      0.959859   \n",
       "47  1.234846e-07  0.000210  0.011253     0.941031  1.000000      0.862967   \n",
       "\n",
       "         minAggR2              Model    Set  Train Size  \n",
       "0     -108.045485              Fresh  Train         180  \n",
       "1    -9924.245758              Fresh   Test         180  \n",
       "2       -0.839688  Transfer learning  Train         180  \n",
       "3     -276.002546  Transfer learning   Test         180  \n",
       "4        0.991977      Random Forest  Train         180  \n",
       "5        0.959730      Random Forest   Test         180  \n",
       "6      -18.464501              Fresh  Train         445  \n",
       "7    -2639.883435              Fresh   Test         445  \n",
       "8       -1.157768  Transfer learning  Train         445  \n",
       "9     -407.906966  Transfer learning   Test         445  \n",
       "10       0.997467      Random Forest  Train         445  \n",
       "11       0.981827      Random Forest   Test         445  \n",
       "12    -138.859218              Fresh  Train           9  \n",
       "13 -316316.061695              Fresh   Test           9  \n",
       "14     -34.796888  Transfer learning  Train           9  \n",
       "15    -495.073811  Transfer learning   Test           9  \n",
       "16       0.878432      Random Forest  Train           9  \n",
       "17       0.284956      Random Forest   Test           9  \n",
       "18     -65.232206              Fresh  Train          92  \n",
       "19  -34491.497420              Fresh   Test          92  \n",
       "20      -1.866629  Transfer learning  Train          92  \n",
       "21    -443.268347  Transfer learning   Test          92  \n",
       "22       0.984915      Random Forest  Train          92  \n",
       "23       0.936244      Random Forest   Test          92  \n",
       "24     -13.763742              Fresh  Train          47  \n",
       "25   -2594.850341              Fresh   Test          47  \n",
       "26      -0.709498  Transfer learning  Train          47  \n",
       "27    -365.735228  Transfer learning   Test          47  \n",
       "28       0.976529      Random Forest  Train          47  \n",
       "29       0.865053      Random Forest   Test          47  \n",
       "30    -276.934450              Fresh  Train           4  \n",
       "31 -131472.776737              Fresh   Test           4  \n",
       "32     -28.433425  Transfer learning  Train           4  \n",
       "33    -117.636579  Transfer learning   Test           4  \n",
       "34       0.750508      Random Forest  Train           4  \n",
       "35      -0.052897      Random Forest   Test           4  \n",
       "36      -6.295137              Fresh  Train         909  \n",
       "37    -207.867096              Fresh   Test         909  \n",
       "38       0.000000  Transfer learning  Train         909  \n",
       "39    -138.789016  Transfer learning   Test         909  \n",
       "40       0.998682      Random Forest  Train         909  \n",
       "41       0.989259      Random Forest   Test         909  \n",
       "42    -192.428279              Fresh  Train          18  \n",
       "43 -238117.364956              Fresh   Test          18  \n",
       "44     -94.359605  Transfer learning  Train          18  \n",
       "45    -411.241520  Transfer learning   Test          18  \n",
       "46       0.910077      Random Forest  Train          18  \n",
       "47       0.680570      Random Forest   Test          18  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(allResults)\n",
    "df = pd.read_csv('results/transferLrn_des7_tower5_01/testResults.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.031741e-03</td>\n",
       "      <td>0.022531</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>-755.943496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1824.017334</td>\n",
       "      <td>-9924.245758</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220445e-05</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>-3.028061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-57.299070</td>\n",
       "      <td>-276.002546</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.297604e-08</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.989085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981748</td>\n",
       "      <td>0.959730</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.824250e-04</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.338595</td>\n",
       "      <td>-107.680914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-410.452558</td>\n",
       "      <td>-2639.883435</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.574461e-05</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.196118</td>\n",
       "      <td>0.295736</td>\n",
       "      <td>0.383904</td>\n",
       "      <td>-72.745692</td>\n",
       "      <td>-407.906966</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.390658e-09</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.995058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992980</td>\n",
       "      <td>0.981827</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.645967e-02</td>\n",
       "      <td>0.108885</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>-6529.935799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-65863.508984</td>\n",
       "      <td>-316316.061695</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.301280e-05</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.204735</td>\n",
       "      <td>-6.353722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-61.949078</td>\n",
       "      <td>-495.073811</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.054910e-07</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.697931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698409</td>\n",
       "      <td>0.284956</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.193004e-04</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.504556</td>\n",
       "      <td>-68.214723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3902.891968</td>\n",
       "      <td>-34491.497420</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.144119e-05</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.151473</td>\n",
       "      <td>-5.178070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41.156471</td>\n",
       "      <td>-443.268347</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.565904e-08</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.982923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969177</td>\n",
       "      <td>0.936244</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.171260e-05</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.267157</td>\n",
       "      <td>-30.596243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-313.186802</td>\n",
       "      <td>-2594.850341</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.603618e-05</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.201086</td>\n",
       "      <td>-24.250164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-77.072700</td>\n",
       "      <td>-365.735228</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.843102e-08</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.967751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946369</td>\n",
       "      <td>0.865053</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.054442e-02</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.822251</td>\n",
       "      <td>-4949.722858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25901.793605</td>\n",
       "      <td>-131472.776737</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.891860e-05</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.197501</td>\n",
       "      <td>-27.705239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.772709</td>\n",
       "      <td>-117.636579</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.955223e-07</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>0.184766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432342</td>\n",
       "      <td>-0.052897</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.179941e-05</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.152432</td>\n",
       "      <td>-2.717348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.161340</td>\n",
       "      <td>-207.867096</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.942709e-06</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.090346</td>\n",
       "      <td>-0.902966</td>\n",
       "      <td>0.406240</td>\n",
       "      <td>-14.647808</td>\n",
       "      <td>-138.789016</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.379187e-09</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.997107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995964</td>\n",
       "      <td>0.989259</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.390252e-03</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.784932</td>\n",
       "      <td>-1770.434508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-41253.389966</td>\n",
       "      <td>-238117.364956</td>\n",
       "      <td>GCN</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.030112e-05</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.163396</td>\n",
       "      <td>-35.044213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-48.687522</td>\n",
       "      <td>-411.241520</td>\n",
       "      <td>GCN with transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.234846e-07</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.011253</td>\n",
       "      <td>0.941031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862967</td>\n",
       "      <td>0.680570</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse       mae       mre       peakR2  maxAggR2     meanAggR2  \\\n",
       "1   1.031741e-03  0.022531  0.481587  -755.943496  0.000000  -1824.017334   \n",
       "3   1.220445e-05  0.002801  0.163308    -3.028061  0.000000    -57.299070   \n",
       "5   2.297604e-08  0.000073  0.003909     0.989085  1.000000      0.981748   \n",
       "7   1.824250e-04  0.010227  0.338595  -107.680914  0.000000   -410.452558   \n",
       "9   1.574461e-05  0.003146  0.196118     0.295736  0.383904    -72.745692   \n",
       "11  8.390658e-09  0.000046  0.002416     0.995058  1.000000      0.992980   \n",
       "13  1.645967e-02  0.108885  0.843348 -6529.935799  0.000000 -65863.508984   \n",
       "15  2.301280e-05  0.003744  0.204735    -6.353722  0.000000    -61.949078   \n",
       "17  4.054910e-07  0.000315  0.015321     0.697931  1.000000      0.698409   \n",
       "19  5.193004e-04  0.018067  0.504556   -68.214723  0.000000  -3902.891968   \n",
       "21  1.144119e-05  0.002553  0.151473    -5.178070  0.000000    -41.156471   \n",
       "23  3.565904e-08  0.000096  0.005099     0.982923  1.000000      0.969177   \n",
       "25  9.171260e-05  0.006767  0.267157   -30.596243  0.000000   -313.186802   \n",
       "27  4.603618e-05  0.004701  0.201086   -24.250164  0.000000    -77.072700   \n",
       "29  5.843102e-08  0.000135  0.007209     0.967751  1.000000      0.946369   \n",
       "31  1.054442e-02  0.087341  0.822251 -4949.722858  0.000000 -25901.793605   \n",
       "33  3.891860e-05  0.004225  0.197501   -27.705239  0.000000    -38.772709   \n",
       "35  9.955223e-07  0.000472  0.023015     0.184766  1.000000      0.432342   \n",
       "37  1.179941e-05  0.002673  0.152432    -2.717348  0.000000    -38.161340   \n",
       "39  3.942709e-06  0.001496  0.090346    -0.902966  0.406240    -14.647808   \n",
       "41  4.379187e-09  0.000032  0.001688     0.997107  1.000000      0.995964   \n",
       "43  6.390252e-03  0.072734  0.784932 -1770.434508  0.000000 -41253.389966   \n",
       "45  4.030112e-05  0.003847  0.163396   -35.044213  0.000000    -48.687522   \n",
       "47  1.234846e-07  0.000210  0.011253     0.941031  1.000000      0.862967   \n",
       "\n",
       "         minAggR2                       Model   Set  Train Size  \n",
       "1    -9924.245758                         GCN  Test         180  \n",
       "3     -276.002546  GCN with transfer learning  Test         180  \n",
       "5        0.959730               Random Forest  Test         180  \n",
       "7    -2639.883435                         GCN  Test         445  \n",
       "9     -407.906966  GCN with transfer learning  Test         445  \n",
       "11       0.981827               Random Forest  Test         445  \n",
       "13 -316316.061695                         GCN  Test           9  \n",
       "15    -495.073811  GCN with transfer learning  Test           9  \n",
       "17       0.284956               Random Forest  Test           9  \n",
       "19  -34491.497420                         GCN  Test          92  \n",
       "21    -443.268347  GCN with transfer learning  Test          92  \n",
       "23       0.936244               Random Forest  Test          92  \n",
       "25   -2594.850341                         GCN  Test          47  \n",
       "27    -365.735228  GCN with transfer learning  Test          47  \n",
       "29       0.865053               Random Forest  Test          47  \n",
       "31 -131472.776737                         GCN  Test           4  \n",
       "33    -117.636579  GCN with transfer learning  Test           4  \n",
       "35      -0.052897               Random Forest  Test           4  \n",
       "37    -207.867096                         GCN  Test         909  \n",
       "39    -138.789016  GCN with transfer learning  Test         909  \n",
       "41       0.989259               Random Forest  Test         909  \n",
       "43 -238117.364956                         GCN  Test          18  \n",
       "45    -411.241520  GCN with transfer learning  Test          18  \n",
       "47       0.680570               Random Forest  Test          18  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Set=='Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('Fresh', 'GCN')\n",
    "df = df.replace('Transfer learning', 'GCN with transfer learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-0cba5d65c3054c5ca8c36d3cc3200d51\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0cba5d65c3054c5ca8c36d3cc3200d51\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0cba5d65c3054c5ca8c36d3cc3200d51\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-1673c4f500025616e7b0b6bf1387bdb5\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Model\"}, {\"type\": \"quantitative\", \"field\": \"mse\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Train Size\", \"scale\": {\"type\": \"log\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".1e\"}, \"field\": \"mse\", \"title\": \"MSE\"}}, \"height\": 200, \"title\": \"Transfer learning - Tower\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-1673c4f500025616e7b0b6bf1387bdb5\": [{\"mse\": 0.0010317409178242087, \"mae\": 0.022531241178512573, \"mre\": 0.4815873205661774, \"peakR2\": -755.9434956547743, \"maxAggR2\": 0.0, \"meanAggR2\": -1824.0173339012085, \"minAggR2\": -9924.245757529909, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 1.2204448466945905e-05, \"mae\": 0.0028010450769215822, \"mre\": 0.16330774128437042, \"peakR2\": -3.028060822299933, \"maxAggR2\": 0.0, \"meanAggR2\": -57.299070230513344, \"minAggR2\": -276.0025456104769, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 2.2976035956337604e-08, \"mae\": 7.331323256149694e-05, \"mre\": 0.003909210077409631, \"peakR2\": 0.9890849260222708, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9817475077566324, \"minAggR2\": 0.959730445097536, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 180}, {\"mse\": 0.00018242500664200636, \"mae\": 0.01022663526237011, \"mre\": 0.338594526052475, \"peakR2\": -107.6809144111863, \"maxAggR2\": 0.0, \"meanAggR2\": -410.45255819472885, \"minAggR2\": -2639.883435359756, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 1.5744608390377834e-05, \"mae\": 0.0031456851866096263, \"mre\": 0.19611825048923487, \"peakR2\": 0.2957356415515078, \"maxAggR2\": 0.3839036897014873, \"meanAggR2\": -72.74569182424845, \"minAggR2\": -407.906966422911, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 8.390658438697235e-09, \"mae\": 4.600029473178543e-05, \"mre\": 0.0024160424435863068, \"peakR2\": 0.9950581018483364, \"maxAggR2\": 1.0, \"meanAggR2\": 0.992979591205948, \"minAggR2\": 0.9818272860310756, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 445}, {\"mse\": 0.01645966619253159, \"mae\": 0.10888450592756273, \"mre\": 0.8433484435081482, \"peakR2\": -6529.935799199926, \"maxAggR2\": 0.0, \"meanAggR2\": -65863.50898382661, \"minAggR2\": -316316.0616945807, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 2.301279891980812e-05, \"mae\": 0.003744259476661682, \"mre\": 0.2047349065542221, \"peakR2\": -6.353722167240077, \"maxAggR2\": 0.0, \"meanAggR2\": -61.94907797717616, \"minAggR2\": -495.0738109934014, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 4.0549099355418985e-07, \"mae\": 0.00031454972184348387, \"mre\": 0.015321339068975762, \"peakR2\": 0.6979312206338365, \"maxAggR2\": 1.0, \"meanAggR2\": 0.6984087445665561, \"minAggR2\": 0.28495635134617114, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 9}, {\"mse\": 0.0005193004035390913, \"mae\": 0.018066739663481712, \"mre\": 0.5045560002326965, \"peakR2\": -68.2147226429931, \"maxAggR2\": 0.0, \"meanAggR2\": -3902.8919675668867, \"minAggR2\": -34491.497420105734, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 1.1441192327765748e-05, \"mae\": 0.0025532783474773173, \"mre\": 0.1514734923839569, \"peakR2\": -5.178069891704737, \"maxAggR2\": 0.0, \"meanAggR2\": -41.15647136145756, \"minAggR2\": -443.26834673248544, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 3.565904411898805e-08, \"mae\": 9.570641929456112e-05, \"mre\": 0.005098534533178443, \"peakR2\": 0.9829225368962348, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9691765647131838, \"minAggR2\": 0.9362439277016108, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 92}, {\"mse\": 9.171260171569884e-05, \"mae\": 0.006767436861991882, \"mre\": 0.2671571969985962, \"peakR2\": -30.59624280080265, \"maxAggR2\": 0.0, \"meanAggR2\": -313.18680179366726, \"minAggR2\": -2594.850340852777, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 4.6036184357944876e-05, \"mae\": 0.004700702615082264, \"mre\": 0.20108579099178314, \"peakR2\": -24.250163887317463, \"maxAggR2\": 0.0, \"meanAggR2\": -77.07270019508016, \"minAggR2\": -365.7352276776039, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 5.843101717591554e-08, \"mae\": 0.00013494259530960314, \"mre\": 0.007208551389434822, \"peakR2\": 0.9677510649846841, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9463687988190852, \"minAggR2\": 0.8650533513885117, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 47}, {\"mse\": 0.010544423945248129, \"mae\": 0.08734091371297836, \"mre\": 0.8222514986991882, \"peakR2\": -4949.722858319397, \"maxAggR2\": 0.0, \"meanAggR2\": -25901.793604642065, \"minAggR2\": -131472.77673677358, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 3.8918598875170574e-05, \"mae\": 0.0042249453254044065, \"mre\": 0.1975010186433792, \"peakR2\": -27.705238548512817, \"maxAggR2\": 0.0, \"meanAggR2\": -38.77270890276425, \"minAggR2\": -117.63657930933714, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 9.955222666667938e-07, \"mae\": 0.00047193656283397306, \"mre\": 0.023015181629274833, \"peakR2\": 0.18476603015475024, \"maxAggR2\": 1.0, \"meanAggR2\": 0.4323418107594328, \"minAggR2\": -0.05289688367329637, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 4}, {\"mse\": 1.179940863949014e-05, \"mae\": 0.002673001028597355, \"mre\": 0.1524323672056198, \"peakR2\": -2.717348072231155, \"maxAggR2\": 0.0, \"meanAggR2\": -38.16133997949992, \"minAggR2\": -207.86709617104037, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 3.94270864489954e-06, \"mae\": 0.0014959395630285144, \"mre\": 0.09034590423107147, \"peakR2\": -0.902965861508508, \"maxAggR2\": 0.4062399524026664, \"meanAggR2\": -14.647808071022775, \"minAggR2\": -138.789016490907, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 4.3791869702940565e-09, \"mae\": 3.2251406881714864e-05, \"mre\": 0.001688392197393646, \"peakR2\": 0.997106940186053, \"maxAggR2\": 1.0, \"meanAggR2\": 0.9959639004131556, \"minAggR2\": 0.9892586919748086, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 909}, {\"mse\": 0.006390252150595188, \"mae\": 0.07273446023464203, \"mre\": 0.7849318385124207, \"peakR2\": -1770.4345080268015, \"maxAggR2\": 0.0, \"meanAggR2\": -41253.389966317736, \"minAggR2\": -238117.3649558316, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 4.03011217713356e-05, \"mae\": 0.0038469235878437757, \"mre\": 0.16339582204818726, \"peakR2\": -35.044212661864876, \"maxAggR2\": 0.0, \"meanAggR2\": -48.68752153915349, \"minAggR2\": -411.241519625881, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 1.2348459229628395e-07, \"mae\": 0.00020956302110985807, \"mre\": 0.011253352500879528, \"peakR2\": 0.9410310113349716, \"maxAggR2\": 1.0, \"meanAggR2\": 0.8629672867418936, \"minAggR2\": 0.6805700122790783, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 18}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df.Set=='Test']).mark_circle().encode(\n",
    "    x=alt.X('Train Size:Q', scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('mse:Q', title='MSE', axis=alt.Axis(format='.1e')),\n",
    "    color='Model',\n",
    "    tooltip=['Model', 'mse']\n",
    ").properties(width=400, height=200, title='Transfer learning - Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
