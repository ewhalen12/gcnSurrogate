{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "sys.path.append('./models')\n",
    "from feastnetSurrogateModel import FeaStNet\n",
    "from pointRegressorSurrogateModel import PointRegressor\n",
    "\n",
    "sys.path.append('./readers')\n",
    "from loadGhGraphs import loadGhGraphs\n",
    "\n",
    "sys.path.append('./visualization')\n",
    "from altTrussViz import plotTruss, interactiveErrorPlot\n",
    "\n",
    "sys.path.append('./util')\n",
    "from gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.199516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.524026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.046981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67.325867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.199516\n",
       "std       2.524026\n",
       "min       0.006570\n",
       "25%       0.015425\n",
       "50%       0.024334\n",
       "75%       0.046981\n",
       "max      67.325867"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "testFile = os.path.join(dataDir, 'design_7_N_1000.csv')\n",
    "allGraphsUnfiltered = loadGhGraphs(testFile, NUM_DV=5)\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.028952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.014951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.021719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.038367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.097861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.028952\n",
       "std      0.019071\n",
       "min      0.006570\n",
       "25%      0.014951\n",
       "50%      0.021719\n",
       "75%      0.038367\n",
       "max      0.097861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGraphs = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testGraphs]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_9\n",
      "loading design_6\n",
      "loading design_8\n",
      "loading design_5\n",
      "loaded 3600 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainFiles = glob.glob(os.path.join(dataDir, '*1000.csv'))\n",
    "pretrainFiles.remove(testFile)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainFile in pretrainFiles:\n",
    "    designName = pretrainFile.split('/')[-1].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    graphsUnfiltered = loadGhGraphs(pretrainFile, NUM_DV=5)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, test = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 8.3074e-01   valLoss:8.8119e-01  time: 9.98e+00\n",
      "epoch: 1   trainLoss: 6.3161e-01   valLoss:8.0567e-01  time: 5.30e+00\n",
      "epoch: 2   trainLoss: 5.0887e-01   valLoss:4.6461e-01  time: 5.19e+00\n",
      "epoch: 3   trainLoss: 4.2253e-01   valLoss:3.9987e-01  time: 5.07e+00\n",
      "epoch: 4   trainLoss: 3.6052e-01   valLoss:3.5567e-01  time: 5.09e+00\n",
      "epoch: 5   trainLoss: 3.1862e-01   valLoss:3.2774e-01  time: 5.07e+00\n",
      "epoch: 6   trainLoss: 2.8169e-01   valLoss:2.9000e-01  time: 5.14e+00\n",
      "epoch: 7   trainLoss: 2.6012e-01   valLoss:2.8451e-01  time: 5.21e+00\n",
      "epoch: 8   trainLoss: 2.3791e-01   valLoss:2.4848e-01  time: 5.27e+00\n",
      "epoch: 9   trainLoss: 2.1157e-01   valLoss:2.2721e-01  time: 5.31e+00\n",
      "epoch: 10   trainLoss: 1.9208e-01   valLoss:2.2545e-01  time: 5.29e+00\n",
      "epoch: 11   trainLoss: 1.7342e-01   valLoss:2.1352e-01  time: 5.28e+00\n",
      "epoch: 12   trainLoss: 1.6153e-01   valLoss:1.9441e-01  time: 5.22e+00\n",
      "epoch: 13   trainLoss: 1.4882e-01   valLoss:1.6108e-01  time: 5.29e+00\n",
      "epoch: 14   trainLoss: 1.4006e-01   valLoss:1.4238e-01  time: 4.94e+00\n",
      "epoch: 15   trainLoss: 1.3313e-01   valLoss:1.3226e-01  time: 4.97e+00\n",
      "epoch: 16   trainLoss: 1.2209e-01   valLoss:1.2983e-01  time: 5.02e+00\n",
      "epoch: 17   trainLoss: 1.1238e-01   valLoss:1.1799e-01  time: 4.90e+00\n",
      "epoch: 18   trainLoss: 1.0696e-01   valLoss:1.1101e-01  time: 4.93e+00\n",
      "epoch: 19   trainLoss: 1.0568e-01   valLoss:1.2431e-01  time: 4.95e+00\n",
      "epoch: 20   trainLoss: 1.0493e-01   valLoss:1.0409e-01  time: 5.00e+00\n",
      "epoch: 21   trainLoss: 9.8657e-02   valLoss:1.0218e-01  time: 4.92e+00\n",
      "epoch: 22   trainLoss: 9.2905e-02   valLoss:1.0356e-01  time: 4.95e+00\n",
      "epoch: 23   trainLoss: 8.9232e-02   valLoss:9.9406e-02  time: 4.92e+00\n",
      "epoch: 24   trainLoss: 8.6734e-02   valLoss:8.3918e-02  time: 4.98e+00\n",
      "epoch: 25   trainLoss: 8.1872e-02   valLoss:9.8688e-02  time: 4.94e+00\n",
      "epoch: 26   trainLoss: 8.8014e-02   valLoss:1.0363e-01  time: 5.01e+00\n",
      "epoch: 27   trainLoss: 8.3592e-02   valLoss:1.1181e-01  time: 5.03e+00\n",
      "epoch: 28   trainLoss: 9.0003e-02   valLoss:1.1666e-01  time: 5.16e+00\n",
      "epoch: 29   trainLoss: 8.2923e-02   valLoss:7.7041e-02  time: 5.03e+00\n",
      "epoch: 30   trainLoss: 7.8415e-02   valLoss:8.7773e-02  time: 5.04e+00\n",
      "epoch: 31   trainLoss: 7.9681e-02   valLoss:1.0562e-01  time: 5.14e+00\n",
      "epoch: 32   trainLoss: 7.8767e-02   valLoss:7.9400e-02  time: 5.04e+00\n",
      "epoch: 33   trainLoss: 6.5671e-02   valLoss:7.7369e-02  time: 4.99e+00\n",
      "epoch: 34   trainLoss: 6.8128e-02   valLoss:6.9712e-02  time: 5.05e+00\n",
      "epoch: 35   trainLoss: 6.8469e-02   valLoss:7.4457e-02  time: 4.97e+00\n",
      "epoch: 36   trainLoss: 6.9107e-02   valLoss:8.9696e-02  time: 4.96e+00\n",
      "epoch: 37   trainLoss: 7.2495e-02   valLoss:7.2278e-02  time: 5.04e+00\n",
      "epoch: 38   trainLoss: 6.2985e-02   valLoss:7.3083e-02  time: 4.91e+00\n",
      "epoch: 39   trainLoss: 6.2814e-02   valLoss:8.5636e-02  time: 4.82e+00\n",
      "epoch: 40   trainLoss: 6.1541e-02   valLoss:7.7875e-02  time: 4.89e+00\n",
      "epoch: 41   trainLoss: 5.9500e-02   valLoss:6.8955e-02  time: 4.88e+00\n",
      "epoch: 42   trainLoss: 5.7664e-02   valLoss:6.2890e-02  time: 4.82e+00\n",
      "epoch: 43   trainLoss: 5.7797e-02   valLoss:6.2260e-02  time: 4.88e+00\n",
      "epoch: 44   trainLoss: 5.7932e-02   valLoss:6.0486e-02  time: 4.91e+00\n",
      "epoch: 45   trainLoss: 5.4683e-02   valLoss:7.1040e-02  time: 4.94e+00\n",
      "epoch: 46   trainLoss: 6.2449e-02   valLoss:6.8475e-02  time: 4.89e+00\n",
      "epoch: 47   trainLoss: 5.7783e-02   valLoss:6.0361e-02  time: 5.00e+00\n",
      "epoch: 48   trainLoss: 5.5629e-02   valLoss:6.3698e-02  time: 4.89e+00\n",
      "epoch: 49   trainLoss: 5.3858e-02   valLoss:5.4783e-02  time: 4.89e+00\n",
      "epoch: 50   trainLoss: 5.2555e-02   valLoss:5.7866e-02  time: 4.90e+00\n",
      "epoch: 51   trainLoss: 5.3066e-02   valLoss:5.5354e-02  time: 4.93e+00\n",
      "epoch: 52   trainLoss: 5.0427e-02   valLoss:5.2948e-02  time: 4.92e+00\n",
      "epoch: 53   trainLoss: 4.8917e-02   valLoss:5.8569e-02  time: 4.88e+00\n",
      "epoch: 54   trainLoss: 5.5066e-02   valLoss:5.1245e-02  time: 4.87e+00\n",
      "epoch: 55   trainLoss: 4.8466e-02   valLoss:5.1252e-02  time: 4.90e+00\n",
      "epoch: 56   trainLoss: 4.7006e-02   valLoss:5.2794e-02  time: 4.87e+00\n",
      "epoch: 57   trainLoss: 4.5160e-02   valLoss:5.5954e-02  time: 4.98e+00\n",
      "epoch: 58   trainLoss: 4.5414e-02   valLoss:5.9088e-02  time: 4.91e+00\n",
      "epoch: 59   trainLoss: 4.5434e-02   valLoss:5.2748e-02  time: 4.86e+00\n",
      "epoch: 60   trainLoss: 4.8141e-02   valLoss:5.9421e-02  time: 4.88e+00\n",
      "epoch: 61   trainLoss: 4.7112e-02   valLoss:5.3023e-02  time: 4.88e+00\n",
      "epoch: 62   trainLoss: 4.1503e-02   valLoss:4.7748e-02  time: 4.85e+00\n",
      "epoch: 63   trainLoss: 4.9519e-02   valLoss:4.9232e-02  time: 4.89e+00\n",
      "epoch: 64   trainLoss: 4.3698e-02   valLoss:4.8908e-02  time: 5.00e+00\n",
      "epoch: 65   trainLoss: 4.4410e-02   valLoss:4.8662e-02  time: 4.88e+00\n",
      "epoch: 66   trainLoss: 4.5066e-02   valLoss:5.2680e-02  time: 4.89e+00\n",
      "epoch: 67   trainLoss: 5.3122e-02   valLoss:6.1396e-02  time: 5.00e+00\n",
      "epoch: 68   trainLoss: 4.8486e-02   valLoss:5.6150e-02  time: 4.91e+00\n",
      "epoch: 69   trainLoss: 4.4607e-02   valLoss:4.8038e-02  time: 4.91e+00\n",
      "epoch: 70   trainLoss: 4.3199e-02   valLoss:4.6000e-02  time: 4.89e+00\n",
      "epoch: 71   trainLoss: 3.9593e-02   valLoss:4.6405e-02  time: 4.89e+00\n",
      "epoch: 72   trainLoss: 4.1993e-02   valLoss:4.8446e-02  time: 4.86e+00\n",
      "epoch: 73   trainLoss: 4.0614e-02   valLoss:4.7514e-02  time: 4.91e+00\n",
      "epoch: 74   trainLoss: 4.3932e-02   valLoss:5.1403e-02  time: 4.86e+00\n",
      "epoch: 75   trainLoss: 4.3062e-02   valLoss:5.3129e-02  time: 4.89e+00\n",
      "epoch: 76   trainLoss: 4.0687e-02   valLoss:4.7471e-02  time: 4.90e+00\n",
      "epoch: 77   trainLoss: 4.7698e-02   valLoss:5.3016e-02  time: 4.97e+00\n",
      "epoch: 78   trainLoss: 4.2839e-02   valLoss:5.8563e-02  time: 4.89e+00\n",
      "epoch: 79   trainLoss: 4.2432e-02   valLoss:4.8221e-02  time: 4.87e+00\n",
      "epoch: 80   trainLoss: 4.3267e-02   valLoss:5.9584e-02  time: 4.90e+00\n",
      "epoch: 81   trainLoss: 4.1535e-02   valLoss:5.3387e-02  time: 4.90e+00\n",
      "epoch: 82   trainLoss: 3.8189e-02   valLoss:5.5410e-02  time: 4.88e+00\n",
      "epoch: 83   trainLoss: 4.4085e-02   valLoss:4.2952e-02  time: 4.88e+00\n",
      "epoch: 84   trainLoss: 4.0846e-02   valLoss:4.5977e-02  time: 4.93e+00\n",
      "epoch: 85   trainLoss: 4.1175e-02   valLoss:4.7638e-02  time: 4.90e+00\n",
      "epoch: 86   trainLoss: 3.6748e-02   valLoss:5.4907e-02  time: 4.92e+00\n",
      "epoch: 87   trainLoss: 3.5873e-02   valLoss:4.2283e-02  time: 4.99e+00\n",
      "epoch: 88   trainLoss: 3.7281e-02   valLoss:4.4546e-02  time: 4.92e+00\n",
      "epoch: 89   trainLoss: 4.1099e-02   valLoss:4.5306e-02  time: 4.90e+00\n",
      "epoch: 90   trainLoss: 4.0872e-02   valLoss:3.8186e-02  time: 4.90e+00\n",
      "epoch: 91   trainLoss: 3.9910e-02   valLoss:3.7185e-02  time: 4.87e+00\n",
      "epoch: 92   trainLoss: 4.0287e-02   valLoss:4.6667e-02  time: 4.88e+00\n",
      "epoch: 93   trainLoss: 4.0268e-02   valLoss:4.4970e-02  time: 4.88e+00\n",
      "epoch: 94   trainLoss: 3.5768e-02   valLoss:4.9893e-02  time: 4.88e+00\n",
      "epoch: 95   trainLoss: 3.8734e-02   valLoss:4.5238e-02  time: 4.86e+00\n",
      "epoch: 96   trainLoss: 4.0373e-02   valLoss:4.5654e-02  time: 4.87e+00\n",
      "epoch: 97   trainLoss: 3.7690e-02   valLoss:4.3101e-02  time: 4.98e+00\n",
      "epoch: 98   trainLoss: 3.7591e-02   valLoss:4.4192e-02  time: 4.90e+00\n",
      "epoch: 99   trainLoss: 4.2472e-02   valLoss:5.6612e-02  time: 4.90e+00\n",
      "loading checkpoint 91\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-e4d0c4c31e0b496e9dd42ac78b8512bf\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e4d0c4c31e0b496e9dd42ac78b8512bf\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e4d0c4c31e0b496e9dd42ac78b8512bf\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a13b6d15313a17b01b27e7a0731619f9\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"metric\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-a13b6d15313a17b01b27e7a0731619f9\": [{\"index\": 0, \"train\": 0.830739215016365, \"val\": 0.8811887585523503}, {\"index\": 1, \"train\": 0.6316109100977579, \"val\": 0.8056656757887039}, {\"index\": 2, \"train\": 0.5088722283641497, \"val\": 0.4646074247615481}, {\"index\": 3, \"train\": 0.422533576687177, \"val\": 0.39987225648602126}, {\"index\": 4, \"train\": 0.360519714653492, \"val\": 0.3556695631739718}, {\"index\": 5, \"train\": 0.31862375140190125, \"val\": 0.3277352926528288}, {\"index\": 6, \"train\": 0.28169239312410355, \"val\": 0.2900013513932074}, {\"index\": 7, \"train\": 0.2601174476246039, \"val\": 0.28451093982870657}, {\"index\": 8, \"train\": 0.23790774742762247, \"val\": 0.24847990775446374}, {\"index\": 9, \"train\": 0.2115746649603049, \"val\": 0.2272074693882907}, {\"index\": 10, \"train\": 0.19208235666155815, \"val\": 0.22545010901090723}, {\"index\": 11, \"train\": 0.1734206328789393, \"val\": 0.21352116457117654}, {\"index\": 12, \"train\": 0.16153153652946153, \"val\": 0.19441301439785294}, {\"index\": 13, \"train\": 0.1488170096029838, \"val\": 0.16108479987036575}, {\"index\": 14, \"train\": 0.14005655174454054, \"val\": 0.1423797323713424}, {\"index\": 15, \"train\": 0.13312784023582935, \"val\": 0.13226348794005052}, {\"index\": 16, \"train\": 0.12208744200567405, \"val\": 0.1298349694426482}, {\"index\": 17, \"train\": 0.11238351526359718, \"val\": 0.11798832209013334}, {\"index\": 18, \"train\": 0.10695895614723365, \"val\": 0.11101165067413132}, {\"index\": 19, \"train\": 0.105684374148647, \"val\": 0.12431415646699154}, {\"index\": 20, \"train\": 0.1049300233523051, \"val\": 0.10409080227362681}, {\"index\": 21, \"train\": 0.0986569623152415, \"val\": 0.10217947114293498}, {\"index\": 22, \"train\": 0.09290510540207227, \"val\": 0.10355570966860762}, {\"index\": 23, \"train\": 0.0892317183315754, \"val\": 0.09940602337225789}, {\"index\": 24, \"train\": 0.08673427253961563, \"val\": 0.0839183801897215}, {\"index\": 25, \"train\": 0.08187156481047471, \"val\": 0.09868849645567092}, {\"index\": 26, \"train\": 0.08801369617382686, \"val\": 0.10363168138148332}, {\"index\": 27, \"train\": 0.08359214942902327, \"val\": 0.11180854747632381}, {\"index\": 28, \"train\": 0.09000270441174507, \"val\": 0.11666275806233493}, {\"index\": 29, \"train\": 0.08292337631185849, \"val\": 0.07704065896363722}, {\"index\": 30, \"train\": 0.07841521874070168, \"val\": 0.08777345447214666}, {\"index\": 31, \"train\": 0.07968135364353657, \"val\": 0.10562111867798699}, {\"index\": 32, \"train\": 0.07876712922006845, \"val\": 0.0793995359009218}, {\"index\": 33, \"train\": 0.06567118596285582, \"val\": 0.07736869607965842}, {\"index\": 34, \"train\": 0.06812843959778547, \"val\": 0.06971172513785186}, {\"index\": 35, \"train\": 0.06846861417094867, \"val\": 0.07445668796275708}, {\"index\": 36, \"train\": 0.06910717735687892, \"val\": 0.08969597389524872}, {\"index\": 37, \"train\": 0.07249497684339683, \"val\": 0.07227763094102826}, {\"index\": 38, \"train\": 0.06298490768919389, \"val\": 0.07308339654140313}, {\"index\": 39, \"train\": 0.06281429467101891, \"val\": 0.08563560651619574}, {\"index\": 40, \"train\": 0.06154077438016733, \"val\": 0.07787477460698466}, {\"index\": 41, \"train\": 0.0594995009402434, \"val\": 0.06895481533310549}, {\"index\": 42, \"train\": 0.057664175828297935, \"val\": 0.0628904117016915}, {\"index\": 43, \"train\": 0.05779723978290955, \"val\": 0.0622603439383157}, {\"index\": 44, \"train\": 0.05793151383598646, \"val\": 0.060486309521581495}, {\"index\": 45, \"train\": 0.054682655880848564, \"val\": 0.07103967782911948}, {\"index\": 46, \"train\": 0.062448794643084206, \"val\": 0.06847460907625241}, {\"index\": 47, \"train\": 0.05778281483799219, \"val\": 0.06036142329642273}, {\"index\": 48, \"train\": 0.055628820632894836, \"val\": 0.06369827192932091}, {\"index\": 49, \"train\": 0.05385812403013309, \"val\": 0.054783486516680566}, {\"index\": 50, \"train\": 0.052554525434970856, \"val\": 0.05786640094571609}, {\"index\": 51, \"train\": 0.053066366662581764, \"val\": 0.055353681103291884}, {\"index\": 52, \"train\": 0.0504270875826478, \"val\": 0.05294760088069813}, {\"index\": 53, \"train\": 0.04891673196107149, \"val\": 0.058569353420709916}, {\"index\": 54, \"train\": 0.055066220151881375, \"val\": 0.051244987783893185}, {\"index\": 55, \"train\": 0.048466392792761326, \"val\": 0.05125194910452356}, {\"index\": 56, \"train\": 0.047005826917787395, \"val\": 0.0527936403419719}, {\"index\": 57, \"train\": 0.0451598729317387, \"val\": 0.05595398240666664}, {\"index\": 58, \"train\": 0.04541373159736395, \"val\": 0.05908809559661205}, {\"index\": 59, \"train\": 0.04543444483230511, \"val\": 0.052747624555033526}, {\"index\": 60, \"train\": 0.04814126901328564, \"val\": 0.05942083648061035}, {\"index\": 61, \"train\": 0.047111675764123596, \"val\": 0.05302303484360756}, {\"index\": 62, \"train\": 0.04150276413808266, \"val\": 0.04774839523080219}, {\"index\": 63, \"train\": 0.04951918808122476, \"val\": 0.049232037690752704}, {\"index\": 64, \"train\": 0.043697782481710114, \"val\": 0.04890786416125829}, {\"index\": 65, \"train\": 0.0444096690043807, \"val\": 0.04866193044085607}, {\"index\": 66, \"train\": 0.0450656625131766, \"val\": 0.05267957682156487}, {\"index\": 67, \"train\": 0.053122478226820626, \"val\": 0.06139638790692617}, {\"index\": 68, \"train\": 0.04848647986849149, \"val\": 0.05615035190356947}, {\"index\": 69, \"train\": 0.044606502167880535, \"val\": 0.04803825603146968}, {\"index\": 70, \"train\": 0.04319923805693785, \"val\": 0.045999819218370784}, {\"index\": 71, \"train\": 0.03959274167815844, \"val\": 0.046405461607328446}, {\"index\": 72, \"train\": 0.04199325433000922, \"val\": 0.048445673742659255}, {\"index\": 73, \"train\": 0.04061445640400052, \"val\": 0.047514312847353586}, {\"index\": 74, \"train\": 0.04393227600182096, \"val\": 0.051403121968636424}, {\"index\": 75, \"train\": 0.043062408765157066, \"val\": 0.05312884185696021}, {\"index\": 76, \"train\": 0.04068659463276466, \"val\": 0.04747136284541166}, {\"index\": 77, \"train\": 0.047697885582844414, \"val\": 0.05301627400331199}, {\"index\": 78, \"train\": 0.04283904563635588, \"val\": 0.05856332755884742}, {\"index\": 79, \"train\": 0.04243206915756067, \"val\": 0.04822067122570135}, {\"index\": 80, \"train\": 0.04326676515241464, \"val\": 0.05958439432067314}, {\"index\": 81, \"train\": 0.041535278471807636, \"val\": 0.05338730323990738}, {\"index\": 82, \"train\": 0.03818905663987001, \"val\": 0.05541043765692006}, {\"index\": 83, \"train\": 0.0440849835673968, \"val\": 0.04295211716445111}, {\"index\": 84, \"train\": 0.04084561594451467, \"val\": 0.045977189255817966}, {\"index\": 85, \"train\": 0.04117483599111438, \"val\": 0.04763800027625014}, {\"index\": 86, \"train\": 0.03674796891088287, \"val\": 0.054906885427233106}, {\"index\": 87, \"train\": 0.03587271304180225, \"val\": 0.04228295332092481}, {\"index\": 88, \"train\": 0.03728061852355798, \"val\": 0.04454639865693429}, {\"index\": 89, \"train\": 0.04109875299036503, \"val\": 0.04530552355270764}, {\"index\": 90, \"train\": 0.040871986808876194, \"val\": 0.038185662548359554}, {\"index\": 91, \"train\": 0.039910230319947004, \"val\": 0.03718534379591734}, {\"index\": 92, \"train\": 0.04028661518047253, \"val\": 0.04666655485570017}, {\"index\": 93, \"train\": 0.040268359861026205, \"val\": 0.044969971795971675}, {\"index\": 94, \"train\": 0.03576821926981211, \"val\": 0.04989290946158925}, {\"index\": 95, \"train\": 0.038734203204512596, \"val\": 0.04523758253565541}, {\"index\": 96, \"train\": 0.0403730304290851, \"val\": 0.045653951112439646}, {\"index\": 97, \"train\": 0.037690280781437956, \"val\": 0.043101452289179346}, {\"index\": 98, \"train\": 0.03759051657592257, \"val\": 0.04419225524050287}, {\"index\": 99, \"train\": 0.0424720204124848, \"val\": 0.056611772499443894}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = './results/transferLrn01/'\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                         epochs=100, \n",
    "                         batch_size=256, \n",
    "                         flatten=True, \n",
    "                         logTrans=False, \n",
    "                         ssTrans=True, \n",
    "                         saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.830739</td>\n",
       "      <td>0.881189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.631611</td>\n",
       "      <td>0.805666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.508872</td>\n",
       "      <td>0.464607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.422534</td>\n",
       "      <td>0.399872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.360520</td>\n",
       "      <td>0.355670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.038734</td>\n",
       "      <td>0.045238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.040373</td>\n",
       "      <td>0.045654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.037690</td>\n",
       "      <td>0.043101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.037591</td>\n",
       "      <td>0.044192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.042472</td>\n",
       "      <td>0.056612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          train       val\n",
       "epoch                    \n",
       "0      0.830739  0.881189\n",
       "1      0.631611  0.805666\n",
       "2      0.508872  0.464607\n",
       "3      0.422534  0.399872\n",
       "4      0.360520  0.355670\n",
       "...         ...       ...\n",
       "95     0.038734  0.045238\n",
       "96     0.040373  0.045654\n",
       "97     0.037690  0.043101\n",
       "98     0.037591  0.044192\n",
       "99     0.042472  0.056612\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histDf = pd.DataFrame(history)\n",
    "histDf.index = histDf.index.rename('epoch')\n",
    "alt.Chart(histDf.reset_index()).transform_fold(\n",
    "        ['train', 'val'],\n",
    "        as_=['metric', 'value']\n",
    "    ).mark_line().encode(\n",
    "        alt.X('index:Q'),\n",
    "        alt.Y('value:Q', axis=alt.Axis(title='loss')),\n",
    "        color=alt.Color('metric:N'),\n",
    "        tooltip=['index:Q', 'value:Q']\n",
    "    ).properties(width=400, height=200)\n",
    "histDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"/home/ewhalen/projects/data/trusses/2D_Truss_v1.3/\"\n",
    "trainDataFiles = glob.glob(os.path.join(dataDir, 'design_7*.csv'))\n",
    "trainDataFiles.remove(doeFile)\n",
    "\n",
    "allResults = []\n",
    "for trainDataFile in trainDataFiles:\n",
    "    trainData = loadGhGraphs(trainDataFile, NUM_DV=5)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                     flatten=False, \n",
    "                     logTrans=False, \n",
    "                     ssTrans=True, \n",
    "                     saveDir=saveDir+f'{trainSize}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['set'] = 'Train'\n",
    "    trainRes['train size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['set'] = 'Test'\n",
    "    testRes['train size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
