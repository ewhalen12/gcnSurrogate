{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning tests\n",
    "Eamon Whalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from gcnSurrogate.models.feastnetSurrogateModel import FeaStNet\n",
    "from gcnSurrogate.models.pointRegressorSurrogateModel import PointRegressor\n",
    "from gcnSurrogate.readers.loadGhGraphs import loadGhGraphs\n",
    "from gcnSurrogate.visualization.altTrussViz import plotTruss, interactiveErrorPlot\n",
    "from gcnSurrogate.util.gcnSurrogateUtil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.574372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.021285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.035028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.072772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.701946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             maxes\n",
       "count  1000.000000\n",
       "mean      0.141483\n",
       "std       1.574372\n",
       "min       0.006253\n",
       "25%       0.021285\n",
       "50%       0.035028\n",
       "75%       0.072772\n",
       "max      48.701946"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = \"data/2D_Truss_v1.3/gh/\"\n",
    "testFile = os.path.join(dataDir, 'design_9_N_1000.csv')\n",
    "allGraphsUnfiltered = loadGhGraphs(testFile, NUM_DV=5)\n",
    "\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.042389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.030990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.020052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.031303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.055108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.140049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            maxes\n",
       "count  900.000000\n",
       "mean     0.042389\n",
       "std      0.030990\n",
       "min      0.006253\n",
       "25%      0.020052\n",
       "50%      0.031303\n",
       "75%      0.055108\n",
       "max      0.140049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in testData]\n",
    "source = pd.DataFrame(maxes, columns=['maxes'])\n",
    "maxDispCutoff = source.max().item()\n",
    "source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading design_7\n",
      "loading design_6\n",
      "loading design_8\n",
      "loading design_5\n",
      "loaded 3600 pretraining graphs\n"
     ]
    }
   ],
   "source": [
    "pretrainFiles = glob.glob(os.path.join(dataDir, '*1000.csv'))\n",
    "pretrainFiles.remove(testFile)\n",
    "\n",
    "allPretrainGraphs = []\n",
    "for pretrainFile in pretrainFiles:\n",
    "    designName = pretrainFile.split('/')[-1].split('_N')[0]\n",
    "    print(f'loading {designName}')\n",
    "    graphsUnfiltered = loadGhGraphs(pretrainFile, NUM_DV=5)\n",
    "    graphs = filterbyDisp(graphsUnfiltered, 0.9)\n",
    "    allPretrainGraphs.extend(graphs)\n",
    "\n",
    "print(f'loaded {len(allPretrainGraphs)} pretraining graphs')\n",
    "pretrainData, pretrainValData, _ = partitionGraphList(allPretrainGraphs, testSize=0.0, valSize=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   trainLoss: 7.1136e-01   valLoss:1.1171e+00  time: 7.39e+00\n",
      "epoch: 1   trainLoss: 3.7696e-01   valLoss:1.3573e+00  time: 7.10e+00\n",
      "epoch: 2   trainLoss: 2.4571e-01   valLoss:3.6096e-01  time: 7.14e+00\n",
      "epoch: 3   trainLoss: 2.0012e-01   valLoss:3.8529e-01  time: 7.11e+00\n",
      "epoch: 4   trainLoss: 1.4104e-01   valLoss:3.6666e-01  time: 7.15e+00\n",
      "epoch: 5   trainLoss: 1.1838e-01   valLoss:2.9482e-01  time: 7.16e+00\n",
      "epoch: 6   trainLoss: 1.1851e-01   valLoss:4.0545e-01  time: 7.12e+00\n",
      "epoch: 7   trainLoss: 1.2444e-01   valLoss:3.0170e-01  time: 7.14e+00\n",
      "epoch: 8   trainLoss: 1.0371e-01   valLoss:3.1434e-01  time: 7.12e+00\n",
      "epoch: 9   trainLoss: 9.7778e-02   valLoss:2.3398e-01  time: 7.17e+00\n",
      "epoch: 10   trainLoss: 9.3861e-02   valLoss:1.9597e-01  time: 7.17e+00\n",
      "epoch: 11   trainLoss: 7.7897e-02   valLoss:3.3217e-01  time: 7.15e+00\n",
      "epoch: 12   trainLoss: 8.2173e-02   valLoss:1.3892e-01  time: 7.21e+00\n",
      "epoch: 13   trainLoss: 7.9765e-02   valLoss:1.5254e-01  time: 7.14e+00\n",
      "epoch: 14   trainLoss: 7.6016e-02   valLoss:2.1298e-01  time: 7.17e+00\n",
      "epoch: 15   trainLoss: 7.4795e-02   valLoss:1.2674e-01  time: 7.13e+00\n",
      "epoch: 16   trainLoss: 6.9417e-02   valLoss:1.1909e-01  time: 7.17e+00\n",
      "epoch: 17   trainLoss: 6.8706e-02   valLoss:1.2300e-01  time: 7.13e+00\n",
      "epoch: 18   trainLoss: 7.9277e-02   valLoss:1.6593e-01  time: 7.15e+00\n",
      "epoch: 19   trainLoss: 7.9470e-02   valLoss:8.0981e-02  time: 7.17e+00\n",
      "epoch: 20   trainLoss: 6.4574e-02   valLoss:9.5908e-02  time: 7.31e+00\n",
      "epoch: 21   trainLoss: 6.1132e-02   valLoss:6.0153e-02  time: 7.15e+00\n",
      "epoch: 22   trainLoss: 5.6229e-02   valLoss:6.8198e-02  time: 7.13e+00\n",
      "epoch: 23   trainLoss: 5.8480e-02   valLoss:8.5135e-02  time: 7.16e+00\n",
      "epoch: 24   trainLoss: 5.7160e-02   valLoss:8.6133e-02  time: 7.13e+00\n",
      "epoch: 25   trainLoss: 4.7165e-02   valLoss:8.8502e-02  time: 7.18e+00\n",
      "epoch: 26   trainLoss: 4.9657e-02   valLoss:1.3672e-01  time: 7.29e+00\n",
      "epoch: 27   trainLoss: 4.7745e-02   valLoss:7.4115e-02  time: 7.12e+00\n",
      "epoch: 28   trainLoss: 5.3792e-02   valLoss:1.5830e-01  time: 7.19e+00\n",
      "epoch: 29   trainLoss: 6.2364e-02   valLoss:1.1174e-01  time: 7.11e+00\n",
      "epoch: 30   trainLoss: 6.0098e-02   valLoss:6.8911e-02  time: 7.14e+00\n",
      "epoch: 31   trainLoss: 6.2323e-02   valLoss:7.9940e-02  time: 7.13e+00\n",
      "epoch: 32   trainLoss: 4.3260e-02   valLoss:5.4779e-02  time: 7.18e+00\n",
      "epoch: 33   trainLoss: 4.8917e-02   valLoss:6.1972e-02  time: 7.21e+00\n",
      "epoch: 34   trainLoss: 5.9566e-02   valLoss:2.2765e-01  time: 7.20e+00\n",
      "epoch: 35   trainLoss: 7.0406e-02   valLoss:1.5120e-01  time: 7.14e+00\n",
      "epoch: 36   trainLoss: 7.0058e-02   valLoss:8.6020e-02  time: 7.19e+00\n",
      "epoch: 37   trainLoss: 6.1671e-02   valLoss:5.6799e-02  time: 7.19e+00\n",
      "epoch: 38   trainLoss: 5.1119e-02   valLoss:8.6860e-02  time: 7.13e+00\n",
      "epoch: 39   trainLoss: 5.4370e-02   valLoss:1.1750e-01  time: 7.12e+00\n",
      "epoch: 40   trainLoss: 5.2881e-02   valLoss:6.6260e-02  time: 7.17e+00\n",
      "epoch: 41   trainLoss: 5.3177e-02   valLoss:5.7930e-02  time: 7.12e+00\n",
      "epoch: 42   trainLoss: 4.9965e-02   valLoss:9.8689e-02  time: 7.13e+00\n",
      "epoch: 43   trainLoss: 5.2395e-02   valLoss:5.9996e-02  time: 7.27e+00\n",
      "epoch: 44   trainLoss: 4.6553e-02   valLoss:1.1866e-01  time: 7.14e+00\n",
      "epoch: 45   trainLoss: 4.5587e-02   valLoss:6.5960e-02  time: 7.14e+00\n",
      "epoch: 46   trainLoss: 5.4933e-02   valLoss:1.0037e-01  time: 7.22e+00\n",
      "epoch: 47   trainLoss: 4.5870e-02   valLoss:8.8792e-02  time: 7.22e+00\n",
      "epoch: 48   trainLoss: 6.1454e-02   valLoss:8.7875e-02  time: 7.13e+00\n",
      "epoch: 49   trainLoss: 5.0127e-02   valLoss:8.6222e-02  time: 7.23e+00\n",
      "epoch: 50   trainLoss: 4.9795e-02   valLoss:5.8413e-02  time: 7.20e+00\n",
      "epoch: 51   trainLoss: 4.9726e-02   valLoss:9.7566e-02  time: 7.14e+00\n",
      "epoch: 52   trainLoss: 5.4871e-02   valLoss:1.2335e-01  time: 7.24e+00\n",
      "epoch: 53   trainLoss: 4.4421e-02   valLoss:6.6832e-02  time: 7.30e+00\n",
      "epoch: 54   trainLoss: 5.3089e-02   valLoss:1.1419e-01  time: 7.20e+00\n",
      "epoch: 55   trainLoss: 4.5577e-02   valLoss:5.7871e-02  time: 7.19e+00\n",
      "epoch: 56   trainLoss: 3.6977e-02   valLoss:5.9696e-02  time: 7.20e+00\n",
      "epoch: 57   trainLoss: 4.3740e-02   valLoss:5.3035e-02  time: 7.17e+00\n",
      "epoch: 58   trainLoss: 4.0081e-02   valLoss:5.1036e-02  time: 7.22e+00\n",
      "epoch: 59   trainLoss: 4.0871e-02   valLoss:5.0173e-02  time: 7.16e+00\n",
      "epoch: 60   trainLoss: 3.6709e-02   valLoss:5.3345e-02  time: 7.32e+00\n",
      "epoch: 61   trainLoss: 5.0687e-02   valLoss:1.3674e-01  time: 7.22e+00\n",
      "epoch: 62   trainLoss: 6.7418e-02   valLoss:1.0116e-01  time: 7.17e+00\n",
      "epoch: 63   trainLoss: 4.8065e-02   valLoss:7.6099e-02  time: 7.32e+00\n",
      "epoch: 64   trainLoss: 4.6805e-02   valLoss:6.2137e-02  time: 7.20e+00\n",
      "epoch: 65   trainLoss: 4.7033e-02   valLoss:5.1037e-02  time: 7.31e+00\n",
      "epoch: 66   trainLoss: 5.2160e-02   valLoss:4.3289e-02  time: 7.23e+00\n",
      "epoch: 67   trainLoss: 4.4731e-02   valLoss:5.4246e-02  time: 7.14e+00\n",
      "epoch: 68   trainLoss: 4.4486e-02   valLoss:9.3577e-02  time: 7.20e+00\n",
      "epoch: 69   trainLoss: 4.3011e-02   valLoss:5.5178e-02  time: 7.32e+00\n",
      "epoch: 70   trainLoss: 4.1234e-02   valLoss:6.9603e-02  time: 7.24e+00\n",
      "epoch: 71   trainLoss: 4.5091e-02   valLoss:9.3763e-02  time: 7.21e+00\n",
      "epoch: 72   trainLoss: 4.4744e-02   valLoss:7.8124e-02  time: 7.15e+00\n",
      "epoch: 73   trainLoss: 3.8491e-02   valLoss:6.9235e-02  time: 7.17e+00\n",
      "epoch: 74   trainLoss: 5.1924e-02   valLoss:7.5008e-02  time: 7.17e+00\n",
      "epoch: 75   trainLoss: 4.3034e-02   valLoss:5.9562e-02  time: 7.26e+00\n",
      "epoch: 76   trainLoss: 3.6987e-02   valLoss:4.6441e-02  time: 7.16e+00\n",
      "epoch: 77   trainLoss: 4.5864e-02   valLoss:6.7687e-02  time: 7.15e+00\n",
      "epoch: 78   trainLoss: 4.8791e-02   valLoss:7.7129e-02  time: 7.21e+00\n",
      "epoch: 79   trainLoss: 5.3273e-02   valLoss:9.4601e-02  time: 7.14e+00\n",
      "epoch: 80   trainLoss: 4.0523e-02   valLoss:4.3781e-02  time: 7.19e+00\n",
      "epoch: 81   trainLoss: 4.2664e-02   valLoss:6.3278e-02  time: 7.14e+00\n",
      "epoch: 82   trainLoss: 3.8323e-02   valLoss:8.6485e-02  time: 7.23e+00\n",
      "epoch: 83   trainLoss: 4.7498e-02   valLoss:8.5747e-02  time: 7.24e+00\n",
      "epoch: 84   trainLoss: 4.7800e-02   valLoss:9.1557e-02  time: 7.18e+00\n",
      "epoch: 85   trainLoss: 5.5092e-02   valLoss:5.9891e-02  time: 7.23e+00\n",
      "epoch: 86   trainLoss: 4.6600e-02   valLoss:4.9759e-02  time: 7.17e+00\n",
      "epoch: 87   trainLoss: 4.2230e-02   valLoss:6.9110e-02  time: 7.20e+00\n",
      "epoch: 88   trainLoss: 4.4555e-02   valLoss:5.8560e-02  time: 7.22e+00\n",
      "epoch: 89   trainLoss: 3.9533e-02   valLoss:8.3416e-02  time: 7.25e+00\n",
      "epoch: 90   trainLoss: 4.5352e-02   valLoss:6.9778e-02  time: 7.22e+00\n",
      "epoch: 91   trainLoss: 3.8264e-02   valLoss:4.2528e-02  time: 7.20e+00\n",
      "epoch: 92   trainLoss: 3.5258e-02   valLoss:5.4601e-02  time: 7.18e+00\n",
      "epoch: 93   trainLoss: 5.1837e-02   valLoss:7.0802e-02  time: 7.18e+00\n",
      "epoch: 94   trainLoss: 4.1110e-02   valLoss:6.5923e-02  time: 7.15e+00\n",
      "epoch: 95   trainLoss: 3.7726e-02   valLoss:5.4897e-02  time: 7.24e+00\n",
      "epoch: 96   trainLoss: 4.1963e-02   valLoss:8.2368e-02  time: 7.17e+00\n",
      "epoch: 97   trainLoss: 4.0239e-02   valLoss:5.4930e-02  time: 7.08e+00\n",
      "epoch: 98   trainLoss: 4.1001e-02   valLoss:4.2566e-02  time: 7.14e+00\n",
      "epoch: 99   trainLoss: 4.1423e-02   valLoss:5.0337e-02  time: 7.16e+00\n",
      "loading checkpoint 91\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-17ff8da233ae4411aac21779b1586d11\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-17ff8da233ae4411aac21779b1586d11\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-17ff8da233ae4411aac21779b1586d11\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-460b332c36448c5bad09cb73cecd2e28\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"set\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"epoch\"}, {\"type\": \"quantitative\", \"field\": \"value\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"loss\"}, \"field\": \"value\"}}, \"height\": 200, \"transform\": [{\"fold\": [\"train\", \"val\"], \"as\": [\"set\", \"value\"]}], \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-460b332c36448c5bad09cb73cecd2e28\": [{\"train\": 0.7113611499468485, \"val\": 1.1171167027757123, \"epoch\": 0}, {\"train\": 0.37695642312367755, \"val\": 1.3573244356515783, \"epoch\": 1}, {\"train\": 0.24570759510000548, \"val\": 0.36096124591961226, \"epoch\": 2}, {\"train\": 0.200122548888127, \"val\": 0.38528892531887526, \"epoch\": 3}, {\"train\": 0.14104289561510086, \"val\": 0.36665567020299256, \"epoch\": 4}, {\"train\": 0.11838142511745293, \"val\": 0.29482458634991887, \"epoch\": 5}, {\"train\": 0.11850867482523124, \"val\": 0.40545134301452584, \"epoch\": 6}, {\"train\": 0.12443822622299194, \"val\": 0.30170087461808215, \"epoch\": 7}, {\"train\": 0.10370677585403125, \"val\": 0.3143391632975114, \"epoch\": 8}, {\"train\": 0.09777773978809516, \"val\": 0.23397883046418427, \"epoch\": 9}, {\"train\": 0.09386125548432271, \"val\": 0.19597424053656007, \"epoch\": 10}, {\"train\": 0.07789699981609981, \"val\": 0.3321699372975639, \"epoch\": 11}, {\"train\": 0.08217296501000722, \"val\": 0.13892109064488775, \"epoch\": 12}, {\"train\": 0.0797646048789223, \"val\": 0.15253851593265103, \"epoch\": 13}, {\"train\": 0.07601640342424314, \"val\": 0.21298324306392008, \"epoch\": 14}, {\"train\": 0.07479493971914053, \"val\": 0.12673869650655736, \"epoch\": 15}, {\"train\": 0.069417386315763, \"val\": 0.1190850353924799, \"epoch\": 16}, {\"train\": 0.06870636530220509, \"val\": 0.12299918241246983, \"epoch\": 17}, {\"train\": 0.07927713791529338, \"val\": 0.16593363283684961, \"epoch\": 18}, {\"train\": 0.07946994869659345, \"val\": 0.08098102450172451, \"epoch\": 19}, {\"train\": 0.06457425250361364, \"val\": 0.09590807562624967, \"epoch\": 20}, {\"train\": 0.06113218826552232, \"val\": 0.06015337899141876, \"epoch\": 21}, {\"train\": 0.05622915054361025, \"val\": 0.06819837379691847, \"epoch\": 22}, {\"train\": 0.05848035806169113, \"val\": 0.08513523931615055, \"epoch\": 23}, {\"train\": 0.05715986372282108, \"val\": 0.08613296464253735, \"epoch\": 24}, {\"train\": 0.0471648375193278, \"val\": 0.08850220959607719, \"epoch\": 25}, {\"train\": 0.04965693006912867, \"val\": 0.1367236039890149, \"epoch\": 26}, {\"train\": 0.04774466622620821, \"val\": 0.07411542979685624, \"epoch\": 27}, {\"train\": 0.05379159158716599, \"val\": 0.1582995092375549, \"epoch\": 28}, {\"train\": 0.062363874477644764, \"val\": 0.11173834695450582, \"epoch\": 29}, {\"train\": 0.06009793374687433, \"val\": 0.06891131966948268, \"epoch\": 30}, {\"train\": 0.06232278700917959, \"val\": 0.07994007985452535, \"epoch\": 31}, {\"train\": 0.04326045513153076, \"val\": 0.05477875199403907, \"epoch\": 32}, {\"train\": 0.04891682912906011, \"val\": 0.06197218343710389, \"epoch\": 33}, {\"train\": 0.059565904860695205, \"val\": 0.2276460033617224, \"epoch\": 34}, {\"train\": 0.07040633137027423, \"val\": 0.15119887002812768, \"epoch\": 35}, {\"train\": 0.07005840974549453, \"val\": 0.08602024783828745, \"epoch\": 36}, {\"train\": 0.061670998421808086, \"val\": 0.056799274996673275, \"epoch\": 37}, {\"train\": 0.05111946786443392, \"val\": 0.08685984601680603, \"epoch\": 38}, {\"train\": 0.05436995904892683, \"val\": 0.11749511515846718, \"epoch\": 39}, {\"train\": 0.05288073824097713, \"val\": 0.06626031707263016, \"epoch\": 40}, {\"train\": 0.05317700964709123, \"val\": 0.05792977129911176, \"epoch\": 41}, {\"train\": 0.04996482407053312, \"val\": 0.09868925692065171, \"epoch\": 42}, {\"train\": 0.05239506841947635, \"val\": 0.059995965893311354, \"epoch\": 43}, {\"train\": 0.046553321493168674, \"val\": 0.11865935218261761, \"epoch\": 44}, {\"train\": 0.045587383676320314, \"val\": 0.06596036507657523, \"epoch\": 45}, {\"train\": 0.054932592126230397, \"val\": 0.10037091562515814, \"epoch\": 46}, {\"train\": 0.04587030690163374, \"val\": 0.08879172400640392, \"epoch\": 47}, {\"train\": 0.06145413654545943, \"val\": 0.08787466361687553, \"epoch\": 48}, {\"train\": 0.050126808074613415, \"val\": 0.08622209095995424, \"epoch\": 49}, {\"train\": 0.049795423013468586, \"val\": 0.058413114049471915, \"epoch\": 50}, {\"train\": 0.04972606742133697, \"val\": 0.097566332746323, \"epoch\": 51}, {\"train\": 0.05487061974902948, \"val\": 0.12334551195338092, \"epoch\": 52}, {\"train\": 0.044421062183876835, \"val\": 0.06683158084094801, \"epoch\": 53}, {\"train\": 0.05308917568375667, \"val\": 0.11418906649492716, \"epoch\": 54}, {\"train\": 0.04557729233056307, \"val\": 0.05787092719951437, \"epoch\": 55}, {\"train\": 0.03697667752082149, \"val\": 0.05969615033827722, \"epoch\": 56}, {\"train\": 0.04373999638482928, \"val\": 0.053034819503038844, \"epoch\": 57}, {\"train\": 0.04008084783951441, \"val\": 0.05103555042210414, \"epoch\": 58}, {\"train\": 0.04087125727285942, \"val\": 0.05017258751214723, \"epoch\": 59}, {\"train\": 0.03670926997438073, \"val\": 0.05334530101282763, \"epoch\": 60}, {\"train\": 0.05068658757954836, \"val\": 0.13674139376531388, \"epoch\": 61}, {\"train\": 0.06741828533510368, \"val\": 0.10116235562427728, \"epoch\": 62}, {\"train\": 0.04806515077749888, \"val\": 0.07609939512244805, \"epoch\": 63}, {\"train\": 0.04680542089045048, \"val\": 0.0621373990393261, \"epoch\": 64}, {\"train\": 0.04703309324880441, \"val\": 0.05103721153551575, \"epoch\": 65}, {\"train\": 0.05216045336176952, \"val\": 0.043288717481105035, \"epoch\": 66}, {\"train\": 0.04473144421353936, \"val\": 0.05424638113709753, \"epoch\": 67}, {\"train\": 0.04448611568659544, \"val\": 0.09357729925821466, \"epoch\": 68}, {\"train\": 0.04301131796091795, \"val\": 0.055178317747561746, \"epoch\": 69}, {\"train\": 0.04123432297880451, \"val\": 0.06960267540524472, \"epoch\": 70}, {\"train\": 0.04509106309463581, \"val\": 0.09376308849486695, \"epoch\": 71}, {\"train\": 0.04474391151840488, \"val\": 0.07812397368683445, \"epoch\": 72}, {\"train\": 0.03849145211279392, \"val\": 0.06923479192493552, \"epoch\": 73}, {\"train\": 0.051923514964679875, \"val\": 0.07500805275243086, \"epoch\": 74}, {\"train\": 0.043034245260059834, \"val\": 0.05956167614591929, \"epoch\": 75}, {\"train\": 0.03698745012904207, \"val\": 0.04644088509225252, \"epoch\": 76}, {\"train\": 0.04586422831440965, \"val\": 0.06768704409780049, \"epoch\": 77}, {\"train\": 0.048791409780581795, \"val\": 0.07712859196687566, \"epoch\": 78}, {\"train\": 0.05327348845700423, \"val\": 0.09460148643853178, \"epoch\": 79}, {\"train\": 0.040523421640197434, \"val\": 0.04378053190022776, \"epoch\": 80}, {\"train\": 0.04266386292874813, \"val\": 0.06327763056874068, \"epoch\": 81}, {\"train\": 0.03832315638040503, \"val\": 0.0864847253024992, \"epoch\": 82}, {\"train\": 0.047498037262509264, \"val\": 0.08574677737541841, \"epoch\": 83}, {\"train\": 0.047800435995062195, \"val\": 0.0915566548597309, \"epoch\": 84}, {\"train\": 0.055092305255432926, \"val\": 0.05989148214036875, \"epoch\": 85}, {\"train\": 0.046599763755997024, \"val\": 0.04975926130255512, \"epoch\": 86}, {\"train\": 0.04222965566441417, \"val\": 0.06910978030279727, \"epoch\": 87}, {\"train\": 0.04455460049211979, \"val\": 0.058559958190933147, \"epoch\": 88}, {\"train\": 0.03953265305608511, \"val\": 0.08341642121387714, \"epoch\": 89}, {\"train\": 0.04535247152671218, \"val\": 0.06977766951227009, \"epoch\": 90}, {\"train\": 0.03826360668366154, \"val\": 0.042527694417879465, \"epoch\": 91}, {\"train\": 0.03525804542005062, \"val\": 0.054600526219345975, \"epoch\": 92}, {\"train\": 0.05183678213506937, \"val\": 0.07080158212144548, \"epoch\": 93}, {\"train\": 0.04110992125545939, \"val\": 0.06592300197538592, \"epoch\": 94}, {\"train\": 0.037725580080101885, \"val\": 0.05489662433952886, \"epoch\": 95}, {\"train\": 0.04196264175698161, \"val\": 0.08236834594496974, \"epoch\": 96}, {\"train\": 0.04023941420018673, \"val\": 0.0549301621906192, \"epoch\": 97}, {\"train\": 0.041000870211670794, \"val\": 0.042566487227601035, \"epoch\": 98}, {\"train\": 0.041422766633331776, \"val\": 0.050337368178750494, \"epoch\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = 'results/transferLrn_des9_01/'\n",
    "epochs = 100\n",
    "ptrGcn = FeaStNet()\n",
    "history = ptrGcn.trainModel(pretrainData, pretrainValData, \n",
    "                            epochs=epochs,\n",
    "                            saveDir=saveDir+f'preTrain/gcn/')\n",
    "\n",
    "ptrGcnCheckptFile = ptrGcn.checkptFile\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>0.947526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.181216</td>\n",
       "      <td>0.764785</td>\n",
       "      <td>0.92666</td>\n",
       "      <td>0.79292</td>\n",
       "      <td>0.482823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mse       mae       mre    peakR2  maxAggR2  meanAggR2  minAggR2\n",
       "train  0.000013  0.002384  0.097664  0.947526       NaN        NaN       NaN\n",
       "test   0.000071  0.005430  0.181216  0.764785   0.92666    0.79292  0.482823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRes = ptrGcn.testModel(pretrainData)\n",
    "testRes = ptrGcn.testModel(testData) # unseen topology\n",
    "pd.DataFrame([trainRes, testRes], index=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train set of size 48\n",
      "epoch: 0   trainLoss: 1.0095e+00   valLoss:9.8591e-01  time: 4.83e-01\n",
      "epoch: 1   trainLoss: 7.6828e-01   valLoss:9.7141e-01  time: 4.54e-01\n",
      "epoch: 2   trainLoss: 6.0922e-01   valLoss:9.5443e-01  time: 4.48e-01\n",
      "epoch: 3   trainLoss: 4.6051e-01   valLoss:9.3796e-01  time: 4.48e-01\n",
      "epoch: 4   trainLoss: 3.5488e-01   valLoss:9.1689e-01  time: 4.49e-01\n",
      "epoch: 5   trainLoss: 2.7095e-01   valLoss:8.9725e-01  time: 4.48e-01\n",
      "epoch: 6   trainLoss: 2.3347e-01   valLoss:8.7435e-01  time: 4.40e-01\n",
      "epoch: 7   trainLoss: 2.1106e-01   valLoss:8.3770e-01  time: 4.46e-01\n",
      "epoch: 8   trainLoss: 1.9763e-01   valLoss:8.0761e-01  time: 4.44e-01\n",
      "epoch: 9   trainLoss: 1.7392e-01   valLoss:7.9918e-01  time: 4.50e-01\n",
      "epoch: 10   trainLoss: 1.5147e-01   valLoss:8.2972e-01  time: 4.52e-01\n",
      "epoch: 11   trainLoss: 1.3430e-01   valLoss:9.2468e-01  time: 4.45e-01\n",
      "epoch: 12   trainLoss: 1.1998e-01   valLoss:1.0653e+00  time: 4.42e-01\n",
      "epoch: 13   trainLoss: 1.0428e-01   valLoss:1.1856e+00  time: 4.44e-01\n",
      "epoch: 14   trainLoss: 9.3196e-02   valLoss:1.2527e+00  time: 4.46e-01\n",
      "epoch: 15   trainLoss: 8.3851e-02   valLoss:1.2706e+00  time: 4.46e-01\n",
      "epoch: 16   trainLoss: 7.7617e-02   valLoss:1.1790e+00  time: 4.41e-01\n",
      "epoch: 17   trainLoss: 7.0522e-02   valLoss:1.0431e+00  time: 4.42e-01\n",
      "epoch: 18   trainLoss: 6.3816e-02   valLoss:8.9502e-01  time: 4.53e-01\n",
      "epoch: 19   trainLoss: 5.7527e-02   valLoss:7.6665e-01  time: 4.42e-01\n",
      "epoch: 20   trainLoss: 5.0848e-02   valLoss:6.6346e-01  time: 4.43e-01\n",
      "epoch: 21   trainLoss: 4.4685e-02   valLoss:5.5448e-01  time: 4.41e-01\n",
      "epoch: 22   trainLoss: 3.9140e-02   valLoss:4.6845e-01  time: 4.54e-01\n",
      "epoch: 23   trainLoss: 3.4273e-02   valLoss:4.0307e-01  time: 4.43e-01\n",
      "epoch: 24   trainLoss: 3.0438e-02   valLoss:3.5454e-01  time: 4.45e-01\n",
      "epoch: 25   trainLoss: 2.6668e-02   valLoss:3.2183e-01  time: 4.49e-01\n",
      "epoch: 26   trainLoss: 2.4365e-02   valLoss:2.9197e-01  time: 4.57e-01\n",
      "epoch: 27   trainLoss: 2.2257e-02   valLoss:2.7648e-01  time: 4.38e-01\n",
      "epoch: 28   trainLoss: 2.1528e-02   valLoss:2.3877e-01  time: 4.42e-01\n",
      "epoch: 29   trainLoss: 2.3322e-02   valLoss:2.4336e-01  time: 4.41e-01\n",
      "epoch: 30   trainLoss: 2.4053e-02   valLoss:2.0839e-01  time: 4.40e-01\n",
      "epoch: 31   trainLoss: 1.8376e-02   valLoss:2.1191e-01  time: 4.43e-01\n",
      "epoch: 32   trainLoss: 1.6906e-02   valLoss:2.1827e-01  time: 4.41e-01\n",
      "epoch: 33   trainLoss: 1.5464e-02   valLoss:1.9839e-01  time: 4.53e-01\n",
      "epoch: 34   trainLoss: 1.4165e-02   valLoss:2.0001e-01  time: 4.50e-01\n",
      "epoch: 35   trainLoss: 1.2289e-02   valLoss:2.1840e-01  time: 4.40e-01\n",
      "epoch: 36   trainLoss: 1.2330e-02   valLoss:2.0400e-01  time: 4.39e-01\n",
      "epoch: 37   trainLoss: 1.0827e-02   valLoss:2.0900e-01  time: 4.42e-01\n",
      "epoch: 38   trainLoss: 1.0275e-02   valLoss:2.1365e-01  time: 4.40e-01\n",
      "epoch: 39   trainLoss: 9.6293e-03   valLoss:1.9232e-01  time: 4.40e-01\n",
      "epoch: 40   trainLoss: 8.4822e-03   valLoss:1.8696e-01  time: 4.39e-01\n",
      "epoch: 41   trainLoss: 8.3657e-03   valLoss:1.9783e-01  time: 4.39e-01\n",
      "epoch: 42   trainLoss: 7.4640e-03   valLoss:1.9486e-01  time: 4.38e-01\n",
      "epoch: 43   trainLoss: 6.7876e-03   valLoss:1.8899e-01  time: 4.39e-01\n",
      "epoch: 44   trainLoss: 6.6894e-03   valLoss:1.9138e-01  time: 4.40e-01\n",
      "epoch: 45   trainLoss: 6.2553e-03   valLoss:1.8010e-01  time: 4.41e-01\n",
      "epoch: 46   trainLoss: 5.5796e-03   valLoss:1.7172e-01  time: 4.39e-01\n",
      "epoch: 47   trainLoss: 5.4747e-03   valLoss:1.8559e-01  time: 4.38e-01\n",
      "epoch: 48   trainLoss: 5.0818e-03   valLoss:1.8765e-01  time: 4.38e-01\n",
      "epoch: 49   trainLoss: 4.7274e-03   valLoss:1.7769e-01  time: 4.34e-01\n",
      "epoch: 50   trainLoss: 4.4908e-03   valLoss:1.7867e-01  time: 4.32e-01\n",
      "epoch: 51   trainLoss: 4.4735e-03   valLoss:1.6706e-01  time: 4.40e-01\n",
      "epoch: 52   trainLoss: 4.1184e-03   valLoss:1.6618e-01  time: 4.36e-01\n",
      "epoch: 53   trainLoss: 3.9166e-03   valLoss:1.6268e-01  time: 4.37e-01\n",
      "epoch: 54   trainLoss: 3.8059e-03   valLoss:1.5819e-01  time: 4.42e-01\n",
      "epoch: 55   trainLoss: 3.9249e-03   valLoss:1.6238e-01  time: 4.40e-01\n",
      "epoch: 56   trainLoss: 4.0942e-03   valLoss:1.7066e-01  time: 4.39e-01\n",
      "epoch: 57   trainLoss: 4.4593e-03   valLoss:1.6817e-01  time: 4.36e-01\n",
      "epoch: 58   trainLoss: 5.2369e-03   valLoss:1.7854e-01  time: 4.38e-01\n",
      "epoch: 59   trainLoss: 6.4192e-03   valLoss:1.6046e-01  time: 4.42e-01\n",
      "epoch: 60   trainLoss: 7.4710e-03   valLoss:1.6479e-01  time: 4.35e-01\n",
      "epoch: 61   trainLoss: 5.8795e-03   valLoss:1.5472e-01  time: 4.32e-01\n",
      "epoch: 62   trainLoss: 3.3494e-03   valLoss:1.5796e-01  time: 4.42e-01\n",
      "epoch: 63   trainLoss: 3.8343e-03   valLoss:1.5717e-01  time: 4.39e-01\n",
      "epoch: 64   trainLoss: 4.8491e-03   valLoss:1.5863e-01  time: 4.39e-01\n",
      "epoch: 65   trainLoss: 3.0636e-03   valLoss:1.5961e-01  time: 4.43e-01\n",
      "epoch: 66   trainLoss: 2.3006e-03   valLoss:1.5406e-01  time: 4.37e-01\n",
      "epoch: 67   trainLoss: 3.7225e-03   valLoss:1.6094e-01  time: 4.52e-01\n",
      "epoch: 68   trainLoss: 3.1788e-03   valLoss:1.5297e-01  time: 4.36e-01\n",
      "epoch: 69   trainLoss: 2.0272e-03   valLoss:1.5416e-01  time: 4.37e-01\n",
      "epoch: 70   trainLoss: 2.6822e-03   valLoss:1.4767e-01  time: 4.34e-01\n",
      "epoch: 71   trainLoss: 2.9663e-03   valLoss:1.5499e-01  time: 4.41e-01\n",
      "epoch: 72   trainLoss: 1.9785e-03   valLoss:1.4585e-01  time: 4.43e-01\n",
      "epoch: 73   trainLoss: 2.2559e-03   valLoss:1.6616e-01  time: 4.36e-01\n",
      "epoch: 74   trainLoss: 3.3629e-03   valLoss:1.4165e-01  time: 4.38e-01\n",
      "epoch: 75   trainLoss: 4.4370e-03   valLoss:1.6915e-01  time: 4.49e-01\n",
      "epoch: 76   trainLoss: 7.1122e-03   valLoss:1.4247e-01  time: 4.56e-01\n",
      "epoch: 77   trainLoss: 1.1813e-02   valLoss:1.8124e-01  time: 4.55e-01\n",
      "epoch: 78   trainLoss: 9.5793e-03   valLoss:1.3770e-01  time: 4.53e-01\n",
      "epoch: 79   trainLoss: 4.3575e-03   valLoss:1.4220e-01  time: 4.46e-01\n",
      "epoch: 80   trainLoss: 4.4805e-03   valLoss:1.6009e-01  time: 4.47e-01\n",
      "epoch: 81   trainLoss: 5.8266e-03   valLoss:1.3072e-01  time: 4.44e-01\n",
      "epoch: 82   trainLoss: 3.1317e-03   valLoss:1.4477e-01  time: 4.40e-01\n",
      "epoch: 83   trainLoss: 3.9407e-03   valLoss:1.5592e-01  time: 4.39e-01\n",
      "epoch: 84   trainLoss: 3.4973e-03   valLoss:1.2700e-01  time: 4.39e-01\n",
      "epoch: 85   trainLoss: 2.7887e-03   valLoss:1.2757e-01  time: 4.41e-01\n",
      "epoch: 86   trainLoss: 3.0064e-03   valLoss:1.4088e-01  time: 4.43e-01\n",
      "epoch: 87   trainLoss: 2.2997e-03   valLoss:1.3021e-01  time: 4.37e-01\n",
      "epoch: 88   trainLoss: 2.5595e-03   valLoss:1.2439e-01  time: 4.39e-01\n",
      "epoch: 89   trainLoss: 1.6392e-03   valLoss:1.3194e-01  time: 4.45e-01\n",
      "epoch: 90   trainLoss: 2.2915e-03   valLoss:1.2730e-01  time: 4.37e-01\n",
      "epoch: 91   trainLoss: 1.6608e-03   valLoss:1.2938e-01  time: 4.39e-01\n",
      "epoch: 92   trainLoss: 1.5713e-03   valLoss:1.3216e-01  time: 4.43e-01\n",
      "epoch: 93   trainLoss: 1.6852e-03   valLoss:1.2313e-01  time: 4.39e-01\n",
      "epoch: 94   trainLoss: 1.3819e-03   valLoss:1.3535e-01  time: 4.46e-01\n",
      "epoch: 95   trainLoss: 1.4596e-03   valLoss:1.3448e-01  time: 4.38e-01\n",
      "epoch: 96   trainLoss: 1.2210e-03   valLoss:1.2371e-01  time: 4.39e-01\n",
      "epoch: 97   trainLoss: 1.3077e-03   valLoss:1.2809e-01  time: 4.38e-01\n",
      "epoch: 98   trainLoss: 1.6574e-03   valLoss:1.3594e-01  time: 4.41e-01\n",
      "epoch: 99   trainLoss: 1.9233e-03   valLoss:1.3047e-01  time: 4.52e-01\n",
      "loading checkpoint 93\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.1470e+00   valLoss:1.3720e+00  time: 4.70e-01\n",
      "epoch: 1   trainLoss: 9.7291e-01   valLoss:1.9112e+00  time: 4.66e-01\n",
      "epoch: 2   trainLoss: 6.9353e-01   valLoss:2.7758e+00  time: 4.59e-01\n",
      "epoch: 3   trainLoss: 4.8860e-01   valLoss:1.4272e+00  time: 4.47e-01\n",
      "epoch: 4   trainLoss: 4.4724e-01   valLoss:2.1091e+00  time: 4.40e-01\n",
      "epoch: 5   trainLoss: 3.4668e-01   valLoss:1.4415e+00  time: 4.54e-01\n",
      "epoch: 6   trainLoss: 2.6021e-01   valLoss:9.3928e-01  time: 4.52e-01\n",
      "epoch: 7   trainLoss: 2.3169e-01   valLoss:1.2561e+00  time: 4.48e-01\n",
      "epoch: 8   trainLoss: 1.8471e-01   valLoss:1.5833e+00  time: 4.68e-01\n",
      "epoch: 9   trainLoss: 1.6144e-01   valLoss:9.0379e-01  time: 4.54e-01\n",
      "epoch: 10   trainLoss: 1.4915e-01   valLoss:4.5874e-01  time: 4.57e-01\n",
      "epoch: 11   trainLoss: 1.3421e-01   valLoss:4.0937e-01  time: 4.51e-01\n",
      "epoch: 12   trainLoss: 1.2557e-01   valLoss:3.0429e-01  time: 4.49e-01\n",
      "epoch: 13   trainLoss: 1.0998e-01   valLoss:2.2317e-01  time: 4.48e-01\n",
      "epoch: 14   trainLoss: 1.0586e-01   valLoss:1.9191e-01  time: 4.49e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15   trainLoss: 9.0670e-02   valLoss:1.7915e-01  time: 4.39e-01\n",
      "epoch: 16   trainLoss: 8.4578e-02   valLoss:1.6388e-01  time: 4.44e-01\n",
      "epoch: 17   trainLoss: 7.4507e-02   valLoss:1.8655e-01  time: 5.81e-01\n",
      "epoch: 18   trainLoss: 7.0147e-02   valLoss:1.4120e-01  time: 4.36e-01\n",
      "epoch: 19   trainLoss: 6.5836e-02   valLoss:1.0592e-01  time: 4.51e-01\n",
      "epoch: 20   trainLoss: 5.9981e-02   valLoss:9.1253e-02  time: 4.59e-01\n",
      "epoch: 21   trainLoss: 5.5810e-02   valLoss:8.2557e-02  time: 4.47e-01\n",
      "epoch: 22   trainLoss: 5.0816e-02   valLoss:1.1018e-01  time: 4.48e-01\n",
      "epoch: 23   trainLoss: 4.8908e-02   valLoss:9.8116e-02  time: 4.37e-01\n",
      "epoch: 24   trainLoss: 4.4775e-02   valLoss:7.4243e-02  time: 4.52e-01\n",
      "epoch: 25   trainLoss: 4.2807e-02   valLoss:6.2059e-02  time: 4.47e-01\n",
      "epoch: 26   trainLoss: 3.9557e-02   valLoss:6.5402e-02  time: 4.35e-01\n",
      "epoch: 27   trainLoss: 3.8201e-02   valLoss:6.6998e-02  time: 4.35e-01\n",
      "epoch: 28   trainLoss: 3.7693e-02   valLoss:5.0902e-02  time: 4.41e-01\n",
      "epoch: 29   trainLoss: 3.4075e-02   valLoss:4.4300e-02  time: 4.48e-01\n",
      "epoch: 30   trainLoss: 3.2849e-02   valLoss:3.9769e-02  time: 4.45e-01\n",
      "epoch: 31   trainLoss: 3.0228e-02   valLoss:4.5526e-02  time: 4.41e-01\n",
      "epoch: 32   trainLoss: 2.9437e-02   valLoss:4.7290e-02  time: 4.46e-01\n",
      "epoch: 33   trainLoss: 2.7790e-02   valLoss:4.1936e-02  time: 4.44e-01\n",
      "epoch: 34   trainLoss: 2.6546e-02   valLoss:3.2914e-02  time: 4.47e-01\n",
      "epoch: 35   trainLoss: 2.4834e-02   valLoss:3.4198e-02  time: 4.43e-01\n",
      "epoch: 36   trainLoss: 2.3876e-02   valLoss:3.2466e-02  time: 4.41e-01\n",
      "epoch: 37   trainLoss: 2.2892e-02   valLoss:2.6324e-02  time: 4.48e-01\n",
      "epoch: 38   trainLoss: 2.1924e-02   valLoss:2.5283e-02  time: 4.45e-01\n",
      "epoch: 39   trainLoss: 2.0692e-02   valLoss:2.5767e-02  time: 4.47e-01\n",
      "epoch: 40   trainLoss: 2.0244e-02   valLoss:2.5483e-02  time: 4.43e-01\n",
      "epoch: 41   trainLoss: 1.9127e-02   valLoss:2.5071e-02  time: 4.47e-01\n",
      "epoch: 42   trainLoss: 1.8763e-02   valLoss:2.3171e-02  time: 4.57e-01\n",
      "epoch: 43   trainLoss: 1.7959e-02   valLoss:2.5983e-02  time: 4.46e-01\n",
      "epoch: 44   trainLoss: 1.7697e-02   valLoss:2.4434e-02  time: 4.45e-01\n",
      "epoch: 45   trainLoss: 1.7200e-02   valLoss:2.4406e-02  time: 4.47e-01\n",
      "epoch: 46   trainLoss: 1.7290e-02   valLoss:3.2898e-02  time: 4.42e-01\n",
      "epoch: 47   trainLoss: 1.7522e-02   valLoss:2.6648e-02  time: 4.46e-01\n",
      "epoch: 48   trainLoss: 1.7899e-02   valLoss:3.1173e-02  time: 4.48e-01\n",
      "epoch: 49   trainLoss: 1.8110e-02   valLoss:3.1173e-02  time: 4.45e-01\n",
      "epoch: 50   trainLoss: 1.8662e-02   valLoss:3.0122e-02  time: 4.35e-01\n",
      "epoch: 51   trainLoss: 1.6917e-02   valLoss:2.3257e-02  time: 4.34e-01\n",
      "epoch: 52   trainLoss: 1.4594e-02   valLoss:2.3500e-02  time: 4.50e-01\n",
      "epoch: 53   trainLoss: 1.2815e-02   valLoss:2.5186e-02  time: 4.36e-01\n",
      "epoch: 54   trainLoss: 1.3664e-02   valLoss:2.8980e-02  time: 4.35e-01\n",
      "epoch: 55   trainLoss: 1.5097e-02   valLoss:3.6707e-02  time: 4.43e-01\n",
      "epoch: 56   trainLoss: 1.3555e-02   valLoss:3.3638e-02  time: 4.47e-01\n",
      "epoch: 57   trainLoss: 1.1662e-02   valLoss:3.4477e-02  time: 4.45e-01\n",
      "epoch: 58   trainLoss: 1.1405e-02   valLoss:3.6136e-02  time: 4.44e-01\n",
      "epoch: 59   trainLoss: 1.1924e-02   valLoss:3.5948e-02  time: 4.41e-01\n",
      "epoch: 60   trainLoss: 1.2381e-02   valLoss:3.4183e-02  time: 4.45e-01\n",
      "epoch: 61   trainLoss: 1.1412e-02   valLoss:2.7365e-02  time: 4.48e-01\n",
      "epoch: 62   trainLoss: 1.0104e-02   valLoss:2.6664e-02  time: 4.43e-01\n",
      "epoch: 63   trainLoss: 9.5677e-03   valLoss:2.9066e-02  time: 4.50e-01\n",
      "epoch: 64   trainLoss: 1.0054e-02   valLoss:3.2638e-02  time: 4.45e-01\n",
      "epoch: 65   trainLoss: 1.0800e-02   valLoss:2.9178e-02  time: 4.47e-01\n",
      "epoch: 66   trainLoss: 1.0879e-02   valLoss:3.2411e-02  time: 4.48e-01\n",
      "epoch: 67   trainLoss: 1.0389e-02   valLoss:3.0055e-02  time: 4.46e-01\n",
      "epoch: 68   trainLoss: 9.2205e-03   valLoss:3.1356e-02  time: 4.47e-01\n",
      "epoch: 69   trainLoss: 8.2050e-03   valLoss:3.1710e-02  time: 4.42e-01\n",
      "epoch: 70   trainLoss: 7.7736e-03   valLoss:3.5712e-02  time: 4.42e-01\n",
      "epoch: 71   trainLoss: 7.7701e-03   valLoss:3.6308e-02  time: 4.45e-01\n",
      "epoch: 72   trainLoss: 8.3006e-03   valLoss:4.1952e-02  time: 4.43e-01\n",
      "epoch: 73   trainLoss: 9.3787e-03   valLoss:4.0948e-02  time: 4.56e-01\n",
      "epoch: 74   trainLoss: 1.2476e-02   valLoss:5.2387e-02  time: 4.34e-01\n",
      "epoch: 75   trainLoss: 1.8035e-02   valLoss:4.8984e-02  time: 4.41e-01\n",
      "epoch: 76   trainLoss: 2.5237e-02   valLoss:3.7166e-02  time: 4.35e-01\n",
      "epoch: 77   trainLoss: 1.6614e-02   valLoss:1.8668e-02  time: 4.37e-01\n",
      "epoch: 78   trainLoss: 8.9292e-03   valLoss:3.1280e-02  time: 4.46e-01\n",
      "epoch: 79   trainLoss: 1.3346e-02   valLoss:2.6878e-02  time: 4.37e-01\n",
      "epoch: 80   trainLoss: 1.0356e-02   valLoss:2.4617e-02  time: 4.35e-01\n",
      "epoch: 81   trainLoss: 1.0463e-02   valLoss:3.4687e-02  time: 4.32e-01\n",
      "epoch: 82   trainLoss: 1.0202e-02   valLoss:3.2417e-02  time: 4.32e-01\n",
      "epoch: 83   trainLoss: 8.0668e-03   valLoss:3.1951e-02  time: 4.33e-01\n",
      "epoch: 84   trainLoss: 1.0062e-02   valLoss:2.6728e-02  time: 4.48e-01\n",
      "epoch: 85   trainLoss: 6.9338e-03   valLoss:3.1555e-02  time: 4.31e-01\n",
      "epoch: 86   trainLoss: 9.1571e-03   valLoss:2.8408e-02  time: 4.41e-01\n",
      "epoch: 87   trainLoss: 6.9689e-03   valLoss:2.8224e-02  time: 4.35e-01\n",
      "epoch: 88   trainLoss: 7.5568e-03   valLoss:2.5442e-02  time: 4.34e-01\n",
      "epoch: 89   trainLoss: 6.9728e-03   valLoss:3.0352e-02  time: 4.39e-01\n",
      "epoch: 90   trainLoss: 6.9046e-03   valLoss:3.0874e-02  time: 4.39e-01\n",
      "epoch: 91   trainLoss: 6.6261e-03   valLoss:2.7948e-02  time: 4.67e-01\n",
      "epoch: 92   trainLoss: 5.6117e-03   valLoss:2.7027e-02  time: 4.47e-01\n",
      "epoch: 93   trainLoss: 6.2958e-03   valLoss:3.3428e-02  time: 4.44e-01\n",
      "epoch: 94   trainLoss: 5.2661e-03   valLoss:3.4836e-02  time: 4.34e-01\n",
      "epoch: 95   trainLoss: 5.7749e-03   valLoss:3.2694e-02  time: 4.32e-01\n",
      "epoch: 96   trainLoss: 5.2967e-03   valLoss:3.2710e-02  time: 4.37e-01\n",
      "epoch: 97   trainLoss: 5.6204e-03   valLoss:3.6665e-02  time: 4.32e-01\n",
      "epoch: 98   trainLoss: 6.2541e-03   valLoss:3.6936e-02  time: 4.33e-01\n",
      "epoch: 99   trainLoss: 7.3531e-03   valLoss:3.3107e-02  time: 4.34e-01\n",
      "loading checkpoint 77\n",
      "trained 38 random forest models in 5.14 seconds\n",
      "loaded train set of size 18\n",
      "epoch: 0   trainLoss: 1.2452e+00   valLoss:1.1423e+00  time: 1.80e-01\n",
      "epoch: 1   trainLoss: 1.0039e+00   valLoss:1.1282e+00  time: 1.80e-01\n",
      "epoch: 2   trainLoss: 8.7416e-01   valLoss:1.0923e+00  time: 1.79e-01\n",
      "epoch: 3   trainLoss: 7.6875e-01   valLoss:1.0307e+00  time: 1.90e-01\n",
      "epoch: 4   trainLoss: 6.4528e-01   valLoss:9.4635e-01  time: 1.83e-01\n",
      "epoch: 5   trainLoss: 5.6150e-01   valLoss:8.5208e-01  time: 1.88e-01\n",
      "epoch: 6   trainLoss: 4.6522e-01   valLoss:7.8383e-01  time: 1.84e-01\n",
      "epoch: 7   trainLoss: 3.9178e-01   valLoss:7.7346e-01  time: 1.91e-01\n",
      "epoch: 8   trainLoss: 3.2803e-01   valLoss:8.3878e-01  time: 1.78e-01\n",
      "epoch: 9   trainLoss: 2.8618e-01   valLoss:9.5187e-01  time: 1.78e-01\n",
      "epoch: 10   trainLoss: 2.5456e-01   valLoss:1.0525e+00  time: 1.79e-01\n",
      "epoch: 11   trainLoss: 2.4181e-01   valLoss:1.1246e+00  time: 1.78e-01\n",
      "epoch: 12   trainLoss: 2.1386e-01   valLoss:1.1938e+00  time: 1.77e-01\n",
      "epoch: 13   trainLoss: 2.0221e-01   valLoss:1.2214e+00  time: 1.83e-01\n",
      "epoch: 14   trainLoss: 1.8447e-01   valLoss:1.1950e+00  time: 1.83e-01\n",
      "epoch: 15   trainLoss: 1.6611e-01   valLoss:1.2208e+00  time: 1.79e-01\n",
      "epoch: 16   trainLoss: 1.4444e-01   valLoss:1.2600e+00  time: 1.78e-01\n",
      "epoch: 17   trainLoss: 1.2908e-01   valLoss:1.3088e+00  time: 1.79e-01\n",
      "epoch: 18   trainLoss: 1.1552e-01   valLoss:1.3905e+00  time: 1.79e-01\n",
      "epoch: 19   trainLoss: 1.0160e-01   valLoss:1.4407e+00  time: 1.79e-01\n",
      "epoch: 20   trainLoss: 9.1800e-02   valLoss:1.3930e+00  time: 1.81e-01\n",
      "epoch: 21   trainLoss: 8.2397e-02   valLoss:1.3321e+00  time: 1.79e-01\n",
      "epoch: 22   trainLoss: 7.6096e-02   valLoss:1.2079e+00  time: 1.79e-01\n",
      "epoch: 23   trainLoss: 6.8556e-02   valLoss:9.5162e-01  time: 1.81e-01\n",
      "epoch: 24   trainLoss: 6.0405e-02   valLoss:7.5894e-01  time: 1.90e-01\n",
      "epoch: 25   trainLoss: 5.6039e-02   valLoss:6.6838e-01  time: 1.79e-01\n",
      "epoch: 26   trainLoss: 5.0363e-02   valLoss:6.0421e-01  time: 1.80e-01\n",
      "epoch: 27   trainLoss: 4.4819e-02   valLoss:4.7873e-01  time: 1.84e-01\n",
      "epoch: 28   trainLoss: 4.0581e-02   valLoss:3.6940e-01  time: 1.84e-01\n",
      "epoch: 29   trainLoss: 3.6306e-02   valLoss:3.4405e-01  time: 1.85e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30   trainLoss: 3.2252e-02   valLoss:3.5199e-01  time: 1.82e-01\n",
      "epoch: 31   trainLoss: 2.9247e-02   valLoss:3.4685e-01  time: 1.82e-01\n",
      "epoch: 32   trainLoss: 2.7147e-02   valLoss:3.9485e-01  time: 1.82e-01\n",
      "epoch: 33   trainLoss: 2.6721e-02   valLoss:3.2250e-01  time: 1.77e-01\n",
      "epoch: 34   trainLoss: 3.0171e-02   valLoss:4.1609e-01  time: 1.82e-01\n",
      "epoch: 35   trainLoss: 2.7271e-02   valLoss:3.6191e-01  time: 1.85e-01\n",
      "epoch: 36   trainLoss: 2.1344e-02   valLoss:2.6813e-01  time: 1.78e-01\n",
      "epoch: 37   trainLoss: 2.1967e-02   valLoss:2.9462e-01  time: 1.80e-01\n",
      "epoch: 38   trainLoss: 1.6005e-02   valLoss:3.3150e-01  time: 1.78e-01\n",
      "epoch: 39   trainLoss: 1.7734e-02   valLoss:2.9390e-01  time: 1.80e-01\n",
      "epoch: 40   trainLoss: 1.3907e-02   valLoss:2.6013e-01  time: 1.79e-01\n",
      "epoch: 41   trainLoss: 1.3640e-02   valLoss:2.6525e-01  time: 1.81e-01\n",
      "epoch: 42   trainLoss: 1.2786e-02   valLoss:2.7836e-01  time: 1.84e-01\n",
      "epoch: 43   trainLoss: 1.2403e-02   valLoss:2.5111e-01  time: 1.80e-01\n",
      "epoch: 44   trainLoss: 1.0669e-02   valLoss:2.3070e-01  time: 1.88e-01\n",
      "epoch: 45   trainLoss: 1.0016e-02   valLoss:2.2445e-01  time: 1.83e-01\n",
      "epoch: 46   trainLoss: 7.4579e-03   valLoss:2.2571e-01  time: 1.85e-01\n",
      "epoch: 47   trainLoss: 8.0167e-03   valLoss:2.1866e-01  time: 1.91e-01\n",
      "epoch: 48   trainLoss: 6.9634e-03   valLoss:1.9964e-01  time: 1.81e-01\n",
      "epoch: 49   trainLoss: 5.8474e-03   valLoss:1.9866e-01  time: 1.87e-01\n",
      "epoch: 50   trainLoss: 4.8701e-03   valLoss:1.9724e-01  time: 1.80e-01\n",
      "epoch: 51   trainLoss: 4.5157e-03   valLoss:1.7813e-01  time: 1.79e-01\n",
      "epoch: 52   trainLoss: 4.3344e-03   valLoss:1.7692e-01  time: 1.85e-01\n",
      "epoch: 53   trainLoss: 4.6815e-03   valLoss:1.6699e-01  time: 1.80e-01\n",
      "epoch: 54   trainLoss: 3.8724e-03   valLoss:1.7021e-01  time: 1.82e-01\n",
      "epoch: 55   trainLoss: 3.2423e-03   valLoss:1.6058e-01  time: 1.82e-01\n",
      "epoch: 56   trainLoss: 2.5788e-03   valLoss:1.5793e-01  time: 1.91e-01\n",
      "epoch: 57   trainLoss: 2.4145e-03   valLoss:1.6113e-01  time: 1.86e-01\n",
      "epoch: 58   trainLoss: 2.3701e-03   valLoss:1.5321e-01  time: 1.93e-01\n",
      "epoch: 59   trainLoss: 2.2466e-03   valLoss:1.4784e-01  time: 1.81e-01\n",
      "epoch: 60   trainLoss: 2.3566e-03   valLoss:1.4166e-01  time: 1.79e-01\n",
      "epoch: 61   trainLoss: 1.8203e-03   valLoss:1.3989e-01  time: 1.81e-01\n",
      "epoch: 62   trainLoss: 1.6099e-03   valLoss:1.3526e-01  time: 1.80e-01\n",
      "epoch: 63   trainLoss: 1.2286e-03   valLoss:1.3228e-01  time: 1.79e-01\n",
      "epoch: 64   trainLoss: 1.2120e-03   valLoss:1.3114e-01  time: 1.81e-01\n",
      "epoch: 65   trainLoss: 1.1824e-03   valLoss:1.2956e-01  time: 1.81e-01\n",
      "epoch: 66   trainLoss: 1.2654e-03   valLoss:1.2861e-01  time: 1.82e-01\n",
      "epoch: 67   trainLoss: 1.5606e-03   valLoss:1.2697e-01  time: 1.78e-01\n",
      "epoch: 68   trainLoss: 1.9666e-03   valLoss:1.2270e-01  time: 1.81e-01\n",
      "epoch: 69   trainLoss: 2.9101e-03   valLoss:1.2540e-01  time: 1.79e-01\n",
      "epoch: 70   trainLoss: 4.2063e-03   valLoss:1.2315e-01  time: 1.77e-01\n",
      "epoch: 71   trainLoss: 5.0265e-03   valLoss:1.2358e-01  time: 1.78e-01\n",
      "epoch: 72   trainLoss: 3.6133e-03   valLoss:1.0928e-01  time: 1.78e-01\n",
      "epoch: 73   trainLoss: 9.8421e-04   valLoss:1.0897e-01  time: 1.97e-01\n",
      "epoch: 74   trainLoss: 1.4285e-03   valLoss:1.1588e-01  time: 1.85e-01\n",
      "epoch: 75   trainLoss: 2.5504e-03   valLoss:1.0540e-01  time: 1.83e-01\n",
      "epoch: 76   trainLoss: 1.1234e-03   valLoss:1.0300e-01  time: 1.84e-01\n",
      "epoch: 77   trainLoss: 8.6484e-04   valLoss:1.0394e-01  time: 1.79e-01\n",
      "epoch: 78   trainLoss: 1.7509e-03   valLoss:9.9124e-02  time: 1.83e-01\n",
      "epoch: 79   trainLoss: 9.8490e-04   valLoss:1.0032e-01  time: 1.83e-01\n",
      "epoch: 80   trainLoss: 6.5804e-04   valLoss:1.0030e-01  time: 1.80e-01\n",
      "epoch: 81   trainLoss: 1.2486e-03   valLoss:9.5867e-02  time: 1.81e-01\n",
      "epoch: 82   trainLoss: 7.7265e-04   valLoss:9.6119e-02  time: 1.83e-01\n",
      "epoch: 83   trainLoss: 5.3612e-04   valLoss:9.3702e-02  time: 1.82e-01\n",
      "epoch: 84   trainLoss: 8.8117e-04   valLoss:8.9248e-02  time: 1.84e-01\n",
      "epoch: 85   trainLoss: 6.5654e-04   valLoss:8.8923e-02  time: 1.84e-01\n",
      "epoch: 86   trainLoss: 3.8652e-04   valLoss:8.7454e-02  time: 1.81e-01\n",
      "epoch: 87   trainLoss: 7.0552e-04   valLoss:8.5262e-02  time: 1.87e-01\n",
      "epoch: 88   trainLoss: 6.7621e-04   valLoss:8.2598e-02  time: 1.80e-01\n",
      "epoch: 89   trainLoss: 3.6285e-04   valLoss:8.1504e-02  time: 1.80e-01\n",
      "epoch: 90   trainLoss: 4.6078e-04   valLoss:8.0700e-02  time: 1.80e-01\n",
      "epoch: 91   trainLoss: 7.1211e-04   valLoss:7.8081e-02  time: 1.80e-01\n",
      "epoch: 92   trainLoss: 5.5449e-04   valLoss:7.8733e-02  time: 1.80e-01\n",
      "epoch: 93   trainLoss: 5.5222e-04   valLoss:7.8705e-02  time: 1.81e-01\n",
      "epoch: 94   trainLoss: 1.1302e-03   valLoss:7.6077e-02  time: 1.84e-01\n",
      "epoch: 95   trainLoss: 2.7257e-03   valLoss:7.7625e-02  time: 1.85e-01\n",
      "epoch: 96   trainLoss: 5.0049e-03   valLoss:7.5117e-02  time: 1.82e-01\n",
      "epoch: 97   trainLoss: 7.6031e-03   valLoss:6.5624e-02  time: 1.83e-01\n",
      "epoch: 98   trainLoss: 4.5154e-03   valLoss:6.7118e-02  time: 1.82e-01\n",
      "epoch: 99   trainLoss: 2.4481e-03   valLoss:6.5864e-02  time: 1.82e-01\n",
      "loading checkpoint 97\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 8.5111e-01   valLoss:1.0616e+00  time: 1.79e-01\n",
      "epoch: 1   trainLoss: 6.5351e-01   valLoss:1.4934e+00  time: 1.81e-01\n",
      "epoch: 2   trainLoss: 4.9095e-01   valLoss:9.4832e-01  time: 1.79e-01\n",
      "epoch: 3   trainLoss: 3.4399e-01   valLoss:4.6396e-01  time: 1.79e-01\n",
      "epoch: 4   trainLoss: 2.2399e-01   valLoss:5.5251e-01  time: 1.80e-01\n",
      "epoch: 5   trainLoss: 1.8233e-01   valLoss:4.2380e-01  time: 1.85e-01\n",
      "epoch: 6   trainLoss: 1.4159e-01   valLoss:3.5966e-01  time: 1.85e-01\n",
      "epoch: 7   trainLoss: 1.1748e-01   valLoss:3.3679e-01  time: 1.84e-01\n",
      "epoch: 8   trainLoss: 9.8102e-02   valLoss:3.5321e-01  time: 1.84e-01\n",
      "epoch: 9   trainLoss: 8.3272e-02   valLoss:2.0365e-01  time: 1.86e-01\n",
      "epoch: 10   trainLoss: 7.1154e-02   valLoss:1.7133e-01  time: 1.83e-01\n",
      "epoch: 11   trainLoss: 7.1094e-02   valLoss:1.8130e-01  time: 1.88e-01\n",
      "epoch: 12   trainLoss: 6.4635e-02   valLoss:1.0483e-01  time: 1.82e-01\n",
      "epoch: 13   trainLoss: 5.6289e-02   valLoss:8.5588e-02  time: 1.88e-01\n",
      "epoch: 14   trainLoss: 5.0277e-02   valLoss:7.3450e-02  time: 1.83e-01\n",
      "epoch: 15   trainLoss: 4.3919e-02   valLoss:6.9342e-02  time: 1.83e-01\n",
      "epoch: 16   trainLoss: 4.0810e-02   valLoss:5.6080e-02  time: 1.85e-01\n",
      "epoch: 17   trainLoss: 3.4799e-02   valLoss:5.8031e-02  time: 1.82e-01\n",
      "epoch: 18   trainLoss: 3.3911e-02   valLoss:5.4943e-02  time: 1.84e-01\n",
      "epoch: 19   trainLoss: 2.9760e-02   valLoss:5.5398e-02  time: 1.84e-01\n",
      "epoch: 20   trainLoss: 2.6053e-02   valLoss:7.3760e-02  time: 1.81e-01\n",
      "epoch: 21   trainLoss: 2.3912e-02   valLoss:7.3791e-02  time: 1.90e-01\n",
      "epoch: 22   trainLoss: 2.2520e-02   valLoss:6.8867e-02  time: 1.92e-01\n",
      "epoch: 23   trainLoss: 2.0519e-02   valLoss:8.0080e-02  time: 1.84e-01\n",
      "epoch: 24   trainLoss: 1.8939e-02   valLoss:7.8840e-02  time: 1.81e-01\n",
      "epoch: 25   trainLoss: 1.7571e-02   valLoss:6.5789e-02  time: 1.84e-01\n",
      "epoch: 26   trainLoss: 1.5893e-02   valLoss:4.9524e-02  time: 1.85e-01\n",
      "epoch: 27   trainLoss: 1.4519e-02   valLoss:5.0767e-02  time: 1.82e-01\n",
      "epoch: 28   trainLoss: 1.4420e-02   valLoss:4.1721e-02  time: 1.80e-01\n",
      "epoch: 29   trainLoss: 1.3345e-02   valLoss:3.4502e-02  time: 1.84e-01\n",
      "epoch: 30   trainLoss: 1.2749e-02   valLoss:2.1956e-02  time: 1.80e-01\n",
      "epoch: 31   trainLoss: 1.1941e-02   valLoss:1.3312e-02  time: 1.83e-01\n",
      "epoch: 32   trainLoss: 1.0751e-02   valLoss:2.0220e-02  time: 1.83e-01\n",
      "epoch: 33   trainLoss: 1.0398e-02   valLoss:1.6773e-02  time: 1.85e-01\n",
      "epoch: 34   trainLoss: 9.9580e-03   valLoss:1.5397e-02  time: 1.81e-01\n",
      "epoch: 35   trainLoss: 9.4224e-03   valLoss:1.7690e-02  time: 1.80e-01\n",
      "epoch: 36   trainLoss: 9.2612e-03   valLoss:1.5050e-02  time: 1.93e-01\n",
      "epoch: 37   trainLoss: 8.8744e-03   valLoss:2.0302e-02  time: 1.79e-01\n",
      "epoch: 38   trainLoss: 8.0190e-03   valLoss:1.9077e-02  time: 1.78e-01\n",
      "epoch: 39   trainLoss: 7.8527e-03   valLoss:2.4297e-02  time: 1.79e-01\n",
      "epoch: 40   trainLoss: 7.3624e-03   valLoss:2.4957e-02  time: 1.78e-01\n",
      "epoch: 41   trainLoss: 7.1549e-03   valLoss:1.8889e-02  time: 1.86e-01\n",
      "epoch: 42   trainLoss: 6.9722e-03   valLoss:1.3622e-02  time: 1.84e-01\n",
      "epoch: 43   trainLoss: 6.5739e-03   valLoss:1.0574e-02  time: 1.83e-01\n",
      "epoch: 44   trainLoss: 6.2490e-03   valLoss:8.2922e-03  time: 1.90e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45   trainLoss: 6.0596e-03   valLoss:1.0726e-02  time: 1.82e-01\n",
      "epoch: 46   trainLoss: 5.8421e-03   valLoss:9.4335e-03  time: 1.81e-01\n",
      "epoch: 47   trainLoss: 5.6577e-03   valLoss:1.4804e-02  time: 1.83e-01\n",
      "epoch: 48   trainLoss: 5.7437e-03   valLoss:1.7036e-02  time: 1.82e-01\n",
      "epoch: 49   trainLoss: 6.4574e-03   valLoss:2.4386e-02  time: 1.81e-01\n",
      "epoch: 50   trainLoss: 8.4161e-03   valLoss:2.0284e-02  time: 1.86e-01\n",
      "epoch: 51   trainLoss: 1.3765e-02   valLoss:2.8691e-02  time: 1.84e-01\n",
      "epoch: 52   trainLoss: 1.4084e-02   valLoss:1.2370e-02  time: 1.82e-01\n",
      "epoch: 53   trainLoss: 8.9679e-03   valLoss:1.6184e-02  time: 1.80e-01\n",
      "epoch: 54   trainLoss: 5.3649e-03   valLoss:2.7126e-02  time: 1.81e-01\n",
      "epoch: 55   trainLoss: 9.7058e-03   valLoss:1.7809e-02  time: 1.84e-01\n",
      "epoch: 56   trainLoss: 6.9031e-03   valLoss:2.3858e-02  time: 1.83e-01\n",
      "epoch: 57   trainLoss: 5.3149e-03   valLoss:2.8055e-02  time: 1.83e-01\n",
      "epoch: 58   trainLoss: 7.9035e-03   valLoss:1.9596e-02  time: 1.84e-01\n",
      "epoch: 59   trainLoss: 4.5170e-03   valLoss:1.8258e-02  time: 1.90e-01\n",
      "epoch: 60   trainLoss: 5.9730e-03   valLoss:2.5113e-02  time: 1.80e-01\n",
      "epoch: 61   trainLoss: 5.0082e-03   valLoss:2.1994e-02  time: 1.79e-01\n",
      "epoch: 62   trainLoss: 4.1246e-03   valLoss:2.6527e-02  time: 1.80e-01\n",
      "epoch: 63   trainLoss: 5.2544e-03   valLoss:2.3824e-02  time: 1.80e-01\n",
      "epoch: 64   trainLoss: 3.8372e-03   valLoss:3.5931e-02  time: 1.81e-01\n",
      "epoch: 65   trainLoss: 5.4897e-03   valLoss:3.4163e-02  time: 1.81e-01\n",
      "epoch: 66   trainLoss: 6.2420e-03   valLoss:3.6590e-02  time: 1.88e-01\n",
      "epoch: 67   trainLoss: 6.7122e-03   valLoss:4.5186e-02  time: 1.80e-01\n",
      "epoch: 68   trainLoss: 7.5953e-03   valLoss:2.6278e-02  time: 1.82e-01\n",
      "epoch: 69   trainLoss: 3.9900e-03   valLoss:3.9432e-02  time: 1.83e-01\n",
      "epoch: 70   trainLoss: 6.4246e-03   valLoss:3.9328e-02  time: 1.78e-01\n",
      "epoch: 71   trainLoss: 4.7152e-03   valLoss:4.3789e-02  time: 1.78e-01\n",
      "epoch: 72   trainLoss: 5.4795e-03   valLoss:2.8249e-02  time: 1.80e-01\n",
      "epoch: 73   trainLoss: 3.3145e-03   valLoss:3.1380e-02  time: 1.82e-01\n",
      "epoch: 74   trainLoss: 5.1634e-03   valLoss:2.7322e-02  time: 1.80e-01\n",
      "epoch: 75   trainLoss: 3.7763e-03   valLoss:3.1289e-02  time: 1.79e-01\n",
      "epoch: 76   trainLoss: 4.1669e-03   valLoss:2.2444e-02  time: 1.78e-01\n",
      "epoch: 77   trainLoss: 5.5253e-03   valLoss:2.5691e-02  time: 1.79e-01\n",
      "epoch: 78   trainLoss: 6.1807e-03   valLoss:2.6228e-02  time: 1.77e-01\n",
      "epoch: 79   trainLoss: 8.0035e-03   valLoss:3.7421e-02  time: 1.79e-01\n",
      "epoch: 80   trainLoss: 6.4028e-03   valLoss:1.7239e-02  time: 1.79e-01\n",
      "epoch: 81   trainLoss: 3.2534e-03   valLoss:2.2468e-02  time: 1.78e-01\n",
      "epoch: 82   trainLoss: 4.1544e-03   valLoss:3.3776e-02  time: 1.79e-01\n",
      "epoch: 83   trainLoss: 3.8749e-03   valLoss:2.8476e-02  time: 1.91e-01\n",
      "epoch: 84   trainLoss: 3.4142e-03   valLoss:2.5809e-02  time: 1.85e-01\n",
      "epoch: 85   trainLoss: 3.4113e-03   valLoss:2.6792e-02  time: 1.79e-01\n",
      "epoch: 86   trainLoss: 2.7365e-03   valLoss:2.9097e-02  time: 1.79e-01\n",
      "epoch: 87   trainLoss: 3.5756e-03   valLoss:2.3805e-02  time: 1.79e-01\n",
      "epoch: 88   trainLoss: 2.2086e-03   valLoss:2.6150e-02  time: 1.80e-01\n",
      "epoch: 89   trainLoss: 2.9588e-03   valLoss:2.6298e-02  time: 1.80e-01\n",
      "epoch: 90   trainLoss: 2.2399e-03   valLoss:3.0201e-02  time: 1.78e-01\n",
      "epoch: 91   trainLoss: 2.6530e-03   valLoss:2.8013e-02  time: 1.93e-01\n",
      "epoch: 92   trainLoss: 2.0888e-03   valLoss:2.6569e-02  time: 1.79e-01\n",
      "epoch: 93   trainLoss: 2.1879e-03   valLoss:3.0907e-02  time: 1.79e-01\n",
      "epoch: 94   trainLoss: 1.9133e-03   valLoss:3.4856e-02  time: 1.82e-01\n",
      "epoch: 95   trainLoss: 2.1905e-03   valLoss:3.1107e-02  time: 1.85e-01\n",
      "epoch: 96   trainLoss: 1.9506e-03   valLoss:3.3260e-02  time: 1.80e-01\n",
      "epoch: 97   trainLoss: 1.7362e-03   valLoss:3.7143e-02  time: 1.83e-01\n",
      "epoch: 98   trainLoss: 1.7603e-03   valLoss:3.2278e-02  time: 1.78e-01\n",
      "epoch: 99   trainLoss: 1.5747e-03   valLoss:3.1713e-02  time: 1.80e-01\n",
      "loading checkpoint 44\n",
      "trained 38 random forest models in 4.56 seconds\n",
      "loaded train set of size 452\n",
      "epoch: 0   trainLoss: 9.0411e-01   valLoss:9.7626e-01  time: 4.08e+00\n",
      "epoch: 1   trainLoss: 5.9364e-01   valLoss:9.3342e-01  time: 4.09e+00\n",
      "epoch: 2   trainLoss: 4.4413e-01   valLoss:8.8560e-01  time: 4.01e+00\n",
      "epoch: 3   trainLoss: 3.5294e-01   valLoss:8.8258e-01  time: 4.06e+00\n",
      "epoch: 4   trainLoss: 2.9849e-01   valLoss:8.9798e-01  time: 4.00e+00\n",
      "epoch: 5   trainLoss: 2.6469e-01   valLoss:8.1100e-01  time: 3.99e+00\n",
      "epoch: 6   trainLoss: 2.3339e-01   valLoss:7.4204e-01  time: 3.99e+00\n",
      "epoch: 7   trainLoss: 2.1506e-01   valLoss:7.2366e-01  time: 3.98e+00\n",
      "epoch: 8   trainLoss: 1.9269e-01   valLoss:6.6519e-01  time: 4.02e+00\n",
      "epoch: 9   trainLoss: 1.8189e-01   valLoss:5.3861e-01  time: 4.03e+00\n",
      "epoch: 10   trainLoss: 1.6311e-01   valLoss:4.5374e-01  time: 3.98e+00\n",
      "epoch: 11   trainLoss: 1.6267e-01   valLoss:3.4096e-01  time: 3.98e+00\n",
      "epoch: 12   trainLoss: 1.4220e-01   valLoss:3.5309e-01  time: 4.02e+00\n",
      "epoch: 13   trainLoss: 1.3086e-01   valLoss:3.1415e-01  time: 3.99e+00\n",
      "epoch: 14   trainLoss: 1.3296e-01   valLoss:3.2149e-01  time: 3.97e+00\n",
      "epoch: 15   trainLoss: 1.1263e-01   valLoss:3.9080e-01  time: 4.01e+00\n",
      "epoch: 16   trainLoss: 1.1480e-01   valLoss:2.8223e-01  time: 3.97e+00\n",
      "epoch: 17   trainLoss: 1.1326e-01   valLoss:4.2934e-01  time: 3.97e+00\n",
      "epoch: 18   trainLoss: 9.6502e-02   valLoss:3.4973e-01  time: 3.96e+00\n",
      "epoch: 19   trainLoss: 9.1722e-02   valLoss:3.7432e-01  time: 3.97e+00\n",
      "epoch: 20   trainLoss: 8.9805e-02   valLoss:3.7166e-01  time: 3.97e+00\n",
      "epoch: 21   trainLoss: 8.1789e-02   valLoss:5.0142e-01  time: 3.92e+00\n",
      "epoch: 22   trainLoss: 8.3273e-02   valLoss:3.5397e-01  time: 3.95e+00\n",
      "epoch: 23   trainLoss: 9.2969e-02   valLoss:3.5201e-01  time: 3.96e+00\n",
      "epoch: 24   trainLoss: 8.0983e-02   valLoss:3.8301e-01  time: 3.97e+00\n",
      "epoch: 25   trainLoss: 7.5809e-02   valLoss:5.5487e-01  time: 3.94e+00\n",
      "epoch: 26   trainLoss: 8.6954e-02   valLoss:3.5398e-01  time: 3.91e+00\n",
      "epoch: 27   trainLoss: 7.4658e-02   valLoss:4.9745e-01  time: 3.95e+00\n",
      "epoch: 28   trainLoss: 7.2555e-02   valLoss:3.7699e-01  time: 3.93e+00\n",
      "epoch: 29   trainLoss: 8.2682e-02   valLoss:7.0047e-01  time: 3.92e+00\n",
      "epoch: 30   trainLoss: 8.8291e-02   valLoss:3.5313e-01  time: 3.97e+00\n",
      "epoch: 31   trainLoss: 7.6875e-02   valLoss:6.1825e-01  time: 3.97e+00\n",
      "epoch: 32   trainLoss: 6.6156e-02   valLoss:3.3375e-01  time: 3.92e+00\n",
      "epoch: 33   trainLoss: 8.1000e-02   valLoss:4.5309e-01  time: 3.92e+00\n",
      "epoch: 34   trainLoss: 6.0471e-02   valLoss:4.1693e-01  time: 3.97e+00\n",
      "epoch: 35   trainLoss: 6.3202e-02   valLoss:3.7015e-01  time: 3.94e+00\n",
      "epoch: 36   trainLoss: 5.1713e-02   valLoss:4.2875e-01  time: 3.98e+00\n",
      "epoch: 37   trainLoss: 4.9652e-02   valLoss:3.5372e-01  time: 3.94e+00\n",
      "epoch: 38   trainLoss: 4.6035e-02   valLoss:4.2143e-01  time: 3.91e+00\n",
      "epoch: 39   trainLoss: 4.5270e-02   valLoss:4.2180e-01  time: 3.95e+00\n",
      "epoch: 40   trainLoss: 4.5339e-02   valLoss:7.7333e-01  time: 3.92e+00\n",
      "epoch: 41   trainLoss: 5.4066e-02   valLoss:4.7464e-01  time: 3.94e+00\n",
      "epoch: 42   trainLoss: 4.3695e-02   valLoss:3.5542e-01  time: 3.95e+00\n",
      "epoch: 43   trainLoss: 4.1421e-02   valLoss:5.5174e-01  time: 3.93e+00\n",
      "epoch: 44   trainLoss: 4.3127e-02   valLoss:4.2665e-01  time: 3.94e+00\n",
      "epoch: 45   trainLoss: 4.9516e-02   valLoss:2.5948e-01  time: 3.97e+00\n",
      "epoch: 46   trainLoss: 5.2295e-02   valLoss:3.4192e-01  time: 4.00e+00\n",
      "epoch: 47   trainLoss: 5.6719e-02   valLoss:5.9142e-01  time: 3.98e+00\n",
      "epoch: 48   trainLoss: 5.0896e-02   valLoss:2.2609e-01  time: 4.00e+00\n",
      "epoch: 49   trainLoss: 5.0833e-02   valLoss:2.6872e-01  time: 3.95e+00\n",
      "epoch: 50   trainLoss: 4.0078e-02   valLoss:3.9380e-01  time: 4.21e+00\n",
      "epoch: 51   trainLoss: 4.1108e-02   valLoss:5.3007e-01  time: 4.03e+00\n",
      "epoch: 52   trainLoss: 3.7063e-02   valLoss:3.0784e-01  time: 3.96e+00\n",
      "epoch: 53   trainLoss: 3.4855e-02   valLoss:3.2952e-01  time: 3.99e+00\n",
      "epoch: 54   trainLoss: 3.2115e-02   valLoss:4.4086e-01  time: 4.02e+00\n",
      "epoch: 55   trainLoss: 3.3814e-02   valLoss:2.8948e-01  time: 3.96e+00\n",
      "epoch: 56   trainLoss: 3.8059e-02   valLoss:3.0325e-01  time: 3.97e+00\n",
      "epoch: 57   trainLoss: 3.6957e-02   valLoss:3.6659e-01  time: 3.92e+00\n",
      "epoch: 58   trainLoss: 3.8254e-02   valLoss:4.4645e-01  time: 3.91e+00\n",
      "epoch: 59   trainLoss: 5.3996e-02   valLoss:3.7120e-01  time: 3.93e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60   trainLoss: 3.8061e-02   valLoss:3.4328e-01  time: 4.03e+00\n",
      "epoch: 61   trainLoss: 3.7284e-02   valLoss:3.4954e-01  time: 3.94e+00\n",
      "epoch: 62   trainLoss: 3.6856e-02   valLoss:3.7524e-01  time: 3.93e+00\n",
      "epoch: 63   trainLoss: 2.8998e-02   valLoss:4.6205e-01  time: 3.92e+00\n",
      "epoch: 64   trainLoss: 3.7340e-02   valLoss:2.7493e-01  time: 3.92e+00\n",
      "epoch: 65   trainLoss: 2.8318e-02   valLoss:3.4414e-01  time: 3.97e+00\n",
      "epoch: 66   trainLoss: 2.8469e-02   valLoss:3.9531e-01  time: 4.06e+00\n",
      "epoch: 67   trainLoss: 2.6688e-02   valLoss:1.9958e-01  time: 3.94e+00\n",
      "epoch: 68   trainLoss: 2.7429e-02   valLoss:3.8636e-01  time: 3.94e+00\n",
      "epoch: 69   trainLoss: 2.4074e-02   valLoss:2.7550e-01  time: 3.94e+00\n",
      "epoch: 70   trainLoss: 3.1552e-02   valLoss:3.1639e-01  time: 4.01e+00\n",
      "epoch: 71   trainLoss: 2.6527e-02   valLoss:3.3329e-01  time: 3.92e+00\n",
      "epoch: 72   trainLoss: 2.2576e-02   valLoss:1.8574e-01  time: 3.97e+00\n",
      "epoch: 73   trainLoss: 2.1580e-02   valLoss:4.1440e-01  time: 4.03e+00\n",
      "epoch: 74   trainLoss: 1.8285e-02   valLoss:2.1629e-01  time: 4.01e+00\n",
      "epoch: 75   trainLoss: 1.7903e-02   valLoss:2.3494e-01  time: 4.03e+00\n",
      "epoch: 76   trainLoss: 1.8889e-02   valLoss:1.7910e-01  time: 4.00e+00\n",
      "epoch: 77   trainLoss: 2.2637e-02   valLoss:2.7046e-01  time: 3.99e+00\n",
      "epoch: 78   trainLoss: 2.0526e-02   valLoss:2.3716e-01  time: 4.03e+00\n",
      "epoch: 79   trainLoss: 1.9223e-02   valLoss:2.0057e-01  time: 3.98e+00\n",
      "epoch: 80   trainLoss: 1.9553e-02   valLoss:2.2631e-01  time: 3.99e+00\n",
      "epoch: 81   trainLoss: 2.4635e-02   valLoss:1.1586e-01  time: 3.96e+00\n",
      "epoch: 82   trainLoss: 1.7664e-02   valLoss:2.0953e-01  time: 4.04e+00\n",
      "epoch: 83   trainLoss: 1.7725e-02   valLoss:2.2717e-01  time: 3.95e+00\n",
      "epoch: 84   trainLoss: 1.8280e-02   valLoss:1.6391e-01  time: 4.02e+00\n",
      "epoch: 85   trainLoss: 2.1670e-02   valLoss:1.9563e-01  time: 3.96e+00\n",
      "epoch: 86   trainLoss: 1.7334e-02   valLoss:1.1386e-01  time: 4.05e+00\n",
      "epoch: 87   trainLoss: 1.9900e-02   valLoss:1.5508e-01  time: 4.00e+00\n",
      "epoch: 88   trainLoss: 1.7415e-02   valLoss:1.3360e-01  time: 4.02e+00\n",
      "epoch: 89   trainLoss: 1.5242e-02   valLoss:1.4144e-01  time: 4.03e+00\n",
      "epoch: 90   trainLoss: 1.9921e-02   valLoss:1.5662e-01  time: 4.07e+00\n",
      "epoch: 91   trainLoss: 2.1663e-02   valLoss:1.8827e-01  time: 4.05e+00\n",
      "epoch: 92   trainLoss: 1.5804e-02   valLoss:1.1237e-01  time: 4.04e+00\n",
      "epoch: 93   trainLoss: 1.7321e-02   valLoss:1.2935e-01  time: 4.03e+00\n",
      "epoch: 94   trainLoss: 1.9408e-02   valLoss:8.0672e-02  time: 3.99e+00\n",
      "epoch: 95   trainLoss: 2.8291e-02   valLoss:1.5304e-01  time: 4.04e+00\n",
      "epoch: 96   trainLoss: 1.9605e-02   valLoss:8.5488e-02  time: 4.06e+00\n",
      "epoch: 97   trainLoss: 1.9515e-02   valLoss:1.4924e-01  time: 4.11e+00\n",
      "epoch: 98   trainLoss: 2.5648e-02   valLoss:7.4458e-02  time: 4.04e+00\n",
      "epoch: 99   trainLoss: 2.4165e-02   valLoss:1.3618e-01  time: 4.00e+00\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 7.2637e-01   valLoss:9.9484e-01  time: 4.06e+00\n",
      "epoch: 1   trainLoss: 3.4935e-01   valLoss:1.4279e+00  time: 4.03e+00\n",
      "epoch: 2   trainLoss: 2.2280e-01   valLoss:1.0267e+00  time: 4.08e+00\n",
      "epoch: 3   trainLoss: 1.7951e-01   valLoss:6.5439e-01  time: 4.02e+00\n",
      "epoch: 4   trainLoss: 1.4746e-01   valLoss:5.5103e-01  time: 4.08e+00\n",
      "epoch: 5   trainLoss: 1.4072e-01   valLoss:4.2515e-01  time: 4.09e+00\n",
      "epoch: 6   trainLoss: 1.1069e-01   valLoss:3.5552e-01  time: 4.05e+00\n",
      "epoch: 7   trainLoss: 9.6110e-02   valLoss:2.1188e-01  time: 4.05e+00\n",
      "epoch: 8   trainLoss: 8.5745e-02   valLoss:1.8984e-01  time: 4.04e+00\n",
      "epoch: 9   trainLoss: 7.8914e-02   valLoss:1.3345e-01  time: 4.03e+00\n",
      "epoch: 10   trainLoss: 7.7813e-02   valLoss:8.7255e-02  time: 4.33e+00\n",
      "epoch: 11   trainLoss: 6.6874e-02   valLoss:7.2654e-02  time: 4.00e+00\n",
      "epoch: 12   trainLoss: 7.1618e-02   valLoss:7.6153e-02  time: 3.99e+00\n",
      "epoch: 13   trainLoss: 6.3562e-02   valLoss:8.4668e-02  time: 4.06e+00\n",
      "epoch: 14   trainLoss: 6.0489e-02   valLoss:6.3281e-02  time: 4.00e+00\n",
      "epoch: 15   trainLoss: 5.6481e-02   valLoss:5.6449e-02  time: 4.09e+00\n",
      "epoch: 16   trainLoss: 5.0322e-02   valLoss:5.9956e-02  time: 4.14e+00\n",
      "epoch: 17   trainLoss: 4.9284e-02   valLoss:6.4005e-02  time: 4.10e+00\n",
      "epoch: 18   trainLoss: 5.7162e-02   valLoss:6.6018e-02  time: 4.04e+00\n",
      "epoch: 19   trainLoss: 4.4250e-02   valLoss:6.5393e-02  time: 4.05e+00\n",
      "epoch: 20   trainLoss: 4.7330e-02   valLoss:8.0232e-02  time: 3.98e+00\n",
      "epoch: 21   trainLoss: 6.5723e-02   valLoss:7.3998e-02  time: 3.96e+00\n",
      "epoch: 22   trainLoss: 4.5806e-02   valLoss:1.1174e-01  time: 3.99e+00\n",
      "epoch: 23   trainLoss: 4.4851e-02   valLoss:9.2606e-02  time: 4.01e+00\n",
      "epoch: 24   trainLoss: 5.4023e-02   valLoss:9.9449e-02  time: 3.99e+00\n",
      "epoch: 25   trainLoss: 4.9510e-02   valLoss:7.4326e-02  time: 4.10e+00\n",
      "epoch: 26   trainLoss: 4.4571e-02   valLoss:7.3461e-02  time: 3.96e+00\n",
      "epoch: 27   trainLoss: 5.1009e-02   valLoss:7.4397e-02  time: 3.98e+00\n",
      "epoch: 28   trainLoss: 5.0295e-02   valLoss:7.5465e-02  time: 3.97e+00\n",
      "epoch: 29   trainLoss: 4.5147e-02   valLoss:6.9746e-02  time: 3.95e+00\n",
      "epoch: 30   trainLoss: 3.9630e-02   valLoss:5.9490e-02  time: 3.91e+00\n",
      "epoch: 31   trainLoss: 4.6983e-02   valLoss:6.1770e-02  time: 3.89e+00\n",
      "epoch: 32   trainLoss: 3.4998e-02   valLoss:6.0959e-02  time: 3.87e+00\n",
      "epoch: 33   trainLoss: 4.5431e-02   valLoss:5.5283e-02  time: 3.85e+00\n",
      "epoch: 34   trainLoss: 4.2889e-02   valLoss:6.7976e-02  time: 3.82e+00\n",
      "epoch: 35   trainLoss: 3.2830e-02   valLoss:6.8740e-02  time: 3.86e+00\n",
      "epoch: 36   trainLoss: 3.6811e-02   valLoss:5.9359e-02  time: 3.84e+00\n",
      "epoch: 37   trainLoss: 4.2431e-02   valLoss:5.5003e-02  time: 3.84e+00\n",
      "epoch: 38   trainLoss: 3.7862e-02   valLoss:5.9771e-02  time: 3.85e+00\n",
      "epoch: 39   trainLoss: 3.4650e-02   valLoss:6.6975e-02  time: 3.82e+00\n",
      "epoch: 40   trainLoss: 3.8092e-02   valLoss:7.3789e-02  time: 3.75e+00\n",
      "epoch: 41   trainLoss: 3.3483e-02   valLoss:7.1443e-02  time: 3.92e+00\n",
      "epoch: 42   trainLoss: 2.6960e-02   valLoss:6.8334e-02  time: 3.78e+00\n",
      "epoch: 43   trainLoss: 3.1561e-02   valLoss:6.1834e-02  time: 3.84e+00\n",
      "epoch: 44   trainLoss: 2.4324e-02   valLoss:5.9457e-02  time: 3.80e+00\n",
      "epoch: 45   trainLoss: 2.2734e-02   valLoss:7.1719e-02  time: 3.76e+00\n",
      "epoch: 46   trainLoss: 3.2514e-02   valLoss:8.3853e-02  time: 3.84e+00\n",
      "epoch: 47   trainLoss: 4.1332e-02   valLoss:4.7912e-02  time: 3.82e+00\n",
      "epoch: 48   trainLoss: 3.1451e-02   valLoss:6.0103e-02  time: 3.88e+00\n",
      "epoch: 49   trainLoss: 2.4733e-02   valLoss:4.5341e-02  time: 3.86e+00\n",
      "epoch: 50   trainLoss: 4.5587e-02   valLoss:6.0633e-02  time: 3.85e+00\n",
      "epoch: 51   trainLoss: 2.8943e-02   valLoss:4.1967e-02  time: 3.93e+00\n",
      "epoch: 52   trainLoss: 2.8832e-02   valLoss:5.1822e-02  time: 3.84e+00\n",
      "epoch: 53   trainLoss: 2.6883e-02   valLoss:5.9473e-02  time: 3.83e+00\n",
      "epoch: 54   trainLoss: 2.8537e-02   valLoss:8.9909e-02  time: 3.85e+00\n",
      "epoch: 55   trainLoss: 3.0882e-02   valLoss:4.7713e-02  time: 3.83e+00\n",
      "epoch: 56   trainLoss: 2.5869e-02   valLoss:5.9350e-02  time: 3.85e+00\n",
      "epoch: 57   trainLoss: 2.5808e-02   valLoss:3.9608e-02  time: 3.96e+00\n",
      "epoch: 58   trainLoss: 2.5401e-02   valLoss:3.8606e-02  time: 3.86e+00\n",
      "epoch: 59   trainLoss: 2.0474e-02   valLoss:3.5492e-02  time: 3.78e+00\n",
      "epoch: 60   trainLoss: 3.8525e-02   valLoss:3.7055e-02  time: 3.79e+00\n",
      "epoch: 61   trainLoss: 3.1158e-02   valLoss:3.7354e-02  time: 3.85e+00\n",
      "epoch: 62   trainLoss: 2.7365e-02   valLoss:5.2354e-02  time: 3.84e+00\n",
      "epoch: 63   trainLoss: 3.1954e-02   valLoss:6.6425e-02  time: 3.85e+00\n",
      "epoch: 64   trainLoss: 2.9979e-02   valLoss:5.6554e-02  time: 3.88e+00\n",
      "epoch: 65   trainLoss: 3.0028e-02   valLoss:4.5225e-02  time: 3.86e+00\n",
      "epoch: 66   trainLoss: 2.5266e-02   valLoss:4.6175e-02  time: 3.81e+00\n",
      "epoch: 67   trainLoss: 3.1261e-02   valLoss:6.7821e-02  time: 3.76e+00\n",
      "epoch: 68   trainLoss: 2.3641e-02   valLoss:6.2681e-02  time: 3.77e+00\n",
      "epoch: 69   trainLoss: 3.0525e-02   valLoss:7.2586e-02  time: 3.77e+00\n",
      "epoch: 70   trainLoss: 5.9867e-02   valLoss:4.9365e-02  time: 3.78e+00\n",
      "epoch: 71   trainLoss: 4.0168e-02   valLoss:6.0761e-02  time: 3.75e+00\n",
      "epoch: 72   trainLoss: 3.2643e-02   valLoss:3.7618e-02  time: 3.76e+00\n",
      "epoch: 73   trainLoss: 3.0170e-02   valLoss:4.6849e-02  time: 3.76e+00\n",
      "epoch: 74   trainLoss: 4.1368e-02   valLoss:2.8472e-02  time: 3.81e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75   trainLoss: 4.2498e-02   valLoss:7.2555e-02  time: 3.82e+00\n",
      "epoch: 76   trainLoss: 4.0295e-02   valLoss:2.8575e-02  time: 3.78e+00\n",
      "epoch: 77   trainLoss: 4.9292e-02   valLoss:8.5648e-02  time: 3.80e+00\n",
      "epoch: 78   trainLoss: 3.0047e-02   valLoss:4.3362e-02  time: 3.76e+00\n",
      "epoch: 79   trainLoss: 3.2247e-02   valLoss:5.6667e-02  time: 3.76e+00\n",
      "epoch: 80   trainLoss: 2.2711e-02   valLoss:7.7137e-02  time: 3.79e+00\n",
      "epoch: 81   trainLoss: 2.2762e-02   valLoss:8.8646e-02  time: 3.76e+00\n",
      "epoch: 82   trainLoss: 2.1204e-02   valLoss:4.6078e-02  time: 3.77e+00\n",
      "epoch: 83   trainLoss: 2.1088e-02   valLoss:3.7021e-02  time: 3.79e+00\n",
      "epoch: 84   trainLoss: 2.3430e-02   valLoss:2.4683e-02  time: 3.76e+00\n",
      "epoch: 85   trainLoss: 1.7431e-02   valLoss:5.4130e-02  time: 3.79e+00\n",
      "epoch: 86   trainLoss: 2.0924e-02   valLoss:3.3905e-02  time: 3.78e+00\n",
      "epoch: 87   trainLoss: 1.8264e-02   valLoss:3.6863e-02  time: 3.83e+00\n",
      "epoch: 88   trainLoss: 1.7208e-02   valLoss:3.9543e-02  time: 3.75e+00\n",
      "epoch: 89   trainLoss: 1.5259e-02   valLoss:3.1999e-02  time: 3.78e+00\n",
      "epoch: 90   trainLoss: 1.5830e-02   valLoss:5.4213e-02  time: 3.97e+00\n",
      "epoch: 91   trainLoss: 2.9020e-02   valLoss:3.6198e-02  time: 3.79e+00\n",
      "epoch: 92   trainLoss: 3.0148e-02   valLoss:6.3550e-02  time: 3.76e+00\n",
      "epoch: 93   trainLoss: 2.5106e-02   valLoss:5.5285e-02  time: 3.78e+00\n",
      "epoch: 94   trainLoss: 2.3135e-02   valLoss:6.1556e-02  time: 3.76e+00\n",
      "epoch: 95   trainLoss: 1.8615e-02   valLoss:5.1268e-02  time: 3.79e+00\n",
      "epoch: 96   trainLoss: 1.9937e-02   valLoss:3.6246e-02  time: 3.79e+00\n",
      "epoch: 97   trainLoss: 4.1850e-02   valLoss:4.2657e-02  time: 3.77e+00\n",
      "epoch: 98   trainLoss: 2.4138e-02   valLoss:4.1290e-02  time: 3.82e+00\n",
      "epoch: 99   trainLoss: 1.5086e-02   valLoss:3.7369e-02  time: 3.82e+00\n",
      "loading checkpoint 84\n",
      "trained 38 random forest models in 16.29 seconds\n",
      "loaded train set of size 178\n",
      "epoch: 0   trainLoss: 9.3918e-01   valLoss:9.7346e-01  time: 1.49e+00\n",
      "epoch: 1   trainLoss: 6.9548e-01   valLoss:9.5392e-01  time: 1.49e+00\n",
      "epoch: 2   trainLoss: 5.5310e-01   valLoss:9.2514e-01  time: 1.49e+00\n",
      "epoch: 3   trainLoss: 4.3903e-01   valLoss:8.7898e-01  time: 1.49e+00\n",
      "epoch: 4   trainLoss: 3.7830e-01   valLoss:8.3616e-01  time: 1.48e+00\n",
      "epoch: 5   trainLoss: 3.3657e-01   valLoss:8.0373e-01  time: 1.50e+00\n",
      "epoch: 6   trainLoss: 3.1127e-01   valLoss:8.0559e-01  time: 1.47e+00\n",
      "epoch: 7   trainLoss: 2.9211e-01   valLoss:8.5673e-01  time: 1.48e+00\n",
      "epoch: 8   trainLoss: 2.6556e-01   valLoss:9.4144e-01  time: 1.48e+00\n",
      "epoch: 9   trainLoss: 2.4541e-01   valLoss:1.0344e+00  time: 1.47e+00\n",
      "epoch: 10   trainLoss: 2.2718e-01   valLoss:1.1007e+00  time: 1.48e+00\n",
      "epoch: 11   trainLoss: 2.0966e-01   valLoss:1.1253e+00  time: 1.47e+00\n",
      "epoch: 12   trainLoss: 2.0057e-01   valLoss:1.1206e+00  time: 1.47e+00\n",
      "epoch: 13   trainLoss: 1.9129e-01   valLoss:1.1222e+00  time: 1.47e+00\n",
      "epoch: 14   trainLoss: 1.8003e-01   valLoss:1.1252e+00  time: 1.49e+00\n",
      "epoch: 15   trainLoss: 1.7325e-01   valLoss:1.1172e+00  time: 1.47e+00\n",
      "epoch: 16   trainLoss: 1.6440e-01   valLoss:1.0704e+00  time: 1.51e+00\n",
      "epoch: 17   trainLoss: 1.5872e-01   valLoss:9.9234e-01  time: 1.48e+00\n",
      "epoch: 18   trainLoss: 1.4965e-01   valLoss:8.5329e-01  time: 1.50e+00\n",
      "epoch: 19   trainLoss: 1.3871e-01   valLoss:7.4036e-01  time: 1.49e+00\n",
      "epoch: 20   trainLoss: 1.2976e-01   valLoss:6.2108e-01  time: 1.47e+00\n",
      "epoch: 21   trainLoss: 1.3087e-01   valLoss:4.9621e-01  time: 1.50e+00\n",
      "epoch: 22   trainLoss: 1.3679e-01   valLoss:5.2602e-01  time: 1.49e+00\n",
      "epoch: 23   trainLoss: 1.2476e-01   valLoss:4.4584e-01  time: 1.48e+00\n",
      "epoch: 24   trainLoss: 1.1005e-01   valLoss:4.0829e-01  time: 1.48e+00\n",
      "epoch: 25   trainLoss: 1.1501e-01   valLoss:4.1206e-01  time: 1.47e+00\n",
      "epoch: 26   trainLoss: 1.0598e-01   valLoss:4.6422e-01  time: 1.47e+00\n",
      "epoch: 27   trainLoss: 1.0550e-01   valLoss:3.2488e-01  time: 1.47e+00\n",
      "epoch: 28   trainLoss: 1.0065e-01   valLoss:3.2112e-01  time: 1.48e+00\n",
      "epoch: 29   trainLoss: 9.1939e-02   valLoss:3.3583e-01  time: 1.48e+00\n",
      "epoch: 30   trainLoss: 8.6948e-02   valLoss:3.3093e-01  time: 1.49e+00\n",
      "epoch: 31   trainLoss: 8.2767e-02   valLoss:2.4877e-01  time: 1.48e+00\n",
      "epoch: 32   trainLoss: 7.8737e-02   valLoss:2.5837e-01  time: 1.50e+00\n",
      "epoch: 33   trainLoss: 7.3990e-02   valLoss:2.8520e-01  time: 1.46e+00\n",
      "epoch: 34   trainLoss: 6.8731e-02   valLoss:2.2106e-01  time: 1.48e+00\n",
      "epoch: 35   trainLoss: 6.4775e-02   valLoss:2.1913e-01  time: 1.49e+00\n",
      "epoch: 36   trainLoss: 6.2671e-02   valLoss:1.9350e-01  time: 1.48e+00\n",
      "epoch: 37   trainLoss: 6.2281e-02   valLoss:2.4675e-01  time: 1.50e+00\n",
      "epoch: 38   trainLoss: 6.1546e-02   valLoss:1.8950e-01  time: 1.49e+00\n",
      "epoch: 39   trainLoss: 6.8660e-02   valLoss:2.8257e-01  time: 1.48e+00\n",
      "epoch: 40   trainLoss: 6.3352e-02   valLoss:2.1241e-01  time: 1.48e+00\n",
      "epoch: 41   trainLoss: 7.2589e-02   valLoss:2.4195e-01  time: 1.48e+00\n",
      "epoch: 42   trainLoss: 6.7245e-02   valLoss:1.8504e-01  time: 1.49e+00\n",
      "epoch: 43   trainLoss: 5.5962e-02   valLoss:1.6843e-01  time: 1.49e+00\n",
      "epoch: 44   trainLoss: 6.2517e-02   valLoss:1.9595e-01  time: 1.51e+00\n",
      "epoch: 45   trainLoss: 4.8541e-02   valLoss:1.8483e-01  time: 1.49e+00\n",
      "epoch: 46   trainLoss: 5.1931e-02   valLoss:1.5153e-01  time: 1.48e+00\n",
      "epoch: 47   trainLoss: 4.9667e-02   valLoss:1.4517e-01  time: 1.49e+00\n",
      "epoch: 48   trainLoss: 4.1364e-02   valLoss:1.7103e-01  time: 1.49e+00\n",
      "epoch: 49   trainLoss: 4.5054e-02   valLoss:1.4054e-01  time: 1.50e+00\n",
      "epoch: 50   trainLoss: 3.7850e-02   valLoss:1.2847e-01  time: 1.46e+00\n",
      "epoch: 51   trainLoss: 3.8564e-02   valLoss:1.3129e-01  time: 1.45e+00\n",
      "epoch: 52   trainLoss: 3.4063e-02   valLoss:1.5767e-01  time: 1.47e+00\n",
      "epoch: 53   trainLoss: 3.6640e-02   valLoss:1.3019e-01  time: 1.48e+00\n",
      "epoch: 54   trainLoss: 3.2276e-02   valLoss:1.3184e-01  time: 1.48e+00\n",
      "epoch: 55   trainLoss: 3.1451e-02   valLoss:1.3904e-01  time: 1.49e+00\n",
      "epoch: 56   trainLoss: 2.8128e-02   valLoss:1.4159e-01  time: 1.48e+00\n",
      "epoch: 57   trainLoss: 2.7033e-02   valLoss:1.2872e-01  time: 1.49e+00\n",
      "epoch: 58   trainLoss: 2.5399e-02   valLoss:1.3590e-01  time: 1.53e+00\n",
      "epoch: 59   trainLoss: 2.6331e-02   valLoss:1.3788e-01  time: 1.50e+00\n",
      "epoch: 60   trainLoss: 2.5485e-02   valLoss:1.3065e-01  time: 1.48e+00\n",
      "epoch: 61   trainLoss: 2.9730e-02   valLoss:1.3983e-01  time: 1.48e+00\n",
      "epoch: 62   trainLoss: 2.4180e-02   valLoss:1.2869e-01  time: 1.46e+00\n",
      "epoch: 63   trainLoss: 2.3088e-02   valLoss:1.4674e-01  time: 1.49e+00\n",
      "epoch: 64   trainLoss: 2.1782e-02   valLoss:1.3989e-01  time: 1.49e+00\n",
      "epoch: 65   trainLoss: 2.4746e-02   valLoss:1.5357e-01  time: 1.48e+00\n",
      "epoch: 66   trainLoss: 2.3908e-02   valLoss:1.2241e-01  time: 1.48e+00\n",
      "epoch: 67   trainLoss: 2.3640e-02   valLoss:1.6281e-01  time: 1.48e+00\n",
      "epoch: 68   trainLoss: 2.0408e-02   valLoss:1.4602e-01  time: 1.48e+00\n",
      "epoch: 69   trainLoss: 1.9760e-02   valLoss:1.4858e-01  time: 1.48e+00\n",
      "epoch: 70   trainLoss: 1.8976e-02   valLoss:1.4416e-01  time: 1.49e+00\n",
      "epoch: 71   trainLoss: 1.5262e-02   valLoss:1.3974e-01  time: 1.47e+00\n",
      "epoch: 72   trainLoss: 1.6983e-02   valLoss:1.4354e-01  time: 1.49e+00\n",
      "epoch: 73   trainLoss: 1.5961e-02   valLoss:1.2422e-01  time: 1.48e+00\n",
      "epoch: 74   trainLoss: 1.4750e-02   valLoss:1.4349e-01  time: 1.48e+00\n",
      "epoch: 75   trainLoss: 1.2613e-02   valLoss:1.3851e-01  time: 1.48e+00\n",
      "epoch: 76   trainLoss: 1.3072e-02   valLoss:1.1738e-01  time: 1.48e+00\n",
      "epoch: 77   trainLoss: 1.2469e-02   valLoss:1.2488e-01  time: 1.48e+00\n",
      "epoch: 78   trainLoss: 1.2299e-02   valLoss:1.2103e-01  time: 1.48e+00\n",
      "epoch: 79   trainLoss: 1.4274e-02   valLoss:1.1853e-01  time: 1.46e+00\n",
      "epoch: 80   trainLoss: 1.6577e-02   valLoss:1.2172e-01  time: 1.46e+00\n",
      "epoch: 81   trainLoss: 1.9786e-02   valLoss:1.2922e-01  time: 1.48e+00\n",
      "epoch: 82   trainLoss: 2.4574e-02   valLoss:1.1637e-01  time: 1.46e+00\n",
      "epoch: 83   trainLoss: 1.4110e-02   valLoss:1.0926e-01  time: 1.48e+00\n",
      "epoch: 84   trainLoss: 1.3735e-02   valLoss:1.1559e-01  time: 1.49e+00\n",
      "epoch: 85   trainLoss: 1.6915e-02   valLoss:1.2014e-01  time: 1.48e+00\n",
      "epoch: 86   trainLoss: 1.5757e-02   valLoss:1.0201e-01  time: 1.52e+00\n",
      "epoch: 87   trainLoss: 1.0402e-02   valLoss:1.1314e-01  time: 1.51e+00\n",
      "epoch: 88   trainLoss: 1.5940e-02   valLoss:1.2287e-01  time: 1.46e+00\n",
      "epoch: 89   trainLoss: 1.3298e-02   valLoss:1.1866e-01  time: 1.49e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90   trainLoss: 1.6036e-02   valLoss:1.4609e-01  time: 1.49e+00\n",
      "epoch: 91   trainLoss: 2.6742e-02   valLoss:1.3630e-01  time: 1.46e+00\n",
      "epoch: 92   trainLoss: 2.9090e-02   valLoss:1.4787e-01  time: 1.51e+00\n",
      "epoch: 93   trainLoss: 2.5409e-02   valLoss:1.1297e-01  time: 1.46e+00\n",
      "epoch: 94   trainLoss: 1.6280e-02   valLoss:1.0799e-01  time: 1.48e+00\n",
      "epoch: 95   trainLoss: 2.4287e-02   valLoss:1.2555e-01  time: 1.48e+00\n",
      "epoch: 96   trainLoss: 2.1451e-02   valLoss:1.1472e-01  time: 1.46e+00\n",
      "epoch: 97   trainLoss: 1.7650e-02   valLoss:1.1069e-01  time: 1.48e+00\n",
      "epoch: 98   trainLoss: 1.6283e-02   valLoss:9.7675e-02  time: 1.48e+00\n",
      "epoch: 99   trainLoss: 1.5711e-02   valLoss:1.0543e-01  time: 1.47e+00\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 8.4991e-01   valLoss:2.6206e+00  time: 1.52e+00\n",
      "epoch: 1   trainLoss: 6.1032e-01   valLoss:1.3816e+00  time: 1.47e+00\n",
      "epoch: 2   trainLoss: 4.3540e-01   valLoss:7.8752e-01  time: 1.46e+00\n",
      "epoch: 3   trainLoss: 3.0398e-01   valLoss:7.9010e-01  time: 1.47e+00\n",
      "epoch: 4   trainLoss: 3.0195e-01   valLoss:1.9359e+00  time: 1.50e+00\n",
      "epoch: 5   trainLoss: 3.1914e-01   valLoss:1.4104e+00  time: 1.48e+00\n",
      "epoch: 6   trainLoss: 2.1126e-01   valLoss:1.1595e+00  time: 1.47e+00\n",
      "epoch: 7   trainLoss: 1.9263e-01   valLoss:5.2751e-01  time: 1.48e+00\n",
      "epoch: 8   trainLoss: 1.7091e-01   valLoss:2.3940e-01  time: 1.48e+00\n",
      "epoch: 9   trainLoss: 1.5513e-01   valLoss:2.4192e-01  time: 1.48e+00\n",
      "epoch: 10   trainLoss: 1.5090e-01   valLoss:2.8090e-01  time: 1.47e+00\n",
      "epoch: 11   trainLoss: 1.2741e-01   valLoss:3.1105e-01  time: 1.48e+00\n",
      "epoch: 12   trainLoss: 1.3067e-01   valLoss:3.1779e-01  time: 1.48e+00\n",
      "epoch: 13   trainLoss: 1.2254e-01   valLoss:2.4642e-01  time: 1.49e+00\n",
      "epoch: 14   trainLoss: 1.0053e-01   valLoss:1.9487e-01  time: 1.48e+00\n",
      "epoch: 15   trainLoss: 1.0227e-01   valLoss:1.8042e-01  time: 1.51e+00\n",
      "epoch: 16   trainLoss: 9.5454e-02   valLoss:1.9507e-01  time: 1.48e+00\n",
      "epoch: 17   trainLoss: 8.9168e-02   valLoss:1.8349e-01  time: 1.47e+00\n",
      "epoch: 18   trainLoss: 8.1203e-02   valLoss:1.2795e-01  time: 1.49e+00\n",
      "epoch: 19   trainLoss: 7.2634e-02   valLoss:9.9587e-02  time: 1.49e+00\n",
      "epoch: 20   trainLoss: 7.5639e-02   valLoss:9.2214e-02  time: 1.65e+00\n",
      "epoch: 21   trainLoss: 7.0569e-02   valLoss:9.7073e-02  time: 1.54e+00\n",
      "epoch: 22   trainLoss: 6.4618e-02   valLoss:1.0696e-01  time: 1.49e+00\n",
      "epoch: 23   trainLoss: 6.2199e-02   valLoss:1.1264e-01  time: 1.48e+00\n",
      "epoch: 24   trainLoss: 5.9614e-02   valLoss:9.6235e-02  time: 1.48e+00\n",
      "epoch: 25   trainLoss: 5.7578e-02   valLoss:7.6144e-02  time: 1.49e+00\n",
      "epoch: 26   trainLoss: 5.4858e-02   valLoss:7.4616e-02  time: 1.48e+00\n",
      "epoch: 27   trainLoss: 5.4360e-02   valLoss:9.0307e-02  time: 1.50e+00\n",
      "epoch: 28   trainLoss: 5.1547e-02   valLoss:1.1162e-01  time: 1.48e+00\n",
      "epoch: 29   trainLoss: 4.8140e-02   valLoss:1.0267e-01  time: 1.45e+00\n",
      "epoch: 30   trainLoss: 4.6400e-02   valLoss:8.5204e-02  time: 1.48e+00\n",
      "epoch: 31   trainLoss: 4.4851e-02   valLoss:7.8441e-02  time: 1.49e+00\n",
      "epoch: 32   trainLoss: 4.2289e-02   valLoss:7.7010e-02  time: 1.45e+00\n",
      "epoch: 33   trainLoss: 4.0501e-02   valLoss:6.9362e-02  time: 1.47e+00\n",
      "epoch: 34   trainLoss: 3.9146e-02   valLoss:6.6332e-02  time: 1.46e+00\n",
      "epoch: 35   trainLoss: 3.7682e-02   valLoss:6.6352e-02  time: 1.46e+00\n",
      "epoch: 36   trainLoss: 3.6464e-02   valLoss:6.5185e-02  time: 1.46e+00\n",
      "epoch: 37   trainLoss: 3.4466e-02   valLoss:5.8063e-02  time: 1.46e+00\n",
      "epoch: 38   trainLoss: 3.3250e-02   valLoss:4.7223e-02  time: 1.46e+00\n",
      "epoch: 39   trainLoss: 3.2323e-02   valLoss:4.5970e-02  time: 1.48e+00\n",
      "epoch: 40   trainLoss: 3.0929e-02   valLoss:4.6650e-02  time: 1.49e+00\n",
      "epoch: 41   trainLoss: 2.9717e-02   valLoss:4.3885e-02  time: 1.48e+00\n",
      "epoch: 42   trainLoss: 2.8487e-02   valLoss:4.3418e-02  time: 1.52e+00\n",
      "epoch: 43   trainLoss: 2.7358e-02   valLoss:4.6333e-02  time: 1.51e+00\n",
      "epoch: 44   trainLoss: 2.6385e-02   valLoss:5.3357e-02  time: 1.51e+00\n",
      "epoch: 45   trainLoss: 2.5596e-02   valLoss:5.4883e-02  time: 1.48e+00\n",
      "epoch: 46   trainLoss: 2.4420e-02   valLoss:6.0044e-02  time: 1.48e+00\n",
      "epoch: 47   trainLoss: 2.3451e-02   valLoss:6.6791e-02  time: 1.48e+00\n",
      "epoch: 48   trainLoss: 2.2736e-02   valLoss:7.2140e-02  time: 1.47e+00\n",
      "epoch: 49   trainLoss: 2.1761e-02   valLoss:7.3712e-02  time: 1.50e+00\n",
      "epoch: 50   trainLoss: 2.1094e-02   valLoss:7.2778e-02  time: 1.53e+00\n",
      "epoch: 51   trainLoss: 2.0506e-02   valLoss:6.7421e-02  time: 1.51e+00\n",
      "epoch: 52   trainLoss: 1.9585e-02   valLoss:6.0518e-02  time: 1.52e+00\n",
      "epoch: 53   trainLoss: 1.9036e-02   valLoss:7.0546e-02  time: 1.49e+00\n",
      "epoch: 54   trainLoss: 1.9061e-02   valLoss:5.4936e-02  time: 1.49e+00\n",
      "epoch: 55   trainLoss: 2.3066e-02   valLoss:1.2732e-01  time: 1.51e+00\n",
      "epoch: 56   trainLoss: 5.1125e-02   valLoss:1.9651e-01  time: 1.48e+00\n",
      "epoch: 57   trainLoss: 8.6537e-02   valLoss:7.4549e-02  time: 1.48e+00\n",
      "epoch: 58   trainLoss: 4.0165e-02   valLoss:1.1782e-01  time: 1.47e+00\n",
      "epoch: 59   trainLoss: 5.7799e-02   valLoss:7.3790e-02  time: 1.48e+00\n",
      "epoch: 60   trainLoss: 2.7624e-02   valLoss:9.4271e-02  time: 1.49e+00\n",
      "epoch: 61   trainLoss: 4.9296e-02   valLoss:4.6974e-02  time: 1.51e+00\n",
      "epoch: 62   trainLoss: 2.8161e-02   valLoss:6.5734e-02  time: 1.52e+00\n",
      "epoch: 63   trainLoss: 3.7462e-02   valLoss:6.2391e-02  time: 1.57e+00\n",
      "epoch: 64   trainLoss: 3.2190e-02   valLoss:5.3352e-02  time: 1.52e+00\n",
      "epoch: 65   trainLoss: 2.9428e-02   valLoss:5.3006e-02  time: 1.53e+00\n",
      "epoch: 66   trainLoss: 3.0187e-02   valLoss:4.2215e-02  time: 1.49e+00\n",
      "epoch: 67   trainLoss: 2.6294e-02   valLoss:4.6030e-02  time: 1.49e+00\n",
      "epoch: 68   trainLoss: 2.9189e-02   valLoss:5.0856e-02  time: 1.47e+00\n",
      "epoch: 69   trainLoss: 2.3288e-02   valLoss:5.7455e-02  time: 1.52e+00\n",
      "epoch: 70   trainLoss: 2.4629e-02   valLoss:5.6881e-02  time: 1.53e+00\n",
      "epoch: 71   trainLoss: 2.4700e-02   valLoss:4.4893e-02  time: 1.51e+00\n",
      "epoch: 72   trainLoss: 2.0756e-02   valLoss:4.7926e-02  time: 1.51e+00\n",
      "epoch: 73   trainLoss: 2.1493e-02   valLoss:4.8413e-02  time: 1.52e+00\n",
      "epoch: 74   trainLoss: 2.1209e-02   valLoss:4.7035e-02  time: 1.54e+00\n",
      "epoch: 75   trainLoss: 1.7990e-02   valLoss:5.2946e-02  time: 1.48e+00\n",
      "epoch: 76   trainLoss: 1.9263e-02   valLoss:5.7885e-02  time: 1.50e+00\n",
      "epoch: 77   trainLoss: 1.7538e-02   valLoss:4.3675e-02  time: 1.47e+00\n",
      "epoch: 78   trainLoss: 1.7221e-02   valLoss:3.8060e-02  time: 1.47e+00\n",
      "epoch: 79   trainLoss: 1.5894e-02   valLoss:5.1460e-02  time: 1.52e+00\n",
      "epoch: 80   trainLoss: 1.6113e-02   valLoss:4.2932e-02  time: 1.54e+00\n",
      "epoch: 81   trainLoss: 1.5071e-02   valLoss:3.4318e-02  time: 1.52e+00\n",
      "epoch: 82   trainLoss: 1.4262e-02   valLoss:3.2265e-02  time: 1.52e+00\n",
      "epoch: 83   trainLoss: 1.4331e-02   valLoss:3.3259e-02  time: 1.48e+00\n",
      "epoch: 84   trainLoss: 1.3387e-02   valLoss:4.2176e-02  time: 1.47e+00\n",
      "epoch: 85   trainLoss: 1.2831e-02   valLoss:5.3949e-02  time: 1.49e+00\n",
      "epoch: 86   trainLoss: 1.2593e-02   valLoss:5.9753e-02  time: 1.48e+00\n",
      "epoch: 87   trainLoss: 1.2123e-02   valLoss:6.5042e-02  time: 1.49e+00\n",
      "epoch: 88   trainLoss: 1.1571e-02   valLoss:7.1416e-02  time: 1.52e+00\n",
      "epoch: 89   trainLoss: 1.1328e-02   valLoss:6.9703e-02  time: 1.49e+00\n",
      "epoch: 90   trainLoss: 1.0795e-02   valLoss:6.8976e-02  time: 1.48e+00\n",
      "epoch: 91   trainLoss: 1.0374e-02   valLoss:6.2207e-02  time: 1.49e+00\n",
      "epoch: 92   trainLoss: 9.9396e-03   valLoss:5.1476e-02  time: 1.50e+00\n",
      "epoch: 93   trainLoss: 9.8610e-03   valLoss:4.2517e-02  time: 1.48e+00\n",
      "epoch: 94   trainLoss: 9.2647e-03   valLoss:3.5730e-02  time: 1.47e+00\n",
      "epoch: 95   trainLoss: 9.1609e-03   valLoss:3.5045e-02  time: 1.47e+00\n",
      "epoch: 96   trainLoss: 9.0771e-03   valLoss:3.1556e-02  time: 1.49e+00\n",
      "epoch: 97   trainLoss: 8.9979e-03   valLoss:3.3354e-02  time: 1.48e+00\n",
      "epoch: 98   trainLoss: 9.1596e-03   valLoss:4.1138e-02  time: 1.49e+00\n",
      "epoch: 99   trainLoss: 1.0437e-02   valLoss:4.4425e-02  time: 1.49e+00\n",
      "loading checkpoint 96\n",
      "trained 38 random forest models in 8.25 seconds\n",
      "loaded train set of size 5\n",
      "epoch: 0   trainLoss: 1.0570e+00   valLoss:1.0196e+00  time: 6.06e-02\n",
      "epoch: 1   trainLoss: 8.0684e-01   valLoss:1.0022e+00  time: 6.08e-02\n",
      "epoch: 2   trainLoss: 6.6954e-01   valLoss:9.6794e-01  time: 6.10e-02\n",
      "epoch: 3   trainLoss: 5.0861e-01   valLoss:9.2816e-01  time: 6.23e-02\n",
      "epoch: 4   trainLoss: 3.9856e-01   valLoss:8.8271e-01  time: 6.24e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5   trainLoss: 2.8230e-01   valLoss:8.3632e-01  time: 6.07e-02\n",
      "epoch: 6   trainLoss: 2.0035e-01   valLoss:7.9273e-01  time: 6.07e-02\n",
      "epoch: 7   trainLoss: 1.6168e-01   valLoss:7.9294e-01  time: 6.04e-02\n",
      "epoch: 8   trainLoss: 1.2785e-01   valLoss:9.3100e-01  time: 6.33e-02\n",
      "epoch: 9   trainLoss: 1.1484e-01   valLoss:1.3123e+00  time: 6.08e-02\n",
      "epoch: 10   trainLoss: 1.0926e-01   valLoss:1.9378e+00  time: 5.96e-02\n",
      "epoch: 11   trainLoss: 1.0306e-01   valLoss:2.6961e+00  time: 6.03e-02\n",
      "epoch: 12   trainLoss: 9.2734e-02   valLoss:3.3812e+00  time: 5.97e-02\n",
      "epoch: 13   trainLoss: 8.1873e-02   valLoss:3.8611e+00  time: 6.07e-02\n",
      "epoch: 14   trainLoss: 7.2576e-02   valLoss:4.1298e+00  time: 6.00e-02\n",
      "epoch: 15   trainLoss: 6.5745e-02   valLoss:4.1513e+00  time: 6.21e-02\n",
      "epoch: 16   trainLoss: 6.1209e-02   valLoss:3.9111e+00  time: 5.95e-02\n",
      "epoch: 17   trainLoss: 5.7757e-02   valLoss:3.4549e+00  time: 6.19e-02\n",
      "epoch: 18   trainLoss: 5.4051e-02   valLoss:2.8610e+00  time: 5.98e-02\n",
      "epoch: 19   trainLoss: 5.0140e-02   valLoss:2.2348e+00  time: 6.17e-02\n",
      "epoch: 20   trainLoss: 4.6823e-02   valLoss:1.6361e+00  time: 5.99e-02\n",
      "epoch: 21   trainLoss: 4.3971e-02   valLoss:1.1076e+00  time: 6.23e-02\n",
      "epoch: 22   trainLoss: 4.1180e-02   valLoss:7.1001e-01  time: 6.07e-02\n",
      "epoch: 23   trainLoss: 3.7891e-02   valLoss:4.4311e-01  time: 6.58e-02\n",
      "epoch: 24   trainLoss: 3.4128e-02   valLoss:2.7535e-01  time: 6.20e-02\n",
      "epoch: 25   trainLoss: 3.0057e-02   valLoss:1.7499e-01  time: 6.08e-02\n",
      "epoch: 26   trainLoss: 2.5874e-02   valLoss:1.2502e-01  time: 6.06e-02\n",
      "epoch: 27   trainLoss: 2.1961e-02   valLoss:1.0045e-01  time: 6.21e-02\n",
      "epoch: 28   trainLoss: 1.8207e-02   valLoss:8.5090e-02  time: 6.08e-02\n",
      "epoch: 29   trainLoss: 1.5329e-02   valLoss:8.5030e-02  time: 6.05e-02\n",
      "epoch: 30   trainLoss: 1.3173e-02   valLoss:8.8964e-02  time: 6.12e-02\n",
      "epoch: 31   trainLoss: 1.1934e-02   valLoss:9.6053e-02  time: 6.01e-02\n",
      "epoch: 32   trainLoss: 9.7803e-03   valLoss:9.9271e-02  time: 6.09e-02\n",
      "epoch: 33   trainLoss: 9.2286e-03   valLoss:9.9709e-02  time: 6.02e-02\n",
      "epoch: 34   trainLoss: 7.6561e-03   valLoss:9.9526e-02  time: 6.14e-02\n",
      "epoch: 35   trainLoss: 6.8733e-03   valLoss:9.7405e-02  time: 5.99e-02\n",
      "epoch: 36   trainLoss: 6.2488e-03   valLoss:9.5553e-02  time: 6.04e-02\n",
      "epoch: 37   trainLoss: 5.8431e-03   valLoss:9.6471e-02  time: 6.12e-02\n",
      "epoch: 38   trainLoss: 5.3646e-03   valLoss:9.9905e-02  time: 6.05e-02\n",
      "epoch: 39   trainLoss: 4.5752e-03   valLoss:1.0200e-01  time: 5.98e-02\n",
      "epoch: 40   trainLoss: 3.8780e-03   valLoss:1.0076e-01  time: 6.21e-02\n",
      "epoch: 41   trainLoss: 3.4133e-03   valLoss:9.7611e-02  time: 6.07e-02\n",
      "epoch: 42   trainLoss: 3.1434e-03   valLoss:9.5059e-02  time: 6.05e-02\n",
      "epoch: 43   trainLoss: 2.6868e-03   valLoss:9.2203e-02  time: 6.08e-02\n",
      "epoch: 44   trainLoss: 2.4387e-03   valLoss:8.7840e-02  time: 6.01e-02\n",
      "epoch: 45   trainLoss: 2.1585e-03   valLoss:8.3584e-02  time: 6.64e-02\n",
      "epoch: 46   trainLoss: 1.9553e-03   valLoss:8.0984e-02  time: 6.19e-02\n",
      "epoch: 47   trainLoss: 1.5954e-03   valLoss:7.8285e-02  time: 6.42e-02\n",
      "epoch: 48   trainLoss: 1.3578e-03   valLoss:7.5324e-02  time: 6.05e-02\n",
      "epoch: 49   trainLoss: 1.1000e-03   valLoss:7.3969e-02  time: 6.04e-02\n",
      "epoch: 50   trainLoss: 9.5704e-04   valLoss:7.3078e-02  time: 6.11e-02\n",
      "epoch: 51   trainLoss: 8.4442e-04   valLoss:7.0618e-02  time: 6.19e-02\n",
      "epoch: 52   trainLoss: 7.5908e-04   valLoss:6.7772e-02  time: 6.13e-02\n",
      "epoch: 53   trainLoss: 6.2653e-04   valLoss:6.6263e-02  time: 6.11e-02\n",
      "epoch: 54   trainLoss: 5.5327e-04   valLoss:6.5369e-02  time: 6.02e-02\n",
      "epoch: 55   trainLoss: 4.8930e-04   valLoss:6.4090e-02  time: 6.15e-02\n",
      "epoch: 56   trainLoss: 4.5649e-04   valLoss:6.2198e-02  time: 6.06e-02\n",
      "epoch: 57   trainLoss: 4.1947e-04   valLoss:6.0584e-02  time: 6.07e-02\n",
      "epoch: 58   trainLoss: 3.5731e-04   valLoss:5.9215e-02  time: 6.07e-02\n",
      "epoch: 59   trainLoss: 3.1315e-04   valLoss:5.7514e-02  time: 6.06e-02\n",
      "epoch: 60   trainLoss: 2.7726e-04   valLoss:5.5805e-02  time: 6.09e-02\n",
      "epoch: 61   trainLoss: 2.4389e-04   valLoss:5.4615e-02  time: 5.99e-02\n",
      "epoch: 62   trainLoss: 2.2914e-04   valLoss:5.3644e-02  time: 6.22e-02\n",
      "epoch: 63   trainLoss: 2.1293e-04   valLoss:5.2791e-02  time: 6.05e-02\n",
      "epoch: 64   trainLoss: 2.0643e-04   valLoss:5.2054e-02  time: 6.05e-02\n",
      "epoch: 65   trainLoss: 1.9256e-04   valLoss:5.1302e-02  time: 6.02e-02\n",
      "epoch: 66   trainLoss: 1.7731e-04   valLoss:5.0632e-02  time: 6.06e-02\n",
      "epoch: 67   trainLoss: 1.6236e-04   valLoss:4.9968e-02  time: 6.07e-02\n",
      "epoch: 68   trainLoss: 1.3894e-04   valLoss:4.8872e-02  time: 6.14e-02\n",
      "epoch: 69   trainLoss: 1.1846e-04   valLoss:4.7835e-02  time: 6.25e-02\n",
      "epoch: 70   trainLoss: 1.0494e-04   valLoss:4.7238e-02  time: 6.13e-02\n",
      "epoch: 71   trainLoss: 1.0114e-04   valLoss:4.6486e-02  time: 6.04e-02\n",
      "epoch: 72   trainLoss: 9.6344e-05   valLoss:4.5692e-02  time: 6.04e-02\n",
      "epoch: 73   trainLoss: 8.4296e-05   valLoss:4.5249e-02  time: 6.47e-02\n",
      "epoch: 74   trainLoss: 8.0843e-05   valLoss:4.4548e-02  time: 6.08e-02\n",
      "epoch: 75   trainLoss: 7.0883e-05   valLoss:4.3998e-02  time: 6.10e-02\n",
      "epoch: 76   trainLoss: 6.2530e-05   valLoss:4.3525e-02  time: 6.13e-02\n",
      "epoch: 77   trainLoss: 6.1924e-05   valLoss:4.3044e-02  time: 6.31e-02\n",
      "epoch: 78   trainLoss: 6.9424e-05   valLoss:4.3066e-02  time: 6.12e-02\n",
      "epoch: 79   trainLoss: 8.7333e-05   valLoss:4.2655e-02  time: 6.83e-02\n",
      "epoch: 80   trainLoss: 1.2893e-04   valLoss:4.2390e-02  time: 6.28e-02\n",
      "epoch: 81   trainLoss: 3.2297e-04   valLoss:4.2379e-02  time: 6.08e-02\n",
      "epoch: 82   trainLoss: 9.4878e-04   valLoss:4.2160e-02  time: 6.06e-02\n",
      "epoch: 83   trainLoss: 2.1857e-03   valLoss:4.0755e-02  time: 6.18e-02\n",
      "epoch: 84   trainLoss: 2.2423e-03   valLoss:4.2319e-02  time: 6.27e-02\n",
      "epoch: 85   trainLoss: 1.2524e-03   valLoss:3.7293e-02  time: 5.99e-02\n",
      "epoch: 86   trainLoss: 1.6964e-03   valLoss:3.8814e-02  time: 6.08e-02\n",
      "epoch: 87   trainLoss: 1.8728e-03   valLoss:3.8631e-02  time: 6.01e-02\n",
      "epoch: 88   trainLoss: 1.1389e-03   valLoss:3.7731e-02  time: 6.07e-02\n",
      "epoch: 89   trainLoss: 7.2472e-04   valLoss:3.8085e-02  time: 6.00e-02\n",
      "epoch: 90   trainLoss: 8.0402e-04   valLoss:3.7582e-02  time: 5.99e-02\n",
      "epoch: 91   trainLoss: 9.3629e-04   valLoss:3.6903e-02  time: 5.97e-02\n",
      "epoch: 92   trainLoss: 3.2484e-04   valLoss:3.8385e-02  time: 6.24e-02\n",
      "epoch: 93   trainLoss: 6.5789e-04   valLoss:3.7400e-02  time: 6.09e-02\n",
      "epoch: 94   trainLoss: 3.3946e-04   valLoss:3.6760e-02  time: 6.09e-02\n",
      "epoch: 95   trainLoss: 4.5857e-04   valLoss:3.7493e-02  time: 6.10e-02\n",
      "epoch: 96   trainLoss: 2.8209e-04   valLoss:3.7531e-02  time: 6.00e-02\n",
      "epoch: 97   trainLoss: 3.3263e-04   valLoss:3.7444e-02  time: 5.98e-02\n",
      "epoch: 98   trainLoss: 2.9499e-04   valLoss:3.6852e-02  time: 5.97e-02\n",
      "epoch: 99   trainLoss: 2.9757e-04   valLoss:3.7245e-02  time: 5.98e-02\n",
      "loading checkpoint 94\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 1.4244e+00   valLoss:3.9222e+00  time: 6.09e-02\n",
      "epoch: 1   trainLoss: 1.3688e+00   valLoss:3.8229e+00  time: 6.10e-02\n",
      "epoch: 2   trainLoss: 9.3844e-01   valLoss:3.1245e+00  time: 6.10e-02\n",
      "epoch: 3   trainLoss: 7.5100e-01   valLoss:1.3577e+00  time: 6.06e-02\n",
      "epoch: 4   trainLoss: 5.2466e-01   valLoss:9.5868e-01  time: 6.03e-02\n",
      "epoch: 5   trainLoss: 4.1726e-01   valLoss:8.5735e-01  time: 6.21e-02\n",
      "epoch: 6   trainLoss: 2.9168e-01   valLoss:9.4237e-01  time: 6.03e-02\n",
      "epoch: 7   trainLoss: 2.1150e-01   valLoss:1.0178e+00  time: 5.94e-02\n",
      "epoch: 8   trainLoss: 1.4939e-01   valLoss:9.3834e-01  time: 6.02e-02\n",
      "epoch: 9   trainLoss: 1.1677e-01   valLoss:1.2617e+00  time: 5.97e-02\n",
      "epoch: 10   trainLoss: 1.0773e-01   valLoss:7.0072e-01  time: 5.96e-02\n",
      "epoch: 11   trainLoss: 1.1048e-01   valLoss:8.8749e-01  time: 6.24e-02\n",
      "epoch: 12   trainLoss: 9.5359e-02   valLoss:4.3219e-01  time: 6.09e-02\n",
      "epoch: 13   trainLoss: 9.6842e-02   valLoss:4.1657e-01  time: 6.13e-02\n",
      "epoch: 14   trainLoss: 9.4106e-02   valLoss:3.6860e-01  time: 6.06e-02\n",
      "epoch: 15   trainLoss: 9.1975e-02   valLoss:3.4922e-01  time: 6.12e-02\n",
      "epoch: 16   trainLoss: 7.6963e-02   valLoss:2.6918e-01  time: 6.07e-02\n",
      "epoch: 17   trainLoss: 7.6885e-02   valLoss:1.4393e-01  time: 6.31e-02\n",
      "epoch: 18   trainLoss: 6.4050e-02   valLoss:1.0107e-01  time: 6.09e-02\n",
      "epoch: 19   trainLoss: 6.3376e-02   valLoss:7.1199e-02  time: 6.14e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20   trainLoss: 5.6506e-02   valLoss:6.0472e-02  time: 6.03e-02\n",
      "epoch: 21   trainLoss: 4.8453e-02   valLoss:5.4196e-02  time: 6.08e-02\n",
      "epoch: 22   trainLoss: 4.0987e-02   valLoss:4.5928e-02  time: 6.11e-02\n",
      "epoch: 23   trainLoss: 3.5828e-02   valLoss:3.7030e-02  time: 6.24e-02\n",
      "epoch: 24   trainLoss: 3.0145e-02   valLoss:3.4564e-02  time: 6.04e-02\n",
      "epoch: 25   trainLoss: 2.6526e-02   valLoss:3.5238e-02  time: 6.12e-02\n",
      "epoch: 26   trainLoss: 2.4633e-02   valLoss:3.2196e-02  time: 6.18e-02\n",
      "epoch: 27   trainLoss: 2.2208e-02   valLoss:3.3663e-02  time: 6.06e-02\n",
      "epoch: 28   trainLoss: 2.0053e-02   valLoss:3.8844e-02  time: 6.01e-02\n",
      "epoch: 29   trainLoss: 1.7436e-02   valLoss:4.0048e-02  time: 6.21e-02\n",
      "epoch: 30   trainLoss: 1.4874e-02   valLoss:3.5052e-02  time: 6.02e-02\n",
      "epoch: 31   trainLoss: 1.3023e-02   valLoss:2.9011e-02  time: 5.98e-02\n",
      "epoch: 32   trainLoss: 1.2097e-02   valLoss:2.4310e-02  time: 7.16e-02\n",
      "epoch: 33   trainLoss: 1.1677e-02   valLoss:2.0694e-02  time: 6.20e-02\n",
      "epoch: 34   trainLoss: 9.8552e-03   valLoss:2.1658e-02  time: 6.20e-02\n",
      "epoch: 35   trainLoss: 9.0319e-03   valLoss:1.8197e-02  time: 6.26e-02\n",
      "epoch: 36   trainLoss: 8.0214e-03   valLoss:1.7175e-02  time: 6.14e-02\n",
      "epoch: 37   trainLoss: 7.4497e-03   valLoss:1.9029e-02  time: 6.13e-02\n",
      "epoch: 38   trainLoss: 7.3728e-03   valLoss:2.0040e-02  time: 6.04e-02\n",
      "epoch: 39   trainLoss: 7.1599e-03   valLoss:2.2441e-02  time: 6.13e-02\n",
      "epoch: 40   trainLoss: 6.7301e-03   valLoss:2.1035e-02  time: 6.03e-02\n",
      "epoch: 41   trainLoss: 6.4197e-03   valLoss:2.2433e-02  time: 6.21e-02\n",
      "epoch: 42   trainLoss: 6.1632e-03   valLoss:2.2292e-02  time: 6.18e-02\n",
      "epoch: 43   trainLoss: 5.4285e-03   valLoss:2.2181e-02  time: 6.13e-02\n",
      "epoch: 44   trainLoss: 5.1229e-03   valLoss:2.2575e-02  time: 6.05e-02\n",
      "epoch: 45   trainLoss: 4.6436e-03   valLoss:2.2888e-02  time: 6.06e-02\n",
      "epoch: 46   trainLoss: 4.6101e-03   valLoss:2.1671e-02  time: 6.06e-02\n",
      "epoch: 47   trainLoss: 4.3218e-03   valLoss:2.3030e-02  time: 6.01e-02\n",
      "epoch: 48   trainLoss: 4.0993e-03   valLoss:2.3448e-02  time: 6.14e-02\n",
      "epoch: 49   trainLoss: 3.8451e-03   valLoss:2.3762e-02  time: 6.05e-02\n",
      "epoch: 50   trainLoss: 3.5449e-03   valLoss:2.5508e-02  time: 6.09e-02\n",
      "epoch: 51   trainLoss: 3.2994e-03   valLoss:2.7739e-02  time: 6.03e-02\n",
      "epoch: 52   trainLoss: 3.1564e-03   valLoss:2.8950e-02  time: 6.01e-02\n",
      "epoch: 53   trainLoss: 3.0300e-03   valLoss:3.0322e-02  time: 6.04e-02\n",
      "epoch: 54   trainLoss: 2.9701e-03   valLoss:3.2179e-02  time: 6.14e-02\n",
      "epoch: 55   trainLoss: 2.8762e-03   valLoss:3.5880e-02  time: 5.97e-02\n",
      "epoch: 56   trainLoss: 2.8024e-03   valLoss:3.5805e-02  time: 6.20e-02\n",
      "epoch: 57   trainLoss: 2.7591e-03   valLoss:3.6871e-02  time: 5.96e-02\n",
      "epoch: 58   trainLoss: 2.7203e-03   valLoss:3.7246e-02  time: 6.13e-02\n",
      "epoch: 59   trainLoss: 2.7096e-03   valLoss:3.7626e-02  time: 6.10e-02\n",
      "epoch: 60   trainLoss: 2.7315e-03   valLoss:3.6417e-02  time: 6.06e-02\n",
      "epoch: 61   trainLoss: 2.9206e-03   valLoss:3.9571e-02  time: 5.97e-02\n",
      "epoch: 62   trainLoss: 3.1000e-03   valLoss:3.7747e-02  time: 6.12e-02\n",
      "epoch: 63   trainLoss: 3.4777e-03   valLoss:4.0057e-02  time: 6.01e-02\n",
      "epoch: 64   trainLoss: 3.6358e-03   valLoss:3.6407e-02  time: 6.06e-02\n",
      "epoch: 65   trainLoss: 3.7696e-03   valLoss:4.0785e-02  time: 6.00e-02\n",
      "epoch: 66   trainLoss: 3.1335e-03   valLoss:3.4927e-02  time: 5.98e-02\n",
      "epoch: 67   trainLoss: 2.3143e-03   valLoss:3.5335e-02  time: 6.24e-02\n",
      "epoch: 68   trainLoss: 1.7367e-03   valLoss:3.7085e-02  time: 6.05e-02\n",
      "epoch: 69   trainLoss: 1.8307e-03   valLoss:3.7112e-02  time: 6.07e-02\n",
      "epoch: 70   trainLoss: 2.2793e-03   valLoss:4.2643e-02  time: 6.00e-02\n",
      "epoch: 71   trainLoss: 2.3659e-03   valLoss:4.1178e-02  time: 5.96e-02\n",
      "epoch: 72   trainLoss: 2.0391e-03   valLoss:4.3373e-02  time: 6.06e-02\n",
      "epoch: 73   trainLoss: 1.5773e-03   valLoss:4.3429e-02  time: 6.06e-02\n",
      "epoch: 74   trainLoss: 1.4550e-03   valLoss:4.1394e-02  time: 6.01e-02\n",
      "epoch: 75   trainLoss: 1.6622e-03   valLoss:4.4596e-02  time: 6.63e-02\n",
      "epoch: 76   trainLoss: 1.8185e-03   valLoss:4.2950e-02  time: 6.54e-02\n",
      "epoch: 77   trainLoss: 1.7222e-03   valLoss:4.3491e-02  time: 6.10e-02\n",
      "epoch: 78   trainLoss: 1.4486e-03   valLoss:4.5643e-02  time: 6.12e-02\n",
      "epoch: 79   trainLoss: 1.2661e-03   valLoss:4.4081e-02  time: 6.08e-02\n",
      "epoch: 80   trainLoss: 1.2604e-03   valLoss:4.8620e-02  time: 5.99e-02\n",
      "epoch: 81   trainLoss: 1.3578e-03   valLoss:4.6847e-02  time: 6.08e-02\n",
      "epoch: 82   trainLoss: 1.4559e-03   valLoss:4.8980e-02  time: 6.02e-02\n",
      "epoch: 83   trainLoss: 1.4863e-03   valLoss:4.8639e-02  time: 6.11e-02\n",
      "epoch: 84   trainLoss: 1.5045e-03   valLoss:4.6771e-02  time: 6.11e-02\n",
      "epoch: 85   trainLoss: 1.5590e-03   valLoss:5.0036e-02  time: 6.01e-02\n",
      "epoch: 86   trainLoss: 1.7038e-03   valLoss:4.2963e-02  time: 6.02e-02\n",
      "epoch: 87   trainLoss: 2.0388e-03   valLoss:5.1799e-02  time: 6.16e-02\n",
      "epoch: 88   trainLoss: 2.3463e-03   valLoss:4.0486e-02  time: 6.44e-02\n",
      "epoch: 89   trainLoss: 2.6932e-03   valLoss:4.8793e-02  time: 6.05e-02\n",
      "epoch: 90   trainLoss: 2.3012e-03   valLoss:4.4098e-02  time: 6.00e-02\n",
      "epoch: 91   trainLoss: 1.8473e-03   valLoss:3.9357e-02  time: 6.00e-02\n",
      "epoch: 92   trainLoss: 2.4367e-03   valLoss:6.1025e-02  time: 6.11e-02\n",
      "epoch: 93   trainLoss: 4.4155e-03   valLoss:3.4432e-02  time: 6.03e-02\n",
      "epoch: 94   trainLoss: 8.2421e-03   valLoss:7.6292e-02  time: 6.01e-02\n",
      "epoch: 95   trainLoss: 1.1599e-02   valLoss:4.1605e-02  time: 6.04e-02\n",
      "epoch: 96   trainLoss: 1.3759e-02   valLoss:4.5607e-02  time: 5.98e-02\n",
      "epoch: 97   trainLoss: 4.1740e-03   valLoss:4.2807e-02  time: 6.02e-02\n",
      "epoch: 98   trainLoss: 2.6840e-03   valLoss:3.7961e-02  time: 6.04e-02\n",
      "epoch: 99   trainLoss: 7.7402e-03   valLoss:4.0538e-02  time: 6.05e-02\n",
      "loading checkpoint 36\n",
      "trained 38 random forest models in 4.24 seconds\n",
      "loaded train set of size 93\n",
      "epoch: 0   trainLoss: 1.0263e+00   valLoss:1.0087e+00  time: 8.19e-01\n",
      "epoch: 1   trainLoss: 8.3144e-01   valLoss:9.8785e-01  time: 7.90e-01\n",
      "epoch: 2   trainLoss: 6.6992e-01   valLoss:9.6031e-01  time: 7.90e-01\n",
      "epoch: 3   trainLoss: 5.3427e-01   valLoss:9.2002e-01  time: 7.77e-01\n",
      "epoch: 4   trainLoss: 4.3655e-01   valLoss:8.6995e-01  time: 7.79e-01\n",
      "epoch: 5   trainLoss: 3.7320e-01   valLoss:8.2594e-01  time: 7.71e-01\n",
      "epoch: 6   trainLoss: 3.3179e-01   valLoss:7.9971e-01  time: 7.66e-01\n",
      "epoch: 7   trainLoss: 3.0554e-01   valLoss:8.0516e-01  time: 7.78e-01\n",
      "epoch: 8   trainLoss: 2.8568e-01   valLoss:8.4855e-01  time: 7.79e-01\n",
      "epoch: 9   trainLoss: 2.6713e-01   valLoss:9.3030e-01  time: 7.79e-01\n",
      "epoch: 10   trainLoss: 2.5024e-01   valLoss:1.0283e+00  time: 7.97e-01\n",
      "epoch: 11   trainLoss: 2.2968e-01   valLoss:1.0703e+00  time: 7.86e-01\n",
      "epoch: 12   trainLoss: 2.0937e-01   valLoss:1.0382e+00  time: 7.78e-01\n",
      "epoch: 13   trainLoss: 1.9372e-01   valLoss:9.9561e-01  time: 7.77e-01\n",
      "epoch: 14   trainLoss: 1.7703e-01   valLoss:9.6978e-01  time: 7.72e-01\n",
      "epoch: 15   trainLoss: 1.6228e-01   valLoss:9.6100e-01  time: 7.73e-01\n",
      "epoch: 16   trainLoss: 1.4893e-01   valLoss:9.4149e-01  time: 7.71e-01\n",
      "epoch: 17   trainLoss: 1.3601e-01   valLoss:9.1529e-01  time: 7.71e-01\n",
      "epoch: 18   trainLoss: 1.2826e-01   valLoss:9.0995e-01  time: 7.74e-01\n",
      "epoch: 19   trainLoss: 1.3466e-01   valLoss:8.3423e-01  time: 7.71e-01\n",
      "epoch: 20   trainLoss: 1.2859e-01   valLoss:8.4905e-01  time: 7.81e-01\n",
      "epoch: 21   trainLoss: 9.9527e-02   valLoss:7.6701e-01  time: 7.86e-01\n",
      "epoch: 22   trainLoss: 9.4834e-02   valLoss:6.2548e-01  time: 7.76e-01\n",
      "epoch: 23   trainLoss: 8.6912e-02   valLoss:5.7130e-01  time: 7.88e-01\n",
      "epoch: 24   trainLoss: 7.4927e-02   valLoss:5.4455e-01  time: 8.47e-01\n",
      "epoch: 25   trainLoss: 7.1449e-02   valLoss:5.0488e-01  time: 8.01e-01\n",
      "epoch: 26   trainLoss: 6.2123e-02   valLoss:5.0382e-01  time: 7.82e-01\n",
      "epoch: 27   trainLoss: 5.8359e-02   valLoss:5.0627e-01  time: 7.77e-01\n",
      "epoch: 28   trainLoss: 5.2191e-02   valLoss:5.0451e-01  time: 7.79e-01\n",
      "epoch: 29   trainLoss: 4.6908e-02   valLoss:4.9472e-01  time: 7.76e-01\n",
      "epoch: 30   trainLoss: 4.3427e-02   valLoss:4.8722e-01  time: 7.71e-01\n",
      "epoch: 31   trainLoss: 3.9462e-02   valLoss:4.8704e-01  time: 7.73e-01\n",
      "epoch: 32   trainLoss: 3.5536e-02   valLoss:5.0097e-01  time: 7.69e-01\n",
      "epoch: 33   trainLoss: 3.3238e-02   valLoss:5.1169e-01  time: 7.66e-01\n",
      "epoch: 34   trainLoss: 3.1504e-02   valLoss:5.1221e-01  time: 7.69e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35   trainLoss: 2.7774e-02   valLoss:5.3745e-01  time: 7.72e-01\n",
      "epoch: 36   trainLoss: 2.6153e-02   valLoss:5.4323e-01  time: 7.74e-01\n",
      "epoch: 37   trainLoss: 2.7015e-02   valLoss:5.3538e-01  time: 7.75e-01\n",
      "epoch: 38   trainLoss: 2.4344e-02   valLoss:5.3210e-01  time: 7.70e-01\n",
      "epoch: 39   trainLoss: 2.2201e-02   valLoss:5.5258e-01  time: 7.91e-01\n",
      "epoch: 40   trainLoss: 1.9242e-02   valLoss:5.4613e-01  time: 7.75e-01\n",
      "epoch: 41   trainLoss: 1.8640e-02   valLoss:5.2742e-01  time: 7.91e-01\n",
      "epoch: 42   trainLoss: 1.8138e-02   valLoss:5.5843e-01  time: 7.83e-01\n",
      "epoch: 43   trainLoss: 1.9094e-02   valLoss:5.0571e-01  time: 7.76e-01\n",
      "epoch: 44   trainLoss: 1.9630e-02   valLoss:5.4832e-01  time: 7.67e-01\n",
      "epoch: 45   trainLoss: 1.7843e-02   valLoss:5.3183e-01  time: 7.74e-01\n",
      "epoch: 46   trainLoss: 1.5325e-02   valLoss:5.2172e-01  time: 7.73e-01\n",
      "epoch: 47   trainLoss: 1.3988e-02   valLoss:5.3168e-01  time: 7.73e-01\n",
      "epoch: 48   trainLoss: 1.2403e-02   valLoss:5.1013e-01  time: 7.66e-01\n",
      "epoch: 49   trainLoss: 1.0799e-02   valLoss:5.1213e-01  time: 7.81e-01\n",
      "epoch: 50   trainLoss: 1.0033e-02   valLoss:5.1188e-01  time: 7.75e-01\n",
      "epoch: 51   trainLoss: 1.0978e-02   valLoss:4.8318e-01  time: 7.81e-01\n",
      "epoch: 52   trainLoss: 8.6441e-03   valLoss:4.8417e-01  time: 7.72e-01\n",
      "epoch: 53   trainLoss: 7.8463e-03   valLoss:4.8139e-01  time: 7.71e-01\n",
      "epoch: 54   trainLoss: 8.5986e-03   valLoss:4.7153e-01  time: 7.76e-01\n",
      "epoch: 55   trainLoss: 7.4397e-03   valLoss:4.7220e-01  time: 7.72e-01\n",
      "epoch: 56   trainLoss: 6.2897e-03   valLoss:4.4890e-01  time: 7.73e-01\n",
      "epoch: 57   trainLoss: 6.7685e-03   valLoss:4.4688e-01  time: 7.85e-01\n",
      "epoch: 58   trainLoss: 5.8286e-03   valLoss:4.5418e-01  time: 7.73e-01\n",
      "epoch: 59   trainLoss: 5.5494e-03   valLoss:4.3120e-01  time: 7.74e-01\n",
      "epoch: 60   trainLoss: 5.3315e-03   valLoss:4.1449e-01  time: 7.72e-01\n",
      "epoch: 61   trainLoss: 5.2778e-03   valLoss:4.3625e-01  time: 7.82e-01\n",
      "epoch: 62   trainLoss: 5.4533e-03   valLoss:3.9990e-01  time: 7.77e-01\n",
      "epoch: 63   trainLoss: 6.8752e-03   valLoss:4.3241e-01  time: 7.75e-01\n",
      "epoch: 64   trainLoss: 9.0940e-03   valLoss:3.8370e-01  time: 7.73e-01\n",
      "epoch: 65   trainLoss: 1.3184e-02   valLoss:3.8493e-01  time: 7.71e-01\n",
      "epoch: 66   trainLoss: 1.2819e-02   valLoss:3.9389e-01  time: 7.73e-01\n",
      "epoch: 67   trainLoss: 9.0025e-03   valLoss:3.5505e-01  time: 7.63e-01\n",
      "epoch: 68   trainLoss: 7.3274e-03   valLoss:3.4965e-01  time: 7.96e-01\n",
      "epoch: 69   trainLoss: 8.4647e-03   valLoss:3.4413e-01  time: 7.73e-01\n",
      "epoch: 70   trainLoss: 5.7979e-03   valLoss:3.4717e-01  time: 7.78e-01\n",
      "epoch: 71   trainLoss: 7.1563e-03   valLoss:3.0703e-01  time: 7.73e-01\n",
      "epoch: 72   trainLoss: 8.3136e-03   valLoss:3.4013e-01  time: 7.69e-01\n",
      "epoch: 73   trainLoss: 8.5307e-03   valLoss:3.0425e-01  time: 7.70e-01\n",
      "epoch: 74   trainLoss: 1.0672e-02   valLoss:3.1818e-01  time: 7.75e-01\n",
      "epoch: 75   trainLoss: 9.9202e-03   valLoss:3.0276e-01  time: 7.75e-01\n",
      "epoch: 76   trainLoss: 8.1058e-03   valLoss:2.9467e-01  time: 7.66e-01\n",
      "epoch: 77   trainLoss: 7.2497e-03   valLoss:2.7574e-01  time: 7.69e-01\n",
      "epoch: 78   trainLoss: 5.7590e-03   valLoss:2.9323e-01  time: 7.70e-01\n",
      "epoch: 79   trainLoss: 5.5996e-03   valLoss:2.6208e-01  time: 7.67e-01\n",
      "epoch: 80   trainLoss: 4.5872e-03   valLoss:2.6530e-01  time: 7.69e-01\n",
      "epoch: 81   trainLoss: 4.1948e-03   valLoss:3.0242e-01  time: 7.78e-01\n",
      "epoch: 82   trainLoss: 4.8299e-03   valLoss:2.7198e-01  time: 7.71e-01\n",
      "epoch: 83   trainLoss: 3.1339e-03   valLoss:2.7307e-01  time: 7.85e-01\n",
      "epoch: 84   trainLoss: 3.7742e-03   valLoss:2.8155e-01  time: 7.77e-01\n",
      "epoch: 85   trainLoss: 2.8655e-03   valLoss:2.8731e-01  time: 7.73e-01\n",
      "epoch: 86   trainLoss: 2.9632e-03   valLoss:2.8620e-01  time: 7.72e-01\n",
      "epoch: 87   trainLoss: 2.6506e-03   valLoss:2.7259e-01  time: 7.91e-01\n",
      "epoch: 88   trainLoss: 2.5051e-03   valLoss:2.8920e-01  time: 7.70e-01\n",
      "epoch: 89   trainLoss: 2.6073e-03   valLoss:2.7886e-01  time: 7.61e-01\n",
      "epoch: 90   trainLoss: 2.0740e-03   valLoss:2.8052e-01  time: 7.63e-01\n",
      "epoch: 91   trainLoss: 2.0950e-03   valLoss:2.7462e-01  time: 7.66e-01\n",
      "epoch: 92   trainLoss: 2.3197e-03   valLoss:2.7369e-01  time: 7.89e-01\n",
      "epoch: 93   trainLoss: 2.1787e-03   valLoss:2.6697e-01  time: 7.74e-01\n",
      "epoch: 94   trainLoss: 1.9377e-03   valLoss:2.8244e-01  time: 7.77e-01\n",
      "epoch: 95   trainLoss: 2.3929e-03   valLoss:2.6202e-01  time: 7.69e-01\n",
      "epoch: 96   trainLoss: 3.0099e-03   valLoss:2.8114e-01  time: 7.74e-01\n",
      "epoch: 97   trainLoss: 4.7990e-03   valLoss:2.5576e-01  time: 7.69e-01\n",
      "epoch: 98   trainLoss: 7.9959e-03   valLoss:3.0704e-01  time: 7.73e-01\n",
      "epoch: 99   trainLoss: 1.2950e-02   valLoss:2.5613e-01  time: 7.78e-01\n",
      "loading checkpoint 97\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 6.9929e-01   valLoss:2.8468e+00  time: 8.20e-01\n",
      "epoch: 1   trainLoss: 5.2234e-01   valLoss:7.1347e+00  time: 8.03e-01\n",
      "epoch: 2   trainLoss: 4.6105e-01   valLoss:8.6025e-01  time: 7.93e-01\n",
      "epoch: 3   trainLoss: 3.1903e-01   valLoss:9.8716e-01  time: 7.85e-01\n",
      "epoch: 4   trainLoss: 2.6462e-01   valLoss:7.1738e-01  time: 7.78e-01\n",
      "epoch: 5   trainLoss: 2.1934e-01   valLoss:6.3455e-01  time: 7.86e-01\n",
      "epoch: 6   trainLoss: 1.8646e-01   valLoss:3.6418e-01  time: 7.79e-01\n",
      "epoch: 7   trainLoss: 1.7261e-01   valLoss:2.7830e-01  time: 7.79e-01\n",
      "epoch: 8   trainLoss: 1.5521e-01   valLoss:2.2933e-01  time: 7.90e-01\n",
      "epoch: 9   trainLoss: 1.4299e-01   valLoss:2.1026e-01  time: 7.75e-01\n",
      "epoch: 10   trainLoss: 1.3378e-01   valLoss:2.3891e-01  time: 7.80e-01\n",
      "epoch: 11   trainLoss: 1.1771e-01   valLoss:2.4458e-01  time: 7.70e-01\n",
      "epoch: 12   trainLoss: 1.0776e-01   valLoss:1.8599e-01  time: 7.69e-01\n",
      "epoch: 13   trainLoss: 9.4990e-02   valLoss:1.6761e-01  time: 7.67e-01\n",
      "epoch: 14   trainLoss: 8.5042e-02   valLoss:2.0698e-01  time: 7.96e-01\n",
      "epoch: 15   trainLoss: 8.1662e-02   valLoss:2.0431e-01  time: 7.87e-01\n",
      "epoch: 16   trainLoss: 7.2846e-02   valLoss:2.1501e-01  time: 7.97e-01\n",
      "epoch: 17   trainLoss: 6.7558e-02   valLoss:2.0221e-01  time: 7.93e-01\n",
      "epoch: 18   trainLoss: 6.2886e-02   valLoss:1.7014e-01  time: 7.84e-01\n",
      "epoch: 19   trainLoss: 5.9169e-02   valLoss:1.1191e-01  time: 7.77e-01\n",
      "epoch: 20   trainLoss: 5.3310e-02   valLoss:9.1048e-02  time: 7.69e-01\n",
      "epoch: 21   trainLoss: 4.8918e-02   valLoss:7.6180e-02  time: 7.66e-01\n",
      "epoch: 22   trainLoss: 4.8309e-02   valLoss:6.0679e-02  time: 7.72e-01\n",
      "epoch: 23   trainLoss: 4.4523e-02   valLoss:6.5663e-02  time: 7.72e-01\n",
      "epoch: 24   trainLoss: 4.1525e-02   valLoss:6.9681e-02  time: 7.68e-01\n",
      "epoch: 25   trainLoss: 3.8092e-02   valLoss:9.9897e-02  time: 7.64e-01\n",
      "epoch: 26   trainLoss: 3.7400e-02   valLoss:8.3431e-02  time: 7.65e-01\n",
      "epoch: 27   trainLoss: 3.4440e-02   valLoss:7.3142e-02  time: 7.60e-01\n",
      "epoch: 28   trainLoss: 3.1834e-02   valLoss:7.3566e-02  time: 7.75e-01\n",
      "epoch: 29   trainLoss: 3.0806e-02   valLoss:4.6539e-02  time: 7.57e-01\n",
      "epoch: 30   trainLoss: 2.9859e-02   valLoss:3.8902e-02  time: 7.69e-01\n",
      "epoch: 31   trainLoss: 2.8359e-02   valLoss:3.0196e-02  time: 7.68e-01\n",
      "epoch: 32   trainLoss: 2.7427e-02   valLoss:5.1182e-02  time: 7.66e-01\n",
      "epoch: 33   trainLoss: 3.0331e-02   valLoss:6.8148e-02  time: 7.66e-01\n",
      "epoch: 34   trainLoss: 4.7703e-02   valLoss:6.9351e-02  time: 7.71e-01\n",
      "epoch: 35   trainLoss: 4.6705e-02   valLoss:4.1798e-02  time: 7.86e-01\n",
      "epoch: 36   trainLoss: 2.5742e-02   valLoss:5.6686e-02  time: 7.72e-01\n",
      "epoch: 37   trainLoss: 3.3334e-02   valLoss:4.0704e-02  time: 7.70e-01\n",
      "epoch: 38   trainLoss: 2.6351e-02   valLoss:5.2871e-02  time: 7.64e-01\n",
      "epoch: 39   trainLoss: 2.4370e-02   valLoss:5.5652e-02  time: 7.69e-01\n",
      "epoch: 40   trainLoss: 2.6911e-02   valLoss:3.9077e-02  time: 7.55e-01\n",
      "epoch: 41   trainLoss: 2.0621e-02   valLoss:3.9976e-02  time: 7.72e-01\n",
      "epoch: 42   trainLoss: 2.4665e-02   valLoss:2.6979e-02  time: 7.77e-01\n",
      "epoch: 43   trainLoss: 1.7468e-02   valLoss:3.2157e-02  time: 7.68e-01\n",
      "epoch: 44   trainLoss: 2.2208e-02   valLoss:2.8203e-02  time: 7.72e-01\n",
      "epoch: 45   trainLoss: 1.6225e-02   valLoss:3.1867e-02  time: 7.74e-01\n",
      "epoch: 46   trainLoss: 1.8745e-02   valLoss:3.1549e-02  time: 7.61e-01\n",
      "epoch: 47   trainLoss: 1.5414e-02   valLoss:3.6073e-02  time: 7.77e-01\n",
      "epoch: 48   trainLoss: 1.6320e-02   valLoss:3.1794e-02  time: 7.62e-01\n",
      "epoch: 49   trainLoss: 1.4206e-02   valLoss:4.4537e-02  time: 7.64e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50   trainLoss: 1.4924e-02   valLoss:4.0695e-02  time: 7.63e-01\n",
      "epoch: 51   trainLoss: 1.2300e-02   valLoss:4.8715e-02  time: 7.71e-01\n",
      "epoch: 52   trainLoss: 1.3013e-02   valLoss:4.8294e-02  time: 7.63e-01\n",
      "epoch: 53   trainLoss: 1.1541e-02   valLoss:4.9524e-02  time: 7.75e-01\n",
      "epoch: 54   trainLoss: 1.1339e-02   valLoss:5.6279e-02  time: 7.62e-01\n",
      "epoch: 55   trainLoss: 1.0823e-02   valLoss:4.8141e-02  time: 7.70e-01\n",
      "epoch: 56   trainLoss: 1.0040e-02   valLoss:5.5287e-02  time: 7.63e-01\n",
      "epoch: 57   trainLoss: 1.0710e-02   valLoss:3.7747e-02  time: 7.65e-01\n",
      "epoch: 58   trainLoss: 9.6575e-03   valLoss:5.1792e-02  time: 7.62e-01\n",
      "epoch: 59   trainLoss: 1.0439e-02   valLoss:2.9104e-02  time: 7.70e-01\n",
      "epoch: 60   trainLoss: 1.1780e-02   valLoss:4.3225e-02  time: 7.64e-01\n",
      "epoch: 61   trainLoss: 1.4771e-02   valLoss:2.5177e-02  time: 7.76e-01\n",
      "epoch: 62   trainLoss: 1.8794e-02   valLoss:2.2605e-02  time: 7.65e-01\n",
      "epoch: 63   trainLoss: 1.4561e-02   valLoss:2.0733e-02  time: 7.61e-01\n",
      "epoch: 64   trainLoss: 9.0253e-03   valLoss:2.8594e-02  time: 7.69e-01\n",
      "epoch: 65   trainLoss: 1.0751e-02   valLoss:3.5492e-02  time: 7.60e-01\n",
      "epoch: 66   trainLoss: 1.0984e-02   valLoss:5.3758e-02  time: 7.64e-01\n",
      "epoch: 67   trainLoss: 9.2564e-03   valLoss:6.1456e-02  time: 7.58e-01\n",
      "epoch: 68   trainLoss: 8.7349e-03   valLoss:5.7409e-02  time: 7.66e-01\n",
      "epoch: 69   trainLoss: 7.6909e-03   valLoss:6.3492e-02  time: 7.73e-01\n",
      "epoch: 70   trainLoss: 9.3962e-03   valLoss:5.5208e-02  time: 7.85e-01\n",
      "epoch: 71   trainLoss: 7.1426e-03   valLoss:5.7721e-02  time: 7.61e-01\n",
      "epoch: 72   trainLoss: 7.1485e-03   valLoss:4.4363e-02  time: 7.73e-01\n",
      "epoch: 73   trainLoss: 7.1695e-03   valLoss:3.6004e-02  time: 7.65e-01\n",
      "epoch: 74   trainLoss: 6.2453e-03   valLoss:3.8534e-02  time: 7.98e-01\n",
      "epoch: 75   trainLoss: 6.2900e-03   valLoss:2.8289e-02  time: 7.75e-01\n",
      "epoch: 76   trainLoss: 5.5465e-03   valLoss:2.1815e-02  time: 7.73e-01\n",
      "epoch: 77   trainLoss: 5.5951e-03   valLoss:2.1015e-02  time: 7.75e-01\n",
      "epoch: 78   trainLoss: 5.3953e-03   valLoss:2.0050e-02  time: 7.55e-01\n",
      "epoch: 79   trainLoss: 5.2729e-03   valLoss:2.0695e-02  time: 7.69e-01\n",
      "epoch: 80   trainLoss: 4.7208e-03   valLoss:2.0235e-02  time: 7.63e-01\n",
      "epoch: 81   trainLoss: 4.4446e-03   valLoss:2.1794e-02  time: 7.67e-01\n",
      "epoch: 82   trainLoss: 4.3954e-03   valLoss:2.2852e-02  time: 7.64e-01\n",
      "epoch: 83   trainLoss: 4.4923e-03   valLoss:2.9992e-02  time: 7.60e-01\n",
      "epoch: 84   trainLoss: 4.8561e-03   valLoss:2.7661e-02  time: 7.72e-01\n",
      "epoch: 85   trainLoss: 4.4167e-03   valLoss:2.4890e-02  time: 7.84e-01\n",
      "epoch: 86   trainLoss: 4.9093e-03   valLoss:3.2723e-02  time: 7.64e-01\n",
      "epoch: 87   trainLoss: 5.2086e-03   valLoss:3.0695e-02  time: 7.74e-01\n",
      "epoch: 88   trainLoss: 5.7163e-03   valLoss:3.1179e-02  time: 7.61e-01\n",
      "epoch: 89   trainLoss: 7.0850e-03   valLoss:3.7075e-02  time: 7.58e-01\n",
      "epoch: 90   trainLoss: 8.8981e-03   valLoss:3.2546e-02  time: 7.55e-01\n",
      "epoch: 91   trainLoss: 9.9284e-03   valLoss:3.0962e-02  time: 7.67e-01\n",
      "epoch: 92   trainLoss: 8.0559e-03   valLoss:2.8473e-02  time: 7.66e-01\n",
      "epoch: 93   trainLoss: 4.3467e-03   valLoss:2.0770e-02  time: 7.63e-01\n",
      "epoch: 94   trainLoss: 4.9000e-03   valLoss:3.3506e-02  time: 7.62e-01\n",
      "epoch: 95   trainLoss: 6.6055e-03   valLoss:2.3529e-02  time: 7.67e-01\n",
      "epoch: 96   trainLoss: 6.6691e-03   valLoss:1.7947e-02  time: 7.63e-01\n",
      "epoch: 97   trainLoss: 5.6923e-03   valLoss:4.1494e-02  time: 7.67e-01\n",
      "epoch: 98   trainLoss: 5.3458e-03   valLoss:2.8145e-02  time: 7.65e-01\n",
      "epoch: 99   trainLoss: 6.2098e-03   valLoss:5.3901e-02  time: 7.58e-01\n",
      "loading checkpoint 96\n",
      "trained 38 random forest models in 6.04 seconds\n",
      "loaded train set of size 914\n",
      "epoch: 0   trainLoss: 7.2319e-01   valLoss:8.9345e-01  time: 7.64e+00\n",
      "epoch: 1   trainLoss: 3.7594e-01   valLoss:8.1982e-01  time: 7.55e+00\n",
      "epoch: 2   trainLoss: 2.7847e-01   valLoss:8.6106e-01  time: 7.47e+00\n",
      "epoch: 3   trainLoss: 2.2029e-01   valLoss:1.0183e+00  time: 7.54e+00\n",
      "epoch: 4   trainLoss: 1.9262e-01   valLoss:9.9418e-01  time: 7.49e+00\n",
      "epoch: 5   trainLoss: 1.4698e-01   valLoss:6.4619e-01  time: 7.48e+00\n",
      "epoch: 6   trainLoss: 1.2137e-01   valLoss:4.4378e-01  time: 7.46e+00\n",
      "epoch: 7   trainLoss: 1.0845e-01   valLoss:2.5275e-01  time: 7.42e+00\n",
      "epoch: 8   trainLoss: 1.0542e-01   valLoss:2.2956e-01  time: 7.46e+00\n",
      "epoch: 9   trainLoss: 8.8769e-02   valLoss:1.9574e-01  time: 7.52e+00\n",
      "epoch: 10   trainLoss: 8.3710e-02   valLoss:2.2164e-01  time: 7.56e+00\n",
      "epoch: 11   trainLoss: 8.3924e-02   valLoss:2.6739e-01  time: 7.53e+00\n",
      "epoch: 12   trainLoss: 8.0033e-02   valLoss:2.0268e-01  time: 7.48e+00\n",
      "epoch: 13   trainLoss: 7.5505e-02   valLoss:1.5751e-01  time: 7.44e+00\n",
      "epoch: 14   trainLoss: 7.6179e-02   valLoss:1.4293e-01  time: 7.43e+00\n",
      "epoch: 15   trainLoss: 7.7097e-02   valLoss:1.5270e-01  time: 7.51e+00\n",
      "epoch: 16   trainLoss: 6.1145e-02   valLoss:1.2878e-01  time: 7.49e+00\n",
      "epoch: 17   trainLoss: 6.3881e-02   valLoss:1.7507e-01  time: 7.48e+00\n",
      "epoch: 18   trainLoss: 5.8889e-02   valLoss:1.4012e-01  time: 7.48e+00\n",
      "epoch: 19   trainLoss: 5.9256e-02   valLoss:1.6555e-01  time: 7.47e+00\n",
      "epoch: 20   trainLoss: 4.5614e-02   valLoss:1.2562e-01  time: 7.51e+00\n",
      "epoch: 21   trainLoss: 5.2424e-02   valLoss:1.2910e-01  time: 7.59e+00\n",
      "epoch: 22   trainLoss: 4.6879e-02   valLoss:1.0862e-01  time: 7.58e+00\n",
      "epoch: 23   trainLoss: 4.5194e-02   valLoss:1.2335e-01  time: 7.63e+00\n",
      "epoch: 24   trainLoss: 4.7817e-02   valLoss:1.0527e-01  time: 7.52e+00\n",
      "epoch: 25   trainLoss: 5.0416e-02   valLoss:1.4829e-01  time: 7.66e+00\n",
      "epoch: 26   trainLoss: 4.9511e-02   valLoss:1.2010e-01  time: 7.55e+00\n",
      "epoch: 27   trainLoss: 4.8066e-02   valLoss:7.2448e-02  time: 7.59e+00\n",
      "epoch: 28   trainLoss: 5.0317e-02   valLoss:5.7170e-02  time: 7.59e+00\n",
      "epoch: 29   trainLoss: 4.8090e-02   valLoss:8.5104e-02  time: 7.59e+00\n",
      "epoch: 30   trainLoss: 4.6835e-02   valLoss:8.8058e-02  time: 7.70e+00\n",
      "epoch: 31   trainLoss: 4.4875e-02   valLoss:6.9428e-02  time: 7.70e+00\n",
      "epoch: 32   trainLoss: 4.1815e-02   valLoss:1.1750e-01  time: 7.46e+00\n",
      "epoch: 33   trainLoss: 4.0730e-02   valLoss:5.6324e-02  time: 7.61e+00\n",
      "epoch: 34   trainLoss: 3.3848e-02   valLoss:1.1513e-01  time: 7.64e+00\n",
      "epoch: 35   trainLoss: 4.1936e-02   valLoss:1.3729e-01  time: 7.52e+00\n",
      "epoch: 36   trainLoss: 4.2263e-02   valLoss:1.3595e-01  time: 7.74e+00\n",
      "epoch: 37   trainLoss: 4.3518e-02   valLoss:9.7414e-02  time: 7.47e+00\n",
      "epoch: 38   trainLoss: 3.9176e-02   valLoss:6.5228e-02  time: 7.68e+00\n",
      "epoch: 39   trainLoss: 3.8447e-02   valLoss:5.2861e-02  time: 7.73e+00\n",
      "epoch: 40   trainLoss: 3.8192e-02   valLoss:7.8367e-02  time: 7.62e+00\n",
      "epoch: 41   trainLoss: 3.5844e-02   valLoss:7.6442e-02  time: 7.71e+00\n",
      "epoch: 42   trainLoss: 3.4258e-02   valLoss:8.5011e-02  time: 7.53e+00\n",
      "epoch: 43   trainLoss: 3.9264e-02   valLoss:7.8194e-02  time: 7.47e+00\n",
      "epoch: 44   trainLoss: 3.8715e-02   valLoss:5.3618e-02  time: 7.72e+00\n",
      "epoch: 45   trainLoss: 3.3350e-02   valLoss:8.8718e-02  time: 7.52e+00\n",
      "epoch: 46   trainLoss: 4.1811e-02   valLoss:6.6998e-02  time: 7.67e+00\n",
      "epoch: 47   trainLoss: 3.2614e-02   valLoss:8.9117e-02  time: 7.51e+00\n",
      "epoch: 48   trainLoss: 3.4246e-02   valLoss:4.9132e-02  time: 7.52e+00\n",
      "epoch: 49   trainLoss: 3.7180e-02   valLoss:9.1821e-02  time: 7.67e+00\n",
      "epoch: 50   trainLoss: 3.1025e-02   valLoss:4.7270e-02  time: 7.63e+00\n",
      "epoch: 51   trainLoss: 3.7129e-02   valLoss:8.7114e-02  time: 7.52e+00\n",
      "epoch: 52   trainLoss: 4.3135e-02   valLoss:8.5517e-02  time: 7.57e+00\n",
      "epoch: 53   trainLoss: 3.3751e-02   valLoss:6.2724e-02  time: 7.53e+00\n",
      "epoch: 54   trainLoss: 3.6773e-02   valLoss:5.7861e-02  time: 7.64e+00\n",
      "epoch: 55   trainLoss: 2.6599e-02   valLoss:8.0449e-02  time: 7.59e+00\n",
      "epoch: 56   trainLoss: 3.8636e-02   valLoss:6.1452e-02  time: 7.64e+00\n",
      "epoch: 57   trainLoss: 3.7161e-02   valLoss:6.0703e-02  time: 7.61e+00\n",
      "epoch: 58   trainLoss: 3.9789e-02   valLoss:4.8937e-02  time: 7.62e+00\n",
      "epoch: 59   trainLoss: 3.8803e-02   valLoss:7.3181e-02  time: 7.65e+00\n",
      "epoch: 60   trainLoss: 4.2792e-02   valLoss:5.8603e-02  time: 7.54e+00\n",
      "epoch: 61   trainLoss: 3.9873e-02   valLoss:1.0182e-01  time: 7.56e+00\n",
      "epoch: 62   trainLoss: 3.6386e-02   valLoss:8.2074e-02  time: 7.64e+00\n",
      "epoch: 63   trainLoss: 3.2783e-02   valLoss:5.8262e-02  time: 7.66e+00\n",
      "epoch: 64   trainLoss: 3.5983e-02   valLoss:8.6671e-02  time: 7.69e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65   trainLoss: 3.7134e-02   valLoss:5.0719e-02  time: 7.68e+00\n",
      "epoch: 66   trainLoss: 3.2988e-02   valLoss:6.1401e-02  time: 7.49e+00\n",
      "epoch: 67   trainLoss: 3.1562e-02   valLoss:9.7060e-02  time: 7.68e+00\n",
      "epoch: 68   trainLoss: 2.7845e-02   valLoss:4.2551e-02  time: 7.60e+00\n",
      "epoch: 69   trainLoss: 3.6683e-02   valLoss:1.2656e-01  time: 7.62e+00\n",
      "epoch: 70   trainLoss: 3.2849e-02   valLoss:7.3578e-02  time: 7.66e+00\n",
      "epoch: 71   trainLoss: 3.5226e-02   valLoss:9.1978e-02  time: 7.64e+00\n",
      "epoch: 72   trainLoss: 3.8125e-02   valLoss:8.6695e-02  time: 7.51e+00\n",
      "epoch: 73   trainLoss: 3.5703e-02   valLoss:1.3566e-01  time: 7.57e+00\n",
      "epoch: 74   trainLoss: 2.9719e-02   valLoss:6.1874e-02  time: 7.65e+00\n",
      "epoch: 75   trainLoss: 2.9807e-02   valLoss:4.5844e-02  time: 7.69e+00\n",
      "epoch: 76   trainLoss: 3.2279e-02   valLoss:2.9030e-02  time: 7.68e+00\n",
      "epoch: 77   trainLoss: 3.7649e-02   valLoss:3.5388e-02  time: 7.70e+00\n",
      "epoch: 78   trainLoss: 2.6476e-02   valLoss:6.1833e-02  time: 7.58e+00\n",
      "epoch: 79   trainLoss: 2.8251e-02   valLoss:6.5689e-02  time: 7.51e+00\n",
      "epoch: 80   trainLoss: 2.7393e-02   valLoss:4.9612e-02  time: 7.67e+00\n",
      "epoch: 81   trainLoss: 2.5318e-02   valLoss:5.2900e-02  time: 7.61e+00\n",
      "epoch: 82   trainLoss: 3.4376e-02   valLoss:9.9993e-02  time: 7.65e+00\n",
      "epoch: 83   trainLoss: 3.3726e-02   valLoss:7.8909e-02  time: 7.53e+00\n",
      "epoch: 84   trainLoss: 2.6190e-02   valLoss:3.9993e-02  time: 7.43e+00\n",
      "epoch: 85   trainLoss: 2.5477e-02   valLoss:5.1340e-02  time: 7.63e+00\n",
      "epoch: 86   trainLoss: 2.7720e-02   valLoss:3.6955e-02  time: 7.62e+00\n",
      "epoch: 87   trainLoss: 2.3222e-02   valLoss:3.9685e-02  time: 7.62e+00\n",
      "epoch: 88   trainLoss: 2.5171e-02   valLoss:4.1112e-02  time: 7.61e+00\n",
      "epoch: 89   trainLoss: 3.0155e-02   valLoss:3.5379e-02  time: 7.69e+00\n",
      "epoch: 90   trainLoss: 3.1622e-02   valLoss:5.2616e-02  time: 7.60e+00\n",
      "epoch: 91   trainLoss: 3.3428e-02   valLoss:7.0645e-02  time: 7.58e+00\n",
      "epoch: 92   trainLoss: 3.6469e-02   valLoss:8.4565e-02  time: 7.62e+00\n",
      "epoch: 93   trainLoss: 2.7857e-02   valLoss:4.3501e-02  time: 7.67e+00\n",
      "epoch: 94   trainLoss: 2.5478e-02   valLoss:5.2412e-02  time: 7.55e+00\n",
      "epoch: 95   trainLoss: 3.8620e-02   valLoss:8.6490e-02  time: 7.53e+00\n",
      "epoch: 96   trainLoss: 3.9055e-02   valLoss:3.1923e-02  time: 7.60e+00\n",
      "epoch: 97   trainLoss: 3.0167e-02   valLoss:3.9832e-02  time: 7.56e+00\n",
      "epoch: 98   trainLoss: 3.0307e-02   valLoss:4.4715e-02  time: 7.63e+00\n",
      "epoch: 99   trainLoss: 3.2311e-02   valLoss:2.4231e-02  time: 7.65e+00\n",
      "loading checkpoint 99\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 6.7110e-01   valLoss:1.3107e+00  time: 7.74e+00\n",
      "epoch: 1   trainLoss: 2.7318e-01   valLoss:6.0830e-01  time: 7.52e+00\n",
      "epoch: 2   trainLoss: 2.3159e-01   valLoss:6.6504e-01  time: 7.48e+00\n",
      "epoch: 3   trainLoss: 1.8113e-01   valLoss:4.7035e-01  time: 7.62e+00\n",
      "epoch: 4   trainLoss: 1.3893e-01   valLoss:2.6714e-01  time: 7.68e+00\n",
      "epoch: 5   trainLoss: 1.1866e-01   valLoss:1.5075e-01  time: 7.54e+00\n",
      "epoch: 6   trainLoss: 1.1241e-01   valLoss:1.4852e-01  time: 7.71e+00\n",
      "epoch: 7   trainLoss: 1.0853e-01   valLoss:8.6780e-02  time: 7.58e+00\n",
      "epoch: 8   trainLoss: 1.0396e-01   valLoss:9.3743e-02  time: 7.61e+00\n",
      "epoch: 9   trainLoss: 9.1680e-02   valLoss:1.0064e-01  time: 7.68e+00\n",
      "epoch: 10   trainLoss: 8.0244e-02   valLoss:8.3430e-02  time: 7.59e+00\n",
      "epoch: 11   trainLoss: 8.0188e-02   valLoss:1.1970e-01  time: 7.67e+00\n",
      "epoch: 12   trainLoss: 7.2193e-02   valLoss:6.6976e-02  time: 7.66e+00\n",
      "epoch: 13   trainLoss: 7.4305e-02   valLoss:8.1568e-02  time: 7.63e+00\n",
      "epoch: 14   trainLoss: 7.2003e-02   valLoss:7.8011e-02  time: 7.55e+00\n",
      "epoch: 15   trainLoss: 6.8913e-02   valLoss:9.1301e-02  time: 7.57e+00\n",
      "epoch: 16   trainLoss: 6.5550e-02   valLoss:8.1142e-02  time: 7.66e+00\n",
      "epoch: 17   trainLoss: 6.2224e-02   valLoss:8.5386e-02  time: 7.63e+00\n",
      "epoch: 18   trainLoss: 6.3355e-02   valLoss:1.0288e-01  time: 7.64e+00\n",
      "epoch: 19   trainLoss: 7.2252e-02   valLoss:7.3081e-02  time: 7.56e+00\n",
      "epoch: 20   trainLoss: 7.1882e-02   valLoss:6.8764e-02  time: 7.68e+00\n",
      "epoch: 21   trainLoss: 6.9258e-02   valLoss:9.1420e-02  time: 7.64e+00\n",
      "epoch: 22   trainLoss: 8.5678e-02   valLoss:9.5630e-02  time: 7.56e+00\n",
      "epoch: 23   trainLoss: 6.8615e-02   valLoss:1.0676e-01  time: 7.58e+00\n",
      "epoch: 24   trainLoss: 6.2607e-02   valLoss:6.8223e-02  time: 7.74e+00\n",
      "epoch: 25   trainLoss: 5.8026e-02   valLoss:9.2577e-02  time: 7.50e+00\n",
      "epoch: 26   trainLoss: 4.7769e-02   valLoss:7.9143e-02  time: 7.73e+00\n",
      "epoch: 27   trainLoss: 5.0930e-02   valLoss:5.3427e-02  time: 7.55e+00\n",
      "epoch: 28   trainLoss: 5.0850e-02   valLoss:8.9362e-02  time: 7.70e+00\n",
      "epoch: 29   trainLoss: 4.4846e-02   valLoss:5.0837e-02  time: 7.68e+00\n",
      "epoch: 30   trainLoss: 4.7781e-02   valLoss:8.9654e-02  time: 7.70e+00\n",
      "epoch: 31   trainLoss: 4.6162e-02   valLoss:5.6705e-02  time: 7.71e+00\n",
      "epoch: 32   trainLoss: 7.3668e-02   valLoss:6.7723e-02  time: 7.64e+00\n",
      "epoch: 33   trainLoss: 5.3094e-02   valLoss:8.0506e-02  time: 7.70e+00\n",
      "epoch: 34   trainLoss: 5.1500e-02   valLoss:9.5040e-02  time: 7.66e+00\n",
      "epoch: 35   trainLoss: 5.9371e-02   valLoss:6.8083e-02  time: 7.56e+00\n",
      "epoch: 36   trainLoss: 4.5114e-02   valLoss:6.6692e-02  time: 7.66e+00\n",
      "epoch: 37   trainLoss: 5.3158e-02   valLoss:5.6209e-02  time: 7.62e+00\n",
      "epoch: 38   trainLoss: 5.1606e-02   valLoss:5.4778e-02  time: 7.62e+00\n",
      "epoch: 39   trainLoss: 4.8842e-02   valLoss:9.3762e-02  time: 7.71e+00\n",
      "epoch: 40   trainLoss: 5.3274e-02   valLoss:5.6115e-02  time: 7.71e+00\n",
      "epoch: 41   trainLoss: 4.7991e-02   valLoss:9.6804e-02  time: 7.75e+00\n",
      "epoch: 42   trainLoss: 4.3966e-02   valLoss:7.0157e-02  time: 7.73e+00\n",
      "epoch: 43   trainLoss: 3.9334e-02   valLoss:5.2726e-02  time: 7.67e+00\n",
      "epoch: 44   trainLoss: 4.1858e-02   valLoss:7.6388e-02  time: 7.68e+00\n",
      "epoch: 45   trainLoss: 4.1579e-02   valLoss:7.4133e-02  time: 7.67e+00\n",
      "epoch: 46   trainLoss: 4.7038e-02   valLoss:9.2487e-02  time: 7.62e+00\n",
      "epoch: 47   trainLoss: 5.6202e-02   valLoss:7.7241e-02  time: 7.65e+00\n",
      "epoch: 48   trainLoss: 6.0746e-02   valLoss:7.2594e-02  time: 7.55e+00\n",
      "epoch: 49   trainLoss: 5.5784e-02   valLoss:4.9808e-02  time: 7.58e+00\n",
      "epoch: 50   trainLoss: 4.8274e-02   valLoss:7.5656e-02  time: 7.64e+00\n",
      "epoch: 51   trainLoss: 4.5911e-02   valLoss:6.5370e-02  time: 7.63e+00\n",
      "epoch: 52   trainLoss: 4.6760e-02   valLoss:7.2529e-02  time: 7.62e+00\n",
      "epoch: 53   trainLoss: 4.0211e-02   valLoss:6.7582e-02  time: 7.72e+00\n",
      "epoch: 54   trainLoss: 4.2880e-02   valLoss:5.1159e-02  time: 7.74e+00\n",
      "epoch: 55   trainLoss: 3.6210e-02   valLoss:4.2670e-02  time: 7.65e+00\n",
      "epoch: 56   trainLoss: 3.9245e-02   valLoss:8.9984e-02  time: 7.60e+00\n",
      "epoch: 57   trainLoss: 3.1201e-02   valLoss:5.7575e-02  time: 7.70e+00\n",
      "epoch: 58   trainLoss: 5.4089e-02   valLoss:4.8091e-02  time: 7.70e+00\n",
      "epoch: 59   trainLoss: 4.4155e-02   valLoss:5.3635e-02  time: 7.69e+00\n",
      "epoch: 60   trainLoss: 4.4431e-02   valLoss:5.9523e-02  time: 7.66e+00\n",
      "epoch: 61   trainLoss: 3.9417e-02   valLoss:6.3335e-02  time: 7.62e+00\n",
      "epoch: 62   trainLoss: 4.0852e-02   valLoss:5.4738e-02  time: 7.73e+00\n",
      "epoch: 63   trainLoss: 4.4214e-02   valLoss:7.4457e-02  time: 7.70e+00\n",
      "epoch: 64   trainLoss: 3.6989e-02   valLoss:8.6970e-02  time: 7.57e+00\n",
      "epoch: 65   trainLoss: 4.0597e-02   valLoss:1.0317e-01  time: 7.66e+00\n",
      "epoch: 66   trainLoss: 4.6350e-02   valLoss:5.3576e-02  time: 7.63e+00\n",
      "epoch: 67   trainLoss: 4.8997e-02   valLoss:7.3922e-02  time: 7.57e+00\n",
      "epoch: 68   trainLoss: 3.9428e-02   valLoss:7.8541e-02  time: 7.57e+00\n",
      "epoch: 69   trainLoss: 4.1044e-02   valLoss:6.9723e-02  time: 7.53e+00\n",
      "epoch: 70   trainLoss: 4.1785e-02   valLoss:6.9882e-02  time: 7.54e+00\n",
      "epoch: 71   trainLoss: 4.1345e-02   valLoss:1.2417e-01  time: 7.56e+00\n",
      "epoch: 72   trainLoss: 4.6933e-02   valLoss:4.7881e-02  time: 7.58e+00\n",
      "epoch: 73   trainLoss: 3.2469e-02   valLoss:4.5008e-02  time: 7.57e+00\n",
      "epoch: 74   trainLoss: 3.8417e-02   valLoss:3.9596e-02  time: 7.53e+00\n",
      "epoch: 75   trainLoss: 3.3428e-02   valLoss:4.0868e-02  time: 7.53e+00\n",
      "epoch: 76   trainLoss: 4.0895e-02   valLoss:4.5977e-02  time: 7.52e+00\n",
      "epoch: 77   trainLoss: 3.6587e-02   valLoss:4.4601e-02  time: 7.50e+00\n",
      "epoch: 78   trainLoss: 3.3367e-02   valLoss:4.0679e-02  time: 7.64e+00\n",
      "epoch: 79   trainLoss: 3.9028e-02   valLoss:6.7801e-02  time: 7.55e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80   trainLoss: 3.5046e-02   valLoss:6.3371e-02  time: 7.58e+00\n",
      "epoch: 81   trainLoss: 3.8229e-02   valLoss:8.5149e-02  time: 7.59e+00\n",
      "epoch: 82   trainLoss: 4.9869e-02   valLoss:8.4540e-02  time: 7.69e+00\n",
      "epoch: 83   trainLoss: 4.2835e-02   valLoss:2.0279e-01  time: 7.54e+00\n",
      "epoch: 84   trainLoss: 4.2697e-02   valLoss:6.2192e-02  time: 7.61e+00\n",
      "epoch: 85   trainLoss: 3.0914e-02   valLoss:4.1289e-02  time: 7.53e+00\n",
      "epoch: 86   trainLoss: 3.4322e-02   valLoss:7.3974e-02  time: 7.54e+00\n",
      "epoch: 87   trainLoss: 3.4048e-02   valLoss:8.1519e-02  time: 7.55e+00\n",
      "epoch: 88   trainLoss: 4.8824e-02   valLoss:1.0491e-01  time: 7.62e+00\n",
      "epoch: 89   trainLoss: 4.1110e-02   valLoss:3.3855e-02  time: 7.58e+00\n",
      "epoch: 90   trainLoss: 3.4265e-02   valLoss:6.9165e-02  time: 7.63e+00\n",
      "epoch: 91   trainLoss: 4.5176e-02   valLoss:5.2259e-02  time: 7.56e+00\n",
      "epoch: 92   trainLoss: 2.9993e-02   valLoss:3.4398e-02  time: 7.59e+00\n",
      "epoch: 93   trainLoss: 3.3490e-02   valLoss:7.1520e-02  time: 7.54e+00\n",
      "epoch: 94   trainLoss: 4.4611e-02   valLoss:3.3880e-02  time: 7.60e+00\n",
      "epoch: 95   trainLoss: 3.8210e-02   valLoss:5.7272e-02  time: 7.57e+00\n",
      "epoch: 96   trainLoss: 4.3654e-02   valLoss:9.8098e-02  time: 7.66e+00\n",
      "epoch: 97   trainLoss: 4.4237e-02   valLoss:6.8428e-02  time: 7.59e+00\n",
      "epoch: 98   trainLoss: 3.9295e-02   valLoss:5.0990e-02  time: 7.52e+00\n",
      "epoch: 99   trainLoss: 3.1060e-02   valLoss:3.9252e-02  time: 7.57e+00\n",
      "loading checkpoint 89\n",
      "trained 38 random forest models in 31.76 seconds\n",
      "loaded train set of size 8\n",
      "epoch: 0   trainLoss: 9.6397e-01   valLoss:9.7210e-01  time: 8.55e-02\n",
      "epoch: 1   trainLoss: 7.8333e-01   valLoss:9.6095e-01  time: 8.68e-02\n",
      "epoch: 2   trainLoss: 6.8258e-01   valLoss:9.4940e-01  time: 8.66e-02\n",
      "epoch: 3   trainLoss: 5.6971e-01   valLoss:9.3435e-01  time: 8.78e-02\n",
      "epoch: 4   trainLoss: 4.7064e-01   valLoss:9.2470e-01  time: 8.59e-02\n",
      "epoch: 5   trainLoss: 3.8329e-01   valLoss:9.2548e-01  time: 9.31e-02\n",
      "epoch: 6   trainLoss: 3.1058e-01   valLoss:9.4718e-01  time: 8.62e-02\n",
      "epoch: 7   trainLoss: 2.6862e-01   valLoss:1.0047e+00  time: 8.64e-02\n",
      "epoch: 8   trainLoss: 2.0608e-01   valLoss:1.1139e+00  time: 8.52e-02\n",
      "epoch: 9   trainLoss: 1.6531e-01   valLoss:1.3225e+00  time: 8.73e-02\n",
      "epoch: 10   trainLoss: 1.4166e-01   valLoss:1.6755e+00  time: 8.83e-02\n",
      "epoch: 11   trainLoss: 1.2767e-01   valLoss:2.1592e+00  time: 8.57e-02\n",
      "epoch: 12   trainLoss: 1.1253e-01   valLoss:2.6881e+00  time: 8.95e-02\n",
      "epoch: 13   trainLoss: 9.8183e-02   valLoss:3.1160e+00  time: 8.84e-02\n",
      "epoch: 14   trainLoss: 8.2644e-02   valLoss:3.3424e+00  time: 8.61e-02\n",
      "epoch: 15   trainLoss: 7.2056e-02   valLoss:3.3556e+00  time: 8.68e-02\n",
      "epoch: 16   trainLoss: 6.2227e-02   valLoss:3.2084e+00  time: 8.82e-02\n",
      "epoch: 17   trainLoss: 5.4831e-02   valLoss:2.9676e+00  time: 8.61e-02\n",
      "epoch: 18   trainLoss: 4.7587e-02   valLoss:2.6428e+00  time: 8.84e-02\n",
      "epoch: 19   trainLoss: 4.3491e-02   valLoss:2.2693e+00  time: 8.85e-02\n",
      "epoch: 20   trainLoss: 4.0155e-02   valLoss:1.8239e+00  time: 8.57e-02\n",
      "epoch: 21   trainLoss: 3.6036e-02   valLoss:1.3358e+00  time: 8.72e-02\n",
      "epoch: 22   trainLoss: 3.1763e-02   valLoss:9.1912e-01  time: 8.49e-02\n",
      "epoch: 23   trainLoss: 2.8428e-02   valLoss:5.9251e-01  time: 8.75e-02\n",
      "epoch: 24   trainLoss: 2.4873e-02   valLoss:3.7226e-01  time: 8.59e-02\n",
      "epoch: 25   trainLoss: 2.0823e-02   valLoss:2.5204e-01  time: 8.59e-02\n",
      "epoch: 26   trainLoss: 1.7823e-02   valLoss:1.9517e-01  time: 8.52e-02\n",
      "epoch: 27   trainLoss: 1.5473e-02   valLoss:1.7129e-01  time: 8.87e-02\n",
      "epoch: 28   trainLoss: 1.3825e-02   valLoss:1.7340e-01  time: 8.54e-02\n",
      "epoch: 29   trainLoss: 1.6120e-02   valLoss:1.6091e-01  time: 8.58e-02\n",
      "epoch: 30   trainLoss: 1.3691e-02   valLoss:1.7592e-01  time: 8.56e-02\n",
      "epoch: 31   trainLoss: 1.1806e-02   valLoss:1.7492e-01  time: 8.50e-02\n",
      "epoch: 32   trainLoss: 8.9743e-03   valLoss:1.5951e-01  time: 8.80e-02\n",
      "epoch: 33   trainLoss: 7.2905e-03   valLoss:1.6842e-01  time: 8.65e-02\n",
      "epoch: 34   trainLoss: 7.1897e-03   valLoss:1.5329e-01  time: 8.51e-02\n",
      "epoch: 35   trainLoss: 7.4473e-03   valLoss:1.4657e-01  time: 8.69e-02\n",
      "epoch: 36   trainLoss: 6.8279e-03   valLoss:1.5740e-01  time: 8.63e-02\n",
      "epoch: 37   trainLoss: 5.5249e-03   valLoss:1.4300e-01  time: 8.89e-02\n",
      "epoch: 38   trainLoss: 4.4145e-03   valLoss:1.4159e-01  time: 8.57e-02\n",
      "epoch: 39   trainLoss: 3.6256e-03   valLoss:1.3217e-01  time: 8.62e-02\n",
      "epoch: 40   trainLoss: 3.5921e-03   valLoss:1.1927e-01  time: 8.54e-02\n",
      "epoch: 41   trainLoss: 3.2141e-03   valLoss:1.3329e-01  time: 8.62e-02\n",
      "epoch: 42   trainLoss: 2.6047e-03   valLoss:1.3029e-01  time: 8.64e-02\n",
      "epoch: 43   trainLoss: 2.2300e-03   valLoss:1.1674e-01  time: 8.48e-02\n",
      "epoch: 44   trainLoss: 2.3095e-03   valLoss:1.2501e-01  time: 8.57e-02\n",
      "epoch: 45   trainLoss: 1.8589e-03   valLoss:1.2851e-01  time: 8.52e-02\n",
      "epoch: 46   trainLoss: 1.3468e-03   valLoss:1.2423e-01  time: 8.44e-02\n",
      "epoch: 47   trainLoss: 1.4907e-03   valLoss:1.2616e-01  time: 8.75e-02\n",
      "epoch: 48   trainLoss: 1.2871e-03   valLoss:1.2084e-01  time: 8.47e-02\n",
      "epoch: 49   trainLoss: 9.6746e-04   valLoss:1.1516e-01  time: 8.47e-02\n",
      "epoch: 50   trainLoss: 9.5323e-04   valLoss:1.1951e-01  time: 8.51e-02\n",
      "epoch: 51   trainLoss: 9.1321e-04   valLoss:1.1783e-01  time: 8.51e-02\n",
      "epoch: 52   trainLoss: 7.2546e-04   valLoss:1.1325e-01  time: 8.45e-02\n",
      "epoch: 53   trainLoss: 7.5440e-04   valLoss:1.1212e-01  time: 8.57e-02\n",
      "epoch: 54   trainLoss: 7.9976e-04   valLoss:1.0805e-01  time: 8.57e-02\n",
      "epoch: 55   trainLoss: 5.6563e-04   valLoss:1.0691e-01  time: 8.56e-02\n",
      "epoch: 56   trainLoss: 4.8967e-04   valLoss:1.0765e-01  time: 8.55e-02\n",
      "epoch: 57   trainLoss: 5.1674e-04   valLoss:1.0487e-01  time: 8.80e-02\n",
      "epoch: 58   trainLoss: 4.6366e-04   valLoss:1.0965e-01  time: 8.63e-02\n",
      "epoch: 59   trainLoss: 3.7715e-04   valLoss:1.1484e-01  time: 8.54e-02\n",
      "epoch: 60   trainLoss: 3.5367e-04   valLoss:1.1335e-01  time: 8.51e-02\n",
      "epoch: 61   trainLoss: 3.8233e-04   valLoss:1.1506e-01  time: 8.57e-02\n",
      "epoch: 62   trainLoss: 3.3457e-04   valLoss:1.1275e-01  time: 8.64e-02\n",
      "epoch: 63   trainLoss: 2.2829e-04   valLoss:1.1318e-01  time: 8.49e-02\n",
      "epoch: 64   trainLoss: 2.2001e-04   valLoss:1.1859e-01  time: 8.80e-02\n",
      "epoch: 65   trainLoss: 2.4183e-04   valLoss:1.1404e-01  time: 8.52e-02\n",
      "epoch: 66   trainLoss: 2.5152e-04   valLoss:1.1579e-01  time: 8.51e-02\n",
      "epoch: 67   trainLoss: 2.7276e-04   valLoss:1.1499e-01  time: 8.45e-02\n",
      "epoch: 68   trainLoss: 3.1636e-04   valLoss:1.1409e-01  time: 8.70e-02\n",
      "epoch: 69   trainLoss: 4.7630e-04   valLoss:1.1568e-01  time: 8.48e-02\n",
      "epoch: 70   trainLoss: 1.1864e-03   valLoss:1.0922e-01  time: 8.53e-02\n",
      "epoch: 71   trainLoss: 4.0272e-03   valLoss:1.2043e-01  time: 8.57e-02\n",
      "epoch: 72   trainLoss: 9.7786e-03   valLoss:1.2117e-01  time: 1.07e-01\n",
      "epoch: 73   trainLoss: 1.5215e-02   valLoss:7.7395e-02  time: 9.13e-02\n",
      "epoch: 74   trainLoss: 5.3554e-03   valLoss:9.1528e-02  time: 9.10e-02\n",
      "epoch: 75   trainLoss: 3.8414e-03   valLoss:1.0366e-01  time: 8.60e-02\n",
      "epoch: 76   trainLoss: 6.3385e-03   valLoss:9.0195e-02  time: 8.64e-02\n",
      "epoch: 77   trainLoss: 2.2991e-03   valLoss:8.1440e-02  time: 8.63e-02\n",
      "epoch: 78   trainLoss: 4.2783e-03   valLoss:8.4862e-02  time: 9.31e-02\n",
      "epoch: 79   trainLoss: 2.0906e-03   valLoss:1.0657e-01  time: 9.03e-02\n",
      "epoch: 80   trainLoss: 3.3975e-03   valLoss:9.7317e-02  time: 1.26e-01\n",
      "epoch: 81   trainLoss: 1.4119e-03   valLoss:8.4489e-02  time: 9.01e-02\n",
      "epoch: 82   trainLoss: 2.6796e-03   valLoss:8.2894e-02  time: 8.55e-02\n",
      "epoch: 83   trainLoss: 1.0567e-03   valLoss:8.7081e-02  time: 8.64e-02\n",
      "epoch: 84   trainLoss: 1.8114e-03   valLoss:8.1345e-02  time: 9.23e-02\n",
      "epoch: 85   trainLoss: 1.0525e-03   valLoss:7.6894e-02  time: 8.68e-02\n",
      "epoch: 86   trainLoss: 1.3514e-03   valLoss:7.6771e-02  time: 8.55e-02\n",
      "epoch: 87   trainLoss: 8.2895e-04   valLoss:7.6664e-02  time: 8.58e-02\n",
      "epoch: 88   trainLoss: 1.0316e-03   valLoss:7.6113e-02  time: 8.60e-02\n",
      "epoch: 89   trainLoss: 7.9618e-04   valLoss:7.3784e-02  time: 8.76e-02\n",
      "epoch: 90   trainLoss: 7.0915e-04   valLoss:7.1618e-02  time: 8.62e-02\n",
      "epoch: 91   trainLoss: 6.5514e-04   valLoss:7.4756e-02  time: 8.55e-02\n",
      "epoch: 92   trainLoss: 5.0969e-04   valLoss:7.3842e-02  time: 8.51e-02\n",
      "epoch: 93   trainLoss: 5.0562e-04   valLoss:6.7877e-02  time: 8.46e-02\n",
      "epoch: 94   trainLoss: 4.7066e-04   valLoss:6.7910e-02  time: 9.02e-02\n",
      "epoch: 95   trainLoss: 4.0915e-04   valLoss:7.3012e-02  time: 8.52e-02\n",
      "epoch: 96   trainLoss: 3.3901e-04   valLoss:7.3792e-02  time: 8.49e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 97   trainLoss: 3.7367e-04   valLoss:6.9584e-02  time: 8.59e-02\n",
      "epoch: 98   trainLoss: 2.3925e-04   valLoss:6.6789e-02  time: 8.53e-02\n",
      "epoch: 99   trainLoss: 2.9850e-04   valLoss:6.8530e-02  time: 8.82e-02\n",
      "loading checkpoint 98\n",
      "loading restart file\n",
      "epoch: 0   trainLoss: 3.2338e-01   valLoss:2.1569e+00  time: 8.60e-02\n",
      "epoch: 1   trainLoss: 2.8858e-01   valLoss:2.0624e+00  time: 8.64e-02\n",
      "epoch: 2   trainLoss: 1.7672e-01   valLoss:5.2127e-01  time: 8.62e-02\n",
      "epoch: 3   trainLoss: 1.5479e-01   valLoss:5.4814e-01  time: 8.96e-02\n",
      "epoch: 4   trainLoss: 1.3949e-01   valLoss:6.0858e-01  time: 8.78e-02\n",
      "epoch: 5   trainLoss: 7.9400e-02   valLoss:3.6663e-01  time: 8.72e-02\n",
      "epoch: 6   trainLoss: 7.1359e-02   valLoss:2.8271e-01  time: 8.80e-02\n",
      "epoch: 7   trainLoss: 6.2293e-02   valLoss:1.8289e-01  time: 8.62e-02\n",
      "epoch: 8   trainLoss: 5.1838e-02   valLoss:1.7411e-01  time: 8.60e-02\n",
      "epoch: 9   trainLoss: 4.7335e-02   valLoss:1.9463e-01  time: 8.69e-02\n",
      "epoch: 10   trainLoss: 4.1310e-02   valLoss:1.9723e-01  time: 8.53e-02\n",
      "epoch: 11   trainLoss: 3.3340e-02   valLoss:1.8297e-01  time: 8.70e-02\n",
      "epoch: 12   trainLoss: 2.8694e-02   valLoss:1.2806e-01  time: 8.57e-02\n",
      "epoch: 13   trainLoss: 2.5271e-02   valLoss:7.8164e-02  time: 8.60e-02\n",
      "epoch: 14   trainLoss: 2.1557e-02   valLoss:5.4586e-02  time: 8.64e-02\n",
      "epoch: 15   trainLoss: 1.8449e-02   valLoss:3.9656e-02  time: 8.93e-02\n",
      "epoch: 16   trainLoss: 1.7160e-02   valLoss:3.2937e-02  time: 8.89e-02\n",
      "epoch: 17   trainLoss: 1.5976e-02   valLoss:2.9778e-02  time: 8.72e-02\n",
      "epoch: 18   trainLoss: 1.5524e-02   valLoss:2.2821e-02  time: 8.80e-02\n",
      "epoch: 19   trainLoss: 1.3727e-02   valLoss:2.2024e-02  time: 8.82e-02\n",
      "epoch: 20   trainLoss: 1.3215e-02   valLoss:2.3110e-02  time: 9.10e-02\n",
      "epoch: 21   trainLoss: 1.2358e-02   valLoss:2.0769e-02  time: 8.72e-02\n",
      "epoch: 22   trainLoss: 1.0853e-02   valLoss:1.8286e-02  time: 8.63e-02\n",
      "epoch: 23   trainLoss: 9.7449e-03   valLoss:1.6606e-02  time: 8.64e-02\n",
      "epoch: 24   trainLoss: 9.2963e-03   valLoss:1.5968e-02  time: 8.56e-02\n",
      "epoch: 25   trainLoss: 8.3945e-03   valLoss:1.2965e-02  time: 8.64e-02\n",
      "epoch: 26   trainLoss: 7.7057e-03   valLoss:1.1014e-02  time: 8.60e-02\n",
      "epoch: 27   trainLoss: 7.6063e-03   valLoss:9.2217e-03  time: 8.72e-02\n",
      "epoch: 28   trainLoss: 7.2004e-03   valLoss:8.9508e-03  time: 8.66e-02\n",
      "epoch: 29   trainLoss: 6.9774e-03   valLoss:8.3856e-03  time: 8.68e-02\n",
      "epoch: 30   trainLoss: 6.4992e-03   valLoss:1.2672e-02  time: 8.54e-02\n",
      "epoch: 31   trainLoss: 5.7496e-03   valLoss:1.5357e-02  time: 8.82e-02\n",
      "epoch: 32   trainLoss: 5.4474e-03   valLoss:1.1290e-02  time: 8.50e-02\n",
      "epoch: 33   trainLoss: 5.1964e-03   valLoss:1.1479e-02  time: 8.55e-02\n",
      "epoch: 34   trainLoss: 4.9064e-03   valLoss:1.0634e-02  time: 8.58e-02\n",
      "epoch: 35   trainLoss: 4.7657e-03   valLoss:1.1446e-02  time: 8.66e-02\n",
      "epoch: 36   trainLoss: 4.3812e-03   valLoss:1.1148e-02  time: 8.47e-02\n",
      "epoch: 37   trainLoss: 4.1524e-03   valLoss:1.2615e-02  time: 9.14e-02\n",
      "epoch: 38   trainLoss: 3.9011e-03   valLoss:1.2686e-02  time: 8.60e-02\n",
      "epoch: 39   trainLoss: 3.6570e-03   valLoss:1.2380e-02  time: 8.67e-02\n",
      "epoch: 40   trainLoss: 3.4261e-03   valLoss:1.6106e-02  time: 8.67e-02\n",
      "epoch: 41   trainLoss: 3.2632e-03   valLoss:1.9036e-02  time: 8.48e-02\n",
      "epoch: 42   trainLoss: 2.9868e-03   valLoss:2.3995e-02  time: 8.64e-02\n",
      "epoch: 43   trainLoss: 2.9280e-03   valLoss:2.2920e-02  time: 8.87e-02\n",
      "epoch: 44   trainLoss: 3.2222e-03   valLoss:3.6150e-02  time: 8.50e-02\n",
      "epoch: 45   trainLoss: 5.6633e-03   valLoss:3.4265e-02  time: 8.79e-02\n",
      "epoch: 46   trainLoss: 1.3526e-02   valLoss:5.3197e-02  time: 8.64e-02\n",
      "epoch: 47   trainLoss: 1.4633e-02   valLoss:3.5657e-02  time: 8.83e-02\n",
      "epoch: 48   trainLoss: 3.6520e-03   valLoss:3.5376e-02  time: 8.55e-02\n",
      "epoch: 49   trainLoss: 1.0237e-02   valLoss:2.1767e-02  time: 9.92e-02\n",
      "epoch: 50   trainLoss: 3.6411e-03   valLoss:2.3212e-02  time: 8.98e-02\n",
      "epoch: 51   trainLoss: 7.1875e-03   valLoss:1.9650e-02  time: 8.56e-02\n",
      "epoch: 52   trainLoss: 4.2771e-03   valLoss:2.2754e-02  time: 8.73e-02\n",
      "epoch: 53   trainLoss: 5.2604e-03   valLoss:2.4476e-02  time: 8.57e-02\n",
      "epoch: 54   trainLoss: 3.8869e-03   valLoss:2.2701e-02  time: 8.53e-02\n",
      "epoch: 55   trainLoss: 4.2927e-03   valLoss:2.1378e-02  time: 8.86e-02\n",
      "epoch: 56   trainLoss: 3.6375e-03   valLoss:2.8638e-02  time: 8.72e-02\n",
      "epoch: 57   trainLoss: 3.3216e-03   valLoss:2.8216e-02  time: 8.54e-02\n",
      "epoch: 58   trainLoss: 3.4275e-03   valLoss:2.3782e-02  time: 8.50e-02\n",
      "epoch: 59   trainLoss: 2.7500e-03   valLoss:1.6808e-02  time: 8.67e-02\n",
      "epoch: 60   trainLoss: 2.8288e-03   valLoss:1.5640e-02  time: 8.60e-02\n",
      "epoch: 61   trainLoss: 2.7866e-03   valLoss:2.0887e-02  time: 8.60e-02\n",
      "epoch: 62   trainLoss: 2.0161e-03   valLoss:2.5909e-02  time: 8.63e-02\n",
      "epoch: 63   trainLoss: 2.7150e-03   valLoss:1.8860e-02  time: 8.58e-02\n",
      "epoch: 64   trainLoss: 1.8689e-03   valLoss:1.5892e-02  time: 8.49e-02\n",
      "epoch: 65   trainLoss: 2.0294e-03   valLoss:1.7787e-02  time: 8.78e-02\n",
      "epoch: 66   trainLoss: 1.9595e-03   valLoss:2.2940e-02  time: 8.60e-02\n",
      "epoch: 67   trainLoss: 1.4242e-03   valLoss:2.8477e-02  time: 8.81e-02\n",
      "epoch: 68   trainLoss: 1.8623e-03   valLoss:2.5083e-02  time: 8.76e-02\n",
      "epoch: 69   trainLoss: 1.4926e-03   valLoss:2.5496e-02  time: 8.76e-02\n",
      "epoch: 70   trainLoss: 1.3272e-03   valLoss:2.4248e-02  time: 8.56e-02\n",
      "epoch: 71   trainLoss: 1.3790e-03   valLoss:2.3882e-02  time: 8.48e-02\n",
      "epoch: 72   trainLoss: 1.0531e-03   valLoss:2.3570e-02  time: 8.79e-02\n",
      "epoch: 73   trainLoss: 1.1293e-03   valLoss:2.4057e-02  time: 8.54e-02\n",
      "epoch: 74   trainLoss: 9.6457e-04   valLoss:2.5467e-02  time: 8.56e-02\n",
      "epoch: 75   trainLoss: 9.9876e-04   valLoss:2.5719e-02  time: 8.71e-02\n",
      "epoch: 76   trainLoss: 9.7376e-04   valLoss:2.7146e-02  time: 8.46e-02\n",
      "epoch: 77   trainLoss: 1.2888e-03   valLoss:3.0626e-02  time: 8.49e-02\n",
      "epoch: 78   trainLoss: 2.2144e-03   valLoss:3.7295e-02  time: 8.80e-02\n",
      "epoch: 79   trainLoss: 4.6359e-03   valLoss:4.0854e-02  time: 8.53e-02\n",
      "epoch: 80   trainLoss: 7.6292e-03   valLoss:3.0678e-02  time: 8.53e-02\n",
      "epoch: 81   trainLoss: 3.0797e-03   valLoss:2.5646e-02  time: 8.59e-02\n",
      "epoch: 82   trainLoss: 2.0083e-03   valLoss:2.5478e-02  time: 8.51e-02\n",
      "epoch: 83   trainLoss: 2.8429e-03   valLoss:1.8658e-02  time: 8.76e-02\n",
      "epoch: 84   trainLoss: 1.9595e-03   valLoss:2.3354e-02  time: 8.60e-02\n",
      "epoch: 85   trainLoss: 1.7871e-03   valLoss:2.3069e-02  time: 8.53e-02\n",
      "epoch: 86   trainLoss: 1.8859e-03   valLoss:2.0701e-02  time: 8.58e-02\n",
      "epoch: 87   trainLoss: 1.3632e-03   valLoss:2.8139e-02  time: 8.55e-02\n",
      "epoch: 88   trainLoss: 1.5864e-03   valLoss:2.9992e-02  time: 8.51e-02\n",
      "epoch: 89   trainLoss: 1.0961e-03   valLoss:2.6320e-02  time: 8.68e-02\n",
      "epoch: 90   trainLoss: 1.4455e-03   valLoss:2.8390e-02  time: 8.87e-02\n",
      "epoch: 91   trainLoss: 1.0383e-03   valLoss:2.9055e-02  time: 8.60e-02\n",
      "epoch: 92   trainLoss: 8.1920e-04   valLoss:2.5627e-02  time: 8.62e-02\n",
      "epoch: 93   trainLoss: 1.2012e-03   valLoss:2.6149e-02  time: 8.84e-02\n",
      "epoch: 94   trainLoss: 6.7522e-04   valLoss:2.6245e-02  time: 8.80e-02\n",
      "epoch: 95   trainLoss: 7.7774e-04   valLoss:2.5877e-02  time: 8.63e-02\n",
      "epoch: 96   trainLoss: 7.5612e-04   valLoss:2.6624e-02  time: 8.60e-02\n",
      "epoch: 97   trainLoss: 5.8482e-04   valLoss:2.3901e-02  time: 8.73e-02\n",
      "epoch: 98   trainLoss: 7.1307e-04   valLoss:2.3943e-02  time: 8.51e-02\n",
      "epoch: 99   trainLoss: 3.6992e-04   valLoss:2.6802e-02  time: 8.67e-02\n",
      "loading checkpoint 29\n",
      "trained 38 random forest models in 4.23 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.196867</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>0.900059</td>\n",
       "      <td>0.724378</td>\n",
       "      <td>-1.629229</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.249037</td>\n",
       "      <td>0.368699</td>\n",
       "      <td>0.687472</td>\n",
       "      <td>0.490062</td>\n",
       "      <td>-1.091653</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.070574</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.996057</td>\n",
       "      <td>0.959733</td>\n",
       "      <td>0.572509</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.142920</td>\n",
       "      <td>0.725684</td>\n",
       "      <td>0.907137</td>\n",
       "      <td>0.790991</td>\n",
       "      <td>0.175821</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.091106</td>\n",
       "      <td>0.925249</td>\n",
       "      <td>0.981562</td>\n",
       "      <td>0.949793</td>\n",
       "      <td>0.874099</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.207026</td>\n",
       "      <td>0.519149</td>\n",
       "      <td>0.756080</td>\n",
       "      <td>0.675091</td>\n",
       "      <td>0.542115</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.931194</td>\n",
       "      <td>0.960480</td>\n",
       "      <td>0.788970</td>\n",
       "      <td>-1.088449</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.284163</td>\n",
       "      <td>0.368963</td>\n",
       "      <td>0.628796</td>\n",
       "      <td>0.219805</td>\n",
       "      <td>-3.422486</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.054617</td>\n",
       "      <td>0.996814</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.966141</td>\n",
       "      <td>0.365412</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.177836</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.834832</td>\n",
       "      <td>0.666008</td>\n",
       "      <td>-0.336928</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.097148</td>\n",
       "      <td>0.883950</td>\n",
       "      <td>0.964350</td>\n",
       "      <td>0.918599</td>\n",
       "      <td>0.804687</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.009002</td>\n",
       "      <td>0.267570</td>\n",
       "      <td>0.237867</td>\n",
       "      <td>0.611386</td>\n",
       "      <td>0.461198</td>\n",
       "      <td>0.222259</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.144164</td>\n",
       "      <td>0.853540</td>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.865954</td>\n",
       "      <td>0.527781</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.161041</td>\n",
       "      <td>0.718541</td>\n",
       "      <td>0.882039</td>\n",
       "      <td>0.788434</td>\n",
       "      <td>0.358945</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.972476</td>\n",
       "      <td>0.985704</td>\n",
       "      <td>0.952068</td>\n",
       "      <td>0.648683</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.096523</td>\n",
       "      <td>0.898213</td>\n",
       "      <td>0.963096</td>\n",
       "      <td>0.918682</td>\n",
       "      <td>0.702692</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.968230</td>\n",
       "      <td>0.992692</td>\n",
       "      <td>0.982136</td>\n",
       "      <td>0.974185</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.721859</td>\n",
       "      <td>0.914127</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.752173</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.844875</td>\n",
       "      <td>0.929080</td>\n",
       "      <td>0.555179</td>\n",
       "      <td>-7.934489</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.198064</td>\n",
       "      <td>0.713126</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.618041</td>\n",
       "      <td>-2.466829</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.078732</td>\n",
       "      <td>0.959935</td>\n",
       "      <td>0.992229</td>\n",
       "      <td>0.933757</td>\n",
       "      <td>0.340679</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.108008</td>\n",
       "      <td>0.860888</td>\n",
       "      <td>0.941949</td>\n",
       "      <td>0.884115</td>\n",
       "      <td>0.545128</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.060702</td>\n",
       "      <td>0.951576</td>\n",
       "      <td>0.990881</td>\n",
       "      <td>0.973120</td>\n",
       "      <td>0.959391</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.154622</td>\n",
       "      <td>0.666280</td>\n",
       "      <td>0.853625</td>\n",
       "      <td>0.791849</td>\n",
       "      <td>0.653210</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.898892</td>\n",
       "      <td>0.992719</td>\n",
       "      <td>0.840834</td>\n",
       "      <td>-1.028646</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.423713</td>\n",
       "      <td>-0.185599</td>\n",
       "      <td>0.264694</td>\n",
       "      <td>-0.779800</td>\n",
       "      <td>-10.230756</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.067613</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>0.999463</td>\n",
       "      <td>0.925670</td>\n",
       "      <td>-1.206370</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009321</td>\n",
       "      <td>0.279191</td>\n",
       "      <td>0.197250</td>\n",
       "      <td>0.593356</td>\n",
       "      <td>0.271317</td>\n",
       "      <td>-3.496308</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.334317</td>\n",
       "      <td>0.534824</td>\n",
       "      <td>0.864375</td>\n",
       "      <td>0.752944</td>\n",
       "      <td>0.662020</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>0.373871</td>\n",
       "      <td>0.054835</td>\n",
       "      <td>0.329798</td>\n",
       "      <td>0.109170</td>\n",
       "      <td>-0.461638</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.232518</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>0.808845</td>\n",
       "      <td>0.517079</td>\n",
       "      <td>-0.703025</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.409182</td>\n",
       "      <td>0.756760</td>\n",
       "      <td>0.484426</td>\n",
       "      <td>-0.484040</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>0.964518</td>\n",
       "      <td>0.992638</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.719800</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.123512</td>\n",
       "      <td>0.742476</td>\n",
       "      <td>0.927324</td>\n",
       "      <td>0.838643</td>\n",
       "      <td>0.533850</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.072813</td>\n",
       "      <td>0.910170</td>\n",
       "      <td>0.973888</td>\n",
       "      <td>0.959266</td>\n",
       "      <td>0.939186</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.192254</td>\n",
       "      <td>0.416747</td>\n",
       "      <td>0.833934</td>\n",
       "      <td>0.673687</td>\n",
       "      <td>0.528036</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.086008</td>\n",
       "      <td>0.943469</td>\n",
       "      <td>0.977666</td>\n",
       "      <td>0.925228</td>\n",
       "      <td>0.223111</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>0.899303</td>\n",
       "      <td>0.963380</td>\n",
       "      <td>0.909080</td>\n",
       "      <td>0.344880</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.080371</td>\n",
       "      <td>0.936057</td>\n",
       "      <td>0.984916</td>\n",
       "      <td>0.943278</td>\n",
       "      <td>0.636293</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.085463</td>\n",
       "      <td>0.920740</td>\n",
       "      <td>0.975743</td>\n",
       "      <td>0.936312</td>\n",
       "      <td>0.707599</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>0.971655</td>\n",
       "      <td>0.994481</td>\n",
       "      <td>0.983597</td>\n",
       "      <td>0.972547</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.104392</td>\n",
       "      <td>0.818948</td>\n",
       "      <td>0.948758</td>\n",
       "      <td>0.891563</td>\n",
       "      <td>0.821109</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.128052</td>\n",
       "      <td>0.807866</td>\n",
       "      <td>0.967331</td>\n",
       "      <td>0.889170</td>\n",
       "      <td>0.550735</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.119227</td>\n",
       "      <td>0.536347</td>\n",
       "      <td>-0.307512</td>\n",
       "      <td>-9.602679</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.061730</td>\n",
       "      <td>0.976483</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.977345</td>\n",
       "      <td>0.856207</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.258339</td>\n",
       "      <td>0.430104</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.344310</td>\n",
       "      <td>-3.811716</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.129745</td>\n",
       "      <td>0.726463</td>\n",
       "      <td>0.926008</td>\n",
       "      <td>0.883565</td>\n",
       "      <td>0.760467</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>0.289496</td>\n",
       "      <td>-0.063022</td>\n",
       "      <td>0.538701</td>\n",
       "      <td>0.327045</td>\n",
       "      <td>0.031889</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2   minAggR2  \\\n",
       "0   0.000087  0.006301  0.196867  0.750303  0.900059   0.724378  -1.629229   \n",
       "1   0.000155  0.008055  0.249037  0.368699  0.687472   0.490062  -1.091653   \n",
       "2   0.000007  0.001848  0.070574  0.975361  0.996057   0.959733   0.572509   \n",
       "3   0.000061  0.004401  0.142920  0.725684  0.907137   0.790991   0.175821   \n",
       "4   0.000022  0.002893  0.091106  0.925249  0.981562   0.949793   0.874099   \n",
       "5   0.000114  0.006645  0.207026  0.519149  0.756080   0.675091   0.542115   \n",
       "6   0.000038  0.004346  0.160157  0.931194  0.960480   0.788970  -1.088449   \n",
       "7   0.000213  0.009103  0.284163  0.368963  0.628796   0.219805  -3.422486   \n",
       "8   0.000003  0.001264  0.054617  0.996814  0.997626   0.966141   0.365412   \n",
       "9   0.000101  0.005767  0.177836  0.601090  0.834832   0.666008  -0.336928   \n",
       "10  0.000027  0.003002  0.097148  0.883950  0.964350   0.918599   0.804687   \n",
       "11  0.000205  0.009002  0.267570  0.237867  0.611386   0.461198   0.222259   \n",
       "12  0.000036  0.004031  0.144164  0.853540  0.957245   0.865954   0.527781   \n",
       "13  0.000067  0.004949  0.161041  0.718541  0.882039   0.788434   0.358945   \n",
       "14  0.000009  0.002044  0.082488  0.972476  0.985704   0.952068   0.648683   \n",
       "15  0.000023  0.002734  0.096523  0.898213  0.963096   0.918682   0.702692   \n",
       "16  0.000006  0.001401  0.050020  0.968230  0.992692   0.982136   0.974185   \n",
       "17  0.000058  0.004106  0.123400  0.721859  0.914127   0.844377   0.752173   \n",
       "18  0.000052  0.004855  0.169254  0.844875  0.929080   0.555179  -7.934489   \n",
       "19  0.000090  0.005993  0.198064  0.713126  0.889299   0.618041  -2.466829   \n",
       "20  0.000011  0.002261  0.078732  0.959935  0.992229   0.933757   0.340679   \n",
       "21  0.000033  0.003286  0.108008  0.860888  0.941949   0.884115   0.545128   \n",
       "22  0.000009  0.001795  0.060702  0.951576  0.990881   0.973120   0.959391   \n",
       "23  0.000073  0.004963  0.154622  0.666280  0.853625   0.791849   0.653210   \n",
       "24  0.000039  0.004108  0.133018  0.898892  0.992719   0.840834  -1.028646   \n",
       "25  0.000509  0.015829  0.423713 -0.185599  0.264694  -0.779800 -10.230756   \n",
       "26  0.000006  0.001775  0.067613  0.989821  0.999463   0.925670  -1.206370   \n",
       "27  0.000200  0.009321  0.279191  0.197250  0.593356   0.271317  -3.496308   \n",
       "28  0.000188  0.010948  0.334317  0.534824  0.864375   0.752944   0.662020   \n",
       "29  0.000334  0.012642  0.373871  0.054835  0.329798   0.109170  -0.461638   \n",
       "30  0.000105  0.006564  0.232518  0.624363  0.808845   0.517079  -0.703025   \n",
       "31  0.000159  0.008082  0.261958  0.409182  0.756760   0.484426  -0.484040   \n",
       "32  0.000006  0.001696  0.069966  0.964518  0.992638   0.957492   0.719800   \n",
       "33  0.000049  0.003821  0.123512  0.742476  0.927324   0.838643   0.533850   \n",
       "34  0.000011  0.001987  0.072813  0.910170  0.973888   0.959266   0.939186   \n",
       "35  0.000120  0.006460  0.192254  0.416747  0.833934   0.673687   0.528036   \n",
       "36  0.000013  0.002338  0.086008  0.943469  0.977666   0.925228   0.223111   \n",
       "37  0.000021  0.002743  0.096844  0.899303  0.963380   0.909080   0.344880   \n",
       "38  0.000012  0.002206  0.080371  0.936057  0.984916   0.943278   0.636293   \n",
       "39  0.000016  0.002408  0.085463  0.920740  0.975743   0.936312   0.707599   \n",
       "40  0.000005  0.001283  0.043661  0.971655  0.994481   0.983597   0.972547   \n",
       "41  0.000038  0.003357  0.104392  0.818948  0.948758   0.891563   0.821109   \n",
       "42  0.000021  0.003146  0.128052  0.807866  0.967331   0.889170   0.550735   \n",
       "43  0.000283  0.011485  0.354778  0.119227  0.536347  -0.307512  -9.602679   \n",
       "44  0.000003  0.001263  0.061730  0.976483  0.998359   0.977345   0.856207   \n",
       "45  0.000162  0.008174  0.258339  0.430104  0.703239   0.344310  -3.811716   \n",
       "46  0.000025  0.003450  0.129745  0.726463  0.926008   0.883565   0.760467   \n",
       "47  0.000249  0.009695  0.289496 -0.063022  0.538701   0.327045   0.031889   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train          48  \n",
       "1               Fresh   Test          48  \n",
       "2   Transfer learning  Train          48  \n",
       "3   Transfer learning   Test          48  \n",
       "4       Random Forest  Train          48  \n",
       "5       Random Forest   Test          48  \n",
       "6               Fresh  Train          18  \n",
       "7               Fresh   Test          18  \n",
       "8   Transfer learning  Train          18  \n",
       "9   Transfer learning   Test          18  \n",
       "10      Random Forest  Train          18  \n",
       "11      Random Forest   Test          18  \n",
       "12              Fresh  Train         452  \n",
       "13              Fresh   Test         452  \n",
       "14  Transfer learning  Train         452  \n",
       "15  Transfer learning   Test         452  \n",
       "16      Random Forest  Train         452  \n",
       "17      Random Forest   Test         452  \n",
       "18              Fresh  Train         178  \n",
       "19              Fresh   Test         178  \n",
       "20  Transfer learning  Train         178  \n",
       "21  Transfer learning   Test         178  \n",
       "22      Random Forest  Train         178  \n",
       "23      Random Forest   Test         178  \n",
       "24              Fresh  Train           5  \n",
       "25              Fresh   Test           5  \n",
       "26  Transfer learning  Train           5  \n",
       "27  Transfer learning   Test           5  \n",
       "28      Random Forest  Train           5  \n",
       "29      Random Forest   Test           5  \n",
       "30              Fresh  Train          93  \n",
       "31              Fresh   Test          93  \n",
       "32  Transfer learning  Train          93  \n",
       "33  Transfer learning   Test          93  \n",
       "34      Random Forest  Train          93  \n",
       "35      Random Forest   Test          93  \n",
       "36              Fresh  Train         914  \n",
       "37              Fresh   Test         914  \n",
       "38  Transfer learning  Train         914  \n",
       "39  Transfer learning   Test         914  \n",
       "40      Random Forest  Train         914  \n",
       "41      Random Forest   Test         914  \n",
       "42              Fresh  Train           8  \n",
       "43              Fresh   Test           8  \n",
       "44  Transfer learning  Train           8  \n",
       "45  Transfer learning   Test           8  \n",
       "46      Random Forest  Train           8  \n",
       "47      Random Forest   Test           8  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataFiles = glob.glob(os.path.join(dataDir, 'design_9*.csv'))\n",
    "trainDataFiles.remove(testFile)\n",
    "\n",
    "allResults = []\n",
    "for trainDataFile in trainDataFiles:\n",
    "    trainDataUnfiltered = loadGhGraphs(trainDataFile, NUM_DV=5)\n",
    "    trainData = filterbyDispValue(trainDataUnfiltered, maxDispCutoff)\n",
    "    trainSize = len(trainData)\n",
    "    print(f'loaded train set of size {trainSize}')\n",
    "    \n",
    "    \n",
    "    ### fresh neural network ###\n",
    "    gcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                         epochs=epochs, \n",
    "                         saveDir=saveDir+f'{trainSize:05}/gcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Fresh'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Fresh'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "\n",
    "    \n",
    "    ### transfer learning ###\n",
    "    ptrGcn = FeaStNet()\n",
    "    history = gcn.trainModel(trainData, trainData, \n",
    "                             restartFile=ptrGcnCheckptFile,\n",
    "                             epochs=epochs, \n",
    "                             saveDir=saveDir+f'{trainSize:05}/ptrGcn/')\n",
    "    \n",
    "    trainRes = gcn.testModel(trainData)\n",
    "    trainRes['Model'] = 'Transfer learning'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = gcn.testModel(testData)\n",
    "    testRes['Model'] = 'Transfer learning'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "    ### random forest ###\n",
    "    rf = PointRegressor('Random Forest')\n",
    "    rf.trainModel(trainData, trainData, \n",
    "                     saveDir=saveDir+f'{trainSize:05}/rf/')\n",
    "\n",
    "    trainRes = rf.testModel(trainData)\n",
    "    trainRes['Model'] = 'Random Forest'\n",
    "    trainRes['Set'] = 'Train'\n",
    "    trainRes['Train Size'] = trainSize\n",
    "    allResults.append(trainRes)\n",
    "    \n",
    "    testRes = rf.testModel(testData)\n",
    "    testRes['Model'] = 'Random Forest'\n",
    "    testRes['Set'] = 'Test'\n",
    "    testRes['Train Size'] = trainSize\n",
    "    allResults.append(testRes)\n",
    "    pd.DataFrame(allResults).to_csv(saveDir+'testResults.csv', index=False)\n",
    "    \n",
    "    \n",
    "pd.DataFrame(allResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mre</th>\n",
       "      <th>peakR2</th>\n",
       "      <th>maxAggR2</th>\n",
       "      <th>meanAggR2</th>\n",
       "      <th>minAggR2</th>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th>Train Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.196867</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>0.900059</td>\n",
       "      <td>0.724378</td>\n",
       "      <td>-1.629229</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.249037</td>\n",
       "      <td>0.368699</td>\n",
       "      <td>0.687472</td>\n",
       "      <td>0.490062</td>\n",
       "      <td>-1.091653</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.070574</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.996057</td>\n",
       "      <td>0.959733</td>\n",
       "      <td>0.572509</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.142920</td>\n",
       "      <td>0.725684</td>\n",
       "      <td>0.907137</td>\n",
       "      <td>0.790991</td>\n",
       "      <td>0.175821</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.091106</td>\n",
       "      <td>0.925249</td>\n",
       "      <td>0.981562</td>\n",
       "      <td>0.949793</td>\n",
       "      <td>0.874099</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.207026</td>\n",
       "      <td>0.519149</td>\n",
       "      <td>0.756080</td>\n",
       "      <td>0.675091</td>\n",
       "      <td>0.542115</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.931194</td>\n",
       "      <td>0.960480</td>\n",
       "      <td>0.788970</td>\n",
       "      <td>-1.088449</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.284163</td>\n",
       "      <td>0.368963</td>\n",
       "      <td>0.628796</td>\n",
       "      <td>0.219805</td>\n",
       "      <td>-3.422486</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.054617</td>\n",
       "      <td>0.996814</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.966141</td>\n",
       "      <td>0.365412</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.177836</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.834832</td>\n",
       "      <td>0.666008</td>\n",
       "      <td>-0.336928</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.097148</td>\n",
       "      <td>0.883950</td>\n",
       "      <td>0.964350</td>\n",
       "      <td>0.918599</td>\n",
       "      <td>0.804687</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.009002</td>\n",
       "      <td>0.267570</td>\n",
       "      <td>0.237867</td>\n",
       "      <td>0.611386</td>\n",
       "      <td>0.461198</td>\n",
       "      <td>0.222259</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.144164</td>\n",
       "      <td>0.853540</td>\n",
       "      <td>0.957245</td>\n",
       "      <td>0.865954</td>\n",
       "      <td>0.527781</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.161041</td>\n",
       "      <td>0.718541</td>\n",
       "      <td>0.882039</td>\n",
       "      <td>0.788434</td>\n",
       "      <td>0.358945</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>0.972476</td>\n",
       "      <td>0.985704</td>\n",
       "      <td>0.952068</td>\n",
       "      <td>0.648683</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.096523</td>\n",
       "      <td>0.898213</td>\n",
       "      <td>0.963096</td>\n",
       "      <td>0.918682</td>\n",
       "      <td>0.702692</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.968230</td>\n",
       "      <td>0.992692</td>\n",
       "      <td>0.982136</td>\n",
       "      <td>0.974185</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.721859</td>\n",
       "      <td>0.914127</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.752173</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.844875</td>\n",
       "      <td>0.929080</td>\n",
       "      <td>0.555179</td>\n",
       "      <td>-7.934489</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.198064</td>\n",
       "      <td>0.713126</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.618041</td>\n",
       "      <td>-2.466829</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.078732</td>\n",
       "      <td>0.959935</td>\n",
       "      <td>0.992229</td>\n",
       "      <td>0.933757</td>\n",
       "      <td>0.340679</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.108008</td>\n",
       "      <td>0.860888</td>\n",
       "      <td>0.941949</td>\n",
       "      <td>0.884115</td>\n",
       "      <td>0.545128</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.060702</td>\n",
       "      <td>0.951576</td>\n",
       "      <td>0.990881</td>\n",
       "      <td>0.973120</td>\n",
       "      <td>0.959391</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.154622</td>\n",
       "      <td>0.666280</td>\n",
       "      <td>0.853625</td>\n",
       "      <td>0.791849</td>\n",
       "      <td>0.653210</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.898892</td>\n",
       "      <td>0.992719</td>\n",
       "      <td>0.840834</td>\n",
       "      <td>-1.028646</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.423713</td>\n",
       "      <td>-0.185599</td>\n",
       "      <td>0.264694</td>\n",
       "      <td>-0.779800</td>\n",
       "      <td>-10.230756</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.067613</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>0.999463</td>\n",
       "      <td>0.925670</td>\n",
       "      <td>-1.206370</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009321</td>\n",
       "      <td>0.279191</td>\n",
       "      <td>0.197250</td>\n",
       "      <td>0.593356</td>\n",
       "      <td>0.271317</td>\n",
       "      <td>-3.496308</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.334317</td>\n",
       "      <td>0.534824</td>\n",
       "      <td>0.864375</td>\n",
       "      <td>0.752944</td>\n",
       "      <td>0.662020</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>0.373871</td>\n",
       "      <td>0.054835</td>\n",
       "      <td>0.329798</td>\n",
       "      <td>0.109170</td>\n",
       "      <td>-0.461638</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.232518</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>0.808845</td>\n",
       "      <td>0.517079</td>\n",
       "      <td>-0.703025</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.409182</td>\n",
       "      <td>0.756760</td>\n",
       "      <td>0.484426</td>\n",
       "      <td>-0.484040</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>0.964518</td>\n",
       "      <td>0.992638</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.719800</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.123512</td>\n",
       "      <td>0.742476</td>\n",
       "      <td>0.927324</td>\n",
       "      <td>0.838643</td>\n",
       "      <td>0.533850</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.072813</td>\n",
       "      <td>0.910170</td>\n",
       "      <td>0.973888</td>\n",
       "      <td>0.959266</td>\n",
       "      <td>0.939186</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.192254</td>\n",
       "      <td>0.416747</td>\n",
       "      <td>0.833934</td>\n",
       "      <td>0.673687</td>\n",
       "      <td>0.528036</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.086008</td>\n",
       "      <td>0.943469</td>\n",
       "      <td>0.977666</td>\n",
       "      <td>0.925228</td>\n",
       "      <td>0.223111</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>0.899303</td>\n",
       "      <td>0.963380</td>\n",
       "      <td>0.909080</td>\n",
       "      <td>0.344880</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.080371</td>\n",
       "      <td>0.936057</td>\n",
       "      <td>0.984916</td>\n",
       "      <td>0.943278</td>\n",
       "      <td>0.636293</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.085463</td>\n",
       "      <td>0.920740</td>\n",
       "      <td>0.975743</td>\n",
       "      <td>0.936312</td>\n",
       "      <td>0.707599</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>0.971655</td>\n",
       "      <td>0.994481</td>\n",
       "      <td>0.983597</td>\n",
       "      <td>0.972547</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.104392</td>\n",
       "      <td>0.818948</td>\n",
       "      <td>0.948758</td>\n",
       "      <td>0.891563</td>\n",
       "      <td>0.821109</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.128052</td>\n",
       "      <td>0.807866</td>\n",
       "      <td>0.967331</td>\n",
       "      <td>0.889170</td>\n",
       "      <td>0.550735</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.119227</td>\n",
       "      <td>0.536347</td>\n",
       "      <td>-0.307512</td>\n",
       "      <td>-9.602679</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.061730</td>\n",
       "      <td>0.976483</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.977345</td>\n",
       "      <td>0.856207</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.258339</td>\n",
       "      <td>0.430104</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.344310</td>\n",
       "      <td>-3.811716</td>\n",
       "      <td>Transfer learning</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.129745</td>\n",
       "      <td>0.726463</td>\n",
       "      <td>0.926008</td>\n",
       "      <td>0.883565</td>\n",
       "      <td>0.760467</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>0.289496</td>\n",
       "      <td>-0.063022</td>\n",
       "      <td>0.538701</td>\n",
       "      <td>0.327045</td>\n",
       "      <td>0.031889</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Test</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mse       mae       mre    peakR2  maxAggR2  meanAggR2   minAggR2  \\\n",
       "0   0.000087  0.006301  0.196867  0.750303  0.900059   0.724378  -1.629229   \n",
       "1   0.000155  0.008055  0.249037  0.368699  0.687472   0.490062  -1.091653   \n",
       "2   0.000007  0.001848  0.070574  0.975361  0.996057   0.959733   0.572509   \n",
       "3   0.000061  0.004401  0.142920  0.725684  0.907137   0.790991   0.175821   \n",
       "4   0.000022  0.002893  0.091106  0.925249  0.981562   0.949793   0.874099   \n",
       "5   0.000114  0.006645  0.207026  0.519149  0.756080   0.675091   0.542115   \n",
       "6   0.000038  0.004346  0.160157  0.931194  0.960480   0.788970  -1.088449   \n",
       "7   0.000213  0.009103  0.284163  0.368963  0.628796   0.219805  -3.422486   \n",
       "8   0.000003  0.001264  0.054617  0.996814  0.997626   0.966141   0.365412   \n",
       "9   0.000101  0.005767  0.177836  0.601090  0.834832   0.666008  -0.336928   \n",
       "10  0.000027  0.003002  0.097148  0.883950  0.964350   0.918599   0.804687   \n",
       "11  0.000205  0.009002  0.267570  0.237867  0.611386   0.461198   0.222259   \n",
       "12  0.000036  0.004031  0.144164  0.853540  0.957245   0.865954   0.527781   \n",
       "13  0.000067  0.004949  0.161041  0.718541  0.882039   0.788434   0.358945   \n",
       "14  0.000009  0.002044  0.082488  0.972476  0.985704   0.952068   0.648683   \n",
       "15  0.000023  0.002734  0.096523  0.898213  0.963096   0.918682   0.702692   \n",
       "16  0.000006  0.001401  0.050020  0.968230  0.992692   0.982136   0.974185   \n",
       "17  0.000058  0.004106  0.123400  0.721859  0.914127   0.844377   0.752173   \n",
       "18  0.000052  0.004855  0.169254  0.844875  0.929080   0.555179  -7.934489   \n",
       "19  0.000090  0.005993  0.198064  0.713126  0.889299   0.618041  -2.466829   \n",
       "20  0.000011  0.002261  0.078732  0.959935  0.992229   0.933757   0.340679   \n",
       "21  0.000033  0.003286  0.108008  0.860888  0.941949   0.884115   0.545128   \n",
       "22  0.000009  0.001795  0.060702  0.951576  0.990881   0.973120   0.959391   \n",
       "23  0.000073  0.004963  0.154622  0.666280  0.853625   0.791849   0.653210   \n",
       "24  0.000039  0.004108  0.133018  0.898892  0.992719   0.840834  -1.028646   \n",
       "25  0.000509  0.015829  0.423713 -0.185599  0.264694  -0.779800 -10.230756   \n",
       "26  0.000006  0.001775  0.067613  0.989821  0.999463   0.925670  -1.206370   \n",
       "27  0.000200  0.009321  0.279191  0.197250  0.593356   0.271317  -3.496308   \n",
       "28  0.000188  0.010948  0.334317  0.534824  0.864375   0.752944   0.662020   \n",
       "29  0.000334  0.012642  0.373871  0.054835  0.329798   0.109170  -0.461638   \n",
       "30  0.000105  0.006564  0.232518  0.624363  0.808845   0.517079  -0.703025   \n",
       "31  0.000159  0.008082  0.261958  0.409182  0.756760   0.484426  -0.484040   \n",
       "32  0.000006  0.001696  0.069966  0.964518  0.992638   0.957492   0.719800   \n",
       "33  0.000049  0.003821  0.123512  0.742476  0.927324   0.838643   0.533850   \n",
       "34  0.000011  0.001987  0.072813  0.910170  0.973888   0.959266   0.939186   \n",
       "35  0.000120  0.006460  0.192254  0.416747  0.833934   0.673687   0.528036   \n",
       "36  0.000013  0.002338  0.086008  0.943469  0.977666   0.925228   0.223111   \n",
       "37  0.000021  0.002743  0.096844  0.899303  0.963380   0.909080   0.344880   \n",
       "38  0.000012  0.002206  0.080371  0.936057  0.984916   0.943278   0.636293   \n",
       "39  0.000016  0.002408  0.085463  0.920740  0.975743   0.936312   0.707599   \n",
       "40  0.000005  0.001283  0.043661  0.971655  0.994481   0.983597   0.972547   \n",
       "41  0.000038  0.003357  0.104392  0.818948  0.948758   0.891563   0.821109   \n",
       "42  0.000021  0.003146  0.128052  0.807866  0.967331   0.889170   0.550735   \n",
       "43  0.000283  0.011485  0.354778  0.119227  0.536347  -0.307512  -9.602679   \n",
       "44  0.000003  0.001263  0.061730  0.976483  0.998359   0.977345   0.856207   \n",
       "45  0.000162  0.008174  0.258339  0.430104  0.703239   0.344310  -3.811716   \n",
       "46  0.000025  0.003450  0.129745  0.726463  0.926008   0.883565   0.760467   \n",
       "47  0.000249  0.009695  0.289496 -0.063022  0.538701   0.327045   0.031889   \n",
       "\n",
       "                Model    Set  Train Size  \n",
       "0               Fresh  Train          48  \n",
       "1               Fresh   Test          48  \n",
       "2   Transfer learning  Train          48  \n",
       "3   Transfer learning   Test          48  \n",
       "4       Random Forest  Train          48  \n",
       "5       Random Forest   Test          48  \n",
       "6               Fresh  Train          18  \n",
       "7               Fresh   Test          18  \n",
       "8   Transfer learning  Train          18  \n",
       "9   Transfer learning   Test          18  \n",
       "10      Random Forest  Train          18  \n",
       "11      Random Forest   Test          18  \n",
       "12              Fresh  Train         452  \n",
       "13              Fresh   Test         452  \n",
       "14  Transfer learning  Train         452  \n",
       "15  Transfer learning   Test         452  \n",
       "16      Random Forest  Train         452  \n",
       "17      Random Forest   Test         452  \n",
       "18              Fresh  Train         178  \n",
       "19              Fresh   Test         178  \n",
       "20  Transfer learning  Train         178  \n",
       "21  Transfer learning   Test         178  \n",
       "22      Random Forest  Train         178  \n",
       "23      Random Forest   Test         178  \n",
       "24              Fresh  Train           5  \n",
       "25              Fresh   Test           5  \n",
       "26  Transfer learning  Train           5  \n",
       "27  Transfer learning   Test           5  \n",
       "28      Random Forest  Train           5  \n",
       "29      Random Forest   Test           5  \n",
       "30              Fresh  Train          93  \n",
       "31              Fresh   Test          93  \n",
       "32  Transfer learning  Train          93  \n",
       "33  Transfer learning   Test          93  \n",
       "34      Random Forest  Train          93  \n",
       "35      Random Forest   Test          93  \n",
       "36              Fresh  Train         914  \n",
       "37              Fresh   Test         914  \n",
       "38  Transfer learning  Train         914  \n",
       "39  Transfer learning   Test         914  \n",
       "40      Random Forest  Train         914  \n",
       "41      Random Forest   Test         914  \n",
       "42              Fresh  Train           8  \n",
       "43              Fresh   Test           8  \n",
       "44  Transfer learning  Train           8  \n",
       "45  Transfer learning   Test           8  \n",
       "46      Random Forest  Train           8  \n",
       "47      Random Forest   Test           8  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(allResults)\n",
    "df = pd.read_csv('results/transferLrn_des9_01/testResults.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('Fresh', 'GCN')\n",
    "df = df.replace('Transfer learning', 'GCN with transfer learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-893ef883f72c4839b7d3bff8ddc3b247\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-893ef883f72c4839b7d3bff8ddc3b247\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-893ef883f72c4839b7d3bff8ddc3b247\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-e0f783c3ac0512c1f1c7e63f3c4cd8cd\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Model\"}, {\"type\": \"quantitative\", \"field\": \"mse\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Train Size\", \"scale\": {\"type\": \"log\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".1e\"}, \"field\": \"mse\", \"title\": \"MSE\"}}, \"height\": 200, \"title\": \"Design Group 9\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-e0f783c3ac0512c1f1c7e63f3c4cd8cd\": [{\"mse\": 0.00015516727580688894, \"mae\": 0.008055133745074272, \"mre\": 0.2490367144346237, \"peakR2\": 0.368698890284106, \"maxAggR2\": 0.6874718898508241, \"meanAggR2\": 0.49006224326908904, \"minAggR2\": -1.0916528351402768, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 6.119624595157802e-05, \"mae\": 0.004400972742587328, \"mre\": 0.14292044937610626, \"peakR2\": 0.7256836898899566, \"maxAggR2\": 0.9071372007126962, \"meanAggR2\": 0.7909914390827947, \"minAggR2\": 0.17582050228501056, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 0.00011399667931276543, \"mae\": 0.00664543312248434, \"mre\": 0.20702606965871032, \"peakR2\": 0.5191490460887206, \"maxAggR2\": 0.7560800580877197, \"meanAggR2\": 0.6750906831522373, \"minAggR2\": 0.5421147069266397, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 48}, {\"mse\": 0.00021324545377865436, \"mae\": 0.009103028103709221, \"mre\": 0.2841626703739166, \"peakR2\": 0.36896334461793295, \"maxAggR2\": 0.6287957029996789, \"meanAggR2\": 0.21980468014199425, \"minAggR2\": -3.4224859406339805, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 0.00010076923354063183, \"mae\": 0.005766655318439007, \"mre\": 0.17783582210540771, \"peakR2\": 0.6010898452922362, \"maxAggR2\": 0.834831882129893, \"meanAggR2\": 0.6660083642899834, \"minAggR2\": -0.3369282865260026, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 0.00020456276423966305, \"mae\": 0.00900180137921571, \"mre\": 0.26757033642565986, \"peakR2\": 0.2378672348317317, \"maxAggR2\": 0.6113863767406179, \"meanAggR2\": 0.4611977297529, \"minAggR2\": 0.22225876411635348, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 18}, {\"mse\": 6.718411896144971e-05, \"mae\": 0.004948869813233614, \"mre\": 0.1610410213470459, \"peakR2\": 0.7185412157948792, \"maxAggR2\": 0.8820385196260044, \"meanAggR2\": 0.7884341618886761, \"minAggR2\": 0.35894484953628303, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 2.3457796487491574e-05, \"mae\": 0.002734175650402904, \"mre\": 0.09652306139469148, \"peakR2\": 0.8982127908799585, \"maxAggR2\": 0.9630961560057932, \"meanAggR2\": 0.9186820424396888, \"minAggR2\": 0.7026924633225613, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 5.775244370402346e-05, \"mae\": 0.004105792392848588, \"mre\": 0.12339952683040488, \"peakR2\": 0.7218594499575828, \"maxAggR2\": 0.9141266094888876, \"meanAggR2\": 0.8443765236543336, \"minAggR2\": 0.7521728006820698, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 452}, {\"mse\": 9.045186743605882e-05, \"mae\": 0.005992776248604059, \"mre\": 0.1980640292167664, \"peakR2\": 0.7131264390553793, \"maxAggR2\": 0.8892992113967049, \"meanAggR2\": 0.6180405811219473, \"minAggR2\": -2.466829099661393, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 3.3241776691284024e-05, \"mae\": 0.003286197781562805, \"mre\": 0.10800820589065553, \"peakR2\": 0.8608882006762436, \"maxAggR2\": 0.9419492117975341, \"meanAggR2\": 0.8841147471227253, \"minAggR2\": 0.5451281876879209, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 7.252792385502035e-05, \"mae\": 0.004963357351382494, \"mre\": 0.15462182517628012, \"peakR2\": 0.666279803338983, \"maxAggR2\": 0.8536247365032391, \"meanAggR2\": 0.7918486048745045, \"minAggR2\": 0.6532102878457473, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 178}, {\"mse\": 0.0005085422308184206, \"mae\": 0.015828743577003482, \"mre\": 0.4237130582332611, \"peakR2\": -0.1855990054464272, \"maxAggR2\": 0.2646941912787604, \"meanAggR2\": -0.7797995831655955, \"minAggR2\": -10.230755990456087, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.00019999075448140502, \"mae\": 0.00932089798152447, \"mre\": 0.27919089794158936, \"peakR2\": 0.1972502750825693, \"maxAggR2\": 0.5933558757343385, \"meanAggR2\": 0.2713165196757801, \"minAggR2\": -3.4963081900664186, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.00033355526675320137, \"mae\": 0.01264163652288955, \"mre\": 0.3738712349150535, \"peakR2\": 0.05483472625356334, \"maxAggR2\": 0.3297981848513515, \"meanAggR2\": 0.10916977672649504, \"minAggR2\": -0.461638339492807, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 5}, {\"mse\": 0.0001591228210600093, \"mae\": 0.008081943728029728, \"mre\": 0.26195818185806274, \"peakR2\": 0.4091819522855977, \"maxAggR2\": 0.7567597025488516, \"meanAggR2\": 0.4844264932103222, \"minAggR2\": -0.4840404795164355, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 4.86294120491948e-05, \"mae\": 0.0038206973113119602, \"mre\": 0.12351195514202118, \"peakR2\": 0.7424758457050746, \"maxAggR2\": 0.9273243171955452, \"meanAggR2\": 0.8386431690505929, \"minAggR2\": 0.5338503209621976, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 0.00011999185980024234, \"mae\": 0.006460457294624732, \"mre\": 0.1922535584315628, \"peakR2\": 0.4167474334562677, \"maxAggR2\": 0.8339339663634284, \"meanAggR2\": 0.6736866489530935, \"minAggR2\": 0.5280357287017414, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 93}, {\"mse\": 2.1408772227005105e-05, \"mae\": 0.002743244869634509, \"mre\": 0.09684422612190248, \"peakR2\": 0.8993030776043724, \"maxAggR2\": 0.9633797809417168, \"meanAggR2\": 0.9090797777449598, \"minAggR2\": 0.3448801443177604, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 1.648949364607688e-05, \"mae\": 0.002407976426184177, \"mre\": 0.08546258509159088, \"peakR2\": 0.9207399951406624, \"maxAggR2\": 0.9757427482126316, \"meanAggR2\": 0.9363122810836294, \"minAggR2\": 0.7075985699387475, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 3.825993806094885e-05, \"mae\": 0.0033574109720321656, \"mre\": 0.10439198986545067, \"peakR2\": 0.8189482205179749, \"maxAggR2\": 0.9487575922272938, \"meanAggR2\": 0.8915632525633488, \"minAggR2\": 0.8211093704618875, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 914}, {\"mse\": 0.0002828380092978477, \"mae\": 0.011484951712191105, \"mre\": 0.3547778129577637, \"peakR2\": 0.11922721120065738, \"maxAggR2\": 0.5363471252407771, \"meanAggR2\": -0.307512166348647, \"minAggR2\": -9.602678643080713, \"Model\": \"GCN\", \"Set\": \"Test\", \"Train Size\": 8}, {\"mse\": 0.00016187084838747978, \"mae\": 0.008173850364983082, \"mre\": 0.2583393156528473, \"peakR2\": 0.4301035247182511, \"maxAggR2\": 0.7032388714322666, \"meanAggR2\": 0.3443098299835228, \"minAggR2\": -3.8117163840952832, \"Model\": \"GCN with transfer learning\", \"Set\": \"Test\", \"Train Size\": 8}, {\"mse\": 0.00024902838461739565, \"mae\": 0.00969540818190918, \"mre\": 0.2894961669775382, \"peakR2\": -0.06302222576510585, \"maxAggR2\": 0.5387008998305595, \"meanAggR2\": 0.3270446694390661, \"minAggR2\": 0.031889008392224534, \"Model\": \"Random Forest\", \"Set\": \"Test\", \"Train Size\": 8}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df[df.Set=='Test']).mark_circle().encode(\n",
    "    x=alt.X('Train Size:Q', scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('mse:Q', title='MSE', axis=alt.Axis(format='.1e')),\n",
    "    color='Model',\n",
    "    tooltip=['Model', 'mse']\n",
    ").properties(width=400, height=200, title='Design Group 9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom",
   "language": "python",
   "name": "ptgeom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
