{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "import pyvista as pv\n",
    "import altair as alt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "sys.path.append('C:/Users/ewhalen/OneDrive - Massachusetts Institute of Technology/research/toolbox/caeSurrogateUtility/')\n",
    "import caeUtility as cu\n",
    "\n",
    "sys.path.append('../readers')\n",
    "from loadGhGraphs import loadGhGraphs\n",
    "\n",
    "sys.path.append('../visualization')\n",
    "from trussViz2D import plotTruss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toss out the wost 10% of designs\n",
    "def filterbyDisp(graphList, pctCutoff):\n",
    "    maxes = [max(np.abs(graph.y.numpy().flatten())) for graph in allGraphsUnfiltered]\n",
    "    df = pd.DataFrame(maxes, columns=['maxes'])\n",
    "    mask = df['maxes'].rank(pct=True) <= pctCutoff\n",
    "    return [g for g,b in zip(graphList, mask) if b]\n",
    "\n",
    "# partition into train, validate and test\n",
    "def partitionGraphList(allGraphs):\n",
    "    allIds = list(range(len(allGraphs)))\n",
    "    other, testIds = train_test_split(allIds, test_size=0.2, shuffle=True, random_state=1234) # 20% test\n",
    "    trainIds, valIds = train_test_split(other, test_size=0.15, shuffle=True, random_state=1234) # 15% val 15% test\n",
    "    trainGraphs = [allGraphs[i] for i in trainIds]\n",
    "    valGraphs = [allGraphs[i] for i in valIds]\n",
    "    testGraphs = [allGraphs[i] for i in testIds]\n",
    "    return trainGraphs, valGraphs, testGraphs\n",
    "\n",
    "logTrans = lambda x: np.sign(x)*np.log(10.0*np.abs(x)+1.0)\n",
    "invLogTrans = lambda y: np.sign(y)*(np.exp(np.abs(y))-1.0)/10.0\n",
    "\n",
    "def fitSS(graphList, logTrans=True, ssTrans=True, flatten=False):\n",
    "    ss = StandardScaler()\n",
    "    if flatten:\n",
    "        allResponses = np.empty((0,1))\n",
    "        for graph in graphList:\n",
    "            allResponses = np.vstack([allResponses, graph.y.reshape(-1,1)])\n",
    "    else:\n",
    "        allResponses = np.empty((0,graphList[0].y.numpy().size))\n",
    "        for graph in graphList:\n",
    "            allResponses = np.vstack([allResponses, graph.y.reshape(1,-1)])\n",
    "    ss.fit(allResponses)\n",
    "    ss.logTrans = logTrans\n",
    "    ss.ssTrans = ssTrans\n",
    "    ss.flatten = flatten\n",
    "    return ss\n",
    "\n",
    "def applySS(ss, graphList):\n",
    "    transformedGraphList = [g.clone() for g in graphList] # deep copy\n",
    "    for graph in transformedGraphList:\n",
    "        if ss.ssTrans:\n",
    "            if ss.flatten:\n",
    "                graph.y = torch.as_tensor(ss.transform(graph.y.reshape(-1,1).cpu()).reshape(-1,2), dtype=torch.float)\n",
    "            else:\n",
    "                graph.y = torch.as_tensor(ss.transform(graph.y.reshape(1,-1).cpu()).reshape(-1,2), dtype=torch.float)\n",
    "        if ss.logTrans: \n",
    "            graph.y = logTrans(graph.y)\n",
    "    return transformedGraphList\n",
    "\n",
    "def applyInvSS(ss, out):\n",
    "    if ss.logTrans: \n",
    "        out = invLogTrans(out)\n",
    "    if ss.ssTrans:\n",
    "        if ss.flatten:\n",
    "            out = ss.inverse_transform(out.reshape(-1,1)).reshape(-1,2)\n",
    "        else:\n",
    "            out = ss.inverse_transform(out.reshape(1,-1)).reshape(-1,2)\n",
    "    return out\n",
    "\n",
    "## define network architecture\n",
    "#############################################################\n",
    "class FeaStNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeaStNet, self).__init__()\n",
    "        self.norm0 = tg.nn.BatchNorm(4, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.lin0 = torch.nn.Linear(4, 16)\n",
    "        self.conv0 = tg.nn.FeaStConv(16, 32, heads=8) # The (translation-invariant) FeaStNet convolution\n",
    "        self.normc0 = tg.nn.BatchNorm(32, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.conv1 = tg.nn.FeaStConv(32, 64, heads=8)\n",
    "        self.normc1 = tg.nn.BatchNorm(64, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.conv2 = tg.nn.FeaStConv(64, 128, heads=8)\n",
    "        self.normc2 = tg.nn.BatchNorm(128, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.conv3 = tg.nn.FeaStConv(128, 256, heads=8)\n",
    "        self.normc3 = tg.nn.BatchNorm(256, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.conv4 = tg.nn.FeaStConv(256, 128, heads=8)\n",
    "        self.normc4 = tg.nn.BatchNorm(128, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.conv5 = tg.nn.FeaStConv(128, 128, heads=8)\n",
    "        self.normc5 = tg.nn.BatchNorm(128, momentum=0.3, affine=True, track_running_stats=True)\n",
    "        self.lin1 = torch.nn.Linear(128, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = torch.cat([data.pos, data.x.float()], 1)\n",
    "        data.x = self.norm0(data.x)\n",
    "        data.x = self.lin0(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.conv0(data.x, data.edge_index)\n",
    "        data.x = self.normc0(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.conv1(data.x, data.edge_index)\n",
    "        data.x = self.normc1(data.x)\n",
    "        data.x = F.relu(data.x)        \n",
    "        data.x = self.conv2(data.x, data.edge_index)\n",
    "        data.x = self.normc2(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.conv3(data.x, data.edge_index)\n",
    "        data.x = self.normc3(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.conv4(data.x, data.edge_index)\n",
    "        data.x = self.normc4(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.conv5(data.x, data.edge_index)\n",
    "        data.x = self.normc5(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.lin1(data.x)\n",
    "        data.x = F.relu(data.x)\n",
    "        data.x = self.lin2(data.x)\n",
    "        return data.x\n",
    "    \n",
    "# configure training\n",
    "def train(model, trainGraphs, valGraphs, device, epochs=10, saveDir=None, batch_size=256, flatten=False):\n",
    "    # prep train data\n",
    "    model.ss = fitSS(trainGraphs, flatten=flatten)\n",
    "    ssFile = None\n",
    "    modelFile = None\n",
    "    if saveDir:\n",
    "        if not os.path.exists(saveDir): os.mkdir(saveDir)\n",
    "        ssFile = os.path.join(saveDir, 'ss.pkl')\n",
    "        pickle.dump(model.ss, open(ssFile, 'wb'))\n",
    "    trainGraphsScaled = applySS(model.ss, trainGraphs)\n",
    "    loader = tg.data.DataLoader(trainGraphsScaled, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # prep validation data\n",
    "    valGraphsScaled = applySS(model.ss, valGraphs)\n",
    "    valLoader = tg.data.DataLoader(valGraphsScaled, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # prep model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=10e-4)\n",
    "    trainHist, valHist = [], []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        ### train ###\n",
    "        model.train()\n",
    "        t = time()\n",
    "        batchHist = []\n",
    "        for batch in loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.mse_loss(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batchHist.append(loss.item())\n",
    "\n",
    "        trainHist.append(np.mean(batchHist))\n",
    "        \n",
    "        ### validate ###\n",
    "        batchHist = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valLoader:\n",
    "                batch.to(device)\n",
    "                out = model(batch)\n",
    "                loss = F.mse_loss(out, batch.y)\n",
    "                batchHist.append(loss.item())\n",
    "        valHist.append(np.mean(batchHist))\n",
    "\n",
    "        print(f'epoch: {epoch}   trainLoss: {trainHist[-1]:.4e}   time: {(time()-t):.2e}')\n",
    "              \n",
    "        if saveDir:\n",
    "            with open(os.path.join(saveDir, 'trainlog.csv'), 'a') as fp: \n",
    "                fp.write(f'{epoch},{trainHist[-1]},{valHist[-1]},{(time()-t)}/n')\n",
    "                \n",
    "            if (np.argmin(valHist) == len(valHist)-1):\n",
    "                modelFile = os.path.join(saveDir, f'checkpoint_{epoch}')\n",
    "                torch.save(model.state_dict(), modelFile) # save best model    \n",
    "\n",
    "    return trainHist, valHist, modelFile, ssFile\n",
    "\n",
    "def predict(model, inputs, device):\n",
    "    # prep data\n",
    "    inputsScaled = applySS(model.ss, inputs)\n",
    "    testLoader = tg.data.DataLoader(inputsScaled, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            batch.to(device)\n",
    "            out = model(batch)\n",
    "            p = applyInvSS(model.ss, out.cpu().numpy())\n",
    "            preds.append(p)\n",
    "    return preds\n",
    "\n",
    "def test(model, inputs, outputs, baselineRef, device, level='set'):\n",
    "    preds = predict(model, inputs, device)\n",
    "    if baselineRef: baselineRef = [b.y.numpy() for b in baselineRef]\n",
    "    return cu.computeFieldLossMetrics([g.y.numpy() for g in outputs], \n",
    "                                      preds, \n",
    "                                      baselineRef=baselineRef, level=level)\n",
    "        \n",
    "def loadModelFromFile(modelFile, ssFile):\n",
    "    bestModel.load_state_dict(torch.load(modelFile, map_location=torch.device('cpu')))\n",
    "    model = FeaStNet()\n",
    "    model.load_state_dict(torch.load(modelFile))\n",
    "    ss = pickle.load(open(ssFile, 'rb'))\n",
    "    model.ss = ss\n",
    "    return model\n",
    "\n",
    "def plotHistory(trainHist, valHist):\n",
    "    histDf = pd.DataFrame({'train': trainHist, 'val': valHist})\n",
    "    return alt.Chart(histDf.reset_index()).transform_fold(\n",
    "            ['train', 'val'],\n",
    "            as_=['metric', 'value']\n",
    "        ).mark_line().encode(\n",
    "            alt.X('index:Q'),\n",
    "            alt.Y('value:Q', axis=alt.Axis(title='loss')),\n",
    "            color=alt.Color('metric:N'),\n",
    "            tooltip=['epoch:Q', 'value:Q']\n",
    "        ).properties(width=400, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1000 graphs\n",
      "loaded 1000 graphs\n"
     ]
    }
   ],
   "source": [
    "# load group 5\n",
    "doeFile = 'C:/Users/ewhalen/OneDrive - Massachusetts Institute of Technology/research/data/trusses/EW/v1.3/design_5_N_1000.csv'\n",
    "allGraphsUnfiltered = loadGhGraphs(doeFile, NUM_DV=5)\n",
    "print(f'loaded {len(allGraphsUnfiltered)} graphs')\n",
    "all5Graphs = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "trainGraphs5, valGraphs5, testGraphs5 = partitionGraphList(all5Graphs)\n",
    "\n",
    "# load group 9\n",
    "doeFile = 'C:/Users/ewhalen/OneDrive - Massachusetts Institute of Technology/research/data/trusses/EW/v1.3/design_9_N_1000.csv'\n",
    "allGraphsUnfiltered = loadGhGraphs(doeFile, NUM_DV=5)\n",
    "print(f'loaded {len(allGraphsUnfiltered)} graphs')\n",
    "all9Graphs = filterbyDisp(allGraphsUnfiltered, 0.9)\n",
    "trainGraphs9, valGraphs9, testGraphs9 = partitionGraphList(all9Graphs)\n",
    "\n",
    "# combine data sets\n",
    "trainSets = {'group 5':trainGraphs5, 'group 9':trainGraphs9, 'comb':trainGraphs5+trainGraphs9}\n",
    "valSets = {'group 5':valGraphs5, 'group 9':valGraphs9, 'comb':valGraphs5+valGraphs9}\n",
    "testSets = {'group 5':testGraphs5, 'group 9':testGraphs9, 'comb':testGraphs5+testGraphs9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile = \"C:/Users/ewhalen/OneDrive - Massachusetts Institute of Technology/research/graphConvolutions/code/pretrained/topoTest04/group 5/checkpoint_95\"\n",
    "ssFile = \"C:/Users/ewhalen/OneDrive - Massachusetts Institute of Technology/research/graphConvolutions/code/pretrained/topoTest04/group 5/ss.pkl\"\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = FeaStNet()\n",
    "model.load_state_dict(torch.load(modelFile, map_location=torch.device('cpu')), strict=False)\n",
    "ss = pickle.load(open(ssFile, 'rb'))\n",
    "model.ss = ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.00053397915,\n",
       " 'mae': 0.01603628,\n",
       " 'mre': 0.44665208,\n",
       " 'peakR2': -1.3475659159016105,\n",
       " 'maxAggR2': 0.0,\n",
       " 'meanAggR2': -0.9085957988128718,\n",
       " 'minAggR2': -5.4398566550491045}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, testGraphs5, testGraphs5, None, device, level='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0012662994,\n",
       " 'mae': 0.02514669,\n",
       " 'mre': 0.61803246,\n",
       " 'peakR2': -1.0462381996868366,\n",
       " 'maxAggR2': -0.05875430087881606,\n",
       " 'meanAggR2': -1.8033689501358083,\n",
       " 'minAggR2': -19.850220066726028}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, testGraphs9, testGraphs9, None, device, level='set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgeom-cpu",
   "language": "python",
   "name": "ptgeom-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
